<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-05-18">18 May 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Guptai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">BM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gu</surname></persName>
							<email>albertgu@stanford.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
							<email>joberant@cs.tau.ac.il</email>
							<affiliation key="aff2">
								<orgName type="institution">Tel Aviv University</orgName>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-05-18">18 May 2022</date>
						</imprint>
					</monogr>
					<note>Diagonal State Spaces are as Effective as Structured State Spaces</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modeling long range dependencies in sequential data is a fundamental step towards attaining human-level performance in many modalities such as text, vision, audio and video. While attention-based models are a popular and effective choice in modeling short-range interactions, their performance on tasks requiring long range reasoning has been largely inadequate. In an exciting result, Gu et al. <ref type="bibr" target="#b12">[GGR22]</ref> proposed the Structured State Space (S4) architecture delivering large gains over state-of-the-art models on several long-range tasks across various modalities. The core proposition of S4 is the parameterization of state matrices via a diagonal plus low rank structure, allowing efficient computation. In this work, we show that one can match the performance of S4 even without the low rank correction and thus assuming the state matrices to be diagonal. Our Diagonal State Space (DSS) model matches the performance of S4 on Long Range Arena tasks, speech classification on Speech Commands dataset, while being conceptually simpler and straightforward to implement.</p><p>Despite S4's achievements, its design is complex and is centered around the HiPPO theory, which is a mathematical framework for long-range modeling [VKE19, <ref type="bibr" target="#b9">GDE`20,</ref><ref type="bibr" target="#b13">GJG`21]</ref>. <ref type="bibr" target="#b12">[GGR22]</ref> work done while author was part of IBM AI Residency program.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Transformer architecture [VSP`17] has been successful across many areas of machine learning. Transformers pre-trained on large amounts of unlabelled text via a denoising objective have become the standard in natural language processing, exhibiting impressive amounts of linguistic and world knowledge [BMR`20, BMH`21]. This recipe has also led to remarkable developments in the areas of vision [RPG`21, RKH`21] and speech <ref type="bibr" target="#b6">[DXX18,</ref><ref type="bibr" target="#b3">BZMA20]</ref>.</p><p>The contextualizing component of the Transformer is the multi-head attention layer which, for inputs of length L, has an expensive ?pL 2 q complexity. This becomes prohibitive on tasks where the model is required to capture long-range interactions of various parts of a long input. To alleviate this issue, several Transformer variants have been proposed with reduced compute and memory requirements [QML`20, KKL20, RSVG21, BPC20, GB20, KVPF20, WLK`20, CLD`21, ZS21, GDG`21] (cf.</p><p>[TDBM20] for a survey). Despite this effort, all these models have reported inadequate performance on benchmarks created to formally evaluate and quantify a model's ability to perform long-range reasoning (such as Long Range Arena [TDA`21] and SCROLLS <ref type="bibr" target="#b21">[SSI`22]</ref>).</p><p>In a recent breakthrough result, Gu et al. <ref type="bibr" target="#b12">[GGR22]</ref> proposed S4, a sequence-to-sequence model that uses linear state spaces for contextualization instead of attention. It has shown remarkable performance on tasks requiring long-range reasoning in domains such as text, images and audio. For instance, on Long Range Arena it advances the state-of-the-art by 19 accuracy points over the best performing Transformer variant. Its remarkable abilities are not limited to text and images and carry over to tasks such as time-series forecasting, speech recognition and audio generation <ref type="bibr" target="#b11">[GGDR22]</ref>.</p><p>showed that state space models with various alternative initializations perform poorly in comparison to initializing the state-space parameters with a particular HiPPO matrix. In order to leverage this matrix, they parameterize the learned state spaces using a Diagonal Plus Low Rank (DLPR) structure and, as a result, need to employ several reduction steps and linear algebraic techniques to be able to compute the state space output efficiently, making S4 difficult to understand, implement and analyze.</p><p>In this work, we show that it is possible to match S4's performance while using a much simpler, fullydiagonal parameterization of state spaces. While we confirm that random diagonal state spaces are less effective, we observe that there do in fact exist effective diagonal state matrices: simply removing the low-rank component of the DPLR HiPPO matrix still preserves its performance. Leveraging this idea, our proposed Diagonal State Space (DSS) model enforces state matrices to be diagonal, making it significantly simpler to formulate, implement and analyze, while being provably as expressive as general state spaces. In contrast to S4, DSS does not assume any specialized background beyond basic linear algebra and can be implemented in just a few lines of code. Our implementation fits in a single page and is provided in ?A.5 ( <ref type="figure" target="#fig_4">Figure 6</ref>).</p><p>We evaluate the performance of DSS on Long Range Arena (LRA) which is a suite of sequencelevel classification tasks with diverse input lengths (1K-16K) requiring similarity, structural, and visual-spatial reasoning over a wide range of modalities such as text, natural/synthetic images, and mathematical expressions. Despite its simplicity, DSS delivers an average accuracy of 81.88 across the 6 tasks of LRA, comparable to the state-of-the-art performance of S4 (80.21). In addition, DSS maintains a comfortable 20 point lead over the best Transformer variant (81.88 vs 61.41).</p><p>In the audio domain, we evaluate the performance of DSS on raw speech classification. On the Speech Commands dataset <ref type="bibr" target="#b26">[War18]</ref>, which consists of raw audio samples of length 16K, we again found the performance of DSS to be comparable to that of S4 (98.2 vs 98.1).</p><p>To summarize, our results demonstrate that DSS is a simple and effective method for modeling long-range interactions in modalities such as text, images and audio. We believe that the effectiveness, efficiency and transparency of DSS can significantly contribute to the adoption of state space models over their attention-based peers. Our code is available at https://github.com/ag1988/dss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>We start by reviewing the basics of time-invariant linear state spaces.</p><p>State Spaces A continuous-time state space model (SSM) parameterized by the state matrix A P R N?N and vectors B P R N?1 , C P R 1?N is given by the differential equation: dx dt ptq " Axptq`Buptq , yptq " Cxptq (1) which defines a function-to-function map uptq ? ? yptq. For a given value of time t P R, uptq P R denotes the value of the input signal u, xptq P R N?1 denotes the state vector and yptq P R denotes the value of the output signal y. We call a state space diagonal if it has a diagonal state matrix.</p><p>Discretization For a given sample time ? P R ?0 , the discretization of a continuous state space (Equation 1) assuming zero-order hold 2 on u is defined as a sequence-to-sequence map from pu 0 , . . . , u L?1 q " u P R L to py 0 , . . . , y L?1 q " y P R L via the recurrence,</p><formula xml:id="formula_0">x k " Ax k?1`B u k , y k " Cx k</formula><p>A " e A? , B " pA?IqA?1B , C " C .</p><p>(2)</p><p>Assuming x?1 " 0 for simplicity, this recurrence can be explicitly unrolled as</p><formula xml:id="formula_1">y k " k ? j"0 CA j B?u k?j .<label>(3)</label></formula><p>For convenience, the scalars CA k B are gathered to define the SSM kernel K P R L as</p><formula xml:id="formula_2">K " pCB, CAB, . . . , CA L?1 Bq " p Ce A?k? pe A??I qA?1B q 0?k?L ,<label>(4)</label></formula><p>where the last equality follows by substituting the values of A, B, C from Equation 2. Hence,</p><formula xml:id="formula_3">y k " k ? j"0 K j?uk?j .<label>(5)</label></formula><p>Given an input sequence u P R L , it is possible to compute the output y P R L sequentially via the recurrence in Equation 2. Unfortunately, sequential computation is prohibitively slow on long inputs and, instead, Equation 5 can be used to compute all elements of y in parallel, provided we have already computed K.</p><p>Computing y from u and K is easy. Given an input sequence u P R L and the SSM kernel K P R L , naively using Equation 5 for computing y would require OpL 2 q multiplications. Fortunately, this can be done much more efficiently by observing that for the univariate polynomials</p><formula xml:id="formula_4">Kpzq " L?1 ? i"0 K i z i and upzq " L?1 ? i"0 u i z i ,</formula><p>y k is the coefficient of z k in the polynomial Kpzq?upzq, i.e. all y k 's can be computed simultaneously by multiplying two degree L?1 polynomials. It is well-known that this can be done in OpL logpLqq time via Fast Fourier Transform (FFT) <ref type="bibr" target="#b5">[CLRS09]</ref>. We denote this fast computation of Equation 5 via the discrete convolution as y " K?u .</p><p>Hence, given the SSM kernel K P R L , the output of a discretized state space can be computed efficiently from the input. The challenging part is computing K itself as it involves computing L distinct matrix powers (Equation 4). Instead of directly using A, B, C, our idea is to use an alternate parameterization of state spaces for which it would be easier to compute K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Having stated the necessary background, we now turn to the main contribution of our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Diagonal State Spaces</head><p>Our model is based on the following proposition which asserts that, under mild technical assumptions, diagonal state spaces are as expressive as general state spaces. We use the operator K ?,L pA, B, Cq P R 1?L to denote the kernel (Equation 4) of length L for state space pA, B, Cq and sample time ?. Proposition 1. Let K P R 1?L be the kernel of length L of a given state space pA, B, Cq and sample time ? ? 0, where A P C N?N is diagonalizable over C with eigenvalues ? 1 , . . . , ? N and @i, ? i ? 0 and e L?i? ? 1. Let P P C N?L be P i,k " ? i k? and ? be the diagonal matrix with ? 1 , . . . , ? N . Then there exist r w, w P C 1?N such that (a) K " K ?,L p?, p1q 1?i?N , r wq " r w???1pe ???I q?elementwise-exppP q,</p><formula xml:id="formula_6">(b) K " K ?,L p?, ppe L?i??1 q?1q 1?i?N , wq " w???1?row-softmaxpP q.</formula><p>The proof of Proposition 1 is elementary and is provided in ?A.1. In the above equations, the last equality follows by using Equation 4 to explicitly compute the expression for the kernel of the corresponding diagonal state space. Hence, for any given state space with a well-behaved state matrix there exists a diagonal state space computing the same kernel. 3 More importantly, the expressions for the kernels of the said diagonal state spaces no longer involve matrix powers but only a structured matrix-vector product.</p><p>Proposition 1(a) suggests that we can parameterize state spaces via ?, r w P C N and simply compute the kernel as shown in Proposition 1(a). Unfortunately, in practice, the real part of the elements of ? can become positive during training making the training unstable on long inputs. This is because the matrix elementwise-exppP q contains terms as large as expp? i ?pL?1qq which even for a modest value of L can be very large. To address this, we propose two methods to model diagonal state spaces. DSS EXP In this variant, we use Proposition 1(a) to model our state space but restrict the real part of elements of ? to be negative. DSS EXP has parameters ? re , ? im P R N , r w P C N and ? log P R. ? is computed as?elementwise-expp? re q`i?? im where i " ??1 <ref type="bibr" target="#b11">[GGDR22]</ref>. ? is computed as expp? log q P R ?0 and the kernel is then computed via Proposition 1(a).</p><formula xml:id="formula_7">Input: parameters ?re, ?im P R N , w P C N , sample time ? log P R Output: SSM kernel K P R L (Proposition 1(b)) 1: ? ? ?re`i??im , ? ? exp p? log q ? ? is positive real 2: PN?L ? p???qN?1?r0, 1, . . .</formula><p>DSS EXP provides a remarkably simple computation of state space kernels but restricts the space of the learned ? (the real part must be negative). It is not clear if such a restriction could be detrimental for some tasks, and we now present an alternate method that provides the simplicity of Proposition 1(a) while being provably as expressive as general state spaces.</p><p>DSS SOFTMAX Instead of restricting the elements of ?, another option for bounding the elements of elementwise-exppP q is to normalize each row by the sum of its elements, which leads us to Proposition 1(b). DSS SOFTMAX has parameters ? re , ? im P R N , w P C N and ? log P R. ? is computed as ? re`i??im , ? is computed as expp? log q P R ?0 , and the kernel is then computed via Proposition 1(b). A sketch of this computation is presented in Algorithm 1. We note that, unlike over R, softmax can have singularities over C and slight care must be taken while computing it (e.g., softmaxp0, i?q is not defined). We instead use a corrected version softmax and detail it in ?A.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Diagonal State Space (DSS) Layer</head><p>We are now ready to describe the DSS layer. We retain the skeletal structure of S4 and simply replace the parameterization and computation of the SSM kernel by one of the methods described in ?3.1.</p><p>Each DSS layer receives a length-L sequence u of H-dimensional vectors as input, i.e., u P R H?L , and produces an output y P R H?L . The parameters of the layer are ? re , ? im P R N , ? log P R H and W P C H?N . For each coordinate h " 1, . . . , H, a state space kernel K h P R L is computed using one of methods described in ?3.1. For example, in case of DSS SOFTMAX , Algorithm 1 is used with parameters ? re , ? im P R N , W h P C N and p? log q h P R. The output y h P R L for coordinate h is computed from u h P R L and K h using Equation 6.</p><p>Following S4, we also add a residual connection from u to y followed by a GELU non-linearity <ref type="bibr" target="#b14">[HG16]</ref>. This is followed by a position-wise linear projection W out P R H?H to enable information exchange among the H coordinates.</p><p>The DSS layer can be implemented in just a few lines of code and our PyTorch implementation of DSS SOFTMAX layer is provided in ?A.5 ( <ref type="figure" target="#fig_4">Figure 6</ref>). The implementation of DSS EXP layer is even simpler and is omitted.</p><p>Complexity For batch size B, sequence length L and hidden size H, the DSS layer requires OpN HLq time and space to compute the kernels, OpBHL logpLqq time for the discrete convolution and OpBH 2 Lq time for the output projection. For small batch size B, the time taken to compute the kernels becomes important whereas for large batches more compute is spent on the convolution and the linear projection. The kernel part of DSS layer has 2N`H`2HN real-valued parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Initialization of DSS layer</head><p>The performance of state spaces models is known to be highly sensitive to initialization <ref type="bibr" target="#b12">[GGR22]</ref>. In line with the past work, we found that carefully initializing the parameters of the DSS layer is crucial to obtain state-of-the-art performance ( ?4).</p><p>The real and imaginary parts of each element of W are initialized from N p0, 1q. Each element of ? log is initialized as e r where r " Uplogp.001q, logp.1qq. ? P C N is initialized using eigenvalues of the normal part of normal plus low-rank form of HiPPO matrix <ref type="bibr" target="#b12">[GGR22]</ref>. Concretely, ? re , ? im are initialized such that the resulting ? is the vector of those N eigenvalues of the following 2N?2N matrix which have a positive imaginary part.</p><formula xml:id="formula_8">$ &amp; % p2i`1q 1{2 p2j`1q 1{2 {2 i ? j 1{2 i " j p2i`1q 1{2 p2j`1q 1{2 {2 i ? j</formula><p>Henceforth, we would refer to the above initialization of ? as Skew-Hippo initialization. In all our experiments, we used the above initialization with N " 64. The initial learning rate of all DSS parameters was 10?3 and weight decay was not applied to them. Exceptions to these settings are noted in ?A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">States of DSS via the Recurrent View</head><p>In ?3, we argued that it can be slow to compute the output of state spaces via Equation 2 and instead leveraged Equation 5 for fast computation. But in situations such as autoregressive decoding during inference, it is more efficient to explicitly compute the states of a state space model similar to a linear RNN (Equation 2). We now show how to compute the states of the DSS models described in ?3.1. Henceforth, we assume that we have already computed ? and ?.</p><p>DSS EXP As stated in Proposition 1(a), DSS EXP computes K ?,L p?, p1q 1?i?N , r wq. For this state space and sample time ?, we use Equation 2 to obtain its discretization</p><formula xml:id="formula_9">A " diagpe ?1? , . . . , e ? N ? q , B "`??1 i pe ?i??1 q?1 ?i?N</formula><p>where diag creates a diagonal matrix of the scalars. We can now compute the states using the SSM recurrence x k " Ax k?1`B u k (Equation 2). As A is diagonal, the N coordinates do not interact and hence can be computed independently. Let us assume x?1 " 0 and say we have already computed x k?1 . Then, for the i'th coordinate independently compute</p><formula xml:id="formula_10">x i,k " e ?i? x i,k?1`??1 i pe ?i??1 qu k .</formula><p>Note that in DSS EXP , Rep? i q ? 0 and hence |e ?i? | " |e Rep?iq? | ? 1. Intuitively, if |? i |? ? 0, we would have x i,k ? x i,k?1 and be able to copy history over many timesteps. On the other hand if Rep? i q? ! 0 then x i,k ????1 i u k and hence the information from the previous timesteps would be forgotten similar to a "forget" gate in LSTMs.</p><p>DSS SOFTMAX As stated in Proposition 1(b), DSS SOFTMAX computes K ?,L p?, ppe L?i?1 q?1q 1?i?N , wq. For this state space and sample time ?, we obtain the discretization</p><formula xml:id="formula_11">A " diagpe ?1? , . . . , e ? N ? q , B "?e ?i??1 ? i pe ?i?L?1 q?1 ?i?N .</formula><p>For the i'th coordinate we can independently compute</p><formula xml:id="formula_12">x i,k " e ?i? x i,k?1`u k pe ?i??1 q ? i pe ?i?L?1 q .</formula><p>Let us drop the coordinate index i for clarity to obtain</p><formula xml:id="formula_13">x k " e ?? x k?1`u k pe ???1 q ?pe ??L?1 q .</formula><p>where x k?1 is a scalar. As the expression involves the term e ??L , where L can be large, directly computing such terms can result in numerical instability and we must avoid exponentiating scalars with a positive real part. We make two cases depending on the sign of Rep?q and compute x k via an intermediate state r</p><p>x k as follows. Let p " IrRep?q ? 0s P t0, 1u. Then, The above equation can be parsed as follows. If Rep?q ? 0 then we have r x k " e ???r x k?1`uk , x k " r x k?p e ???1 q ?pe ??L?1 q and hence if Rep?q ! 0 then r x k ? u k and x k ? u k {?. In this case, information from the previous timesteps would be ignored and the information used would be local. On the other hand if Rep?q ? 0 then we would have</p><formula xml:id="formula_14">r x k " e ??p1?pq?r x k?1`e?k ??p?u k , x k " r x k?e ??ppk?pL?1qq ??e ??p1?2pq?1 e ??p1?2pqL?1 .</formula><formula xml:id="formula_15">r x k " r x k?1`e?k ???u k , x k " r x k?e ??pk?pL?1qq ??e?? ??1 e?? ?L?1</formula><p>and hence if Rep?q " 0 then r x 0 ? u 0 and r x k ? r x k?1 ? u 0 , x k?L?1 ? 0 and x L?1 ? u 0 {?. Hence, the model would be able to copy information even from extremely distant positions, allowing it to capture long-range dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate the performance of DSS on sequence-level classification tasks over text, images, audio. Overall, we find its performance is comparable to S4.</p><p>Long Range Arena (LRA) LRA [TDA`21] is a standard benchmark for assessing the ability of models to process long sequences. LRA contains 6 tasks with diverse input lengths 1K-16K, encompassing modalities such as text and images. Several Transformer variants have been benchmarked on LRA but all have underperformed due to factors such as their high compute-memory requirements, implicit locality bias and inability to capture long-range dependencies. Interestingly, despite being less expressive than DSS SOFTMAX , DSS EXP also reports an impressive performance which makes it specially appealing given its simplicity during training and inference. We investigate an even simpler version of DSS EXP , denoted as DSS EXP-NO-SCALE , which is same as DSS EXP except that we omit the term ??1pe ???I q in the expression of the kernel shown in Proposition 1(a) and instead compute it as r w?elementwise-exppP q. As shown in <ref type="table" target="#tab_2">Table 1</ref>, the performance of DSS EXP-NO-SCALE is generally inferior to that of DSS EXP with the model failing on the challenging PATH-X task which requires the model to capture extremely long-range interactions.</p><p>Raw Speech Classification Audio is typically digitized using a high sampling rate resulting in very long sequences. This provides an interesting domain for investigating the abilities of long-range models. We evaluate the performance of DSS on the Speech Commands (SC) dataset <ref type="bibr" target="#b26">[War18]</ref>, consisting of raw audio samples of length 16000, modeled as a 10-way classification task. As shown in <ref type="table">Table 2</ref>, the performance of all DSS variants is comparable to that of S4 (98.2 vs 98.1).</p><p>In all experiments and ablations, S4 and DSS use identical model hyperparameters such as hidden size, number of layers, etc. Our experimental setup was built on top of the training framework provided by the S4 authors and for our S4 runs we followed their official instructions. 5 Details about model initialization, and hyperparameters are provided in ?A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Analyzing the Performance of DSS</head><p>While the experimental results presented above are encouraging, and clearly demonstrate the effectiveness of DSS at modeling long-range dependencies, it is not clear what exactly are the main factors contributing to its performance. To investigate this further, we performed an ablation analysis aimed at answering the following questions:</p><p>? How significant is the Skew-Hippo initialization ( ?3.3) to the model performance? Would initializing ? randomly work just as well? ? Is the main source of superior performance of state space models (S4, DSS), compared to previous models, their ability to model long-range dependencies? Would restricting DSS to only model local interactions hurt its performance on the above tasks?</p><p>Random Initialization To answer the first question, we repeated the experiments after initializing each element of ? re and ? im in DSS SOFTMAX by randomly sampling from N p0, 1q.</p><p>Truncated Kernels To answer the second question, instead of constructing a kernel of length equal to the length L of the input, we restricted the length of the kernel constructed in DSS SOFTMAX (Algorithm 1) to 128, significantly shorter than the length of the input. To understand the implication of this restriction recall Equation 5 which states that y k " ? k j"0 K j?uk?j . For a given context size c " 128, restricting K ?c " 0 would imply   As shown in <ref type="table" target="#tab_5">Table 3</ref>, randomly initializing the ? parameters of DSS leads to significant performance degradation on the majority of tasks, with the model failing to perform on PATH-X. This is inline with the findings of <ref type="bibr" target="#b12">[GGR22]</ref> who also reported the initialization of S4 to be critical to its performance. Interestingly, despite this performance reduction, DSS manages to outperform all non state-spacebased models on every task.</p><p>Truncating the length of the kernel also leads to a significant reduction in performance across most tasks <ref type="table" target="#tab_5">(Table 3</ref>), suggesting that the superior performance of state-space models on these tasks can indeed be partly attributed to their ability to capture long-range dependencies. Moreover, on some tasks such as LISTOPS and IMAGE, using a truncated kernel still manages to outperform all Transformer variants, which is surprising as Transformer layers are known to be effective at capturing interactions at such short ranges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Analysis of Learned DSS Parameters</head><p>To further explore the inner workings of DSS, we visually inspected the trained parameters and kernels of DSS SOFTMAX .</p><p>The kernels of all layers of the trained DSS SOFTMAX are shown in <ref type="figure">Figure 2</ref> and reveal a stark contrast between the tasks. On the tasks IMAGE and SC, for almost all kernels, the absolute values of the first 128 positions are significantly higher than the later positions indicating that these kernels are mostly local. On the other hand for PATH-X, for a significant proportion of kernels the opposite is true, indicating that these kernels are modeling long-range interactions.</p><p>This partially explains why the performance of DSS SOFTMAX on IMAGE and SC does not decrease significantly even after limiting the kernel length to 128, whereas PATHFINDER and PATH-X report significant degradation <ref type="table" target="#tab_5">(Table 3</ref>). The last row of <ref type="table" target="#tab_5">Table 3</ref> shows the 95th percentile of the set targmax 0?k?L |K k | : K is a kernel in DSS SOFTMAX u, i.e., the set of positions over all kernels, at which for a given kernel its absolute value is maximized. As expected, most kernels of IMAGE and SC are local whereas for PATH-X they are mostly long range.</p><p>The values of the real and imaginary parts of ? are shown in <ref type="figure">Figure 3</ref> (and ?A.4, <ref type="figure">Figure 4</ref>). We observe that, although all elements of ? re are initialized as?0.5, their values can change significantly during training and can become positive (see plots for LISTOPS, SC). It can also be observed that a ? with a more negative Rep?q generally tends to have a larger Imp?q, which interestingly is a property not satisfied by Skew-Hippo initialization.</p><p>The values of the trained ? log corresponding to the real and imaginary parts of ? are shown in ?A.4 ( <ref type="figure">Figure 5</ref>) and, in this case as well, change significantly during training. The values of ? log on short-range tasks such as IMAGE and SC generally tend to be larger compared to long-range tasks such as PATHFINDER and PATH-X, inline with the intuition that, when Rep?q ? 0, larger values of ? correspond to modeling long-range dependence ( ?3.4).</p><p>We note that the plot for LISTOPS reveals an outlier with a value of 22 which after exponentiation in Algorithm 1 would result in an extreme large ?. This can potentially lead to training instabilities and we plan to address this issue in future work. introduced S4, a new type of model that leverages linear state spaces for contextualization instead of attention. Our work is inspired from S4 but uses a diagonal parameterization of state spaces. As a result, our method is significantly simpler compared to S4 and we do not require (1) Pad? approximations to A " e A? (Euler, Bilinear, etc), (2) Woodbury Identity reductions to compute matrix inverse, and (3) fourier analysis for computing the SSM kernel efficiently via truncated generating functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Limitations and future work In this work, we evaluated DSS on sequence-level classification tasks. In future work, we plan to include token-level generation tasks such as language modeling, forecasting, etc. Another natural and important future direction is to pretrain models based on DSS over large amounts of raw data. Lastly, we found that the initialization and learning rates of DSS parameters ? re , ? im , ? log ( ?3.3) play an important role in the performance and convergence of the model. Informally, for tasks such as PATH-X that require very long-range interactions, a smaller initialization of ? log was beneficial. An in-depth analysis of this phenomenon could be helpful and remains for future work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Supplemental Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Diagonal State Spaces</head><p>We restate Proposition 1 for convenience. Proposition. Let K P R 1?L be the kernel of length L of a given state space pA, B, Cq and sample time ? ? 0, where A P C N?N is diagonalizable over C with eigenvalues ? 1 , . . . , ? N and @i, ? i ? 0 and e L?i? ? 1. Let P P C N?L be P i,k " ? i k? and ? be the diagonal matrix with ? 1 , . . . , ? N . Then there exist r w, w P C 1?N such that (a) K " K ?,L p?, p1q 1?i?N , r wq " r w???1pe ???I q?elementwise-exppP q,</p><formula xml:id="formula_16">(b) K " K ?,L p?, ppe L?i??1 q?1q 1?i?N , wq " w???1?row-softmaxpP q.</formula><p>Proof. Let A be diagonalizable over C as A " V ?V?1 with eigenvalues ? 1 , . . . , ? N P C. From Equation 4 we have</p><formula xml:id="formula_17">K " p Ce A?k? pe A??I qA?1B q 0?k?L ,</formula><p>where K k " Ce A?k? pe A??I qA?1B " pCV qe ?k? pe ???I q??1pV?1Bq .</p><p>For CV P C 1?N and V?1B P C N?1 let pCV q J?p V?1Bq " r w P C N be the element-wise product of CV and V?1B. Then,</p><formula xml:id="formula_18">K k " N ? i"1 e ?ik? pe ?i??1 q ? i?r w i (7) " N ? i"1 e ?ik? pe ?i??1 q ? i pe L?i??1 q?p pe L?i??1 q r w i q (8) " N ? i"1 p r w i?p e L?i??1 qq?1 ? i?e ?ik? p ? L?1 r"0 e r?i? q<label>(9)</label></formula><p>where the last equality follows from pz L?1 q " pz?1qpz 0`. . .`z L?1 q and using z L ? 1.</p><p>Let P P C N?L be the matrix P i,k " ? i?k ? and let E " elementwise-exppP q. It is easy to verify that Equation 7 can be re-written as a vector-matrix product as</p><formula xml:id="formula_19">K " r w???1pe ???I q?E .</formula><p>Similarly, for the state space p?, p1q 1?i?N , r wqq and sample time ? its kernel r K can be obtained from Equation 4 as</p><formula xml:id="formula_20">r K k " r w?e ??k? pe ???I q??1?r1, . . . , 1s N?1 " N ? i"1 r w i?e ?ik? pe ?i??1 q ? i</formula><p>which is also the expression for K k <ref type="figure">(Equation 7</ref>). This proves part (a) and we now consider part (b). Let w P C N be defined as w i " r w i?p e L?i??1 q. Then from Equation 9,</p><formula xml:id="formula_21">K k " N ? i"1 w?1 ? i?e ?ik? p ? L?1 r"0 e r?i? q .<label>(10)</label></formula><p>Let S " row-softmaxpP q denote the matrix obtained after applying softmax on the rows of P , i.e.</p><formula xml:id="formula_22">S i,k " e ?ik? ? L?1 r"0 e r?i? .</formula><p>It is easy to verify that Equation 10 can be expressed as a vector-matrix product</p><formula xml:id="formula_23">K " w???1?S .</formula><p>Similarly, for the state space p?, ppe L?i??1 q?1q 1?i?N , wqq and sample time ? its kernel p K can be obtained from Equation 4 as</p><formula xml:id="formula_24">p K k " w?e ??k? pe ???I q??1?r. . . , pe L?i??1 q?1, . . .s N?1 " N ? i"1 w i?e ?ik? pe ?i??1 q ? i pe L?i??1 q " N ? i"1 r w i?e ?ik? pe ?i??1 q ? i</formula><p>which is also the expression for K k <ref type="figure">(Equation 7)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Numerically Stable softmax</head><p>As noted in ?3.1, softmax can have singularities over C. To address this issue, we use a simple correction to make it well-defined over the entire domain:</p><p>? softmax : Given px 0 , . . . , x L?1 q " x P C L , let softmaxpxq P C L be defined as psoftmaxpxqq k " e x k pe x0`. . .`e x L?1 q?1. Note that for any c P C, softmaxpx 0 , . . . , x L?1 q " softmaxpx 0? , . . . , x L?1?c q. Unlike over R, softmax can have singularities over C as sum of exponentials can vanish. E.g. e 0`ei? " 0 and hence softmaxp0, i?q is not defined. ? max : Given px 0 , . . . , x L?1 q " x P C L , let maxpxq be the x i with the maximum real part, i.e.</p><p>x argmax i Repxiq .</p><p>? reciprocal : Given x P C and P R ?0 , let reciprocal pxq " x x?x` where x is the complex conjugate of x. The denominator is always in R ? and |reciprocal | ? p2 ? q?1. ? softmax : Given px 0 , . . . , x L?1 q " x P C L let m " maxpxq and r</p><p>x i " x i?m . Note that |e r xi | ? 1. Given P R ?0 , let softmax pxq P C L be psoftmax pxqq k " e r x k?r eciprocal ?L?1 ?</p><formula xml:id="formula_25">r"0 e r xr?.</formula><p>softmax is always bounded and differentiable.</p><p>In our implementation, we use softmax with " 10?7.</p><p>SSM Softmax In our current implementation of softmaxpxq ( <ref type="figure" target="#fig_4">Figure 6</ref>), we exploit the specific structure of x that arises in Algorithm 1. We now describe an alternate method based on FFT which also uses this specific structure and might lead to a faster implementation in the future. Claim 1 (SSM Softmax). Given c P C, let p " IrRepcq ? 0s, n " 1?p, e " exppc?pn?pqq and r " pn?peq{pp?neq. Let ? " expp?2?i{Lq where i " ??1 . Then, softmaxpc?0, . . . , c?pL?1qq " inverseFFT?1?e n?pe`pp?neq? k?0</p><formula xml:id="formula_26">?k?L " inverseFFT?r`1 r`? k?0 ?k?L .</formula><p>Proof. There are 2 cases depending on sign of Repcq.</p><p>Case 1 (p " 0, n " 1): In this case we have e " exppcq. For the map</p><formula xml:id="formula_27">F pzq " 1?e 1?e L L?1 ? k"0 pezq k " p1?eqp1?pezq L q p1?e L qp1?ezq</formula><p>we get the coefficients of F pzq as invFFTpF p? k q 0?k?L q "?p 1?eq p1?e L q e k?0 ?k?L " softmaxpc?0, . . . , c?pL?1qq.</p><p>We have,</p><formula xml:id="formula_28">F p? k q " p1?eqp1?pe? k q L q p1?e L qp1?e?? k q " 1?e 1?e?? k</formula><p>where last equality follows from ? L " 1.</p><p>Case 2 (p " 1, n " 0): In this case we have e " expp?cq. For the map</p><formula xml:id="formula_29">F pzq " 1?e 1?e L L?1 ? k"0 e k z L?1?k " p1?eqz L?1 1?e L L?1 ? k"0?e z?k " p1?eqz L?1 1?e L 1?`e z?L 1?e z " p1?eq p1?e L q pz L?eL q pz?eq</formula><p>we get the coefficients of F pzq as</p><formula xml:id="formula_30">invFFTpF p? k q 0?k?L q "?p 1?eq 1?e L e L?1?k?k "?p e?1?1q pe?1q L?1 pe?1q k?0 ?k?L " softmaxpc?0, . . . , c?pL?1qq</formula><p>as e?1 " exppcq. Moreover, we have</p><formula xml:id="formula_31">F p? k q " p1?eqp? k?L?eL q p1?e L qp? k?e q " 1?? e`? k</formula><p>where last equality follows from ? L " 1.</p><p>Finally, the second equality of the main Claim follows from 1?e " n?pe`p?ne.</p><p>The computation of softmax in Claim 1 is numerically stable and we always exponentiate scalars with a negative real part. The computed function has singularities at c P t?2?ik{L, 0 ? k ? Lu.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Experimental Setup</head><p>We now describe the training details for DSS and S4 on LRA and Speech Commands ( ?4).</p><p>Sequence Classification Head: Both LRA and Speech Commands are sequence classification tasks. The final layer of the DSS stack outputs a sequence which is aggregated into a single vector via mean pooling along the length dimension. Exceptions to this were TEXT and PATHFINDER tasks where the rightmost token was used as the aggregate.</p><p>For all datasets, we used AdamW optimizer with a constant learning rate schedule with decay on validation plateau. However, for the DSS parameters ( ?3.2) initial learning rate was 10?3 and weight decay was not used, with a few exceptions noted below.</p><p>We used hyperparameters such as model sizes, number of update steps, etc as recommended by the S4 authors on their official repository and are listed in <ref type="table" target="#tab_7">Table 4</ref>. We made the following exceptions for DSS trainings:</p><p>? LISTOPS: learning rate of ? log was 0.02 instead of 10?3.</p><p>? TEXT: learning rate of ? log 0.02 instead of 10?3.</p><p>? IMAGE: we used seed 0 and trained for 200 epochs instead of 100.</p><p>? PATHFINDER: we used Patience " 13.</p><p>? PATH-X: we used batch size 16 and trained for 35 epochs. ? log was initialized as e r where r " Uplogp.0001q, logp.01qq and its learning rate was 10?4. This was beneficial in early convergence of the model.</p><p>For our experiments, the test accuracy that we report in ?4 was measured at the checkpoint with the highest validation accuracy.</p><p>All our experiments were conducted on a single A100 GPU (40GiB).    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>hence the output y k at position k would only depend on u k , . . . , u k?c`1 . This would restrict each DSS SOFTMAX layer to only model local interactions and the model would require several layers to have a broader receptive field and capture long-range interactions.MODEL LISTOPS TEXT IMAGE PATHFINDER PATH-X SC</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Related work In a long line of work, several variants of the Transformer have been proposed to address its quadratic complexity (cf.<ref type="bibr" target="#b8">[GB21]</ref> and references therein). Recently, Gu et al.<ref type="bibr" target="#b12">[GGR22]</ref> Kernels of trained DSSSOFTMAX . Each row of pixels corresponds to one of the (number-of-layers?H) kernels. For a row (kernel) K P R L the element K k P R is shown as |K k |{||K||8 to enable visualization. To enable comparison across tasks, only the first 1024 positions are shown. Values of ? in trained DSSSOFTMAX for tasks described in ?4. For ? " x`iy, we show y on log-scale for better visualization by plotting ? as px, arcsinhpy{2qq " px, logpy{2`apy{2q 2`1 qq. Black dots correspond to Skew-Hippo initialization ( ?3.3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Radu Soricut. H-transformer-1D: Fast one-dimensional hierarchical attention for sequences. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3801-3815, Online, 2021. Association for Computational Linguistics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Trained ? in DSSSOFTMAX for tasks described in ?4. Trained ? log in DSSSOFTMAX for tasks described in ?4.A.5 Implementation of DSSSOFTMAX def reciprocal(x, epsilon=1e-7): x_conj = x.conj() # conjugate return x_conj / (x*x_conj + epsilon) def dss_kernel(L): # L: kernel length # Lambda: [N 2], log_dt: [H], W: [H N 2] (floats) Lambda, log_dt, W = get_layer_parameters() # complex parameter stored as 2 floats denoting real, # imaginary parts as ADAM moments are non-linear # convert reals to complex Lambda, W = map(torch.view_as_complex, (Lambda, W)) # [N], [H N] dt_Lambda = log_dt.exp().unsqueeze(-1) * Lambda # [H L] pos = torch.arange(L, device=W.device) # [L] P = dt_Lambda.unsqueeze(-1) * pos # [H N L] # fast softmax using structure of P Lambda_gt_0 = Lambda.real &gt; 0 # [N] if Lambda_gt_0.any(): with torch.no_grad(): P_max = dt_Lambda * (Lambda_gt_0 * (L-1)) # [H N] P = P -P_max.unsqueeze(-1) S = P.exp() # [H N L] dt_Lambda_neg = dt_Lambda * (1 -2*Lambda_gt_0) # [H N] # 1 / S.sum(-1) == num / den num = dt_Lambda_neg.exp() -1 # [H N] den = (dt_Lambda_neg * L).exp() -1 # [H N] W = W * num * reciprocal(den * Lambda) # [H N] # mixture of softmaxes return torch.einsum('hn,hnl-&gt;hl', W, S).real # [H L] def state_space(u): # u: batch of input sequences # B: batch size, H: hidden size, L: sequence length B, H, L = u.shape # compute state space kernel for each of H coordinates K = dss_kernel(L) # [H L] # multiply two degree L-1 polynomials # (u0 + u1*z ... uL-1*z^L-1)(K0 + K1*z ... KL-1*z^L-1) # zero-pad them to degree 2L-1 to avoid wrap-around K_f = torch.fft.rfft(K, n=2*L) # [H L+1] u_f = torch.fft.rfft(u, n=2*L) # [B H L+1] y_f = K_f * u_f # [B H L+1] y = torch.fft.irfft(y_f, n=2*L)[..., :L] # [B H L] # yi = ui*K0 + ... u0*Ki # residual connection, non-linearity, output projection not shown return y</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Core implementation of DSSSOFTMAX layer ( ?3.2) in PyTorch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 compares</head><label>1</label><figDesc>DSS against S4, the Transformer variants reported in [TDA`21], as well as followup work. State space models (S4, DSS) shown in Table 1 are left-to-right unidirectional whereas other models could be bidirectional. Despite its simplicity, DSS delivers state-of-the-art performance on LRA. Its performance is comparable to that of S4, with a modest improvement in test accuracy averaged across the 6 tasks (81.88 vs 80.21). 4 In addition, DSS maintains a comfortable 20 point lead over the best performing Transformer variant (81.88 vs 61.41).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Speech Commands (SC)) Transformer, CTM, RNN, CNN, and SSM models. (MFCC) Standard pre-processed MFCC features (length-161). (Raw) Unprocessed signals (length-16000). denotes not applicable or computationally infeasible on a single GPU. All results except ours are reproduced from [GGR22].</figDesc><table><row><cell>MODEL</cell><cell cols="2">MFCC RAW</cell><cell>MODEL</cell><cell cols="2">MFCC RAW</cell></row><row><cell>Transformer</cell><cell>90.75</cell><cell></cell><cell>WaveGAN-D</cell><cell></cell><cell>96.25</cell></row><row><cell>Performer ODE-RNN NRDE</cell><cell>80.85 65.9 89.8</cell><cell>30.77 16.49</cell><cell>S4 (as in [GGR22]) S4 (our run)</cell><cell>93.96</cell><cell>98.32 98.1</cell></row><row><cell>ExpRNN</cell><cell>82.13</cell><cell>11.6</cell><cell>DSSSOFTMAX (ours)</cell><cell></cell><cell>97.7</cell></row><row><cell cols="2">LipschitzRNN 88.38</cell><cell></cell><cell>DSSEXP (ours)</cell><cell></cell><cell>98.2</cell></row><row><cell>CKConv</cell><cell>95.3</cell><cell>71.66</cell><cell>DSSEXP-NO-SCALE (ours)</cell><cell></cell><cell>97.7</cell></row><row><cell>Table 2: (</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>(Top) Ablation analysis of DSSSOFTMAX . "Non-State-Space best" is the best performance among models in top and middle sections of Table 1. (Bottom) "argmax k?L |K k |-95-percentile" is the 95th percentile of targmax 0?k?L |K k | : K is a kernel in DSSSOFTMAX u and is explained in ?4.2.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Depth Features H Norm Pre-norm Dropout LR Batch Size Epochs WD Patience</figDesc><table><row><cell>ListOps</cell><cell>6</cell><cell>128</cell><cell>BN</cell><cell>False</cell><cell>0</cell><cell>0.01</cell><cell>50</cell><cell>50</cell><cell cols="2">0.01 5</cell></row><row><cell>Text</cell><cell>4</cell><cell>128</cell><cell>BN</cell><cell>True</cell><cell>0</cell><cell>0.01</cell><cell>50</cell><cell>40</cell><cell>0</cell><cell>10</cell></row><row><cell>Retrieval</cell><cell>6</cell><cell>256</cell><cell>BN</cell><cell>True</cell><cell>0</cell><cell>0.002</cell><cell>64</cell><cell>25</cell><cell>0</cell><cell>20</cell></row><row><cell>Image</cell><cell>6</cell><cell>512</cell><cell>LN</cell><cell>False</cell><cell>0.2</cell><cell>0.004</cell><cell>50</cell><cell>200</cell><cell cols="2">0.01 10</cell></row><row><cell>Pathfinder</cell><cell>6</cell><cell>256</cell><cell>BN</cell><cell>True</cell><cell>0.1</cell><cell>0.004</cell><cell>100</cell><cell>200</cell><cell>0</cell><cell>10</cell></row><row><cell>Path-X</cell><cell>6</cell><cell>256</cell><cell>BN</cell><cell>True</cell><cell>0.0</cell><cell cols="2">0.0005 32</cell><cell>100</cell><cell>0</cell><cell>40</cell></row><row><cell cols="2">Speech Commands (Raw) 6</cell><cell>128</cell><cell>BN</cell><cell>True</cell><cell>0.1</cell><cell>0.01</cell><cell>20</cell><cell>200</cell><cell>0</cell><cell>20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Hyperparameters for the S4 and DSS models. Exceptions for DSS are detailed in ?A.3. (Top) LRA and (Bottom) Speech Commands. LR is initial learning rate and WD is weight decay. BN and LN refer to Batch Normalization and Layer Normalization.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">assumes the value of a sample of u is held constant for a duration of one sample interval ?.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">It is possible for norms of the parameters of the resulting diagonal state spaces to be much larger than that of the original state space. For example, this occurs for the HiPPO matrix<ref type="bibr" target="#b12">[GGR22]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">The large gap between S4 and DSS on TEXT is due to the use of a larger learning rate for ? log in DSS. For our S4 runs, we decided to use the official hyperparameters as provided by<ref type="bibr" target="#b12">[GGR22]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/HazyResearch/state-spaces</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>We thank Ramon Fernandez Astudillo for carefully reviewing the preliminary draft and suggesting several helpful edits. We thank Omer Levy, Achille Fokoue and Luis Lastras for their support. Our experiments were conducted on IBM's Cognitive Computing Cluster, with additional resources from Tel Aviv University. This research was supported by (1) IBM AI Residency program and (2) Defense Advanced Research Projects Agency (DARPA) through Cooperative Agreement D20AC00004 awarded by the U.S. Department of the Interior (DOI), Interior Business Center.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katie</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Lespiau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Damoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>De Las</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelia</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">W</forename><surname>Ring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saffron</forename><surname>Hennigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Maggiore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albin</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Cassirer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michela</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Paganini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sifre</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Improving language models by retrieving from trillions of tokens. ArXiv, abs/2112.04426, 2021</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ilya Sutskever, and Dario Amodei. Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<editor>Hugo Larochelle, Marc&apos;Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin</editor>
		<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford; NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer. ArXiv preprint, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">wav2vec 2.0: A framework for self-supervised learning of speech representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<editor>Hugo Larochelle, Marc&apos;Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin</editor>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rethinking attention with performers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerii</forename><surname>Krzysztof Marcin Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyou</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreea</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tam?s</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Sarl?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><forename type="middle">Quincy</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afroz</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">Benjamin</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><forename type="middle">J</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Colwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Introduction to Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">E</forename><surname>Cormen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">L</forename><surname>Leiserson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clifford</forename><surname>Rivest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
	<note>3rd edition</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Speech-transformer: A no-recurrence sequence-to-sequence model for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>Calgary, AB, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-04-15" />
			<biblScope unit="page" from="5884" to="5888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">GMAT: Global memory augmentation for transformers. ArXiv preprint, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Value-aware approximate attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9567" to="9574" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hippo: Recurrent memory with optimal polynomial projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<editor>Hugo Larochelle, Marc&apos;Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin</editor>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Memory-efficient transformers via top-k attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Dar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaya</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ciprut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Simple and Efficient Natural Language Processing</title>
		<meeting>the Second Workshop on Simple and Efficient Natural Language Processing</meeting>
		<imprint>
			<publisher>Virtual</publisher>
			<date type="published" when="2021-11" />
			<biblScope unit="page" from="39" to="52" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">It&apos;s raw! audio generation with state-space models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
		<idno>abs/2202.09729</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficiently modeling long sequences with structured state spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Combining recurrent, convolutional, and continuous-time models with linear state-space layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isys</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khaled</forename><surname>Saab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Gaussian error linear units (gelus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno>abs/1606.08415</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Transformers are rnns: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelos</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="5156" to="5165" />
		</imprint>
	</monogr>
	<note>Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Blockwise selfattention for long document understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinong</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<biblScope unit="page" from="2555" to="2565" />
		</imprint>
	</monogr>
	<note>Online, 2020. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
		<editor>Marina Meila and Tong Zhang</editor>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021-07" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
	<note>RKH`21</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
		<editor>Marina Meila and Tong Zhang</editor>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021-07-24" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="8821" to="8831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient content-based sparse attention with routing transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="53" to="68" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Scrolls: Standardized comparison over long language sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Shaham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Segal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maor</forename><surname>Ivgi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avia</forename><surname>Efrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ori</forename><surname>Yoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adi</forename><surname>Haviv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<pubPlace>Mor Geva, Jonathan Berant, and Omer Levy</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Long range arena : A benchmark for efficient transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Efficient transformers: A survey. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Legendre memory units: Continuous-time representation in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Voelker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivana</forename><surname>Kajic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Eliasmith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d&apos;Alch?-Buc, Emily B. Fox, and Roman Garnett</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="15544" to="15553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<editor>Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett</editor>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Speech commands: A dataset for limited-vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<idno>abs/1804.03209</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belinda</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>WLK`20</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
