<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BORSE ET AL.: HIERARCHICALLY SUPERVISED SEMANTIC SEGMENTATION HS3: Learning with Proper Task Complexity in Hierarchically Supervised Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubhankar</forename><surname>Borse</surname></persName>
							<email>sborse@qti.qualcomm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Qualcomm AI Research San Diego</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Cai</surname></persName>
							<email>hongcai@qti.qualcomm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Qualcomm AI Research San Diego</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
							<email>yizhe.zhang.cs@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="department">Work done at Qualcomm AI Research</orgName>
								<orgName type="institution">Nanjing University of Science and Technology Nanjing</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
							<email>fporikli@qti.qualcomm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Qualcomm AI Research San Diego</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">BORSE ET AL.: HIERARCHICALLY SUPERVISED SEMANTIC SEGMENTATION HS3: Learning with Proper Task Complexity in Hierarchically Supervised Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While deeply supervised networks are common in recent literature, they typically impose the same learning objective on all transitional layers despite their varying representation powers.</p><p>In this paper, we propose Hierarchically Supervised Semantic Segmentation (HS3), a training scheme that supervises intermediate layers in a segmentation network to learn meaningful representations by varying task complexity. To enforce a consistent performance vs. complexity trade-off throughout the network, we derive various sets of class clusters to supervise each transitional layer of the network. Furthermore, we devise a fusion framework, HS3-Fuse, to aggregate the hierarchical features generated by these layers. This provides rich semantic contexts and further enhance the final segmentation. Extensive experiments show that our proposed HS3 scheme considerably outperforms deep supervision with no added inference cost. Our proposed HS3-Fuse framework further improves segmentation predictions and achieves state-of-the-art results on two large segmentation benchmarks: NYUD-v2 and Cityscapes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Aiming at labeling each pixel to a target category, semantic segmentation is a fundamental task in computer vision for various real-world applications, such as autonomous driving, AR/VR, photography, medical imaging, scene understanding, and real-time surveillance.</p><p>Notable advancements in semantic segmentation originated with the end-to-end fully convolutional networks <ref type="bibr" target="#b14">[15]</ref>. Researchers have since then looked extensively into various ways to further improve performance by adding different kinds of context to such networks. Some examples are the HRNet <ref type="bibr" target="#b18">[19]</ref> branches and hierarchical multi-scale attention <ref type="bibr" target="#b21">[22]</ref>, which add context based on scale; another similar direction is Object Contextual Representations (OCR) <ref type="bibr" target="#b26">[27]</ref>, which adds context related to label representations.</p><p>In these approaches, deep architectures play a key role, but at the same time, bring challenges to training. For instance, the gradients can vanish as they back-propagate through a deep network. In addition, the intermediate layers are highly unconstrained and lack prediction capability. In order to address these issues, deep supervision <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24]</ref> is recently adopted in training, where auxiliary supervisions are imposed on a few selected intermediate layers.</p><p>To enable intermediate supervision, a separate segmentation head is constructed based on the features of each selected intermediate layer and supervised directly with the original ground truth annotations. In this way, the intermediate layers are tightly regularized by the target task, and more expressive gradients are generated to train the network.</p><p>Nevertheless, deep supervision neglects the fact that the intermediate layers have weaker representation powers as compared to the final layer since their features are computed by smaller sub-networks. As such, it can be highly complex for sub-networks to learn to solve the same segmentation problem as the overall network. This prevents the sub-networks from learning meaningful features, and in some cases, can even degrade the overall accuracy (as shown in Section 4). Moreover, the different representation powers of intermediate layers are not taken into account in deep supervision.</p><p>In this paper, we propose Hierarchically Supervised Semantic Segmentation (HS3), the goal of which is to find the right learning task for each intermediate layer to be supervised. We attain these segmentation tasks by clustering semantic labels to form a set containing fewer classes, thus less complexity. Specifically, an earlier layer is supervised with a smaller set of classes to match the corresponding sub-network's (the part of the network up to the current layer) learning capacity. We propose a principled approach to determine the number of class clusters using a two-step training process. This approach utilizes the confusion matrices obtained after training a deep supervision baseline to perform automatic hierarchical grouping of classes. Hierarchical supervision is then applied in the second (final) training phase. We show the effectiveness of our method over deep supervision as well as over clustering based on the manual assignment using single-step training. Now that each intermediate supervised layer can be trained with the suitable grouping of classes, we further propose a framework, HS3-Fuse, to fully utilize the hierarchical features generated by these layers. More specifically, we use lightweight Object Contextual Representation (OCR) modules to process the segmentation features of the supervised intermediate layers. These processed features are then aggregated and fed into the output layer to provide rich hierarchical semantic information and enhance the final segmentation performance.</p><p>The contributions of this work are summarized as follows:</p><p>? We propose a novel hierarchical supervision scheme, HS3, for training semantic segmentation networks, which allows the supervised intermediate layers to learn with the right task complexities in terms of the sets of classes. This enhances the feature learning of the intermediate layers without incurring additional inference costs. ? We devise a novel framework, HS3-Fuse, to fully exploit the hierarchical features generated by the intermediate supervised layers. The fused features contain proper and useful hierarchical semantic context and are fed into the output layer to enhance the overall segmentation performance. ? We evaluate our proposed approach on the common benchmarks of Cityscapes, NYUD-v2 and CamVid. The results show that by utilizing HS3, we considerably improve upon the common deep supervision. HS3-Fuse then further improves the accuracy of the segmentation and achieves state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Semantic Segmentation: The introduction of fully convolutional networks (FCNs) paved the way for significant progress in semantic segmentation <ref type="bibr" target="#b14">[15]</ref>. More recent works aim to maximize segmentation accuracy while maintaining a low inference cost, e.g., DeepLab <ref type="bibr" target="#b2">[3]</ref>, PSPNet <ref type="bibr" target="#b31">[32]</ref>, and HRNet <ref type="bibr" target="#b23">[24]</ref>. Several works then build upon these backbone architectures to incorporate diverse contextual models. The added context could be based on boundaries <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28]</ref>, multi-scale context <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref> or relational context <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>Deep Supervision: Deep supervision was initially proposed to train classification networks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20]</ref> and later extended to other tasks, e.g., segmentation <ref type="bibr" target="#b23">[24]</ref>, depth estimation <ref type="bibr" target="#b5">[6]</ref>. These methods, however, assign the same task for all intermediate supervisions, ignoring the weaker learning abilities of sub-networks. Recently, <ref type="bibr" target="#b11">[12]</ref> proposes to use intermediate geometric concepts to deeply supervise a key-point estimation network. While the different capacities of intermediate layers are considered, this method is not applicable to segmentation. Coarse-to-Fine methods: Some recent works apply different coarse-to-fine ideas to improve segmentation, e.g., increasing spatial resolution <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14]</ref>, mask refinement <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16]</ref>. Our method differs from these as we develop a strategy of class grouping. We use a method of matching task complexities based on sub-network capability, and hence the refinement occurs in an implicit manner. <ref type="bibr" target="#b6">[7]</ref> proposes to use different sets of classes for supervision during training. However, its class grouping is manually and specifically designed for the face segmentation problem, and hence not applicable to other segmentation scenarios (e.g., driving, indoors). Furthermore, this grouping is static and cannot adapt to different networks. In contrast, our HS3 derives class grouping in an automated, data-driven manner, which can be applied to any segmentation application and adapt to the learning capacity of any network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Hierarchically Supervised Semantic Segmentation</head><p>In this section, we describe the Hierarchically Supervised Semantic Segmentation (HS3) training strategy. The first step involves identifying intermediate or transitional layers in deep networks. We use the approach illustrated by deeply supervised networks <ref type="bibr" target="#b10">[11]</ref> to obtain transitional layers, which are demarcated by scale. For instance, the HRNet architecture <ref type="bibr" target="#b11">[12]</ref> contains four stages with different scale groupings, which we identify as transitional layers. Note that our method would extend to other segmentation architectures since the identification of transitional layers can also be based on the depth of the layers. Once we identify transitional layers, we train our backbone network by imposing auxiliary supervision through segmentation heads attached to these intermediate layers.</p><p>Consider a network trained with the HS3 method for N stages. If S is the set of ground truth predictions, we obtain S i , which is a smaller set of grouped semantic labels for every stage i, ?i ? {1, ..., N}. The resulting loss function for HS3 training is given as follows:</p><formula xml:id="formula_0">L total = N ? i=1 ? i L S i i + L S final ,<label>(1)</label></formula><p>where L S i i is the segmentation loss for the ith intermediate supervision stage, ? i is the weight of the ith intermediate segmentation loss and L S final is the segmentation loss for the final network output. This approach is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Our approach differs from deeply supervised networks, for which the set of classes considered for each intermediate supervision is the same as the full set, i.e., S i = S, ?i ? {1, ..., N}. However, this scheme imposes the same task complexity on all the intermediate sub-networks, in spite of their weaker and different learning capabilities. Instead, our approach supervises each intermediate layer with an optimal task complexity in terms of the set of semantic classes. We illustrate our approach to finding intermediate semantic sets next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Redefining Segmentation Tasks: Learning with the Right Classes</head><p>When applying auxiliary supervisions to a deep segmentation network, we allow each supervised intermediate layer to perform a segmentation task that is of the right complexity to it, in terms of the set of classes. We show in this part how to determine this right complexity by analyzing the trade-off between task performance and task complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Segmentation Accuracy vs. Segmentation Complexity</head><p>In order to understand the capabilities of the intermediate layers, we first perform a study on the segmentation performance as a function of the task complexity for each of these layers. From the available training data, we reserve a small subset as an analysis set and use the rest as a reduced training set. <ref type="bibr" target="#b0">1</ref> First, we train the full segmentation network using vanilla (existing) deep supervision on the reduced training set, where all the intermediate supervision stages use the full set of classes. Once the network is trained, we compute the confusion matrix, C i , for each supervised intermediate layer i, as well as for the final layer, based on the analysis set.</p><p>Next, we study how the segmentation accuracy varies as a function of the number of classes. This indicates the task complexity. Given a target number of classes, we apply spectral clustering <ref type="bibr" target="#b22">[23]</ref> to the full set of classes based on an affinity matrix A i = (C i +C T i )/2, which is the symmetric version of the confusion matrix, for each intermediate supervision stage. As the clustering algorithm merges similar classes (e.g., person and rider), we are able to obtain sets of classes with sizes from 2 to K ? 1, where K is the number of classes in the full set. Note that for any two intermediate supervision stages, the sets of classes can be different even when their sizes are the same.</p><p>Based on these reduced sets of classes, we re-evaluate the segmentation accuracy for each intermediate stage in terms of mean Intersection-over-Union (mIoU). This analysis reveals the trade-off between segmentation accuracy and segmentation task complexity for each intermediate stage, as well as for the final output layer. <ref type="figure" target="#fig_1">Figure 2</ref> shows the trade-off analysis for an HRNetv2-w18-OCR network on NYUD-v2. It can be seen that the accuracy reduces as the task complexity increases (in terms of the number/set of classes) and that an earlier intermediate layer shows weaker capability as compared to a later layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Choosing Proper Task Complexity</head><p>If the learning task is either too complex or too simple, intermediate layers will not be able to generate useful features to aid the final segmentation. To address this, we utilize the performance-complexity trade-off considering the final output as a reference and enforce the same trade-off across all the intermediate supervision stages. More specifically, we quantify this trade-off using the ratio between the segmentation mIoU and the number of classes. Then, for intermediate supervision stages, we find the trade-off points that match the ratio of the reference point, as highlighted by the green and red dots in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>Once the trade-off points are identified, we can readily determine the corresponding numbers of classes, as well as the sets of classes (based on spectral clustering) for the intermediate supervisions. These sets of classes are then used to construct the segmentation losses for the respective auxiliary supervision stages in Eq. 1. We then train the network on the full training set, using the total loss derived from our proposed hierarchical supervisions, which produces the final segmentation model.</p><p>In <ref type="figure" target="#fig_1">Figure 2</ref>, it can be seen that our proposed approach of enforcing consistent performancecomplexity trade-offs can be represented by a line through the origin and the reference point (blue dot). We denote the angle between this line and the vertical line through the reference point by ? . By changing ? , one can adjust the trade-offs across the layers. For instance, a larger (smaller) ? places more emphasis on task accuracy (task complexity). In particular, deep supervision corresponds to setting ? = 0 ? , which requires all the intermediate layers to work on the full segmentation task. As shown in Section 4.4, our proposed approach of enforcing consistent trade-offs achieves a performance very close to the optimum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Using Other Clustering Methods</head><p>Our proposed HS3 framework is general and can be used with any clustering algorithm, as shown in Section 4.4. For instance, instead of running spectral clustering on the confusion matrix, one can perform k-means clustering based on the features generated by a supervised intermediate layer. This also allows us to analyze the performance-complexity trade-off for each layer, where the merging of the semantic classes is conducted via k-means.</p><p>When using spectral clustering or k-means clustering, a two-phase training process is required. It is also possible to train the network only once within our HS3 framework, by utilizing a non-data-driven approach to determine the set/number of classes for each intermediate supervision stage. For instance, we can utilize human intuition to manually cluster similar classes and derive reduced sets for the intermediate layers. Another possible way is to set a constant reduction ratio of the number of classes across the layers. For instance, we set the number of classes for each intermediate stage to be 1/2 of that in the next stage and apply manual clustering based on the given number of classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Fusing Hierarchical Features</head><p>By utilizing our proposed HS3 approach, the intermediate layers can learn with the right sets of classes, which allows them to generate features of hierarchical semantic contexts at no additional computational cost. We design a fusion framework for aggregating these features to provide richer semantic information to the final segmentation. More specifically, for each intermediate supervision, we feed the segmentation features into an Object Contextual Representation (OCR) block <ref type="bibr" target="#b26">[27]</ref>, which enhances the features via relational context attention. These enhanced intermediate features are then fused and provided to the final segmentation layer. To reduce computational cost with the task complexity, we set the number of channels in an intermediate OCR block to be 1/2 of that in the immediate next stage. As we shall see in Section 4, our proposed HS3 and feature fusion allow us to outperform state-of-the-art methods considerably. We illustrate the fusion process in <ref type="figure" target="#fig_2">Figure 3</ref> for the case of two intermediate supervision stages. We refer to combining HS3 and feature fusion as HS3-Fuse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section (and in the supplementary file), we present extensive performance evaluations of our proposed approach. We compare HS3 and HS3-Fuse methods with their baseline networks, deep supervision, as well as the latest state of the art. We further conduct ablation studies on our proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Datasets: We analyze semantic segmentation performance on two datasets, NYU-Depth-v2 (NYUD-v2) <ref type="bibr" target="#b17">[18]</ref> and Cityscapes <ref type="bibr" target="#b4">[5]</ref>. We use the original 795 training and 654 testing images for NYUD-v2. We further split the training set into 695 reduced training samples and 100 analysis samples. For Cityscapes, we use their 2975/500/1525 train/val/test splits to report Metrics: Our primary metric for measuring performance is the mean Intersection-over-Union (mIoU). We also show the mean Pixel Accuracy for our results on NYUD-v2, and the instance IoU (iIoU) for results on Cityscapes. We use GMAC (Multiply-Accumulative Operations in 10 9 ) to measure computation cost. Networks: On NYUD-v2, we use HRNetv2-w48 <ref type="bibr" target="#b23">[24]</ref>, HRNetv2-w18 <ref type="bibr" target="#b23">[24]</ref>, HRNetv2-w18-OCR <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b26">27]</ref>, and SA-Gate-ResNet-101 <ref type="bibr" target="#b16">[17]</ref>. On Cityscapes, we use HRNetv2-w18, HRNetv2-w18-OCR, HRNetv2-w48-OCR, and DeepLab-v3+ <ref type="bibr" target="#b2">[3]</ref> with WideResNet-38 (WRN-38) <ref type="bibr" target="#b28">[29]</ref> backbone. We apply the intermediate supervisions to layers which transition in scale. For instance, for HRNet backbones, we attach intermediate segmentation heads to the outputs of stages 2 and 3 (and stage 4 generates the final output) <ref type="bibr" target="#b23">[24]</ref>. Training: When applying HS3 for training, we select the sets of classes for the intermediate supervision based on our trade-off analysis and spectral clustering in Section 3.1. More details on the training and hyperparameters can be found in supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results on NYUD-v2</head><p>We report results on the NYUD-v2 validation set. As shown in <ref type="table" target="#tab_0">Table 1</ref>, training with our proposed HS3 method consistently improves the performance as compared to deep supervision and the baseline of no intermediate supervision. For HRNet-w48, we observed that deep supervision could even degrade the segmentation performance compared to baseline. Our fusion framework is not used in this comparison. As such, our HS3 approach improves the segmentation accuracy without incurring extra computation cost at inference. Next, we incorporate the hierarchical predictions into the proposed HS3-Fuse framework. More specifically, we use an SA-Gate-ResNet101 backbone with the proposed fusion unit discussed in Section 3.2. As shown in <ref type="table" target="#tab_2">Table 2</ref>, our segmentation performance is 1.2% mIoU more than the SA-Gates baseline. Our HS3-Fuse also achieves better performance when comparing to the latest SOTA on NYUD-v2 using RGB-D inputs, such as Inverse-Form <ref type="bibr" target="#b0">[1]</ref>. Furthermore, we evaluate both single-scale and multi-scale inference schemes (as proposed by <ref type="bibr" target="#b16">[17]</ref>) for mIoU and pixel accuracy. Overall, the results indicate that our proposed approach consistently improves segmentation performance in different settings and sets the new SOTA score on NYUD-v2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results on Cityscapes</head><p>We provide results on Cityscapes val and test splits. The results on val with several backbones are summarized in <ref type="table" target="#tab_4">Table 3</ref>. We also perform inference by domain adaptation to CamVid dataset <ref type="bibr" target="#b1">[2]</ref> using the same weights, as shown in the supplementary file. By using   our proposed HS3 training scheme, we are able to consistently improve baseline scores as compared to deep supervision. These improvements come with no added inference cost. We further use our proposed HS3-Fuse approach to fully utilize the hierarchical semantic features for the case of HRNetv2-w48-OCR. We achieve an improvement in performance but with additional computational cost during inference. Hence, we also show a lighter version of HS3-Fuse by reducing the number of channels in all OCR modules by a constant factor, such that we match the GMACs required by the baseline model. It can be seen in <ref type="table" target="#tab_4">Table 3</ref> that with the same inference cost, our lighter HS3-Fuse still considerably outperforms the baseline HRNetv2-w18-OCR. Using this technique of scaling OCR channels, we measure performance v/s computational cost at various operating points. As shown in <ref type="figure" target="#fig_3">Figure 4</ref>, when using the same amount of computation, our proposed approach significantly outperforms the baseline since HS3-Fuse provides richer hierarchical semantic information by fusing our extracted intermediate features.</p><p>To evaluate on the test-set, we upload predictions to Cityscapes benchmark server. We use the HS3-Fuse architecture trained using an HRNetv2-w48 <ref type="bibr" target="#b11">[12]</ref> with OCR <ref type="bibr" target="#b26">[27]</ref> and Hierarchical Multi-scale attention(HMS) <ref type="bibr" target="#b21">[22]</ref> model as backbone. As seen in <ref type="table" target="#tab_5">Table 4</ref>, We achieve a gain of 0.3 mIoU and 1.7 iIoU over this baseline. We also outperform the previous state-of-the-art model(InverseForm <ref type="bibr" target="#b0">[1]</ref>) by a margin of 0.1 mIoU and 0.4 iIoU. Our model ranks top in both categories among published results. We also show visual results comparing our approach to these methods in <ref type="figure" target="#fig_4">Figure 5</ref>. Details on the predictions obtained from other methods are mentioned in the supplementary file.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Studies</head><p>Finding Optimal Number of Clusters: We analyze the segmentation accuracy w.r.t. different performance-complexity trade-offs at the intermediate layers, by varying the parameter of ? . We use an HRNetv2-w18-OCR model on NYUD-v2. Based on the choice of ? , we obtain the numbers of classes K 1 and K 2 for the first and second intermediate stages. As shown in <ref type="table" target="#tab_7">Table 5</ref>, the network achieves optimal segmentation mIoU of 41.8% when ? = 80 ? . By using our proposed approach of enforcing consistent trade-offs across layers from Section 3.1.2, we obtain ? = 76 ? . This allows us to achieve a near-optimal mIou of 41.7%. When ? = 0 ? , we recover the vanilla deep supervision which assigns over-complex tasks to intermediate layers. When ? = 90 ? , it is required that these intermediate stages achieve the same mIoU as the final output, which results in over-simplified tasks for them. As shown in <ref type="table" target="#tab_7">Table 5</ref>, both baselines perform considerably worse as compared to our proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Choice of Clustering Methods:</head><p>We study the effect of using other clustering methods within our hierarchical supervision framework: 1) k-means OCR feature clustering and 2) manual assignment. These are mentioned in Section 3.1.3. We use an HRNetv2-w18 backbone trained using the HS3 scheme and vary our clustering approach. We report our results on the Cityscapes val set.</p><p>As shown in <ref type="table" target="#tab_8">Table 6</ref>, HS3 with any of the clustering methods outperforms deep supervision and the case of no auxiliary supervision. Manual assignment under-performs compared to k-means clustering and spectral clustering, as it is based on human intuition and does not properly align with the sub-networks' capabilities. While k-means clustering performs on par with spectral clustering, it requires class-wise embeddings (e.g., the object representations derived in OCR) at each stage. These representations may not be always available in a given network. In contrast, spectral clustering only requires the confusion matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>SegFix <ref type="bibr" target="#b27">[28]</ref> HMS <ref type="bibr" target="#b21">[22]</ref> Ours    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this work, we have presented a training method that supervises the transitional layers of a segmentation network to learn meaningful representations adaptively by varying task complexity. We derived various sets of class clusters to supervise each transitional layer of the network to facilitate this. Furthermore, we devised a fusion framework to leverage additional context offered by our derived hierarchical features. We showed empirically that our proposed training scheme considerably outperforms baselines and also deep supervision with no added inference cost. The proposed fusion architecture offers superior performance on public benchmarks. For future work, we plan to extend our scheme to various tasks, including classification. We're also looking for an acceptable method for single-stage training.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Hierarchical Supervision: Training using our proposed HS3 scheme, where each intermediate supervision uses the right set of classes for its segmentation task, e.g., earlier layers are trained with smaller sets of classes. We show two sample intermediate stages in the figure, with i ? {1, ..., N}, and the final output stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Performance-Complexity Trade-off: We perform the analysis for an HRNetv2-w18-OCR backbone on NYUD-v2. The two intermediate layers are selected based on the scale transitions (more details in Section 4.1). The blue dot indicates the reference trade-off point from the final output. The red and green dots indicate trade-off points for the first and second intermediate supervision stages respectively. The x-axis shows the number of classes after clustering.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>HS3-Fuse: Using the OCR Segmentation Transformer<ref type="bibr" target="#b26">[27]</ref> to fuse hierarchical features back into the network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>On Cityscapes val: Analyzing mIoU v/s GMACs performance with and without the proposed HS-Fuse architecture. We use a backbone HRNetv2-w18-OCR model and tune the OCR parameters to equalize GMAC costs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>On Cityscapes test: Showing visual effect of training an HRNet-OCR-HMS model within the HS3 Fuse framework. Notice the improvement in highlighted regions as compared to previous state-of-the-art works on Cityscapes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>On NYUD-v2: Training with the proposed Hierarchical Supervision (HS3) scheme improves performance as compared to various baselines, and also outperforms the Deep Supervision (DS) approach. The improvements come with no added inference cost. performance. We further split the training set into 2675 reduced training samples and 300 analysis samples. Models reported on test set are trained using train+val set.</figDesc><table><row><cell>Network</cell><cell cols="2">DS HS3 mIoU GMACs (Inference)</cell></row><row><cell>HRNetv2-w18-OCR</cell><cell>40.6</cell><cell>22</cell></row><row><cell>HRNetv2-w18-OCR</cell><cell>41.2</cell><cell>22</cell></row><row><cell>HRNetv2-w18-OCR</cell><cell>41.7</cell><cell>22</cell></row><row><cell>HRNetv2-w48</cell><cell>47.2</cell><cell>110</cell></row><row><cell>HRNetv2-w48</cell><cell>47.0</cell><cell>110</cell></row><row><cell>HRNetv2-w48</cell><cell>47.6</cell><cell>110</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>On NYUD-v2: Comparison with recent state-of-the art RGB-D methods, both with sin-</cell></row><row><cell>gle scale and multi-scale inference. Our proposed HS3-Fuse architecture with a SA-Gates backbone</cell></row><row><cell>outperforms all other backbones.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>On Cityscapes val: Training with the proposed Hierarchical Supervision (HS3) method improves performance compared to various baselines, and also outperforms the Deep Supervision (DS) approach with no added inference cost.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="2">mIoU iIoU</cell></row><row><cell>SegFix</cell><cell>HRNet48-OCR</cell><cell>84.5</cell><cell>65.9</cell></row><row><cell>Panoptic-DeepLab</cell><cell>Scaled WideResNet</cell><cell>85.1</cell><cell>71.2</cell></row><row><cell>Naive Student</cell><cell>WideResNet41</cell><cell>85.2</cell><cell>68.8</cell></row><row><cell>Densely-Connected NAS</cell><cell>DCNAS-ASPP</cell><cell>85.3</cell><cell>70.0</cell></row><row><cell>Hierarcical Multi-scale attention</cell><cell>HRNet48-OCR-HMS</cell><cell>85.4</cell><cell>70.4</cell></row><row><cell>InverseForm</cell><cell>HRNet48-OCR-HMS</cell><cell>85.6</cell><cell>71.4</cell></row><row><cell>HS3-Fuse(Ours)</cell><cell>HRNet48-OCR-HMS</cell><cell>85.7</cell><cell>71.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>On Cityscapes test: Training with the proposed Hierarchical Supervision (HS3) framework achieves state-of-the-art scores among published methods on the live benchmark.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>On NYUD-v2: Effect of varying ? (i.e., trade-off parameter) in HS3 to train HRNetv2-w18-OCR. Our approach derives a near-optimal ? = 76 ? with 41.7% mIoU.</figDesc><table><row><cell>Auxiliary Supervision</cell><cell cols="2">Clustering mIoU Method</cell></row><row><cell>None</cell><cell>-</cell><cell>77.6</cell></row><row><cell>DS</cell><cell>-</cell><cell>77.7</cell></row><row><cell>HS3</cell><cell>manual</cell><cell>77.9</cell></row><row><cell>HS3</cell><cell>k-means</cell><cell>78.1</cell></row><row><cell>HS3</cell><cell>spectral</cell><cell>78.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>On Cityscapes val: Using various clustering methods with HS3 to train HRNetv2-w18. For k-means, OCR modules are used to extract embeddings.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">? 2021. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For instance, in the case of the Cityscapes dataset, we use 90% of the training data as the reduced training set and the remaining 10% as analysis set. Note that these sets are always disjoint from the validation/test data.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Inverseform: A loss function for structured boundary-aware segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubhankar</forename><surname>Borse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="5901" to="5911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic object classes in video: A high-definition ground truth database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="88" to="97" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A2-nets: Double attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.11579</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The Cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Digging into self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3828" to="3838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Progressive refinement: A method of coarse-to-fine image parsing using stacked network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiagao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlong</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Multimedia and Expo</title>
		<meeting>the IEEE International Conference on Multimedia and Expo</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Label refinement network for coarse-to-fine semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujon</forename><surname>Md Amirul Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrigank</forename><surname>Naha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00551</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Coarse-to-fine semantic segmentation from image-level labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longlong</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingli</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="225" to="236" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">PointRend: Image segmentation as rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9799" to="9808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deeply-supervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="562" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep supervision with intermediate concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc-Huy</forename><surname>Zeeshan Zia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1828" to="1843" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Zigzagnet: Fusing top-down and bottom-up context for object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingguo</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siting</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7490" to="7499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1925" to="1934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Coarse-to-fine annotation enrichment for semantic segmentation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 27th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="237" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bi-directional cross-modality feature propagation with separation-and-aggregation gate for RGB-D semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04514</idno>
		<title level="m">Wenyu Liu, and Jingdong Wang. High-resolution representations for labeling pixels and regions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gated-SCNN: gated shape CNNs for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Towaki</forename><surname>Takikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Acuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5229" to="5238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Hierarchical multi-scale attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10821</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrike</forename><surname>Von</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luxburg</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep multimodal fusion by channel exchanging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Malleable 2.5D convolution: Learning receptive fields along the depth-axis for RGB-D scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajie</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Denseaspp for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoke</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3684" to="3692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Object-contextual representations for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Segfix: Model-agnostic boundary refinement for segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="489" to="506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Wide residual networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Acfnet: Attentional class feature network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibin</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingtuo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6798" to="6807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Nonlocal aggregation for rgb-d semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing-Hao</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengwei</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sifan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guijin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="658" to="662" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
