<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Modality-Aware Contrastive Instance Learning with Self-Distillation for Weakly-Supervised Audio-Visual Violence Detection ACM Reference Format</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 10-14, 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashuo</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyu</forename><surname>Liu</surname></persName>
							<email>jinyuliu20@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Cheng</surname></persName>
							<email>chengy18@fudan.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Academy for Engineering and Technology</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Feng</surname></persName>
							<email>fengrui@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Academy for Engineering and Technology</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shanghai Collaborative Innovation Center of Intelligent Visual Computing</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuejie</forename><surname>Zhang</surname></persName>
							<email>yjzhang@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shanghai Collaborative Innovation Center of Intelligent Visual Computing</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashuo</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyu</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Academy for Engineering and Technology</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Academy for Engineering and Technology</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shanghai Collaborative Innovation Center of Intelligent Visual Computing</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuejie</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shanghai Collaborative Innovation Center of Intelligent Visual Computing</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Modality-Aware Contrastive Instance Learning with Self-Distillation for Weakly-Supervised Audio-Visual Violence Detection ACM Reference Format</title>
					</analytic>
					<monogr>
						<title level="m">MM&apos;2022</title>
						<meeting> <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<date type="published">October 10-14, 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3503161.3547868</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS ? Computing methodologies ? Scene anomaly detection KEYWORDS Multi-Modality</term>
					<term>Contrastive Learning</term>
					<term>Violence Detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Weakly-supervised audio-visual violence detection aims to distinguish snippets containing multimodal violence events with videolevel labels. Many prior works perform audio-visual integration and interaction in an early or intermediate manner, yet overlooking the modality heterogeneousness over the weakly-supervised setting. In this paper, we analyze the modality asynchrony and undifferentiated instances phenomena of the multiple instance learning (MIL) procedure, and further investigate its negative impact on weakly-supervised audio-visual learning. To address these issues, we propose a modality-aware contrastive instance learning with self-distillation (MACIL-SD) strategy . Specifically, we leverage a lightweight two-stream network to generate audio and visual bags, in which unimodal background, violent, and normal instances are clustered into semi-bags in an unsupervised way. Then audio and visual violent semi-bag representations are assembled as positive pairs, and violent semi-bags are combined with background and normal instances in the opposite modality as contrastive negative pairs. Furthermore, a self-distillation module is applied to transfer unimodal visual knowledge to the audio-visual model, which alleviates noises and closes the semantic gap between unimodal and multimodal features. Experiments show that our framework outperforms previous methods with lower complexity on the large-scale XD-Violence dataset. Results also demonstrate that our proposed approach can be used as plug-in modules to enhance other networks. Codes are available at https://github.com/JustinYuu/MACIL_SD.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: a) An example of the modality asynchrony. During the violent event abuse, the abuser first hits the victim, where the violent message is reflected in the visual modality. Then the scream of the victim occurs, indicating the auditory violence information. b) The illustration of the undifferentiated instances. In each bag, violent cues are distributed in some instances while others contain background noises, and the discrepancy between normal segments and background noises also exists. We argue that adding additional constraints could enhance model discrimination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>cues. Despite numerous modality fusion and interaction methods have shown promising results, the modality discrepancy of the multiple instance learning (MIL) <ref type="bibr" target="#b37">[38]</ref> framework under the weaklysupervised setting remains to be explored. To alleviate the appetite for fine-labeled data, MIL is widely adopted for the weakly-supervised violence detection, where the output of each video sequence is formed into a bag containing multiple snippet-level instances. In the audio-visual scenarios, all prior works share a general scheme that regards each audio-visual snippet as an integral instance and averaging the top-K audiovisual logits as the final video-level scores. However, we analyze that this formula suffers from two defects: modality asynchrony and undifferentiated instances. Modality asynchrony indicates the temporal inconsistency between auditory and visual violence cues. Taking the typical violent event abuse in <ref type="figure">Figure 1</ref>(a) as an example, when the abuser hits the victim, the scream occurs afterward, and the entire procedure is regarded as a violent event. In this situation, scenes in part of the visual modality (2nd-3rd snippets) and audio modality (4th-5th snippets) contain violent clues. We argue that directly leveraging an audio-visual pair as an instance could introduce data noise to the video-level optimization. The other defect we discovered is undifferentiated instances, that is, picking the top-K instances for optimization results in numerous disengaged instances. As shown in <ref type="figure">Figure 1(b)</ref>, in a sequence of violent videos, the violent event can be reflected in some audio/visual instances. In contrast, others contain irrelevant elements such as background noises. On the contrary, in the videos of normal events, a few snippets contain elements of normal events, while others include background information. In this case, the K-max activation abandons the instances containing background elements, and the discrepancy between violent and normal instances is not explicitly revealed. To this end, we argue that adding contrastive constraints among the violent, normal, and background instances could contribute to the discrimination toward violent content.</p><p>Driven by preliminary analysis, we propose a simple yet effective framework constructed by modality-aware contrastive instance learning (MA-CIL) and self-distillation (SD) module. To address the modality asynchrony, we form the unimodal bags apart from the original audio-visual bags, compute unimodal logits, and cluster embeddings of top-K and bottom-K unimodal instances as semi-bags. To differentiate instances, we propose a modality-aware contrastivebased method. In detail, the audio and visual violent semi-bags are constructed as the positive pairs, while the violent semi-bags are assembled with embeddings of instances in the background and normal semi-bags as negative pairs. Furthermore, a self-distillation module is applied to distill unimodal knowledge to the audio-visual model, which closes the semantic gap between modalities and alleviates the data noise introduced by the abundant cross-modality interactions. In summary, our contributions are as follows:</p><p>? We analyze the modality asynchrony and undifferentiated instances phenomena of the widely-used MIL framework in audiovisual scenarios, further elaborating their disadvantages for the weakly-supervised audio-visual violence detection. ? We propose a modality-aware contrastive instance learning with self-distillation framework to introduce feature discrimination and alleviate modality noise.</p><p>? Equipped with a lightweight network, our framework outperforms the state-of-the-art methods on the XD-Violence dataset, and our model also shows the generalizability as plug-in modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS 2.1 Weakly-Supervised Violence Detection</head><p>Weakly-supervised violence detection requires identifying violent snippets under video-level labels, where the MIL <ref type="bibr" target="#b37">[38]</ref> framework is widely used for denoising irrelevant information. Some previous works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b67">68]</ref> regard violence detection as a pure vision task and leverage CNN-based networks to encode visual features. Among these methods, various feature integration and amelioration methods are proposed to enhance the robustness of MIL. Tian et al. <ref type="bibr" target="#b53">[54]</ref> propose RTFM, a robust temporal feature magnitude learning method to refine the capacity of recognizing positive instances. Li et al. <ref type="bibr" target="#b32">[33]</ref> design a Transformer <ref type="bibr" target="#b56">[57]</ref>-based multisequence learning network to reduce the probability of instance selection errors. However, these models neglect the corresponding auditory information as well as the cross-modality interactions, thereby restricting the performance of violence prediction.</p><p>Recently, Wu et al. <ref type="bibr" target="#b61">[62]</ref> curate a large-scale audio-visual dataset XD-Violence and establish an audio-visual benchmark. However, they integrate audio and visual features in an early fusion way, thereby limiting further inter-modality interactions. To facilitate multimodal fusion, Pang et al. <ref type="bibr" target="#b42">[43]</ref> propose an attention-based network to adaptively integrate audio and visual features with mutual learning module in an intermediate manner. Different from prior methods, we perform inter-modality interactions via a lightweight two-stream network and conduct discriminative multimodal learning via modality-aware contrast and self-distillation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Contrastive Learning</head><p>Contrastive learning is formulated by contrasting positive pairs against negative pairs without data supervisory. In the unimodal field, several visual methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b34">35]</ref> leverage the augmentation of visual data as a contrast to increase model discrimination. Furthermore, some natural language processing methods utilize the token-and sentence-level contrasts to enhance the performance of pre-trained models <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b49">50]</ref> and supervised tasks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b45">46]</ref>. For the multimodal fields, some works introduce modality-aware contrasts to vision-language tasks, such as image captioning <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b57">58]</ref>, visual question answering <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b59">60]</ref>, and representation learning <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b65">66]</ref>. Moreover, recent literature <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42]</ref> utilizes the temporal consistency of audio-visual streams as contrastive pretext tasks to learn robust audio-visual representations. Based on existing instance-level contrastive frameworks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b62">63]</ref>, we put forward the concept of semi-bags and leverage the cross-modality contrast to obtain model discrimination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Cross-Modality Knowledge Distillation</head><p>Knowledge distillation is first proposed to transfer knowledge from large-scale architectures to lightweight models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b27">28]</ref>. However, the cross-modality distillation aims to transfer unimodal knowledge to multimodal models for alleviating the semantic gap between modalities. Several methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b28">29]</ref>   Our approach consists of three parts: the lightweight two-stream network, modality-aware contrastive learning (MA-CIL), and self-distillation (SD) module. Taking audio and visual features extracted from pretrained networks as inputs, we design a simple yet effective attention-based network to perform audio-visual interaction. Then a modality-aware contrasting-based method is used to cluster instances of different types into several semi-bags and further obtain model discrimination. Finally, a self-distillation module is deployed to transfer visual knowledge to our audio-visual network, aiming to alleviate modality noise and close the semantic gap between unimodal and multimodal features. The entire framework is trained jointly in a weakly supervised manner, and we adopt the multiple instance learning (MIL) strategy for optimization.</p><p>modality missing and noisy phenomena. Chen et al. <ref type="bibr" target="#b12">[13]</ref> propose an audio-visual distillation strategy, which learns the compositional embedding and transfers knowledge across semantic-uncorrelated modalities. Recently, Multimodal Knowledge Expansion <ref type="bibr" target="#b64">[65]</ref> is proposed as a two-stage distillation strategy, which transfers knowledge from unimodal teacher networks to the multimodal student network by generating pseudo labels. Inspired by the methodology of self-distillation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b63">64]</ref>, we propose the parameter integration paradigm to transfer visual knowledge to our audiovisual model via two similar lightweight networks, which reduces the modality noise and benefits robust audio-visual representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARIES</head><p>Given an audio-visual video sequence = ( , ), where is the audio channel, and denotes the visual channel, the entire sequence is divided into non-overlapping segments { , } =1 . For an audio-visual pair ( , ), weakly-supervised violence detection task requires to distinguish whether it contains violent events via an event relevance label ? {0, 1}, where = 1 means at least one modality in the current segment includes violent cues. In the training phase, only video-level labels are available for optimization. Hence, a general scheme is to utilize the multiple instance learning (MIL) procedure to satisfy the weak supervision.</p><p>In the MIL framework, each video sequence is regarded as a bag, and video segments { , } =1 are taken as instances. Then instances are aggregated via a specific feature-level/score-level pooling method to generate video-level predictions . In this paper, we utilize the K-max activation with average pooling rather than attention-based methods <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b52">53]</ref> and global pooling <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b66">67]</ref> as the aggregation function. To be specific, given the audio and visual feature , extracted by CNN networks, we use a multimodal network to generate unimodal logits , , and audio-visual logits . The embeddings of audio and visual instances are symbolized as ? and ? . Then we average maximum logits and use the sigmoid activation to generate the video-level prediction . Due to the additional constraint of our proposed contrastive learning method, we define the unimodal bags B , B . In each unimodal bag, instances are clustered into several semi-bags , ? { , } based on their intrinsic characteristics, and the corresponding semi-bag representations are noted as B , ? { , }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHODOLOGY</head><p>Our proposed framework consists of three parts, a lightweight two-stream network, modality-aware contrastive instance learning (MA-CIL), and the self-distillation (SD) module. An illustration of our framework shown in <ref type="figure" target="#fig_2">Figure 2</ref> is detailed as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Two-Stream Network</head><p>Considering prior methods suffer from the parameter redundancy of the large-scale networks, we design an encoder-agnostic lightweight architecture to achieve feature aggregation and modality interaction. Taking the visual and auditory feature , extracted by pre-trained networks (e.g., I3D and VGGish for visual and audio features, respectively) as input, our proposed network consists of three parts, linear layers to keep the dimension of input features identical, cross-modality attention layer to perform inter-modality interactions, and MIL module for the weakly-supervised training. Among these modules, the cross-modality attention layer is ameliorated from the encoder part of Transformer <ref type="bibr" target="#b56">[57]</ref>, which includes the multihead self-attention, feed-forward layer, residual connection <ref type="bibr" target="#b25">[26]</ref>, and layer normalization <ref type="bibr" target="#b2">[3]</ref>. In the raw self-attention block, features are projected by three different parameter matrices as query, key, and value vectors, respectively. Then the scale dot-product attention score is computed by</p><formula xml:id="formula_0">( , , ) = ( ? ) , where</formula><p>, , denotes the query, key, and value vectors, is the dimension of query vectors, denotes the matrix transpose operation. To enforce cross-modality interactions, we change the key and value vectors of the self-attention block to features in other modalities:</p><formula xml:id="formula_1">? = ( , , ),<label>(1)</label></formula><formula xml:id="formula_2">? = ( , , ),<label>(2)</label></formula><p>where ? , ? are updated audio and visual features, , , and are learnable parameters. We adopt the sharing parameter strategy for feature projection to reduce computation.</p><p>We adopt the MIL procedure under the weakly-supervised setting to obtain video-level scores. Unlike prior works, we process unimodal features individually to alleviate modality asynchrony. To be specific, fully-connected layers are used in each modality to generate unimodal logits. Then we take the summation of unimodal logits as the fused audio-visual logits while reserving the unimodal logits for the following contrastive learning. Finally, the top-K audio-visual logits are average-pooled and put into a sigmoid activation to generate video-level scores for optimization. The entire procedure is formulated as:</p><formula xml:id="formula_3">, = + , + (3) = ?(? ( ( ? )))<label>(4)</label></formula><p>where , , , are learnable parameters, ? is the K-max activation, denotes the sigmoid function, ? is the summation operation, ? denotes average pooling, and is video-level prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">MA-CIL</head><p>To utilize more disengaged instances, we propose the MA-CIL module, which is shown on the right side of <ref type="figure" target="#fig_2">Figure 2</ref>. Given the embeddings ? , ? , we perform unsupervised clustering to divide them into violent, normal, and background semi-bag representations based on the visual and audio logits. We argue that the discrepancy between semantic-irrelevant instances can be exploited to enrich model's capacity for discrimination.</p><p>To be specific, we first leverage the video-level probabilities to distinguish whether the given video contains violent events. In each mini-batch, for the video sequence that &gt; 0.5, top-K instances with highest logits are clustered as the violence semibag</p><formula xml:id="formula_4">( ) = {? ( )} =1 , ? { , }.</formula><p>For the sequence that ? 0.5, top-K instances are selected as the normal semi-bag</p><formula xml:id="formula_5">( ) = {? ( )} =1 , ? { , }.</formula><p>We hope adding contrast to the normal and violent events could help the model distinguish the violent extent of percepted signals.</p><p>Moreover, we argue that both normal and violent videos contain background snippets, and learning the difference between eventrelated segments and background noises could benefit the localization. Therefore, we select the bottom-K instances of the whole minibatch as the background semi-bag = {? ( )} =1 , ? { , }. In each mini-batch, the model should contrast violent audio-visual instances against negative pairs constructed by violent instances and other instances (background and normal).</p><p>An intuitive way is to randomly pick intra-and inter-semi-bag instances in the opposite modality as positive and negative pairs. However, we argue that audio and visual violent instances with diverse positions could be semantically mismatched, such as expressing the beginning and ending of a violent event, respectively. Therefore, it is unnatural to assume that they share the same implication. In contrast, we conduct average pooling to embeddings of all violence instances in each bag and form a semi-bag-level representation B , ? { , }. By doing so, the audio and visual representation both express event-level semantics, thereby alleviating the noise issue. To this end, we construct semi-bag-level positive pairs, which are assembled by audio and visual violent semi-bag representations B , B . We also construct semi-bag-to-instance negative pairs to maintain numerous contrastive samples, where violent semi-bag representations are combined with background and normal instance embeddings ? , ? , ? { , } in the opposite modality as negative pairs.</p><p>We use the InfoNCE <ref type="bibr" target="#b54">[55]</ref> as the training objective of this part, which closes the distance between positive pairs and enlarges the distance between negatives. The objective for audio violent semibag representation ( ) against visual normal instance embeddings {? ( )} =1 is formulated as:</p><formula xml:id="formula_6">L 2 ( ( )) = ? ( ( ), ( ))/ ) ( ( ), ( ))/ ) + =1 ( ( ),? ( )/ ) ,<label>(5)</label></formula><p>where denotes cosine similarity function, is the temperature hyperparameter, denotes the normal instances number in the whole mini-batch. Similarly, the objective for audio violent semibag representation ( ) against visual background instances embeddings {? ( )} =1 is formulated as:</p><formula xml:id="formula_7">L 2 ( ( )) = ? ( ( ), ( ))/ ) ( ( ), ( ))/ ) + =1 ( ( ),? ( )/ ) ,<label>(6)</label></formula><p>where denotes the background instance number in the whole mini-batch. The visual-against-audio counterparts are highly similar, thus we omit these for concise writing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Self-Distillation</head><p>The audio-visual interactions provided by the former parts could introduce abundant modality noises, and modality asynchrony also results in the semantic mismatch of multimodal and unimodal features in the same temporal position. To address these issues, we argue that training a similar visual network simultaneously enables the model to ensemble unimodal and multimodal knowledge. With a controllable co-distillation strategy, our proposed module warrants modality noise reduction and robust modality-agnostic knowledge.</p><p>Specifically, we propose an analogous unimodal network that contains comparable architecture with our two-stream network. The cross-modality attention block is substituted by the standard transformer encoder block including self-attention. During training, the unimodal network is trained with a relatively small learning rate, and parameters of the same layers are infused into the audiovisual network with an exponential moving average strategy:</p><formula xml:id="formula_8">? + (1 ? )<label>(7)</label></formula><p>where and denotes parameters of the audio-visual model and visual model, respectively, denotes the control hyperparameter following a cosine scheduler that increases from the original valu? to 1 during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Learning Objective</head><p>The entire framework is optimized in a joint-training manner. For the video-level prediction , we leverage binary cross-entropy L B as the training objective and use a linearly growing strategy to control the weight of contrastive loss. The total objective is:</p><formula xml:id="formula_9">L = 2 ( ) ?? (L 2 ( ( )) + L 2 ( ( )))+ 2 ( ) ?? (L 2 ( ( )) + (L 2 ( ( ))) + L (8) ( ) = ( * , ?)<label>(9)</label></formula><p>where denotes the number of violence semi-bags in the whole mini-batch, ( ) is a controller to increase weight within a few epochs linearly, denotes the growing ratio, is the current epoch, and ? denotes the maximum weight.</p><p>The visual network is optimized via the BCE loss with videolevel labels to distill unimodal knowledge. The two objectives are optimized simultaneously during training while in the inference phase, only the audio-visual network is used for prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENT</head><p>We design experiments to verify our model from two perspectives, the end-to-end framework compared with state-of-the-art methods and assembling with other networks as plug-in modules. Experimental details and analyses are introduced as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset and Evaluation Metric</head><p>XD-Violence <ref type="bibr" target="#b61">[62]</ref> dataset is by far the only available large-scale audio-visual dataset for violence detection, which is also the largest dataset compared with other unimodal datasets. XD-Violence consists of 4,757 untrimmed videos (217 hours) and six types of violent events, which are curated from real-life movies and in-the-wild scenes on YouTube. Although previous methods adopt some popular datasets <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b50">51]</ref> as benchmarks, we argue that these datasets only contain unimodal visual contents, which cannot perform crossmodality interactions and further verify our proposed multimodal framework. Hence, following <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b61">62]</ref>, we select the large-scale audio-visual dataset XD-Violence as benchmark. During inference, we utilize the frame-level average precision (AP) as evaluation metrics following previous works <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b61">62]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Implementation Details</head><p>To make a fair comparison, we adopt the same feature extracting procedure as prior methods <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b61">62]</ref>. Concretely, we use the I3D <ref type="bibr" target="#b6">[7]</ref> network pretrained on the Kinetics-400 dataset to extract visual features. Audio features are extracted via the VGGish <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b26">27]</ref> network pretrained on a large YouTube dataset. The visual sample rate is set to be 24 fps, and visual features are extracted by a sliding window with a size of 16 frames. For the auditory data, we first The entire network is trained on an NVIDIA Tesla V100 GPU for 50 epochs. We set the batch size as 128 and the initial learning rate as 4e-4, which is dynamically adjusted by a cosine annealing scheduler. For the visual distillation network, the learning rate is set as 8e-5. We use Adam <ref type="bibr" target="#b30">[31]</ref> as the optimizer without weight decay. During optimization, the weighted hyperparameter , ? 2 , ? 2 are 0.1, 1.5, and 1.5, respectively. The initial distillation weight is set to 0.91. The temperature of InfoNCE <ref type="bibr" target="#b54">[55]</ref> is set to be 0.1. The hidden dimension of our two-stream network is 128, and the dropout rate is 0.1. For the MIL, we set the value of K-max activation as 16 + 1 , where denotes the length of input feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparisons with State-of-the-Arts</head><p>We compare our proposed approach with state-of-the-art models, including (1) unsupervised methods: SVM baseline, OCSVM <ref type="bibr" target="#b47">[48]</ref>, and Hasan et al. <ref type="bibr" target="#b23">[24]</ref>; (2) unimodal weakly-supervised methods: Sultani et al. <ref type="bibr" target="#b50">[51]</ref>, RTFM <ref type="bibr" target="#b53">[54]</ref>, Li et al. <ref type="bibr" target="#b32">[33]</ref>, and Wu et al. <ref type="bibr" target="#b60">[61]</ref>; <ref type="formula">(3)</ref> audio-visual weakly-supervised methods: Wu et al. <ref type="bibr" target="#b61">[62]</ref> and Pang et al. <ref type="bibr" target="#b42">[43]</ref>. We report the AP results on XD-Violence dataset in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>With video-level supervisory signals, our method outperforms all previous unsupervised approaches by a large margin. Moreover, compared with previous unimodal weakly-supervised methods, our model surpasses prior results with a minimum of 5.12%, showing the necessity of utilizing multimodal cues for violent detection.</p><p>To further demonstrate the efficacy of our modality-aware contrastive instance learning and cross-modality distillation, we select state-of-the-art methods <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b61">62]</ref> as audio-visual baselines and reimplement SOTA unimodal MIL method <ref type="bibr" target="#b53">[54]</ref> with two modalityexpansion strategies. First, following <ref type="bibr" target="#b61">[62]</ref>, we fuse the audio and <ref type="table">Table 2</ref>: Results on proposed MA-CIL and SD modules as plug-in modules. * indicates results re-implemented by fusing audio and visual features as inputs. ? denotes reimplemented by integrating logits of two identical networks with audio and visual inputs, respectively. ? is the ablated model that removes the fusion module and mutual loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>MA-CIL SD AP (%) Param. Wu et al. <ref type="bibr" target="#b61">[62]</ref> 78.64 0.843M Wu et al. ? <ref type="bibr" target="#b61">[62]</ref> 78.66 1.539M Wu et al. <ref type="bibr" target="#b61">[62]</ref> 80.07 (1.43?) 1.612M Wu et al. ? <ref type="bibr" target="#b61">[62]</ref> 79.98 (1.32?) 1.539M RTFM <ref type="bibr" target="#b53">[54]</ref> 77.81 12.067M RTFM* <ref type="bibr" target="#b53">[54]</ref> 78.10 13.510M RTFM ? <ref type="bibr" target="#b53">[54]</ref> 78. <ref type="bibr" target="#b53">54</ref> 13.190M RTFM* <ref type="bibr" target="#b53">[54]</ref> 80.40 (2.30?) 25.577M RTFM ? <ref type="bibr" target="#b53">[54]</ref> 80.00 (1.46?) 13.190M Pang et al. <ref type="bibr" target="#b45">[46]</ref> 81.69 1.876M Pang et al. ? <ref type="bibr" target="#b45">[46]</ref> 80.03 1.086M Pang et al. <ref type="bibr" target="#b45">[46]</ref> 81.21 (1.18?) 2.138M Pang et al. <ref type="bibr" target="#b45">[46]</ref> 80.90 (0.87?) 1.086M Pang et al. <ref type="bibr" target="#b45">[46]</ref> 82.21 (2.18?) 1.613M visual features in an early way as model inputs. This approach forbids the intermediate modality interaction in the network, aiming to show the performance of simply integrating multimodal data. Considering some networks may be unsuitable for multimodal inputs, we put forward another strategy to train two unimodal networks simultaneously and generate audio and visual logits, respectively. The audio-visual predictions are generated by fusing unimodal logits. Results show that our framework achieves 1.71% higher performance against state-of-the-art method Pang et al. <ref type="bibr" target="#b42">[43]</ref>, which verifies that our MA-CIL and SD modules are practical for violence detection. Our method outperforms RTFM* and Wu et al. by 5.30% and 4.76% for multimodal variants using audio-visual inputs. For variants using two-stream architecture, we observe that our model surpasses RTFM ? and Wu et al. ? by 4.86% and 4.74%, respectively, which suggests that modality-aware interactions are indispensable for multimodal scenarios. To conclude, using the same input features, our method achieves superior performance compared with all audio-visual methods, showing the effectiveness of our entire proposed audio-visual framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Plug-in Module</head><p>We also argue that our proposed modules have satisfying generalizability and are capable of enhancing other networks. To this end, we combine our framework with state-of-the-art methods and evaluate the performance. First, we re-implement the state-of-the-art audiovisual method <ref type="bibr" target="#b42">[43]</ref> using the official implementations provided by the original paper. Then we select the unimodal method with publicly available codes RTFM <ref type="bibr" target="#b53">[54]</ref> as the unimodal baseline, which is ameliorated to multimodal networks by two means we mentioned above (* and ?). For the multimodal method Wu et al. <ref type="bibr" target="#b61">[62]</ref>, we use the two-stream variant to examine the performance of our MA-CIL module and use the native version for combining with SD. Since the unimodal network RTFM <ref type="bibr" target="#b53">[54]</ref> and the audio-visual method Wu et al. <ref type="bibr" target="#b61">[62]</ref> can only be amalgamated with MA-CIL in the two-stream network manner ( ?), while SD should be assembled in an early modality fusion way (*), we can only combine these frameworks with our modules separately. For the multimodal approach <ref type="bibr" target="#b42">[43]</ref>, we both testify the joint and independent enhancement performances of our MA-CIL and SD modules. We report the results on the XD-Violence dataset in <ref type="table">Table 2</ref>. First, we observe that MA-CIL boosts the unimodal baselines Wu et al. <ref type="bibr" target="#b61">[62]</ref> and RTFM [54] for 1.32% and 1.46%, respectively, showing that our contrastive learning method improves the discrimination of models. We also note that equipped with the SD module, the performances of <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b61">62]</ref> also gain an increase of 1.43% and 2.30%. For the multimodal baseline <ref type="bibr" target="#b42">[43]</ref>, we remove the mutual loss and multimodal fusion modules and leverage the vanilla attention-based variant ( ?) for comparison. Results show that enhanced with MA-CIL and SD separately or jointly both achieve accuracy boosts. In summary, we conclude that integrating our MA-CIL and SD modules is beneficial to numerous networks and our modules can be utilized flexibly depending on specific usages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Complexity Analysis</head><p>As we mentioned before, we propose a computation-friendly framework that does not introduce too many parameters. To support our claims, we compare parameter amounts with previous methods, which are shown in the Param. column of <ref type="table" target="#tab_0">Table 1</ref>, 2. In <ref type="table" target="#tab_0">Table 1</ref>, we report the parameter amounts of previous works we re-implement and our proposed framework, where Ours (light) denotes the ablated model without self-distillation, and Ours (full) indicates the full model with MA-CIL and SD. In <ref type="table">Table 2</ref>, we provide parameter amounts of the raw methods and our enhancement variants.</p><p>From the comparison with other methods, we observe that Ours (light) holds the smallest model size (0.347M) while outperforming all previous methods. Combined with the SD module, our full model still has fewer parameter amounts and achieves the best performance. This result demonstrates the efficiency of our framework, which leverages a much simpler network yet gains better performance. As shown in <ref type="table">Table 2</ref>, we note that the MA-CIL method does not include any parameters, which exploits the intrinsic prior of multimodal instances and obtains model discrimination with no computation cost. When boosting the multimodal model <ref type="bibr" target="#b42">[43]</ref>, the enhanced model has comparable size to the raw model due to the analogous model structure. This suggests that our proposed modules are flexible to be adapted to multimodal networks.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Ablation Studies</head><p>To further investigate the contribution of our proposed modules, we conduct ablation experiments to demonstrate how each aspect of our framework affects the overall performance. We first conduct experiments on the effectiveness of each component, and the results are shown in <ref type="table" target="#tab_1">Table 3</ref>. The vanilla two-stream network without MA-CIL and SD achieves a performance of 71.37%. We argue that the limited performance is driven by the small-scale model architecture. Equipped with MA-CIL, we observe a remarkable performance boost from 71.37% to 82.17%, proving that our proposed contrastive method benefits model discrimination and further improves the detection performance. We then investigate the role of our SD module. Combining our SD module to the raw two-stream network and network with MA-CIL, the ablated models achieve the AP increase of 2.64% and 1.23%, respectively. This indicates that the SD module is effective both with and without contrastive learning, and the two modules complement each other for a better violence detection performance.</p><p>Then we perform ablation studies on the loss control strategy of our modality-aware contrastive instance learning. As shown in <ref type="table" target="#tab_2">Table 4</ref>, ? 2 , ? 2 denote the maximum weights of L 2 , L 2 , respectively. is the linearly increasing ratio. <ref type="table" target="#tab_2">Table 4</ref> shows the results of different settings about ? 2 , ? 2 , and . We observe that the optimal setting is ? 2 = 1.5, ? 2 = 1.5, = 0.1, while training with the full weights from the very beginning (r=3.0) brings worse performance. This suggests that gently raising the proportion of contrastive loss is a plausible training strategy, where the model focuses more on the quality of audio and visual embeddings in the early stage and learning feature discrimination afterwards. Finally, we investigate the control hyperparameter of the selfdistillation block in our proposed method as shown in <ref type="figure" target="#fig_3">Figure 3</ref>. Results show that the best performance achieves at = 0.91.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MA-CIL Loss BCE Loss Accuracy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Epoch</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Qualitative Analysis</head><p>We first visualize the variation of the training loss and video-level accuracy on the XD-Violence dataset. Results are shown in <ref type="figure">Figure 4</ref>, where the red curve denotes the video-level accuracy, and the blue and green curves denote BCE loss and contrastive loss, respectively. For the prediction accuracy, we observe a sudden decrease in the first 10 training epochs, where the contrastive learning constraints are gradually applied with the increasing weights. After learning the discrimination for a few epochs, the training accuracy begins to increase and finally outperforms previous results. A similar conclusion also appears in the loss curves. The reduction of the BCE loss comes from the first few epochs, where the model is required to generate high-quality embeddings. The contrastive loss has a lasting decline in dozens of epochs, which means the constraints enforce the model to differentiate instances for a long training period. These curves also denote that the two objectives are co-optimized without interfering with each other. We argue that contrastive learning plays a complementary role to traditional MIL learning, and this insight further demonstrates the generalizability of our methods. We also provide t-SNE <ref type="bibr" target="#b55">[56]</ref> visualizations about the distributions of audio and visual features on the XD-Violence test set. Results are shown in <ref type="figure">Figure 5</ref>, where yellow dots denote background segments and purple dots are violent features. We can find that the violent and non-violent features are clearly clustered, and the distance between uncorrelated features is enlarged after the training procedure. This reveals that aided by our proposed network, instances are successfully differentiated in both audio and visual modalities, further indicating the effectiveness of our proposed framework.</p><p>Finally, we provide visualizations of prediction results presented in <ref type="figure" target="#fig_5">Figure 6</ref>. Our model accurately localizes the anomalous events and even identifies normal events of a very short duration between two violent events. In non-violent videos, the magnitudes between normal and background segments are also evident. Scores of normal events will be a little higher than the background segments yet far less than the violent segments, and our method generates nearly zero predictions for the background snippets. These results show that our proposed approach enables the model to perceive the discrepancy between segments in different types (violent, normal, and background), and further contribute to the violent detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we investigate the model asynchrony and undifferentiated instances phenomena of MIL under audio-visual scenarios, and further show the impact on weakly-supervised audio-visual learning. Then a modality-aware contrastive instance learning with a self-distillation framework is proposed to address these issues. To be specific, we design a lightweight two-stream network to generate audio and visual embedding and logits. Furthermore, a cross-modality contrast is applied to audio and visual instances of different semantics, which involves more unused instances for better discrimination and alleviates the modality inconsistency. To diminish training noises, a self-distillation module is leveraged to transfer visual knowledge to the audio-visual network, by which the semantic gaps between unimodal and multimodal features are narrowed. Our framework outperforms previous methods on the XD-Violence dataset with minor expenses. Besides, assembled with our contrastive learning and self-distillation modules, several prior methods achieve higher detection accuracy, showing the capability as plug-in modules to ameliorate other networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported by National Natural Science Foundation of China (No. 62172101, No. 61976057). This work was supported (in part) by the Science and Technology Commission of Shanghai Municipality (No. 21511101000, No. 21511100602), and the SPMI Innovation and Technology Fund Projects (SAST2020-110).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>ACM</head><label></label><figDesc>Reference Format: Jiashuo Yu 1, * , Jinyu Liu 1, * , Ying Cheng 2 , Rui Feng 1,2,3, ? , Yuejie Zhang 1,3, ? . 2022. Modality-Aware Contrastive Instance Learning with Self-Distillation for Weakly-Supervised Audio-Visual Violence Detection. In Proceedings of ACM MULTIMEDIA CONFERENCE 2022 (MM'2022). ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3503161.3547868</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>distill depth features to the RGB representations via hallucination networks to address the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>An illustration of our proposed Modality-Aware Contrastive Instance Learning with Self-Distillation framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Ablation studies of different settings for control hyperparameter in our self-distillation module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Illustration of the accuracy and loss curves in 50 epochs during training. The red curve denotes the videolevel prediction accuracy. The ranges of BCE loss and contrastive loss are shown in blue and green curves, respectively. Feature space visualizations of the vanilla features and the output of our model on XD-Violence testing videos.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Visualization of results on the XD-Violence test set. Red regions are the temporal ground-truths of violent events.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of the frame-level AP performance with unsupervised and weakly-supervised baselines. ? denotes results re-implemented by integrating logits of two identical networks with audio and visual inputs, and * indicates re-implemented by fusing audio and visual features as inputs.</figDesc><table><row><cell cols="2">Manner Method</cell><cell cols="3">Modality AP (%) Param.</cell></row><row><cell></cell><cell>SVM baseline</cell><cell>V</cell><cell>50.78</cell><cell>/</cell></row><row><cell>Unsup.</cell><cell>OCSVM [48]</cell><cell>V</cell><cell>27.25</cell><cell>/</cell></row><row><cell></cell><cell>Hasan et al. [24]</cell><cell>V</cell><cell>30.77</cell><cell>/</cell></row><row><cell></cell><cell>Sultani et al. [51]</cell><cell>V</cell><cell>73.20</cell><cell>/</cell></row><row><cell></cell><cell>Wu et al. [61]</cell><cell>V</cell><cell>75.90</cell><cell>/</cell></row><row><cell></cell><cell>RTFM [54]</cell><cell>V</cell><cell cols="2">77.81 12.067M</cell></row><row><cell></cell><cell>RTFM* [54]</cell><cell>A+V</cell><cell cols="2">78.10 13.510M</cell></row><row><cell></cell><cell>RTFM ? [54]</cell><cell>A+V</cell><cell cols="2">78.54 13.190M</cell></row><row><cell>W. Sup.</cell><cell>Li et al. [33]</cell><cell>V</cell><cell>78.28</cell><cell>/</cell></row><row><cell></cell><cell>Wu et al. [62]</cell><cell>A+V</cell><cell>78.64</cell><cell>0.843M</cell></row><row><cell></cell><cell>Wu et al. ? [62]</cell><cell>A+V</cell><cell>78.66</cell><cell>1.539M</cell></row><row><cell></cell><cell>Pang et al. [43]</cell><cell>A+V</cell><cell>81.69</cell><cell>1.876M</cell></row><row><cell></cell><cell>Ours (light)</cell><cell>A+V</cell><cell cols="2">82.17 0.347M</cell></row><row><cell></cell><cell>Ours (full)</cell><cell>A+V</cell><cell cols="2">83.40 0.678M</cell></row></table><note>divide each audio into 960-ms overlapped segments and compute the log-mel spectrogram with 96 ? 64 bins.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Ablation studies on different components of our proposed framework.</figDesc><table><row><cell>Index</cell><cell>Two-Stream</cell><cell>MA-CIL</cell><cell>SD</cell><cell>AP (%)</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell>71.37</cell></row><row><cell>2</cell><cell></cell><cell></cell><cell></cell><cell>74.01</cell></row><row><cell>3</cell><cell></cell><cell></cell><cell></cell><cell>82.17</cell></row><row><cell>4</cell><cell></cell><cell></cell><cell></cell><cell>83.40</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Ablation study for the hyperparameters in the proposed modality-aware contrastive instance learning.</figDesc><table><row><cell></cell><cell></cell><cell>Index</cell><cell>? 2</cell><cell>? 2</cell><cell>ratio ( )</cell><cell cols="2">AP (%)</cell></row><row><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell>0.1</cell><cell>82.62</cell><cell></cell></row><row><cell></cell><cell></cell><cell>2</cell><cell>1.0</cell><cell>1.0</cell><cell>0.3</cell><cell>82.67</cell><cell></cell></row><row><cell></cell><cell></cell><cell>3</cell><cell></cell><cell></cell><cell>3.0</cell><cell>82.09</cell><cell></cell></row><row><cell></cell><cell></cell><cell>4</cell><cell></cell><cell></cell><cell>0.1</cell><cell>82.95</cell><cell></cell></row><row><cell></cell><cell></cell><cell>5</cell><cell>1.5</cell><cell>1.0</cell><cell>0.3</cell><cell>81.37</cell><cell></cell></row><row><cell></cell><cell></cell><cell>6</cell><cell></cell><cell></cell><cell>3.0</cell><cell>82.15</cell><cell></cell></row><row><cell></cell><cell></cell><cell>7</cell><cell></cell><cell></cell><cell>0.1</cell><cell>83.21</cell><cell></cell></row><row><cell></cell><cell></cell><cell>8</cell><cell>1.0</cell><cell>1.5</cell><cell>0.3</cell><cell>82.62</cell><cell></cell></row><row><cell></cell><cell></cell><cell>9</cell><cell></cell><cell></cell><cell>3.0</cell><cell>81.68</cell><cell></cell></row><row><cell></cell><cell></cell><cell>10</cell><cell></cell><cell></cell><cell>0.1</cell><cell cols="2">83.40</cell></row><row><cell></cell><cell></cell><cell>11</cell><cell>1.5</cell><cell>1.5</cell><cell>0.3</cell><cell>81.61</cell><cell></cell></row><row><cell></cell><cell></cell><cell>12</cell><cell></cell><cell></cell><cell>3.0</cell><cell>82.14</cell><cell></cell></row><row><cell></cell><cell></cell><cell>83.5</cell><cell></cell><cell></cell><cell>83.4</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>83.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>82.5</cell><cell>82.57</cell><cell>82.49</cell><cell></cell><cell>82.40</cell><cell></cell></row><row><cell>.92</cell><cell>AP(%)</cell><cell>82.0</cell><cell>82.22</cell><cell>82.32</cell><cell>82.35</cell><cell>82.06</cell><cell>82.34</cell></row><row><cell>81.53</cell><cell></cell><cell>81.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>81.32</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>81.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.12</cell><cell></cell><cell cols="3">0.85 0.86 0.87 0.88 0.89</cell><cell cols="3">0.9 0.91 0.92 0.93 0.94</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">m (control hyperparameter)</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Look, listen and learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="609" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Objects that sound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="435" to="451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Violence detection in video using computer vision techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Bermejo Nievas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><forename type="middle">Deniz</forename><surname>Suarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gloria</forename><surname>Bueno Garc?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on Computer analysis of images and patterns</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="332" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Bucilu?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="535" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9650" to="9660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Wasserstein contrastive representation distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16296" to="16305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.01013</idno>
		<title level="m">Counterfactual samples synthesizing and training for robust visual question answering</title>
		<imprint>
			<date type="published" when="2021-06" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning. PMLR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22243" to="22255" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haizhou</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.10855</idno>
		<title level="m">CIL: Contrastive Instance Learning Framework for Distantly Supervised Relation Extraction</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distilling audio-visual knowledge by compositional contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Koepke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7016" to="7025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Look, listen, and attend: Co-attention network for self-supervised audio-visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruize</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuejie</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3884" to="3892" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10555</idno>
		<title level="m">Electra: Pre-training text encoders as discriminators rather than generators</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Contrastive learning for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarkar</forename><surname>Snigdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarathi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arzoo</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><forename type="middle">J</forename><surname>Passonneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.07589</idno>
		<title level="m">CONTaiNER: Few-Shot Named Entity Recognition via Contrastive Learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast violence detection in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Deniz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ismael</forename><surname>Serrano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gloria</forename><surname>Bueno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Kyun</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 international conference on computer vision theory and applications (VISAPP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="478" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Seed: Self-supervised distillation for visual representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yezhou</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.04731</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mist: Multiple instance self-training framework for video anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Chang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fa-Ting</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14009" to="14018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Modality distillation with multiple stream networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nuno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Morerio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV</title>
		<meeting>the European Conference on Computer Vision (ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="103" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Audio set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jort F Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Channing</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="776" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent-a new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="21271" to="21284" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning temporal regularity in video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmudul</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghyun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="733" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">CNN architectures for large-scale audio classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourish</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Channing</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devin</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seybold</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="131" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning with side information through modality hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="826" to="834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cover the violence: A novel Deep-Learning-Based approach towards violence-detection in movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ijaz</forename><surname>Samee Ullah Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungmin</forename><surname>Ul Haq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Wook</forename><surname>Rho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mi</forename><forename type="middle">Young</forename><surname>Baik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">4963</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cooperative Learning of Audio and Video Models from Self-Supervised Synchronization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7774" to="7785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Self-Training Multi-Sequence Learning with Transformer for Weakly Supervised Video Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Jiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Unimo: Towards unified-modal understanding and generation via cross-modal contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guocheng</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15409</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Self-Supervised Video Representation Learning with Motion-Contrastive Perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuejie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui-Wei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.04607</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Future frame prediction for anomaly detection-a new baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongze</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6536" to="6545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Active Contrastive Learning of Audio-Visual Video Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Mcduff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=OMizHuea_HB" />
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A framework for multiple-instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oded</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom?s</forename><surname>Lozano-P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems 10</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Robust audio-visual instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Morgado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12934" to="12945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Audio-visual instance discrimination with cross-modal agreement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Morgado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12475" to="12486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Weakly supervised action localization by sparse temporal pooling network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phuc</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautam</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6752" to="6761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Audio-visual scene analysis with self-supervised multisensory features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="631" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Violence Detection in Videos Based on Fusing Visual and Audio Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Feng</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Hua</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Jian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan-Xiong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2260" to="2264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multimodal violence detection in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Peixoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bahram</forename><surname>Lavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Bestagini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zanoni</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anderson</forename><surname>Rocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2957" to="2961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Toward subjective violence detection in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Peixoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bahram</forename><surname>Lavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o Paulo Pereira</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Avila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zanoni</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anderson</forename><surname>Rocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8276" to="8280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01923</idno>
		<title level="m">Learning from context or names? an empirical study on neural relation extraction</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neelu</forename><surname>Nicolae-Catalin Ristea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Madan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tudor</forename><surname>Radu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamal</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Nasrollahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">B</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Moeslund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.09099</idno>
		<title level="m">Self-Supervised Predictive Convolutional Attentive Block for Anomaly Detection</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Support vector method for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Shuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengkai</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuohui</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.13135</idno>
		<title level="m">Contrastive visual-linguistic pretraining</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiqiao</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.04198</idno>
		<title level="m">Ehsan Shareghi, and Nigel Collier. 2021. TaCL: Improving BERT Pre-training with Token-aware Contrastive Learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Real-world anomaly detection in surveillance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waqas</forename><surname>Sultani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6479" to="6488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10699</idno>
		<title level="m">Contrastive representation distillation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Unified multisensory perception: Weakly-supervised audio-visual video parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingzeyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="436" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Weakly-supervised video anomaly detection with robust temporal feature magnitude learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guansong</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajvinder</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><forename type="middle">W</forename><surname>Verjans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4975" to="4986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Attention is all you need. Advances in neural information processing systems 30</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">On Distinctive Image Captioning via Comparing and Reweighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiuniu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingzhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">COOKIE: Contrastive Cross-Modal Knowledge Sharing Pre-training for Vision-Language Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyu</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2208" to="2217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Separating skills and concepts for novel visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spencer</forename><surname>Whitehead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5632" to="5641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning causal temporal relation and feature discrimination for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3513" to="3527" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Not only look, but also listen: Learning multimodal violence detection under weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangtao</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="322" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Selftraining with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10687" to="10698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Multimodal knowledge expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihui</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sucheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengqi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="854" to="863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiali</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Son</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampath</forename><surname>Chanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belinda</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.10401</idno>
		<title level="m">Trishul Chilimbi, and Junzhou Huang. 2022. Vision-Language Pre-Training with Triple Contrastive Learning</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Temporal convolutional network with complementary inner bag loss for weakly supervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laiyun</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4030" to="4034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">A new method for violence detection in surveillance scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangjian</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="7327" to="7349" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
