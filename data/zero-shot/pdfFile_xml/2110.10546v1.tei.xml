<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Trash or Treasure? An Interactive Dual-Stream Strategy for Single Image Reflection Separation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiming</forename><surname>Hu</surname></persName>
							<email>huqiming@tju.edu.cnxj.max.guo@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">College of Intelligence and Computing</orgName>
								<orgName type="institution">Tianjin University</orgName>
								<address>
									<postCode>300350</postCode>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Intelligence and Computing</orgName>
								<orgName type="institution">Tianjin University</orgName>
								<address>
									<postCode>300350</postCode>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Trash or Treasure? An Interactive Dual-Stream Strategy for Single Image Reflection Separation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Single image reflection separation (SIRS), as a representative blind source separation task, aims to recover two layers, i.e., transmission and reflection, from one mixed observation, which is challenging due to the highly ill-posed nature. Existing deep learning based solutions typically restore the target layers individually, or with some concerns at the end of the output, barely taking into account the interaction across the two streams/branches. In order to utilize information more efficiently, this work presents a general yet simple interactive strategy, namely your trash is my treasure (YTMT), for constructing dual-stream decomposition networks. To be specific, we explicitly enforce the two streams to communicate with each other block-wisely. Inspired by the additive property between the two components, the interactive path can be easily built via transferring, instead of discarding, deactivated information by the ReLU rectifier from one stream to the other. Both ablation studies and experimental results on widely-used SIRS datasets are conducted to demonstrate the efficacy of YTMT, and reveal its superiority over other state-of-the-art alternatives. The implementation is quite simple and our code is publicly available at https://github.com/mingcv/YTMT-Strategy. * Corresponding Author 1 Many problems follow the same additive model, such as denoising (I = B + N , where B and N denote clean image and noise, respectively), and intrinsic image decomposition (log I = log A + log S, where A and S stand for albedo and shading, respectively.) The proposed strategy can be potentially applied to all these tasks, but due to page limit, we concentrate on the task of SIRS to verify primary claims in this paper.</p><p>35th</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Blind source separation, a long-standing problem in signal processing, aims to recover multiple intrinsic components from their mixture, the difficulty of which comes from its ill-posedness, i.e., without extra information, there is an infinite number of feasible decompositions. Particularly in computer vision, image reflection separation (IRS) is a representative scenario that often occurs when taking pictures through a transparent medium such as glass. In such cases, the captured images will contain both the scene transmitted through the medium (transmission) and reflection. On the one hand, reflections are annoying for high-quality imaging, and may interfere with the performance of most, if not all, of classic and contemporary vision oriented algorithms such as object detection and segmentation. On the other hand, one may also want to see what happens in the reflection. Hence, developing effective transmission-reflection decomposition techniques is desired.</p><p>Formally, the captured superimposed image I can be typically modeled as a linear combination of a transmission layer T and a reflection R, i.e. I = T + R. <ref type="bibr" target="#b0">1</ref> Over last decades, a large number of schemes have been devised to solve the decomposition problem. Various statistical priors and regularizers have been proposed to mitigate the ill-posed dilemma, while diverse deep networks have been recently built for the sake of performance improvement, please see Sec. 2 for details. However, in the literature, the additive property, say I = T + R, has been hardly investigated, except for acting as a reconstruction constraint.</p><p>Let us consider that, for any estimation pairT andR satisfying the additive property, there always exists an error/residual Q subject to I =T +R = (T + Q) + (R ? Q). Once Q is somehow obtained, the only thing needed to do is subtracting it fromT , and adding it intoR instead of simply discarding. Under the circumstances, no information is trash, only misplaced. To be symmetric, we rewrite Q as Q := Q T ? Q R , yielding T =T ? Q T + Q R and R =R ? Q R + Q T . In other words, the two targets includingT andR can gain from each other by exchanging, rather than discarding, their respective "trash" factors Q T and Q R . Driven by the above fact, a question naturally arises: Can such an interaction/exchange be applied to intermediate deep features of dual-stream networks?</p><p>Contributions. This paper answers the above question by designing a general interactive dualstream/branch strategy, namely your trash is my treasure (YTMT). An obstacle to realizing YTMT was how to determine exchanging information. Intuitively, activation functions are competent for the job, which are developed to select (activate) a part of features from inputs. In this work, we adopt the ReLU that is arguably the most representative and widely-used activation manner, while others could be also qualified like <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b1">2]</ref>. Please notice that, instead of simply discarding the deactivated features (trash) of the one stream, we alternatively deliver them to the other stream as compensation (treasure). By doing so, there are two main merits: 1) the information losing and dead ReLU problems can be consequently mitigated, and 2) the decreasing speed in training error can be significantly accelerated. The implementation is quite simple and flexible. We provide two optional YTMT blocks as examples and apply them on both plain and UNet architectures to verify the primary claims. Both ablation studies and experimental results on widely-used SIRS datasets are conducted to demonstrate the efficacy of YTMT, and reveal its superiority over other state-of-the-art alternatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Over the past decades, much attention to resolving the image reflection separation problem has been drawn from the community. From the perspective of the required input amount, existing methods can be divided into two classes, i.e., multi-image based and single image based methods.</p><p>Multiple Image Reflection Separation (MIRS). Utilizing multiple images <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b9">10]</ref> used to be a prevalent way to cope with the task, as more information complementary to single superimposed images, like varied conditions and relative motions, can be explored from a sequence of images to accomplish the task. For instance, Agrawal et al. <ref type="bibr" target="#b0">[1]</ref> use flash and no-flash image pairs to remove reflections and highlights, while Szeliski et al. <ref type="bibr" target="#b32">[33]</ref> employ focused and defocused pairs. A variety of approaches <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref> have been proposed to seek relationships between different images, among which Farid and Adelson <ref type="bibr" target="#b6">[7]</ref> employ independent component analysis to reduce reflections and lighting. Gai et al. <ref type="bibr" target="#b7">[8]</ref> alternatively develop an algorithm to estimate layer motions and linear mixing coefficients for the recovery. Li and Brown <ref type="bibr" target="#b21">[22]</ref> adopt the SIFT-flow to align the images, while Xue et al. <ref type="bibr" target="#b39">[40]</ref> enforce a heavy tailed distribution on the obstruction and background components to penalize the overlapped gradients, making the two layers independent. Moreover, Yang et al. <ref type="bibr" target="#b40">[41]</ref> introduce a generalized double-layer brightness consistency constraint to connect optical flow estimation and layer separation. Despite the satisfactory performance of MIRS methods, the need for specified shooting conditions and/or professional tools heavily limits their applicability.</p><p>Single Image Reflection Separation (SIRS). In practice, single image based schemes are more attractive. No doubt that, compared with those MIRS techniques, single image based ones are also more challenging due to less information available, demanding extra priors to regularize the solution.</p><p>To mitigate the ill-posedness, Levin et al. <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b17">18]</ref> favor decompositions that have fewer edges and corners by imposing sparse gradients on the layers, in the same spirit as <ref type="bibr" target="#b39">[40]</ref>. Levin and Weiss <ref type="bibr" target="#b16">[17]</ref> allow users to manually annotate some dominant edges for the layers as explicit constraints for the problem, which requires careful human efforts to obtain favorable results. Li and Brown <ref type="bibr" target="#b22">[23]</ref> assume one layer is smoother than the other, and penalize differently on the two layers in the gradient domain to split the reflection and transmission. Despite a progress made toward addressing the problem, the assumption and requirement could be frequently violated in real situations. Besides, these methods all rely on handcrafted features that may considerably restrict their performance. With the emergence of deep neural networks, learning based methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b38">39]</ref> have shown their power and become dominant for the task. Concretely, CEILNet <ref type="bibr" target="#b5">[6]</ref> comprises of two stages, similarly to <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b36">37]</ref>. An edge map is estimated in the first stage, which performs as the guidance for the second stage to produce the final transmission layer. However, the network is hard to capture high-level information, thus having trouble guaranteeing the perceptual quality. Zhang et al. <ref type="bibr" target="#b43">[44]</ref> further take the perception cue into consideration by coordinating hyper-column features <ref type="bibr" target="#b10">[11]</ref>, the perceptual and adversarial losses. The exclusivity loss (see also the gradient production penalization in <ref type="bibr" target="#b39">[40]</ref>) is designed for assuring gradients to be exclusive between the two components. This manner is also adopted by <ref type="bibr" target="#b37">[38]</ref>. The main drawback is the insufficient capacity in processing cases with large transmission-reflection overlapped regions. Recently, ERRNet <ref type="bibr" target="#b37">[38]</ref> was proposed, which enlarges the receptive field by introducing channel attention and pyramid pooling modules. It does not explicitly predict the reflection layer, which directly discards possible benefit from the supervision of reflection. IBCLN <ref type="bibr" target="#b19">[20]</ref> proposes an iterative boost convolutional LSTM network to progressively split the two layers. However, such an iterative scheme slows down the training and predicting procedure. The errors will also be accumulated due to the high dependency on the outputs of previous stages. Though the learning based strategies have further stepped forward in single image reflection separation compared with the traditional methods, they either restore the target layers individually, or together with some concerns dealing with outputs (e.g., enforcing the linear combination constraint and/or concatenating them together as input). In other words, they barely take into account the interaction across the two streams/branches, which is key to the target task and also other dual-stream decomposition tasks. This study is mainly to demonstrate the effectiveness of such an interaction consideration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Deep Interactive Dual-Stream Learning for SIRS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">YTMT Strategy</head><p>In this part, the proposed deep interactive dual-stream strategy will be detailed by centering around the concept that your trash is my treasure, which would be beneficial to a variety of two-component decomposition tasks using dual-branch networks. Prior works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b34">35]</ref> have shown evidence on the effectiveness of passing information between two branches though, our YTMT strategy performs in a novel and principled way. We first give the definition of the negative ReLU function as follows:</p><formula xml:id="formula_0">ReLU ? (x) := x ? ReLU(x) = min(x, 0),<label>(1)</label></formula><p>where ReLU(x) := max(x, 0). By the negative ReLU, the deactivated features can be easily retained. <ref type="figure" target="#fig_0">Figure 1</ref> (a) exhibits the behaviors of the ReLU and negative ReLU.</p><p>Here, let x 0 be the input to the first layer of the network, andx l i (i ? {1, 2} for two branches) denotes the feature obtained by the i-th branch after l stacked layers, i.e.x l i := H l i (x 0 ). The inputs to the (l + 1)-th layer are as follows:</p><formula xml:id="formula_1">x l 1 := ReLU(x l 1 ) ? ReLU ? (x l 2 ); x l 2 := ReLU(x l 2 ) ? ReLU ? (x l 1 ),<label>(2)</label></formula><p>where ? can be either the concatenation operation or the addition between features activated by the ReLU function (called normal connection) and those by the ReLU ? (YTMT connection), as shown in <ref type="figure" target="#fig_0">Fig. 1 (b)</ref> and (c). As can be obtained from Eq. <ref type="formula" target="#formula_1">(2)</ref>, the amount of information in x l 1 and x l 2 is equivalent to that inx l 1 andx l 2 . This property guarantees no information flowing away from the interaction, which substantially avoids the problems of vanishing/exploding gradients and dead ReLU. <ref type="figure" target="#fig_1">Figure 2</ref> (c) depicts a visual example of producing</p><formula xml:id="formula_2">x L 1 and x L 2 fromx L 1 andx L 2 (in this case, L = 6). It shows that ReLU ? (x L 1 ) is complementary to the ReLU(x L 2 ) and ReLU ? (x L 2 ) is complementary to the ReLU(x L 1 )</formula><p>. By merging the complementary counterparts, there is no information wasted by the rectifiers. In addition, this strategy can significantly speed up the decreasing of training error similarly to the ResNet design <ref type="bibr" target="#b12">[13]</ref>, which will be empirically validated in Sec. 4. Moreover, generally speaking, our strategy can be compatible with most, if not all, of the activation pairs (e.g. Softmax and Softmin). But for the additive problems, the pair of ReLU and negative ReLU is more suitable due to its "either A or B" nature that satisfies the task of SIRS.</p><p>We again emphasize that the proposed YTMT strategy is general and flexible, which can be implemented in various forms according to different demands. <ref type="figure" target="#fig_0">Figure 1</ref> provides two YTMT block options. The one in (b) fuses features from the normal and YTMT connections by channel concatenation and 1 ? 1 convolutions, while the second in (c) simply employs feature addition. In addition, pixel and channel attention mechanism is introduced in both (b) and (c) to select and re-weight the merged features (see <ref type="bibr" target="#b24">[25]</ref> for details). Moreover, the YTMT blocks can be applied to most, if not all, of dualstream backbones by simple modification. In <ref type="figure" target="#fig_1">Fig. 2 (a)</ref> and (b), two commonly-used architectures, i.e. U-shaped <ref type="bibr" target="#b25">[26]</ref> and plain <ref type="bibr" target="#b42">[43]</ref> networks, are present. We will shortly show how to construct these YTMT based networks, and demonstrate their improvement over the backbones and the superior performance on specific applications over other competitors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">YTMT based Design for SIRS</head><p>As shown in <ref type="figure" target="#fig_1">Fig. 2 (a)</ref>, we adopt the U-shaped network as the backbone for the task of SIRS, which can be readily implemented by replacing the convolutional blocks in the UNet architecture with proposed YTMT block options. Following the prior work, the input images are first augmented by the hypercolumn <ref type="bibr" target="#b43">[44]</ref>, gaining 1475 channels, then mapped to 64 via a 1 ? 1 convolution to fuse VGG features and the original input. Each YTMT block is an interactive dual-stream module containing two convolutional layers, both followed by a dual-ReLU rectifier pair. The activations produced by the negative ReLU rectifiers are exchanged between the two streams, then merged by feature addition or concatenation operator before being fed into the attention block. We here use max-pooling and bilinear interpolation to squeeze and expand the feature maps. Like in the single-stream UNet, there are skip connections between the encoder layers and the decoder layers (represented by dashed arrows in orange in <ref type="figure" target="#fig_1">Fig. 2 (a)</ref>), but an extra skip connection is added between each encoder-decoder layer pair for the dual-stream design (represented by dashed arrows in blue). The features in skip connections are first fused with the up-sampled features and then fed into the YTMT blocks.</p><p>After training the first stage to converge, we then freeze its parameters and initialize the second stage with them. The outputs of the first stage are then fed into the second one for training until it also converges. Under this design, the information of the transmission and reflection pairs will further interact during passing through the second stage. To be concluded, with consideration of YTMT block options ("C" for channel concatenation and "A" for feature addition) and using two-stage or not ("S" for single-stage and "T" for two-stage), YTMT based design for SIRS has several variants, including YTMT-{UCS, UAS, UCT, UAT}, based on the choice of model architecture ("U" stands for the U-shaped architecture. We omit the plain architecture for its inferior efficiency compared with the U-shaped on the task of SIRS). The performance difference between these YTMT variants will be studied in Sec. 4.3.</p><p>The objective function considered for SIRS consists of reconstruction loss, perceptual loss, exclusion loss, and adversarial loss. In what follows, each term is explained.</p><p>Reconstruction loss. Preceding methods have revealed that edges are essential for a valid separation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b34">35]</ref>. To make our model sensitive to the gradients, we follow <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b37">38]</ref> to penalize the gradient difference between predictions and targets besides the MSE term, via:</p><formula xml:id="formula_3">L rec := a T ? T 2 2 + b R ? R 2 2 + c ?T ? ?T 1 + d T +R ? I 1 ,<label>(3)</label></formula><p>where ? 1 stands for the 1 norm, and ? 2 the 2 norm. In addition, we empirically set a = 0.3, b = 0.9, c = 0.6, d = 0.2 in all of our experiments. Since reflections are usually weak, the penalization on ?R ? ?R is omitted for stable training.</p><p>Perceptual Loss. The perceptual loss <ref type="bibr" target="#b13">[14]</ref> assists models in achieving high perceptual quality. We minimize the 1 difference between the features of predicted components and those of ground-truths at layers 'conv2_2', 'conv3_2', 'conv4_2', and 'conv5_2' of a VGG-19 model <ref type="bibr" target="#b29">[30]</ref> pretrained on the ImageNet dataset <ref type="bibr" target="#b2">[3]</ref>. Denoting the features of the input at layer i as ? i (?), we have the following function:</p><formula xml:id="formula_4">L per := j ? j ? j (T ) ? ? j (T ) 1 + j ? j ? j (R) ? ? j (R) 1 ,<label>(4)</label></formula><p>where ? j s balance the weights of different layers, {0.38, 0.21, 0.27, 0.18, 6.67} as default.</p><p>Exclusion Loss. Prior work <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b43">44]</ref> shows that enforcing the exclusivity on the two components in the gradient domain is beneficial to separation tasks, which is defined as:</p><formula xml:id="formula_5">L exc := 1 N N ?1 n=0 ?(T ?n ,R ?n ) 2 2 with ?(T,R) := tanh ? T |?T | ? tanh ? R |?R| ,<label>(5)</label></formula><p>whereT ?n andR ?n areT andR down-sampled by 2 n times. In addition, ? T and ? R are normalization factors and ? represents element-wise multiplication.   Our overall objective function turns out to be:</p><formula xml:id="formula_6">L tot := L rec + ? 1 L per + ? 2 L exc + ? 3 L adv ,<label>(7)</label></formula><p>with ? 1 = 0.1, ? 3 = 1, and ? 3 = 0.01 empirically set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Validation on SIRS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>Our training data consists of both real-world and synthesis images, as used in <ref type="bibr" target="#b37">[38]</ref>. Among these data, 90 pairs of input and transmission groundtruth are collected by Zhang et al. <ref type="bibr" target="#b43">[44]</ref>, and 7,643 image pairs chosen from the PASCAL VOC dataset <ref type="bibr" target="#b4">[5]</ref> to synthesize superimposed images following CEILNet <ref type="bibr" target="#b5">[6]</ref>. The models are implemented in PyTorch and optimized with Adam optimizer, keeping ? 1 = 0.9, and ? 2 = 0.999. The learning rate is initialized as 10 ?4 and then reduced by half at epoch 60, 80, and 100, respectively. The training is stopped at epoch 120. All the models are trained on a single RTX 2080 Ti graphics card.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with State-of-the-art Methods</head><p>We select the proposed YTMT-UCT network to compare with the state-of-the-art methods, including Zhang et al. <ref type="bibr" target="#b43">[44]</ref>, BDN <ref type="bibr" target="#b41">[42]</ref>, ERRNet <ref type="bibr" target="#b37">[38]</ref>, IBCLN <ref type="bibr" target="#b19">[20]</ref> and Lei et al. <ref type="bibr" target="#b15">[16]</ref>, on four real-world dataset, involving Real20 <ref type="bibr" target="#b43">[44]</ref> and three subsets of SIR 2 <ref type="bibr" target="#b33">[34]</ref>. The PSNR and SSIM metrics are utilized to evaluate all the competing methods as shown in <ref type="table" target="#tab_0">Table 1</ref>. Generally speaking, each component for blind source separation tasks should be treated equally. For the SIRS task, the reflection layer Input Zhang et al. <ref type="bibr" target="#b43">[44]</ref> BDN <ref type="bibr" target="#b41">[42]</ref> ERRNet <ref type="bibr" target="#b37">[38]</ref> IBCLN <ref type="bibr" target="#b19">[20]</ref> Ours GT <ref type="figure">Figure 4</ref>: Visual comparison of state-of-the-art methods and ours on real-world testing dataset, including Real20 <ref type="bibr" target="#b43">[44]</ref> and SIR 2 <ref type="bibr" target="#b5">[6]</ref>. It shows that the results by our method are more visually striking compared with those by the competitors, with less artifacts and color distortion (please pay attention to the regions framed by red boxes). can be a critical part of the information to reproduce the image shooting scene, as discussed in <ref type="bibr" target="#b35">[36]</ref>. Therefore, we report the performance difference of the reflection recovery in <ref type="table" target="#tab_1">Table 2</ref>, among the methods with the top-3 transmission recovery. The visual comparison is also conducted in <ref type="figure">Fig. 6</ref>. Given ERRNet does not output reflection layers and the predicted reflections of IBCLN still mingle with large parts of transmission layers, we compare their reflection layers by I ?T for fairness.</p><p>It turns out that the YTMT-UCT achieves the best results on almost all of the testing datasets, in terms of the recovery of both transmission and the reflection layers, which indicates that the YTMT strategy gives the most accurate separations with the help of frequent feature interaction between the transmission and reflection streams. The difficulty raising in Real20 comes from the largely overlapped regions and various reflection patterns, while the challenge of SIR 2 is mainly low-quality transmission layers and reflections blended with the monochromatic background. Therefore, we further conduct the qualitative comparison among these methods in <ref type="figure">Fig. 4</ref>   <ref type="figure">Figure 5</ref>: Visual comparison of YTMT-UAT and state-of-the-art methods on real45 dataset <ref type="bibr" target="#b5">[6]</ref>. It shows that our method produces results with fewer residual reflections.</p><p>separations stage-by-stage or iteration-by-iteration, where new outputs are concatenated to be fed into the next stage/iteration, instead of inner interaction or feature exchanging. As a consequence, the error would be rather accumulated. However, in the YTMT strategy, there are more chances for the network (e.g., 14 times in the YTMT-UCT). Moreover, the separations are estimated in a single network, rather than several sub-networks. We train the first stage of the network to converge, and then use it to initialize the second stage. The training of the second stage depends on not only the preliminary separations, but also the knowledge learned in the first stage. If there is no better separation the network can exploit, the second stage will produce a solution close to the preliminary separations. ERRNet introduces some useful network building blocks, but still leaves some obvious reflections. It merely estimates the transmission layer, discarding the interaction between two components. In conclusion, feature interaction is highly desired for the satisfactory separations. Moreover, the training error shown in <ref type="figure" target="#fig_2">Fig. 3</ref> further accounts for the information efficacy of YTMT strategy, given the large margin of the convergence between the dual-stream UNet without any feature interaction and the YTMT-UCS solution. To evaluate our generalization ability to different data distributions, we also follow the training setting of IBCLN <ref type="bibr" target="#b19">[20]</ref> and report our results on the testing data of the Nature dataset proposed by it. We achieve 23.85 and 0.810 for PSNR and SSIM, respectively, compared with 23.57 and 0.783 of IBCLN, and surpass previous methods more than a lot as shown in <ref type="table" target="#tab_2">Table 3</ref>. Note that we reuse the numerical results proposed in the paper of IBCLN, since the same experimental setup is kept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>To further assess the capacity of the YTMT strategy for the task, we have developed two baselines to illustrate the effectiveness of our proposed YTMT strategy (see results in <ref type="table" target="#tab_4">Table 4</ref>). The first model is a dual-stream UNet without any feature interaction. One might think it is identical to a single UNet. However, constrained by the reconstruction loss term T +R ? I 1 , the estimations of the two layersT andR are highly related to each other. Moreover, supervised by the ground-truth of both transmission and reflection layers, the optimization constrained by multiple regularizers are more likely to improve the generalization ability of the network. Thus we adopt it as one of the baseline networks, which means any interactive method should not have lower performance than it.</p><p>The second model replaces all the negative ReLU rectifiers by the normal ReLU in the YTMT-UCS to validate if the YTMT strategy really works to preserve the discarded information. The rest four models are the YTMT variants introduced in the Sec. 3.2.  <ref type="bibr" target="#b19">[20]</ref> Ours GT <ref type="figure">Figure 6</ref>: Visual comparison on the layer decomposition between YTMT-UCT and state-of-the-art methods that output transmission-reflection layer pairs.</p><p>With approximate number of parameters, YTMT solutions show obvious superiority over other alternatives, benefiting from the strategy introducing the complementary interaction between dual streams. Obviously, without any feature interaction, a dual-stream UNet is apparently inferior to those with feature interaction, which partly explains the performance gap between the IBCLN and ours. With only the positive ReLU rectifiers, the dual streams in the network will have the same activations, leading to degraded performance. Moreover, it can be seen that the YTMT-UAT shows leading results on the first two datasets, while YTMT-UCT exhibits better overall performance over other state-of-the-art methods, thus it is opted as the default architecture for SIRS tasks. Also, it shows that a two-stage architecture can indeed further improve the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed a general rule for deep interactive dual-stream/branch learning, namely your trash is my treasure (denoted as YTMT), which says that two branches should communicate with each other frequently by exchanging, rather than discarding, the information useless to themselves. Activation functions are deemed to be suitable for determining which part of information to exchange.</p><p>As an example, the widely-used ReLU was chosen to validate the primary claims of this work. In addition, we offered several YTMT based designs based on both U-shaped and plain backbones to show the flexibility. Extensive experimental results on public SIRS datasets have been provided to verify the effectiveness of YTMT, and demonstrate the clear advantages of our method in comparison with other state-of-the-art alternatives. Another merit of YTMT comes from its acceleration in decreasing error during training. It is positive that our strategy can derive diverse designs and be beneficial to many two-component decomposition tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The functional behaviors of ReLU and negative ReLU are plotted in (a). Two YTMT block options are given in (b) and (c). To fuse features from the normal and YTMT connections, the first option uses channel concatenation and 1 ? 1 convolutions, while the second simply uses feature addition. Pixel and channel attention mechanism (denoted by Att.) is utilized in both (b) and (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of the YTMT networks. (a) and (b) offer the YTMT versions modified on the U-shape and plain network architectures, respectively. The input is first augmented by hyper column [44] and then fed into the YTMT blocks. (c) shows a visualized example of producing x L 1 and x L 2 fromx L 1 andx L 2 at the sixth YTMT block of the trained U-shaped YTMT network. For better view, all the features are normalized by softmax.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Training error on our training data of YTMT-UCS and a dual-stream UNet without any feature interaction (w/o FI). The proposed interaction strategy leads to a much faster decreasing speed. Adversarial Loss. The adversarial loss regulates the SIRS solution on the real-world image manifold. With the adversarial training of the main network G and a discriminator D, they seek the equilibrium by optimizing the adversarial loss L adv [15], as follows: L G adv := ? log(D(T,T )) ? log(1 ? D(T, T )); L D adv := ? log(1 ? D(T,T )) ? log(D(T, T )), (6) where D stays invariant with [38].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Quantitative results on four real-world benchmark datasets of different methods. The best results are indicated in red and the second best results in blue.DatasetsMetricsInput CEILNet Zhang et al. BDN ERRNet IBCLN Lei et al. YTMT-UCT</figDesc><table><row><cell>Real20 (20)</cell><cell>PSNR 19.16 SSIM 0.732</cell><cell>18.45 0.690</cell><cell>22.55 0.788</cell><cell>18.41 0.726</cell><cell>22.89 0.803</cell><cell>21.86 0.762</cell><cell>22.35 0.793</cell><cell>23.26 0.806</cell></row><row><cell>Objects (200)</cell><cell>PSNR 23.74 SSIM 0.878</cell><cell>23.62 0.867</cell><cell>22.68 0.879</cell><cell>22.72 0.856</cell><cell>24.87 0.896</cell><cell>24.87 0.893</cell><cell>23.81 0.882</cell><cell>24.87 0.896</cell></row><row><cell>Postcard (199)</cell><cell>PSNR 21.31 SSIM 0.877</cell><cell>21.24 0.834</cell><cell>16.81 0.797</cell><cell>20.71 0.859</cell><cell>22.04 0.876</cell><cell>23.39 0.875</cell><cell>21.48 0.873</cell><cell>22.91 0.884</cell></row><row><cell>Wild (55)</cell><cell>PSNR 26.06 SSIM 0.890</cell><cell>22.36 0.821</cell><cell>21.52 0.832</cell><cell>22.36 0.830</cell><cell>24.25 0.853</cell><cell>24.71 0.886</cell><cell>23.84 0.866</cell><cell>25.48 0.890</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Quantitative comparison in reflection recovery between the top-3 methods in transmission recovery. The best results are highlighted in red and the second best results in blue.</figDesc><table><row><cell>Datasets</cell><cell cols="4">Metrics ERRNet IBCLN YTMT-UCT</cell></row><row><cell>Real20 (20)</cell><cell>PSNR SSIM</cell><cell>23.55 0.446</cell><cell>22.36 0.469</cell><cell>24.30 0.542</cell></row><row><cell>Objects (200)</cell><cell>PSNR SSIM</cell><cell>26.02 0.446</cell><cell>19.97 0.226</cell><cell>26.45 0.499</cell></row><row><cell>Postcard (199)</cell><cell>PSNR SSIM</cell><cell>22.47 0.419</cell><cell>13.16 0.230</cell><cell>23.41 0.478</cell></row><row><cell>Wild (55)</cell><cell>PSNR SSIM</cell><cell>25.52 0.460</cell><cell>20.83 0.298</cell><cell>27.33 0.590</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Quantitative results on the testing samples from Nature dataset of different methods. The best results are highlighted in red and the second best results in blue.</figDesc><table><row><cell cols="8">Metrics CEILNet-F Zhang et al. BDN-F RmNet ERRNet-F IBCLN YTMT-UCS</cell></row><row><cell>PSNR</cell><cell>19.33</cell><cell>19.56</cell><cell>18.92</cell><cell>19.36</cell><cell>22.18</cell><cell>23.57</cell><cell>23.85</cell></row><row><cell>SSIM</cell><cell>0.745</cell><cell>0.736</cell><cell>0.737</cell><cell>0.725</cell><cell>0.756</cell><cell>0.783</cell><cell>0.810</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>to show our capacity for these challenging samples. As can be observed, the method proposed by Zhang et al. produces results with severe color distortion and cannot handle the aforementioned globally overlapping problem. BDN and IBCLN generalize unsatisfyingly on the Real20 dataset, failing to remove conspicuous reflections in several cases, and some reflections are even enhanced by the BDN due to the inaccurate reflection estimation amplified by the cascade structure. Meanwhile, both the methods refine the</figDesc><table><row><cell>Input</cell><cell>CEILNet [6] Zhang et al. [44] BDN [42]</cell><cell>ERRNet [38] IBCLN [20]</cell><cell>Ours</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on different network architectures, including a dual-stream UNet without any feature interaction (FI), a YTMT-UCS with all the negative ReLU rectifiers replaced by the normal ReLU and four YTMT variants. The best results are highlighted in red and the second best results in blue.Datasets Metrics w/o FI ReLU only YTMT-UCS YTMT-UCT YTMT-UAS YTMT-UAT</figDesc><table><row><cell>Real20</cell><cell>PSNR SSIM</cell><cell>20.57 0.752</cell><cell>22.79 0.802</cell><cell>23.05 0.805</cell><cell>23.09 0.802</cell><cell>23.26 0.801</cell><cell>23.39 0.809</cell></row><row><cell>Objects</cell><cell>PSNR SSIM</cell><cell>23.85 0.883</cell><cell>24.06 0.886</cell><cell>24.46 0.891</cell><cell>24.58 0.891</cell><cell>24.71 0.893</cell><cell>25.40 0.899</cell></row><row><cell>Postcard</cell><cell>PSNR SSIM</cell><cell>20.82 0.863</cell><cell>22.02 0.869</cell><cell>22.66 0.885</cell><cell>22.75 0.884</cell><cell>22.45 0.871</cell><cell>23.01 0.874</cell></row><row><cell>Wild</cell><cell>PSNR SSIM</cell><cell>24.20 0.881</cell><cell>24.71 0.859</cell><cell>25.24 0.887</cell><cell>25.46 0.892</cell><cell>24.49 0.870</cell><cell>25.19 0.880</cell></row><row><cell cols="2">Zhang et al. [44]</cell><cell cols="2">BDN [42]</cell><cell>IBCLN</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Removing photography artifacts using gradient projection and flash-exposure sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhen</forename><surname>Nayar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="828" to="835" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djork-Arn?</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Incorporating second-order functional knowledge for better option pricing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Dugas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>B?lisle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claude</forename><surname>Nadeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="472" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">M</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A generic deep architecture for single image reflection removal and image smoothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingnan</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">P</forename><surname>Wipf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3258" to="3267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Separating reflections and lighting using independent components analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hany</forename><surname>Farid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="1262" to="1267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Blind separation of superimposed moving images using image statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenwei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="19" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Robust separation of reflection from multiple images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2195" to="2202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Reflection removal using low-rank matrix completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeong-Ju</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae-Young</forename><surname>Sim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3872" to="3880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><forename type="middle">Andr?s</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="447" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The relativistic discriminator: a key element missing from standard GAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexia</forename><surname>Jolicoeur-Martineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Polarized reflection removal with perfect alignment in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuhua</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengdi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1747" to="1755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">User assisted separation of reflections from a single image using a sparsity prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anat</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1647" to="1654" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to perceive transparency from the statistics of natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anat</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Assaf</forename><surname>Zomet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1247" to="1254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Separating reflections from a single image using local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anat</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Assaf</forename><surname>Zomet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="306" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Single image reflection removal through cascaded refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">E</forename><surname>Hopcroft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3562" to="3571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Single image rain removal via a deep decomposition-composition network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">186</biblScope>
			<biblScope unit="page" from="48" to="57" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Exploiting reflection change for automatic reflection removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2432" to="2439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Single image layer separation using relative smoothness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2752" to="2759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Awni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. icml</title>
		<meeting>icml</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ffa-net: Feature fusion attention network for single image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Xu Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizhu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11908" to="11915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Separating transparent layers through layer information exchange</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Sarel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="328" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Separation of transparent layers using focus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nahum</forename><surname>Schechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronen</forename><surname>Kiryati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Basri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="39" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Reflection removal for in-vehicle black box videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">Kyu</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4231" to="4239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Image-based rendering for scenes with reflections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sudipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Goesele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szeliski</surname></persName>
		</author>
		<idno>100:1-100:10</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Automatic reflection removal using gradient intensity and motion cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taotao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanghui</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="466" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Layer extraction from multiple images containing reflections and transparency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shai Avidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anandan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page">1246</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Benchmarking single-image reflection removal algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling-Yu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah-Hwee</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3942" to="3950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">CRRN: multi-scale guided concurrent reflection removal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling-Yu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah-Hwee</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4777" to="4785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Reflection scene separation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling-Yu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Depth of field guided reflection removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah-Hwee</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Single image reflection removal exploiting misaligned training data and network enhancements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">P</forename><surname>Wipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8178" to="8187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Single image reflection removal beyond linearity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinjie</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqiang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengfeng</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3771" to="3779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A computational approach for obstruction-free photography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<idno>79:1-79:11</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Robust optical flow estimation of double-layer images under transparency or reflection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robby</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1410" to="1419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Seeing deeply and bidirectionally: A deep learning approach for single image reflection removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinfeng</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="675" to="691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Beyond a gaussian denoiser: Residual learning of deep CNN for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3142" to="3155" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Single image reflection separation with perceptual losses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cecilia</forename><surname>Xuaner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4786" to="4794" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
