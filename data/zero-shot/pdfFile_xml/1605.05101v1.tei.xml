<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Recurrent Neural Network for Text Classification with Multi-Task Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
							<email>xpqiu@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
							<email>xjhuang@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Recurrent Neural Network for Text Classification with Multi-Task Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural network based methods have obtained great progress on a variety of natural language processing tasks. However, in most previous works, the models are learned based on single-task supervised objectives, which often suffer from insufficient training data. In this paper, we use the multitask learning framework to jointly learn across multiple related tasks. Based on recurrent neural network, we propose three different mechanisms of sharing information to model text with task-specific and shared layers. The entire network is trained jointly on all these tasks. Experiments on four benchmark text classification tasks show that our proposed models can improve the performance of a task with the help of other related tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Distributed representations of words have been widely used in many natural language processing (NLP) tasks. Following this success, it is rising a substantial interest to learn the distributed representations of the continuous words, such as phrases, sentences, paragraphs and documents <ref type="bibr" target="#b4">[Socher et al., 2013;</ref><ref type="bibr" target="#b4">Le and Mikolov, 2014;</ref><ref type="bibr">Kalchbrenner et al., 2014;</ref><ref type="bibr" target="#b4">Liu et al., 2015a]</ref>. The primary role of these models is to represent the variable-length sentence or document as a fixedlength vector. A good representation of the variable-length text should fully capture the semantics of natural language.</p><p>The deep neural networks (DNN) based methods usually need a large-scale corpus due to the large number of parameters, it is hard to train a network that generalizes well with limited data. However, the costs are extremely expensive to build the large scale resources for some NLP tasks. To deal with this problem, these models often involve an unsupervised pre-training phase. The final model is fine-tuned with respect to a supervised training criterion with a gradient based optimization. Recent studies have demonstrated significant accuracy gains in several NLP tasks <ref type="bibr" target="#b0">[Collobert et al., 2011]</ref> with the help of the word representations learned from the large unannotated corpora. Most pre-training methods * Corresponding author. are based on unsupervised objectives such as word prediction for training <ref type="bibr" target="#b0">[Collobert et al., 2011;</ref><ref type="bibr">Turian et al., 2010;</ref><ref type="bibr" target="#b4">Mikolov et al., 2013]</ref>. This unsupervised pre-training is effective to improve the final performance, but it does not directly optimize the desired task.</p><p>Multi-task learning utilizes the correlation between related tasks to improve classification by learning tasks in parallel. Motivated by the success of multi-task learning <ref type="bibr">[Caruana, 1997]</ref>, there are several neural network based NLP models <ref type="bibr" target="#b0">[Collobert and Weston, 2008;</ref><ref type="bibr" target="#b4">Liu et al., 2015b]</ref> utilize multitask learning to jointly learn several tasks with the aim of mutual benefit. The basic multi-task architectures of these models are to share some lower layers to determine common features. After the shared layers, the remaining layers are split into the multiple specific tasks.</p><p>In this paper, we propose three different models of sharing information with recurrent neural network (RNN). All the related tasks are integrated into a single system which is trained jointly. The first model uses just one shared layer for all the tasks. The second model uses different layers for different tasks, but each layer can read information from other layers. The third model not only assigns one specific layer for each task, but also builds a shared layer for all the tasks. Besides, we introduce a gating mechanism to enable the model to selectively utilize the shared information. The entire network is trained jointly on all these tasks.</p><p>Experimental results on four text classification tasks show that the joint learning of multiple related tasks together can improve the performance of each task relative to learning them separately.</p><p>Our contributions are of two-folds:</p><p>? First, we propose three multi-task architectures for RNN. Although the idea of multi-task learning is not new, our work is novel to integrate RNN into the multilearning framework, which learns to map arbitrary text into semantic vector representations with both taskspecific and shared layers.</p><p>? Second, we demonstrate strong results on several text classification tasks. Our multi-task models outperform most of state-of-the-art baselines. The primary role of the neural models is to represent the variable-length text as a fixed-length vector. These models generally consist of a projection layer that maps words, subword units or n-grams to vector representations (often trained beforehand with unsupervised methods), and then combine them with the different architectures of neural networks. There are several kinds of models to model text, such as Neural Bag-of-Words (NBOW) model, recurrent neural network (RNN) <ref type="bibr" target="#b0">[Chung et al., 2014]</ref>, recursive neural network (RecNN) <ref type="bibr" target="#b4">[Socher et al., 2012;</ref><ref type="bibr" target="#b4">Socher et al., 2013]</ref> and convolutional neural network (CNN) <ref type="bibr" target="#b0">[Collobert et al., 2011;</ref><ref type="bibr">Kalchbrenner et al., 2014]</ref>. These models take as input the embeddings of words in the text sequence, and summarize its meaning with a fixed length vectorial representation.</p><p>Among them, recurrent neural networks (RNN) are one of the most popular architectures used in NLP problems because their recurrent structure is very suitable to process the variable-length text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Recurrent Neural Network</head><p>A recurrent neural network (RNN) <ref type="bibr">[Elman, 1990]</ref> is able to process a sequence of arbitrary length by recursively applying a transition function to its internal hidden state vector h t of the input sequence. The activation of the hidden state h t at time-step t is computed as a function f of the current input symbol x t and the previous hidden state h t?1</p><formula xml:id="formula_0">h t = 0 t = 0 f (h t?1 , x t ) otherwise<label>(1)</label></formula><p>It is common to use the state-to-state transition function f as the composition of an element-wise nonlinearity with an affine transformation of both x t and h t?1 .</p><p>Traditionally, a simple strategy for modeling sequence is to map the input sequence to a fixed-sized vector using one RNN, and then to feed the vector to a softmax layer for classification or other tasks <ref type="bibr" target="#b0">[Cho et al., 2014]</ref>.</p><p>Unfortunately, a problem with RNNs with transition functions of this form is that during training, components of the gradient vector can grow or decay exponentially over long sequences <ref type="bibr" target="#b2">[Hochreiter et al., 2001;</ref><ref type="bibr" target="#b2">Hochreiter and Schmidhuber, 1997</ref>]. This problem with exploding or vanishing gradients makes it difficult for the RNN model to learn longdistance correlations in a sequence.</p><p>Long short-term memory network (LSTM) was proposed by <ref type="bibr" target="#b2">[Hochreiter and Schmidhuber, 1997</ref>] to specifically address this issue of learning long-term dependencies. The LSTM maintains a separate memory cell inside it that updates and exposes its content only when deemed necessary. A number of minor modifications to the standard LSTM unit have been made. While there are numerous LSTM variants, here we describe the implementation used by <ref type="bibr" target="#b2">Graves [2013]</ref>.</p><p>We define the LSTM units at each time step t to be a collection of vectors in R d : an input gate i t , a forget gate f t , an output gate o t , a memory cell c t and a hidden state h t . d is the number of the LSTM units. The entries of the gating vectors i t , f t and o t are in [0, 1]. The LSTM transition equations <ref type="figure">Figure 1</ref>: Recurrent Neural Network for Classification are the following:</p><formula xml:id="formula_1">h 1 h 2 h 3 ? ? ? h T softmax y x1 x2 x3 xT</formula><formula xml:id="formula_2">i t = ?(W i x t + U i h t?1 + V i c t?1 ), (2) f t = ?(W f x t + U f h t?1 + V f c t?1 ), (3) o t = ?(W o x t + U o h t?1 + V o c t ), (4) c t = tanh(W c x t + U c h t?1 ),<label>(5)</label></formula><formula xml:id="formula_3">c t = f i t c t?1 + i t c t , (6) h t = o t tanh(c t ),<label>(7)</label></formula><p>where x t is the input at the current time step, ? denotes the logistic sigmoid function and denotes elementwise multiplication. Intuitively, the forget gate controls the amount of which each unit of the memory cell is erased, the input gate controls how much each unit is updated, and the output gate controls the exposure of the internal memory state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Task-Specific Output Layer</head><p>In a single specific task, a simple strategy is to map the input sequence to a fixed-sized vector using one RNN, and then to feed the vector to a softmax layer for classification or other tasks.</p><p>Given a text sequence x = {x 1 , x 2 , ? ? ? , x T }, we first use a lookup layer to get the vector representation (embeddings) x i of the each word x i . The output at the last moment h T can be regarded as the representation of the whole sequence, which has a fully connected layer followed by a softmax non-linear layer that predicts the probability distribution over classes. <ref type="figure">Figure 1</ref> shows the unfolded RNN structure for text classification.</p><p>The parameters of the network are trained to minimise the cross-entropy of the predicted and true distributions.</p><formula xml:id="formula_4">L(?, y) = ? N i=1 C j=1 y j i log(? j i ),<label>(8)</label></formula><p>where y j i is the ground-truth label;? j i is prediction probabilities; N denotes the number of training samples and C is the class number.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Three Sharing Models for RNN based</head><p>Multi-Task Learning</p><formula xml:id="formula_5">x (m) 1 x (s) 1 x (m) 2 x (s) 2 x (m) 3 x (s) 3 x (m) T x (s) T softmax1 y (m) h (s) 1 h (s) 2 h (s) 3 ? ? ? h (s) T x (n) 1 x (s) 1 x (n) 2 x (s) 2 x (n) 3 x (s) 3 x (n) T x (s) T softmax2 y (n) (a) Model-I: Uniform-Layer Architecture x 1 x 2 x 3 x T h (m) 1 h (m) 2 h (m) 3 ? ? ? h (m) T softmax1 y (m) h (n) 1 h (n) 2 h (n) 3 ? ? ? h (n) T softmax2 y (n) x 1 x 2 x 3 x T (b) Model-II: Coupled-Layer Architecture x 1 x 2 x 3 x T h (m) 1 h (m) 2 h (m) 3 ? ? ? h (m) T softmax1 y (m) h (s) 1 h (s) 2 h (s) 3 h (s) T h (n) 1 h (n) 2 h (n) 3 ? ? ? h (n) T softmax2 y (n) x 1 x 2 x 3 x T</formula><p>(c) Model-III: Shared-Layer Architecture <ref type="figure">Figure 2</ref>: Three architectures for modelling text with multitask learning.</p><p>Motivated by the success of multi-task learning <ref type="bibr">[Caruana, 1997]</ref>, we propose three multi-task models to leverage supervised data from many related tasks. Deep neural model is well suited for multi-task learning since the features learned from a task may be useful for other tasks. <ref type="figure">Figure 2</ref> gives an illustration of our proposed models.</p><p>Model-I: Uniform-Layer Architecture In Model-I, the different tasks share a same LSTM layer and an embedding layer besides their own embedding layers.</p><p>For task m, the inputx t consists of two parts:</p><formula xml:id="formula_6">x (m) t = x (m) t ? x (s) t ,<label>(9)</label></formula><formula xml:id="formula_7">where x (m) t , x (s) t</formula><p>denote the task-specific and shared word embeddings respectively, ? denotes the concatenation operation.</p><p>The LSTM layer is shared for all tasks. The final sequence representation for task m is the output of LSMT at step T .</p><formula xml:id="formula_8">h (m) T = LST M (x (m) ).<label>(10)</label></formula><p>Model-II: Coupled-Layer Architecture In Model-II, we assign a LSTM layer for each task, which can use the information for the LSTM layer of the other task. Given a pair of tasks (m, n), each task has own LSTM in the task-specific model. We denote the outputs at step t of two coupled LSTM layer are h (m) t and h (n) t . To better control signals flowing from one task to another task, we use a global gating unit which endows the model with the capability of deciding how much information it should accept. We re-define Eqs. (5) and the new memory content of an LSTM at m-th task is computed by:</p><formula xml:id="formula_9">ct (m) = tanh ? ? W (m) c xt + i?{m,n} g (i?m) U (i?m) c h (i) t?1 ? ? (11) where g (i?m) = ?(W (m) g x t +U (i) g h (i) t?1 )</formula><p>. The other settings are same to the standard LSTM.</p><p>This model can be used to jointly learning for every two tasks. We can get two task specific representations h Model-III: Shared-Layer Architecture Model-III also assigns a separate LSTM layer for each task, but introduces a bidirectional LSTM layer to capture the shared information for all the tasks.</p><p>We denote the outputs of the forward and backward LSTMs at step t as</p><formula xml:id="formula_10">? ? h (s) t and ? ? h (s) t respectively. The output of shared layer is h (s) t = ? ? h (s) t ? ? ? h (s) t .</formula><p>To enhance the interaction between task-specific layers and the shared layer, we use gating mechanism to endow the neurons in task-specific layer with the ability to accept or refuse the information passed by the neuron in shared layers. Unlike Model-II, we compute the new state for LSTM as follows:</p><formula xml:id="formula_11">c (m) t = tanh W (m) c xt + g (m) U (m) c h (m) t?1 + g (s?m) U (s) c h (s) t ,<label>(12)</label></formula><p>where g (m) = ?(W (m)</p><formula xml:id="formula_12">g x t + U (m) g h (m) t?1 ) and g (s?m) = ?(W (m) g x t + U (s?m) g h (s) t ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Training</head><p>The task-specific representations, which emittd by the mutitask architectures of all of the above, are ultimately fed into different output layers, which are also task-specific.</p><formula xml:id="formula_13">y (m) = softmax(W (m) h (m) + b (m) ),<label>(13)</label></formula><p>where? (m) is prediction probabilities for task m, W (m) is the weight which needs to be learned, and b (m) is a bias term. Our global cost function is the linear combination of cost function for all joints.</p><formula xml:id="formula_14">? = M m=1 ? m L(? (m) , y (m) )<label>(14)</label></formula><p>where ? m is the weights for each task m respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Type Train Size Dev. Size Test Size Class Averaged Length Vocabulary Size <ref type="table" target="#tab_3">SST-1  Sentence  8544  1101  2210  5  19  18K  SST-2  Sentence  6920  872  1821  2  18  15K  SUBJ  Sentence  9000  -1000  2  21  21K  IMDB Document  25,000  -25,000  2  294  392K   Table 1</ref>: Statistics of the four datasets used in this paper.</p><p>It is worth noticing that labeled data for training each task can come from completely different datasets. Following <ref type="bibr" target="#b0">[Collobert and Weston, 2008]</ref>, the training is achieved in a stochastic manner by looping over the tasks:</p><p>1. Select a random task. 2. Select a random training example from this task. 3. Update the parameters for this task by taking a gradient step with respect to this example. 4. Go to 1. Fine Tuning For model-I and model-III, there is a shared layer for all the tasks. Thus, after the joint learning phase, we can use a fine tuning strategy to further optimize the performance for each task. Pre-training of the shared layer with neural language model For model-III, the shared layer can be initialized by an unsupervised pre-training phase. Here, for the shared LSTM layer in Model-III, we initialize it by a language model <ref type="bibr" target="#b0">[Bengio et al., 2007]</ref>, which is trained on all the four task dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment</head><p>In this section, we investigate the empirical performances of our proposed three models on four related text classification tasks and then compare it to other state-of-the-art models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>To show the effectiveness of multi-task learning, we choose four different text classification tasks about movie review. Each task have own dataset, which is briefly described as follows.</p><p>? SST- ? IMDB The IMDB dataset 2 consists of 100,000 movie reviews with binary classes <ref type="bibr" target="#b4">[Maas et al., 2011]</ref>. One key aspect of this dataset is that each movie review has several sentences.</p><p>The first three datasets are sentence-level, and the last dataset is document-level. The detailed statistics about the four datasets are listed in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Hyperparameters and Training</head><p>The network is trained with backpropagation and the gradient-based optimization is performed using the Adagrad update rule <ref type="bibr" target="#b1">[Duchi et al., 2011]</ref>. In all of our experiments, the word embeddings are trained using word2vec <ref type="bibr" target="#b4">[Mikolov et al., 2013]</ref> on the Wikipedia corpus (1B words). The vocabulary size is about 500,000. The word embeddings are fine-tuned during training to improve the performance <ref type="bibr" target="#b0">[Collobert et al., 2011]</ref>. The other parameters are initialized by randomly sampling from uniform distribution in [-0.1, 0.1]. The hyperparameters which achieve the best performance on the development set will be chosen for the final evaluation. For datasets without development set, we use 10-fold crossvalidation (CV) instead.</p><p>The final hyper-parameters are as follows. The embedding size for specific task and shared layer are 64. For Model-I, there are two embeddings for each word, and both their sizes are 64. The hidden layer size of LSTM is 50. The initial learning rate is 0.1. The regularization weight of the parameters is 10 ?5 .   <ref type="table">Table 3</ref>: Results of the coupled-layer architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Effect of Multi-task Training</head><p>We first compare our our proposed models with the standard LSTM for single task classification. We use the implementation of <ref type="bibr" target="#b2">Graves [2013]</ref>. The unfolded illustration is shown in <ref type="figure">Figure 1</ref>.   Uniform-layer Architecture For the first uniform-layer architecture, we train the model on four datasets simultaneously. The LSTM layer is shared across all the tasks. The average improvement of the performances on four datasets is 0.8%. With the further fine-tuning phase, the improvement achieves 2.0% on average.</p><p>Coupled-layer Architecture For the second coupled-layer architecture, the information is shared with a pair of tasks. Therefore, there are six combinations for the four datasets. We train six models on the different pairs of datasets. We can find that the pair-wise joint learning also improves the performances. The more relevant the tasks are, the more significant the improvements are. Since SST-1 and SST-2 are from the same corpus, their improvements are more significant than the other combinations. The improvement is 2.3% on average with simultaneously learning on SST-1 and SST-2.</p><p>Shared-layer Architecture The shared-layer architecture is more general than uniform-layer architecture. Besides a shared layer for all the tasks, each task has own task-specific layer. As shown in <ref type="table" target="#tab_5">Table 4</ref>, we can see that the average improvement of the performances on four datasets is 1.4%, which is better than the uniform-layer architecture. We also investigate the strategy of unsupervised pre-training towards shared LSTM layer. With the LM pre-training, the performance is improved by an extra 0.5% on average. Besides, the further fine-tuning can significantly improve the performance by an another 0.9%.</p><p>To recap, all our proposed models outperform the baseline of single-task learning. The shared-layer architecture gives the best performances. Moreover, compared with vanilla LSTM, our proposed three models don't cause much extra computational cost while converge faster. In our experiment, the most complicated model-III, costs 2.5 times as long as vanilla LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Comparisons with State-of-the-art Neural Models</head><p>We compare our model with the following models:</p><p>? NBOW The NBOW sums the word vectors and applies a non-linearity followed by a softmax classification layer.</p><p>? MV-RNN Matrix-Vector Recursive Neural Network with parse trees <ref type="bibr" target="#b4">[Socher et al., 2012]</ref>.  <ref type="table">Table 5</ref>: Results of shared-layer multi-task model against state-of-the-art neural models.</p><p>? RNTN Recursive Neural Tensor Network with tensorbased feature function and parse trees <ref type="bibr" target="#b4">[Socher et al., 2013]</ref>.</p><p>? DCNN Dynamic Convolutional Neural Network with dynamic k-max pooling <ref type="bibr">[Kalchbrenner et al., 2014]</ref>.</p><p>? PV Logistic regression on top of paragraph vectors <ref type="bibr" target="#b4">[Le and Mikolov, 2014]</ref>. Here, we use the popular open source implementation of PV in Gensim 3 .</p><p>? Tree-LSTM A generalization of LSTMs to treestructured network topologies. <ref type="bibr" target="#b4">[Tai et al., 2015]</ref>  <ref type="table">Table 5</ref> shows the performance of the shared-layer architecture compared with the competitor models, which shows our model is competitive for the neural-based state-of-the-art models.</p><p>Although Tree-LSTM outperforms our model on SST-1, it needs an external parser to get the sentence topological structure. It is worth noticing that our models are compatible with the other RNN based models. For example, we can easily extend our models to incorporate the Tree-LSTM model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Case Study</head><p>To get an intuitive understanding of what is happening when we use the single LSTM or the shared-layer LSTM to predict the class of text, we design an experiment to analyze the output of the single LSTM and the shared-layer LSTM at each time step. We sample two sentences from the SST-2 test dataset, and the changes of the predicted sentiment score at different time steps are shown in <ref type="figure" target="#fig_1">Figure 3</ref>. To get more insights into how the shared structures influences the specific task. We observe the activation of global gates g (s) , which controls signals flowing from one shared LSTM layer to taskspcific layer, to understand the behaviour of neurons. We plot evolving activation of global gates g (s) through time and sort the neurons according to their activations at the last time step.</p><p>For the sentence "A merry movie about merry period people's life.", which has a positive sentiment, while the standard LSTM gives a wrong prediction. The reason can be inferred from the activation of global gates g <ref type="bibr">(s)</ref> . As shown in <ref type="figure" target="#fig_1">Figure 3</ref>-(c), we can see clearly the neurons are activated much when they take input as "merry", which indicates the task-specific layer takes much information from shared layer towards the word "merry", and this ultimately makes the model give a correct prediction. Another case "Not everything works, but the average is higher than in Mary and most other recent comedies." is positive and has a little complicated semantic composition. As shown in <ref type="figure" target="#fig_1">Figure  3-(b,d)</ref>, simple LSTM cannot capture the structure of "but ... higher than " while our model is sensitive to it, which indicates the shared layer can not only enrich the meaning of certain words, but can teach some information of structure to specific task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Error Analysis</head><p>We analyze the bad cases induced by our proposed sharedlayer model on SST-2 dataset. Most of the bad cases can be generalized into two categories Complicated Sentence Structure Some sentences involved complicated structure can not be handled properly, such as double negation "it never fails to engage us." and subjunctive sentences "Still, I thought it could have been more.". To solve these cases, some architectural improvements are necessary, such as tree-based LSTM <ref type="bibr" target="#b4">[Tai et al., 2015]</ref>.</p><p>Sentences Required Reasoning The sentiments of some sentences can be mislead if only considering the literal meaning. For example, the sentence "I tried to read the time on my watch." expresses negative attitude towards a movie, which can be understood correctly by reasoning based on common sense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Neural networks based multi-task learning has proven effective in many NLP problems <ref type="bibr" target="#b0">[Collobert and Weston, 2008;</ref><ref type="bibr" target="#b4">Liu et al., 2015b]</ref>.</p><p>Collobert and Weston <ref type="bibr">[2008]</ref> used a shared representation for input words and solve different traditional NLP tasks such as part-of-Speech tagging and semantic role labeling within one framework. However, only one lookup table is shared, and the other lookup-tables and layers are task specific. To deal with the variable-length text sequence, they used window-based method to fix the input size.</p><p>Liu et al.</p><p>[2015b] developed a multi-task DNN for learning representations across multiple tasks. Their multi-task DNN approach combines tasks of query classification and ranking for web search. But the input of the model is bag-of-word representation, which lose the information of word order.</p><p>Different with the two above methods, our models are based on recurrent neural network, which is better to model the variable-length text sequence.</p><p>More recently, several multi-task encoder-decoder networks were also proposed for neural machine translation <ref type="bibr" target="#b1">[Dong et al., 2015;</ref><ref type="bibr" target="#b1">Firat et al., 2016]</ref>, which can make use of cross-lingual information. Unlike these works, in this paper we design three architectures, which can control the information flow between shared layer and task-specific layer flexibly, thus obtaining better sentence representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>In this paper, we introduce three RNN based architectures to model text sequence with multi-task learning. The differences among them are the mechanisms of sharing information among the several tasks. Experimental results show that our models can improve the performances of a group of related tasks by exploring common features.</p><p>In future work, we would like to investigate the other sharing mechanisms of the different tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>T</head><label></label><figDesc>for tasks m and n receptively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>3Figure 3 :</head><label>3</label><figDesc>https://github.com/piskvorky/gensim/ (a)(b) The change of the predicted sentiment score at different time steps. Y-axis represents the sentiment score, while X-axis represents the input words in chronological order. The red horizontal line gives a border between the positive and negative sentiments. (c)(d) Visualization of the global gate's (g (s) ) activation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>The movie reviews with binary classes. It is also from the Stanford Sentiment Treebank.? SUBJ Subjectivity data set where the goal is to classify each instance (snippet) as being subjective or objective.</figDesc><table><row><cell>1 The movie reviews with five classes (negative,</cell></row><row><cell>somewhat negative, neutral, somewhat positive, posi-</cell></row><row><cell>tive) in the Stanford Sentiment Treebank 1 [Socher et al.,</cell></row><row><cell>2013].</cell></row><row><cell>? SST-2</cell></row></table><note>[Pang and Lee, 2004]</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results of the uniform-layer architecture.</figDesc><table><row><cell>Model</cell><cell cols="5">SST-1 SST-2 SUBJ IMDB Avg?</cell></row><row><cell>Single Task</cell><cell>45.9</cell><cell>85.8</cell><cell>91.6</cell><cell>88.5</cell><cell>-</cell></row><row><cell>SST1-SST2</cell><cell>48.9</cell><cell>87.4</cell><cell>-</cell><cell>-</cell><cell>+2.3</cell></row><row><cell>SST1-SUBJ</cell><cell>46.3</cell><cell>-</cell><cell>92.2</cell><cell>-</cell><cell>+0.5</cell></row><row><cell>SST1-IMDB</cell><cell>46.9</cell><cell>-</cell><cell>-</cell><cell>89.5</cell><cell>+1.0</cell></row><row><cell>SST2-SUBJ</cell><cell>-</cell><cell>86.5</cell><cell>92.5</cell><cell>-</cell><cell>+0.8</cell></row><row><cell>SST2-IMDB</cell><cell>-</cell><cell>86.8</cell><cell>-</cell><cell>89.8</cell><cell>+1.2</cell></row><row><cell>SUBJ-IMDB</cell><cell>-</cell><cell>-</cell><cell>92.7</cell><cell>89.3</cell><cell>+0.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Results of the shared-layer architecture.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 -</head><label>2</label><figDesc>4 show the classification accuracies on the four datasets. The second line ("Single Task") of each table shows the result of the standard LSTM for each individual task.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Most existing neural network methods are based on supervised training objectives on a single task<ref type="bibr" target="#b0">[Collobert et al., 2011;</ref><ref type="bibr" target="#b4">Socher et al., 2013;</ref> Kalchbrenner et al., 2014]. These methods often suffer from the limited amounts of training data. To deal with this problem, these models often involve an unsupervised pre-training phase. This unsupervised pretraining is effective to improve the final performance, but it does not directly optimize the desired task.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://nlp.stanford.edu/sentiment. 2 http://ai.stanford.edu/?amaas/data/ sentiment/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous reviewers for their valuable comments. This work was partially funded by National Natural Science Foundation of China (No. 61532011, 61473092, and 61472088), the National High Technology Research and Development Program of China (No. 2015AA015408).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of ICML. Collobert et al.. Natural language processing (almost) from scratch</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-way, multilingual neural machine translation with a shared attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.01073</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL</title>
		<meeting>the ACL</meeting>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="179" to="211" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Adaptive subgradient methods for online learning and stochastic optimization</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Gradient flow in recurrent nets: the difficulty of learning long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Graves ; Sepp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
	</analytic>
	<monogr>
		<title level="m">Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J?rgen Schmidhuber</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Long short-term memory. Kalchbrenner et al., 2014] Nal Kalchbrenner</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bo Pang and Lillian Lee. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<idno>arXiv:1503.00075</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing<address><addrLine>Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts</addrLine></address></meeting>
		<imprint>
			<publisher>Richard Socher</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of ACL</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
