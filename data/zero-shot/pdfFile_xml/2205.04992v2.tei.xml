<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">KeypointNeRF: Generalizing Image-based Volumetric Avatars using Relative Spatial Encoding of Keypoints</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marko</forename><surname>Mihajlovic</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ETH Z?rich</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aayush</forename><surname>Bansal</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhoefer</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ETH Z?rich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunsuke</forename><surname>Saito</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<country>KeypointNeRF</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">KeypointNeRF: Generalizing Image-based Volumetric Avatars using Relative Spatial Encoding of Keypoints</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Neural Radiance Field, Pixel-Aligned Features</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Image-based volumetric humans using pixel-aligned features promise generalization to unseen poses and identities. Prior work leverages global spatial encodings and multi-view geometric consistency to reduce spatial ambiguity. However, global encodings often suffer from overfitting to the distribution of the training data, and it is difficult to learn multi-view consistent reconstruction from sparse views. In this work, we investigate common issues with existing spatial encodings and propose a simple yet highly effective approach to modeling high-fidelity volumetric humans from sparse views. One of the key ideas is to encode relative spatial 3D information via sparse 3D keypoints. This approach is robust to the sparsity of viewpoints and cross-dataset domain gap. Our approach outperforms state-of-the-art methods for head reconstruction. On human body reconstruction for unseen subjects, we also achieve performance comparable to prior work that uses a parametric human body model and temporal feature aggregation. Our experiments show that a majority of errors in prior work stem from an inappropriate choice of spatial encoding and thus we suggest a new direction for high-fidelity image-based human modeling.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>3D renderable human representations are an important component for social telepresence, mixed reality, and a new generation of entertainment platforms. Classical mesh-based methods require dense multi-view stereo <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b59">60]</ref> or depth sensor fusion <ref type="bibr" target="#b69">[70]</ref>. The fidelity of these methods is limited due to the difficulty of accurate geometry reconstruction. Recently, neural volumetric representations <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b41">42]</ref> have enabled high-fidelity human reconstruction, especially where accurate geometry is difficult to obtain (e.g. hair). By injecting humanspecific parametric shape models <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b33">34]</ref>, extensive multi-view data capture can  be reduced to sparse camera setups <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b44">45]</ref>. However, these learning-based approaches are subject-specific and require days of training for each individual subject, which severely limits their scalability. Democratizing digital volumetric humans requires an ability to instantly create a personalized reconstruction of a user from two or three snaps (from different views) taken from their phone. Towards this goal, we learn to generalize metrically accurate image-based volumetric humans from two or three views. Fully convolutional pixel-aligned features utilizing multi-scale information have enabled better performance for various 2D computer vision tasks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b42">43]</ref>, including the generalizable reconstruction of unseen subjects <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b68">69]</ref> Pixel-aligned neural fields infer field quantities given a pixel location and spatial encoding function (to avoid ray-depth ambiguity). Different spatial encoding functions <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b47">48]</ref> have been proposed in the literature. However, the effect of spatial encoding is not fully understood. In this paper, we provide an extensive analysis of spatial encodings for modeling pixel-aligned neural radiance fields for human faces. Our experiments show that the choice of spatial encoding influences the reconstruction quality and generalization to novel identities and views. The models that use camera depth overfit to the training distribution, and multi-view stereo constraints are less robust to sparse views with large baselines.</p><p>We present a simple yet highly effective approach based on sparse 3D keypoints to address the limitations of existing approaches. 3D keypoints are easy to obtain using an off-the-shelf 2D keypoint detector <ref type="bibr" target="#b10">[11]</ref> and a simple triangulation of multi-views <ref type="bibr" target="#b17">[18]</ref>. We treat 3D keypoints as spatial anchors and encode relative 3D spatial information to those keypoints. Unlike global spatial encoding <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b68">69]</ref>, the relative spatial information is agnostic to camera parameters. This property allows the proposed approach to be robust to changes in pose. 3D keypoints also allow us to use the same approach for both human faces and bodies. Our approach achieves state-of-the-art performance for generating volumetric humans for unseen subjects from sparse-and-wide two or three views, and we can also incorporate more views to further improve performance. We also achieve performance comparable to Neural Human Performer (NHP) <ref type="bibr" target="#b28">[29]</ref> when it comes to full-body human reconstruction. NHP relies on accurate parametric body fitting and temporal feature aggregation, whereas our approach employs 3D keypoints alone. Our method is not biased <ref type="bibr" target="#b57">[58]</ref> to the distribution of the training data. We can use the learned model (without modification) to never-before-seen iPhone captures. We attribute our ability to generalize image-based volumetric humans to an unseen data distribution to our choice of spatial encoding. Our key contributions include:</p><p>-A simple approach that leverages sparse 3D keypoints and allows us to create high-fidelity state-of-the-art volumetric humans for unseen subjects from widely spread out two or three views. -Extensive analysis on the use of spatial encodings to understand their limitations and the efficacy of the proposed approach. -We demonstrate generalization to never-before-seen iPhone captures by training with only a studio-captured dataset. To our knowledge, no prior work has shown these results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our goal is to create high-fidelity volumetric humans for unseen subjects from as few as two views. Classical Template-based Techniques: Early work on human reconstruction <ref type="bibr" target="#b22">[23]</ref> required dense 3D reconstruction from a large number of images of the subject and non-rigid registration to align a template mesh to 3D point clouds. Cao et al. <ref type="bibr" target="#b9">[10]</ref> employ coarse geometry along with face blendshapes and a morphable hair model to address restrictions posed by dense 3D reconstruction. Hu et al. <ref type="bibr" target="#b20">[21]</ref> retrieve hair templates from a database and carefully compose facial and hair details. Video Avatar <ref type="bibr" target="#b0">[1]</ref> obtains a full-body avatar based on a monocular video captured using silhouette-based modeling. The dependence on geometry and meshes restricts the applicability of these methods to faithfully reconstruct regions such as the hair, mouth, teeth, etc., where it is non-trivial to obtain accurate geometry. Neural Rendering: Neural rendering <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b56">57]</ref> has tackled some of the challenges classical template-based approaches struggle with by directly learning compo-nents of the image formation process from raw sensor measurements. 2D neural rendering approaches <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47]</ref> employ surface rendering and a convolutional network to bridge the gap between rendered and real images. The downside of these 2D techniques is that they struggle to synthesize novel viewpoints in a temporally coherent manner. Deep Appearance Models <ref type="bibr" target="#b29">[30]</ref> employ a coarse 3D proxy mesh in combination with view-dependent texture mapping to learn personalized face avatars from dense multi-view supervision. Using a 3D proxy mesh significantly helps with viewpoint generalization, but the approach faces challenges in synthesizing certain regions for which it is hard to obtain good 3D reconstruction, such as the hair and inside the mouth. Current state-of-theart methods such as NeuralVolumes <ref type="bibr" target="#b30">[31]</ref> and NeRF <ref type="bibr" target="#b41">[42]</ref> employ differentiable volumetric rendering instead of relying on meshes. Due to their volumetric nature, these methods enable high-quality results even for regions where estimating 3D geometry is challenging. Various extensions <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b64">65]</ref> have further improved quality. These methods require dense multi-view supervision for person-specific training and take 3-4 days to train a single model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sparse View Reconstruction:</head><p>Large scale deployment requires approaches that allow a user to take two or three pictures of themselves from multi-views and generate a digital human from this data. The use of pixel-aligned features <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52]</ref> further allows the use of large datasets for learning generalized models from sparse views. Different approaches <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b68">69]</ref> have combined multi-view constraints and pixel-aligned features alongside NeRF to learn generalizable viewsynthesis. In this work, we observe that these approaches struggle to generate fine details given sparse views for unseen human faces and bodies.</p><p>Learning Face and Body Reconstruction: Generalizable parametric mesh <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b67">68]</ref> and implicit <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b66">67]</ref> body models can provide additional constraints for learning details from sparse views. Recent approaches have incorporated priors specific to human faces <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b71">72]</ref> and human bodies <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b73">74]</ref> to reduce the dependence on multi-view captures. Approaches such as H3DNet <ref type="bibr" target="#b48">[49]</ref> and SIDER <ref type="bibr" target="#b11">[12]</ref> use signed-distance functions (SDFs) for learning geometry priors from large datasets and perform test-time fine-tuning on the test subject. PaMIR <ref type="bibr" target="#b73">[74]</ref> uses volumetric features guided by a human body model for better generalization. Neural Human Performer <ref type="bibr" target="#b28">[29]</ref> employs SMPL with pixel-aligned features and temporal feature aggregation. In this work, we observe that the use of human 3D-keypoints provides necessary and sufficient constraints for learning from sparse-view inputs. Our approach has high flexibility since it only relies on 3D keypoints alone and thus enables us to work both on human faces and bodies. Prior methods have also employed various forms of spatial encoding for better learning. For example, PVA <ref type="bibr" target="#b47">[48]</ref> and PortraitNeRF <ref type="bibr" target="#b15">[16]</ref> use face-centric coordinates. ARCH/ARCH++ <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22]</ref> use canonical body coordinates. In this work, we extensively study the role of spatial encoding, and found that the use of a relative depth encoding using 3D keypoints leads to the best results. Our findings enable us to learn a representation that generalizes to never-before-seen iPhone camera captures for unseen human faces. In addition to achieving state-of-the-art results on volumetric face reconstruction from as few as two images, our approach can also be used for synthesizing novel views of unseen human bodies and achieves competitive performance to prior work <ref type="bibr" target="#b28">[29]</ref> in this setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries: Neural Radiance Fields</head><p>NeRF <ref type="bibr" target="#b41">[42]</ref> is a continuous function that represents the scene as a volumetric radiance field of color and density. Given a 3D point and a viewing direction d ? R 3 , NeRF estimates RGB color values and density (c, ?) that are then accumulated via quadrature to calculate the expected color of each camera ray r(t) = o + td:</p><formula xml:id="formula_0">C(r) = t f tn exp ? t tn ?(s) ds ?(t)c(t, d) dt ,<label>(1)</label></formula><p>where t n and t f define near and far bounds.</p><p>Pixel-aligned NeRF. One of the core limitations of NeRF is that the approach requires per-scene optimization and does not work well for extremely sparse input views (e.g., two images). To address these challenges, several recent methods <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b68">69]</ref> propose to condition NeRF on pixel-aligned image features and generalize to novel scenes without retraining. Spatial Encoding. To avoid the ray-depth ambiguity, pixel-aligned neural fields <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b68">69]</ref> attach spatial encoding to the pixel-aligned feature. PIFu <ref type="bibr" target="#b50">[51]</ref> and related methods <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b68">69]</ref> use depth value in the camera coordinate space as spatial encoding, while PVA <ref type="bibr" target="#b47">[48]</ref> uses coordinates relative to the head position. However, we argue that such spatial encodings are global and sub-optimal for learning generalizable volumetric humans. In contrast, our proposed relative spatial encoding provides a localized context that enables better learning and is more robust to changes in human pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">KeypointNeRF</head><p>Our method is based on a radiance field function:</p><formula xml:id="formula_1">f X, d|{(I n , P n )} N n=1 = (c, ?)<label>(2)</label></formula><p>that infers a color c ? R 3 and a density ? ? R value for any point in 3D space given its position X ? R 3 and its viewing direction d ? R 3 as input. In addition, the function has access to the N input images I n and their camera calibrations P n . We model the function f as a neural network that consists of four main parts; a spatial keypoint encoder (Sec. 4.1), two convolutional image encoders that extract pixel-aligned features (Sec. 4.2), an MLP fusion network that aggregates multiple pixel-aligned features (Sec. 4.3), and two MLPs that predict density ? and color values c (Sec. 4.4). The high-level overview of our method is illustrated in <ref type="figure" target="#fig_3">Fig. 2</ref> and in the following we further describe its components. </p><formula xml:id="formula_2">Fig. 2. Overview.</formula><p>We learn a generalizable image-based neural radiance representation for volumetric humans. Given a sparse set of input images {In} N n=1 and their respective camera parameters Pn, we first detect keypoints and lift them to 3D P. The keypoints are used to provide the relative spatial encoding (Sec 4.1). The input images are simultaneously encoded via convolutional encoders and provide image-guided pixel-aligned features (Sec 4.2). These two types of encoding are fused (Sec. 4.3) and condition the radiance field (Sec. 4.4). The radiance field is decoupled by two MLPs, one that directly predicts view-independent density value ?, and the other one which predicts blending weights that are used to output the final color value c by blending image pixel values {?(X|In)} N n=1 . The predicted color and density values are accumulated along the ray via volume rendering <ref type="bibr" target="#b40">[41]</ref> to render the volumetric representation from novel views. The rendered example in the figure is an actual output of our method for the displayed two input images of an unseen subject.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Relative Spatial Keypoint Encoding</head><p>Our method first leverages an off-the-shelf keypoint regressor <ref type="bibr" target="#b10">[11]</ref> to extract K 2D keypoints from at least two input views. Then these points are triangulated and lifted to 3D P = {p k ? R 3 } K k=1 by using the direct linear transformation algorithm <ref type="bibr" target="#b17">[18]</ref>. To spatially encode the query point X, we first compute the depth values of the query point and all keypoints w.r.t each input view z(p k |P n ) by the 2D projection and then estimate their relative depth difference ? n (p k , X) = z(p k |P n ) ? z(X|P n ). We further employ positional encoding ?(?) <ref type="bibr" target="#b41">[42]</ref> and the Gaussian exponential kernel to create the final relative spatial encoding for the query point X as:</p><formula xml:id="formula_3">s n (X|P) = exp ?l 2 (p k , X) 2 2? 2 ? ? n (p k , X) K k=1 ,<label>(3)</label></formula><p>where ? is a fixed hyper-parameter that controls the impact of each keypoint. We set this value to 5cm for facial keypoints and to 10cm for the human skeleton.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Convolutional Pixel-aligned Features</head><p>In addition to the spatial encoding s n (X), we extract pixel-aligned features for the query point X by encoding the input images I n ? R H?W ?3 using two convolutional encoders. Image Encoders. The first image encoder uses a single HourGlass <ref type="bibr" target="#b42">[43]</ref> network that generates both deep low-resolution F gl n ? R H/8?W/8?64 and shallow high-resolution F gh n ? R H/2?W/2?8 feature maps. This network learns a geometric prior of humans and its output is used to condition the density estimation network. The second encoder is a convolutional network with residual connections <ref type="bibr" target="#b24">[25]</ref> that encodes input images F a n ? R H/4?W/4?8 and provides an alternative pathway for the appearance information which is independent of the density prediction branch in the spirit of DoubleField <ref type="bibr" target="#b53">[54]</ref>. Please see the supplemental material for further architectural details. Pixel-aligned Features. To compute the pixel-aligned features, we project the query point on each feature plane x = ?(X|P n ) ? R 2 and bi-linearly interpolate the grid values. We define this operation of computing the pixel-aligned features (2D projection and interpolation) by the operator ? n (X|F ), where F can represent any grid of vectors for the nth camera: F gl n , F gh n , F a n , I n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Multi-view Feature Fusion</head><p>To model a multi-view consistent radiance field, we need to fuse the per-view spatial encodings s n (Eq. 3) and the pixel-aligned features ? n . The spatial encoding s n is first translated into a feature vector via a singlelayer perceptron. This feature is then jointly blended with the deep low-resolution pixel-aligned feature ? n (X|F gl n ) by a two-layer perceptron. The output is then concatenated with the shallow high-resolution feature ? n (X|F gh n ) and processed by an additional two-layer perceptron that outputs per-view 64-dimensional feature vector that jointly encodes the blended spatial encoding and the pixelaligned information. These multi-view features are then fused into a single feature vector G X ? R 128 by the mean and variance pooling operators as in <ref type="bibr" target="#b61">[62]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Modeling Radiance Fields</head><p>The radiance field is modeled via decoupled MLPs for density ? and color c prediction. Density Fields. The density network is implemented as a four-layer MLP that takes as input the geometry feature vector G X and predicts the density value ?.</p><p>View-dependent Color Fields. We implement an additional MLP to output the consistent color value c for a given query point X and its viewing direction d by blending image pixel values {?(X|P n )} N n=1 similarly to IBRNet <ref type="bibr" target="#b61">[62]</ref>. The input to this MLP is 1) the extracted geometry feature vector G X that ensures geometrically consistent renderings, 2) the additional pixel-aligned features ? n (X|F a n ), 3) the corresponding pixel values ? n (X|I n ), and 4) the view direction that is encoded as the difference between the view direction d and the camera views along with their dot product.</p><p>These inputs are concatenated and augmented with the mean and variance vectors computed over the multi-view pixel-aligned features, and jointly propagated through a nine-layer perceptron with residual connections which predicts blending weights for each input view {w n } N n=1 . These blending weights form the final color prediction by fusing the corresponding pixel-aligned color values:</p><formula xml:id="formula_4">c = N n=1 exp(w n )? n (X|I n ) N i=1 exp(w i )</formula><p>.</p><p>(4)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Novel View Synthesis</head><p>Given our radiance field function f (X, d) = (c, ?), we render novel views via the volume rendering equation <ref type="formula" target="#formula_0">(1)</ref>, in which we define the near and far bound by analytically computing the intersection of the pixel ray and a geometric proxy that over-approximates the volumetric human and use the entrance and exit points as near and far bounds respectively. For the experiments on human heads, we use a sphere with a radius of 30 centimeters centered around the keypoints, while for the human bodies we follow the prior work <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b44">45]</ref> and use a 3D bounding box. Similar to NeRF <ref type="bibr" target="#b41">[42]</ref>, we employ a coarse-to-fine rendering strategy, but we employ the same network weights for both levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Training and Implementation Details</head><p>To train our network, we render H ? ? W ? patches (as in <ref type="bibr" target="#b52">[53]</ref>) by accumulating color and density values for 64 sampled points along the ray for the coarse and 64 more for the fine rendering. Our method is trained end-to-end by minimizing the mean ? 1 -distance between the rendered and the ground truth pixel values and the VGG [55]-loss applied over rendered and ground truth image patches:</p><formula xml:id="formula_5">L = L RGB + L VGG .<label>(5)</label></formula><p>The use of the VGG loss for NeRF training was also leveraged by the concurrent methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b65">66]</ref> to better capture high-frequency details. The final loss L is minimized by the Adam optimizer <ref type="bibr" target="#b27">[28]</ref> with a learning rate of 1e ?4 and a batch size of one. For the other parameters, we use their defaults. The background from all training and test input images is removed via an off-the-shelf matting network <ref type="bibr" target="#b25">[26]</ref>. Additionally for more temporally coherent novel-view synthesis at inference time, we clip the maximum of the dot product (introduced in Sec. 4.4) to 0.8 when the number of input images is two in the supplementary video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>In this section, we validate our method on three different reconstruction tasks and datasets: 1) reconstruction of human heads from images captured in a multicamera studio, 2) reconstruction of human heads from in-the-wild images taken with the iPhone's camera, and 3) reconstruction of human bodies on the public ZJU-MoCap dataset <ref type="bibr" target="#b44">[45]</ref>. As evaluation metrics, we follow prior work <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b61">62]</ref> and report the standard SSIM and PSNR metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Reconstruction of Human Heads from Studio Data</head><p>Dataset and Experimental Setup. Our captured data consists of 29 1280 ? 768-resolution cameras positioned in front of subjects. We use a total of 351 identities and 26 cameras for training and 38 novel identities for evaluation. At inference time, we reconstruct humans only from 2-3 input views.</p><p>Baselines. As baseline, we employ the current state-of-the-art model IBR-Net <ref type="bibr" target="#b61">[62]</ref>. In addition, we add several other baselines by varying different types of encoding for the query points in our proposed reconstruction pipeline. Specifically, 1) our pipeline without any encoding, 2) with the camera z encoding used in <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52]</ref>, 3) with the encoding of xyz coordinates in the canonical space of a human head that is used in <ref type="bibr" target="#b47">[48]</ref>, 4) relative encoding of xyz as the distance between the query point and estimated keypoints, 5) our relative spatial encoding without distance weighing (? ? ? in Eq. 3), and 6) the proposed weighted relative encoding as described in the method section (Sec. 4.3). The last three models use a total of 13 facial keypoints that are visualized in <ref type="figure" target="#fig_1">Fig. 1</ref>. All methods are trained with a batch size of one for 150k training steps, except IBRNet <ref type="bibr" target="#b61">[62]</ref> which was trained for 200k iterations. For more comparisons and baselines we refer the reader to the supplementary material and video. Results. We provide novel view synthesis <ref type="figure" target="#fig_2">(Fig. 3</ref>) results for unseen identities that have been reconstructed from only two input images. The results clearly demonstrate that the rendered images of our method are significantly sharper compared to the baselines and are of significantly higher quality. This improvement is confirmed by the quantitative evaluation (Tab. 1) which further indicates that the proposed distance weighting of the relative spatial encoding improves the reconstruction quality. The third-best performing method is our pipeline without any spatial encoding. However, such a method does not generalize well to other capture systems as we will demonstrate in the next section on in-thewild captured data. Robustness to Different Noise Levels. We evaluate the robustness of our relative spatial encoding and the encoding in canonical space proposed by Raj et al. <ref type="bibr" target="#b47">[48]</ref> by adding different noise levels to the estimated keypoints and the head center respectively. The results reported in Tab. 2 show that our proposed encoding based on keypoints is significantly more robust compared to modeling in an object-specific canonical space. Note that the canonical encoding requires head template fitting for which we used the ground truth estimation from all views and which, in practice, is very erroneous or even infeasible from two views alone. Dynamic scenes. Although our model is trained only with a neutral face, it generalizes well to dynamic expressions and outperforms the baseline methods.</p><p>We evaluate the trained models on 38 test subjects performing eight different expressions and report results in Tab. 3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Reconstruction from In-the-wild Captures</head><p>Setup. To tackle the problem of reconstructing humans in the wild, we acquire a small dataset of subjects by taking several photos with an iPhone and estimate camera parameters. We directly use intrinsic from manufacturing information of iPhone and extrinsic is computed by multi-view RGB-D fitting as in <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inputs</head><p>IBRNet <ref type="bibr" target="#b61">[62]</ref> Ours (no keypoints) KeypointNeRF GT <ref type="figure">Fig. 4</ref>. In-the-wild Captures. Our approach produces high-quality reconstructions from three iPhone images, while all baselines show significant artifacts.</p><p>We evaluate the reconstruction methods trained on the studio captured data in Sec. 6.1 without any retraining. As input, all methods take three 1920?1024-resolution images of a person and predict a radiance field that is then rendered from novel views. In <ref type="figure">Fig. 4</ref>, we display rendered novel views of IBRNet <ref type="bibr" target="#b61">[62]</ref>, our method without any spatial encoding, and our method with the proposed spatial encoding. The baseline methods produce significantly worse results with lots of blur and cloudy artifacts, whereas our method can reliably reconstruct the human heads. This improvement is quantitatively supported by computed SSIM and PSNR on novel held-out views of the visualized four subjects (Tab. 4). This experiment demonstrates that our relative spatial encoding is the crucial component for cross-dataset generalization. Please see the supplementary material for more visualizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Reconstruction of Human Bodies</head><p>Additionally, we demonstrate that our method is suitable for reconstructing full volumetric human bodies without relying on template fitting of parametric human bodies <ref type="bibr" target="#b33">[34]</ref>. We use the public ZJU <ref type="bibr" target="#b44">[45]</ref> dataset in order to follow the experimental setup used in <ref type="bibr" target="#b28">[29]</ref>, so that we could closely compare our method's ability to reconstruct human bodies to the current state-of-the-art method without changing any experimental variables. We follow the standard training-test split of frames and use a total of seven subjects for training and three for validation. At inference time, all methods use three input views. We compare our method with the generalizable volumetric methods: pixelNeRF <ref type="bibr" target="#b68">[69]</ref>, PVA <ref type="bibr" target="#b47">[48]</ref>, the current state-of-the-art Neural Human Performer (NHP) <ref type="bibr" target="#b28">[29]</ref>, and our method without weighting the relative spatial encoding in Eq. 3. We report results on unseen identities for 438 novel views in <ref type="table">Table 5</ref> and side-by-side qualitative comparisons with NHP in <ref type="figure">Fig. 5</ref>. The results demonstrate that weighting the spatial encoding benefits reconstruction of human bodies as well. Our method is on par with the significantly more complex NHP, which relies on the accurate registration of the SMPL body model <ref type="bibr" target="#b33">[34]</ref> and temporal feature fusion, whereas ours only requires skeleton keypoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We present a simple yet highly effective approach for generating high-fidelity volumetric humans from as few as two input images. The key to our approach is a novel spatial encoding based on relative information extracted from 3D keypoints. Our approach outperforms state-of-the-art methods for head reconstruction and better generalizes to challenging out-of-domain inputs, such as selfies captured in the wild by an iPhone. Since our approach does not require <ref type="table">Table 5</ref>. Human Body Experiment. Comparison of our method with the baseline methods pixelNeRF <ref type="bibr" target="#b68">[69]</ref>, PVA <ref type="bibr" target="#b47">[48]</ref>, and Neural Human Performer (NHP) <ref type="bibr" target="#b28">[29]</ref>; "no w." in the  <ref type="bibr" target="#b28">[29]</ref> and our method on unseen identities from the ZJU-MoCap dataset <ref type="bibr" target="#b44">[45]</ref>. Best viewed in electronic format.</p><p>a parametric template mesh, it can be applied to the task of body reconstruction without modification, where it achieves performance comparable to more complicated prior work that has to rely on parametric human body models and temporal feature aggregation. We believe that our local spatial encoding based on keypoints might also be useful for many other category-specific neural rendering applications.</p><p>KeypointNeRF: Generalizing Image-based Volumetric Avatars using Relative Spatial Encoding of Keypoints -Supplementary Material -</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Overview</head><p>In this document we provide additional implementation details (Sec. B), information about the baseline methods (Sec. C), more qualitative and quantitative results (Sec. D), and reflect on the limitations of KeypointNeRF and future work (Sec. E).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Implementation Details</head><p>Image Encoders. We employ a single HourGlass <ref type="bibr" target="#b42">[43]</ref> network to learn a geometric prior of humans and condition the density estimation network. The input image is normalized to [?1, 1] range and processed by four convolutional blocks (256 filters) interleaved with group normalization. We then employ an HourGlass block (down-sampling rate of four) with group normalization layers and refine the final output with four convolutional layers to produce the deep feature map F gl n ? R H/8?W/8?64 . Additionally, after the second convolutional block, we employ the transposed convolutional layer to produce the shallow high-resolution feature map F gh n ? R H/2?W/2?8 . As activation function we use ReLU for all layers. We implemented a second convolutional encoder that is independent of the density prediction branch to produce an alternative pathway for the appearance information F a n ? R H/4?W/4?8 as in DoubleField <ref type="bibr" target="#b53">[54]</ref>. We follow the design of <ref type="bibr" target="#b24">[25]</ref> and implement this encoder as a 15-layer convolutional network with residual connections and ReLU activations. Multi-view Feature Fusion. The feature fusion network is implemented as a four-layer MLP (128, 136, 120, and 64 neurons with Softplus activations) that aggregates features from multiple views. Its output is aggregated via mean-variance pooling <ref type="bibr" target="#b61">[62]</ref> to produce the geometry feature vector G X ? R 128 . Density Fields. The geometry feature vector is decoded as density value ? via a four-layer MLP (64 neurons with Softplus activations). View-dependent Color Fields. To produce the final color prediction c for a query point X, we implemented an additional MLP that predicts blending weights as an intermediate step which are used to blend the input pixel colors. This network follows the design proposed in IBRNet to communicate information among multi-view features by using the mean-variance pooling operator. The per-view input feature vectors (described in Sec. 4.4) are first fused into a Inputs MVSNeRF <ref type="bibr" target="#b12">[13]</ref> PVA <ref type="bibr" target="#b47">[48]</ref> IBRNet <ref type="bibr" target="#b61">[62]</ref> KeypointNeRF <ref type="figure" target="#fig_1">Fig. B.1</ref>. Studio Capture Results. Reconstruction results on held-out subjects from only two input views. Our method produces much sharper results with fewer artifacts compared to prior work. Best viewed in electronic format. global feature vector via the mean-variance pooling operator. Then this feature is attached to the pixel-aligned feature vectors ?(X|F a n ) and propagated through a nine-layer MLP with residual connections and an exponential linear unit as activation to predict the blending weights (Eq. 4). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Baseline Methods</head><p>We used the publicly released code of MVSNeRF <ref type="bibr" target="#b12">[13]</ref> and IBRNet <ref type="bibr" target="#b61">[62]</ref> with their default parameters. We re-implemented PVA <ref type="bibr" target="#b47">[48]</ref> since their code is not public and we directly used the public results of NHP <ref type="bibr" target="#b28">[29]</ref> for the experiments on the ZJU-MoCap dataset <ref type="bibr" target="#b44">[45]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Additional Results</head><p>Multi-view studio Capture Results. We further provide qualitative results for two more baseline methods (MVSNeRF <ref type="bibr" target="#b12">[13]</ref> and PVA <ref type="bibr" target="#b47">[48]</ref>) for the experimental setup described in Sec. 6.1. The results in <ref type="figure" target="#fig_1">Fig. B.1</ref> demonstrate that the best performing baseline (IBRNet) produces incomplete images with lots of blur and foggy artifacts. PVA yields consistent, but overly smoothed renderings, while MVSNeRF does not work well for the widely spread-out input views. For more qualitative results, we refer the reader to the supplementary video. Keypoint perturbation. To evaluate the sensitivity of our method on a less accurate estimation of keypoints, we perturb them with different Gaussian noise levels (ranging from 1 to 20mm) for unseen subjects from Sec. 6.1 and observe that the rendered images ( <ref type="figure">Fig. D.</ref>2) occasionally tend to become blurry around the keypoints (e.g. eyes) for large noise levels (&gt; 10mm). The impact of the iPhone calibration for the in-the-wild capture. We evaluate the robustness of KeypointNeRF to a nosier camera calibration by estimating the iPhone camera parameters without the depth term for the experimental setup presented in Sec. 6.2. We observe (Tab. D.1) a negligible drop (PSNR/SSIM by -0.04/-0.5) in performance for our method, demonstrating the robustness of our method under noisy camera calibration. Convolutional feature encoders. We further measure the impact of the Hour-Glass feature extractor and compare it with the U-Net encoder that is used by the other baseline methods <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b61">62]</ref>. We follow the experimental setup from subsections 6.1 and 6.2 and report quantitative results in Tab. D.2 and D.3 respectively. We observe that HourGlass encoder consistently improves the reconstruction quality.  <ref type="bibr" target="#b61">[62]</ref>, our method without any spatial encoding, and our method with the proposed keypoint encoding; visual results are provided in <ref type="figure">Fig. 4</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Limitations and Future Work</head><p>While our method offers an efficient way of reconstructing volumetric avatars from as few as two input images, it still has several difficulties. The imagebased rendering formulation of our method parametrizes the color prediction as blending of available pixels, which ensures good color generalization at inference time, however it makes the method sensitive to occlusions. The method itself has also difficulties reconstructing challenging thin geometries (e.g. glasses) and is less robust to highly articulated human motions (see <ref type="figure" target="#fig_2">Fig. E.3)</ref>. As future work we consider addressing these challenges and additionally integrating learnable 3D lifting methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b23">24]</ref> with the proposed relative spatial encoding for more optimal end-to-end network training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inputs</head><p>IBRNet <ref type="bibr" target="#b61">[62]</ref> KeypointNeRF NHP <ref type="bibr" target="#b28">[29]</ref> KeypointNeRF <ref type="figure" target="#fig_2">Fig. E.3</ref>. Limitations. Our method struggles to reconstruct the thin frames of the glasses (left) and has difficulties reconstructing human articulations that are outside of the training distribution.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>studio capture In-the-wild iPhone capture ZJU-MoCap capture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>(a) Our approach allows us to synthesize high-fidelity volumetric humans from two or three views of unseen subjects. (b) A model learned on studio captures can be used without modification on in-the-wild iPhone captures; and (c) finally, the same approach also allows us to synthesize novel views of unseen human subjects (faces are blurred). The figure is best viewed in electronic format.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Studio Capture Results. Reconstruction results on held-out subjects from only two input views. Best viewed in electronic format.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. D. 2 .</head><label>2</label><figDesc>Keypoint perturbation via different noise levels (from left to right: 1mm, 2mm, 3mm, 4mm, 5mm, 10mm, and 20mm). The rendered images tend to become blurry around the keypoints (e.g. eyes) for large noise levels (&gt; 10mm).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Studio Capture Results. Quantitative comparison with IBRNet<ref type="bibr" target="#b61">[62]</ref> and different types of spatial encoding. Visual results are provided inFig. 3</figDesc><table><row><cell>SSIM? PSNR?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Robustness to Different Noise Levels. Perturbing our keypoints vs. perturbing head position in the canonical xyz encoding used in<ref type="bibr" target="#b47">[48]</ref>. Our proposed encoding demonstrates significantly slower performance degradation as the noise increases</figDesc><table><row><cell>Noise level</cell><cell cols="2">Canonical xyz encoding</cell><cell cols="2">Our keypoint encoding</cell></row><row><cell>[mm]</cell><cell>SSIM?</cell><cell>PSNR?</cell><cell>SSIM?</cell><cell>PSNR?</cell></row><row><cell>no noise</cell><cell>83.65</cell><cell>27.05</cell><cell>85.19</cell><cell>27.64</cell></row><row><cell>1</cell><cell>82.79</cell><cell>26.24</cell><cell>85.20</cell><cell>27.64</cell></row><row><cell>2</cell><cell>82.26</cell><cell>26.05</cell><cell>85.08</cell><cell>27.62</cell></row><row><cell>3</cell><cell>81.58</cell><cell>25.48</cell><cell>84.96</cell><cell>27.59</cell></row><row><cell>4</cell><cell>80.92</cell><cell>25.08</cell><cell>84.95</cell><cell>27.56</cell></row><row><cell>5</cell><cell>80.36</cell><cell>25.16</cell><cell>84.80</cell><cell>27.51</cell></row><row><cell>10</cell><cell>76.62</cell><cell>22.23</cell><cell>83.69</cell><cell>27.10</cell></row><row><cell>15</cell><cell>73.33</cell><cell>20.26</cell><cell>82.33</cell><cell>26.47</cell></row><row><cell>20</cell><cell>70.40</cell><cell>18.87</cell><cell>81.17</cell><cell>25.77</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Dynamic expressions. Our model is more accurate than the baselines</figDesc><table><row><cell></cell><cell>SSIM?PSNR?</cell></row><row><cell>IBRNet [62]</cell><cell>82.64 26.79</cell></row><row><cell cols="2">Ours (no keypoints) 84.97 27.14</cell></row><row><cell>Ours</cell><cell>85.31 27.30</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>In-the-wild Captures.</figDesc><table><row><cell></cell><cell cols="2">SSIM? PSNR?</cell></row><row><cell>IBRNet [62]</cell><cell>81.74</cell><cell>18.45</cell></row><row><cell>Ours (no keypoints)</cell><cell>79.50</cell><cell>19.79</cell></row><row><cell>KeypointNeRF</cell><cell>86.73</cell><cell>25.29</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>table means our method without weighting the relative spatial encoding (Eq 3)</figDesc><table><row><cell>pixelNeRF</cell><cell>PVA</cell><cell>NHP</cell><cell>Ours</cell><cell>GT</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PSNR? SSIM?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>pixelNeRF</cell><cell>23.17</cell><cell>86.93</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>PVA</cell><cell>23.15</cell><cell>86.63</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>NHP</cell><cell>24.75</cell><cell>90.58</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Ours (no w.)</cell><cell>24.66</cell><cell>89.30</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Ours</cell><cell>25.03</cell><cell>89.69</cell></row><row><cell>NHP [29]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>KeypointNeRF</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GT</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Fig. 5. Human Body Experiment. Comparison of NHP</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table D .</head><label>D</label><figDesc>1. In-the-wild Captures. Quantitative comparison of IBRNet</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Table D.2. Studio Capture Results. HourGlass [43] vs U-Net [48, 62] encoder for the experiment conducted in Sec. 6.1. Net encoder [48, 62]) 84.34 26.23 KeypointNeRF (w. HourGlass encoder [43]) 85.19 27.64 Table D.3. In-the-wild Captures. HourGlass [43] vs U-Net [48, 62] encoder for the experiment conducted in Sec. 6.2 Net encoder [48, 62]) 84.20 25.67 KeypointNeRF (w. HourGlass encoder [43]) 86.22 25.25</figDesc><table><row><cell></cell><cell></cell><cell cols="3">for the iPhone calibration with the</cell></row><row><cell>depth term</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">RGB calibration RGB-D calibration</cell></row><row><cell></cell><cell cols="3">SSIM? PSNR? SSIM?</cell><cell>PSNR?</cell></row><row><cell>IBRNet [62]</cell><cell>81.72</cell><cell>18.41</cell><cell>81.74</cell><cell>18.45</cell></row><row><cell>Ours (no keypoints)</cell><cell>79.36</cell><cell>19.85</cell><cell>79.50</cell><cell>19.79</cell></row><row><cell>KeypointNeRF</cell><cell>86.22</cell><cell>25.25</cell><cell>86.73</cell><cell>25.29</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">SSIM? PSNR?</cell></row><row><cell>PVA [48]</cell><cell></cell><cell></cell><cell cols="2">81.95 25.87</cell></row><row><cell>IBRNet [62]</cell><cell></cell><cell></cell><cell cols="2">82.39 27.14</cell></row><row><cell cols="5">KeypointNeRF (w. U-SSIM? PSNR?</cell></row><row><cell>IBRNet [62]</cell><cell></cell><cell></cell><cell cols="2">81.72 18.41</cell></row><row><cell cols="2">KeypointNeRF (w. U-</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. We thank Chen Cao for the help with the in-the-wild iPhone capture. M. M. and S. T. acknowledge the SNF grant 200021 204840.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Video based reconstruction of 3d people models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Alldieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Magnor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">imghum: Implicit generative models of 3d human shape and articulated pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Alldieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2021)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2021)</meeting>
		<imprint>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Photorealistic monocular 3d reconstruction of humans wearing clothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Alldieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scape: shape completion and animation of people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rodgers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2005 Papers</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Athar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.04913</idno>
		<title level="m">Flame-in-nerf : Neural control of radiance fields for free view face animation</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.06506</idno>
		<title level="m">Pixelnet: Representation of the pixels, by the pixels, and for the pixels</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A morphable model for the synthesis of 3d faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 26th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Varitex: Variational neural face textures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Meka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Beeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Authentic volumetric avatars from a phone scan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhoefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lombardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Belko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Real-time facial animation with image-based dynamic avatars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Sider: Singleimage neural optimization for facial geometric detail recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chatziagapi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Athar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.05465</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mvsnerf: Fast generalizable radiance field reconstruction from multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV) (2021) 4</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV) (2021) 4</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Stereo radiance fields (srf): Learning view synthesis from sparse views of novel scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chibane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lazova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dynamic neural radiance fields for monocular 4d facial avatar reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gafni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niessner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2021)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2021)</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.05903</idno>
		<title level="m">Portrait neural radiance fields from a single image</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural head avatars from monocular rgb videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Grassal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Prinzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge university press</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Arch++: Animation-ready clothed human reconstruction revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV) (2021)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV) (2021)</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Epipolar transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ieee/cvf conference on computer vision and pattern recognition</title>
		<meeting>the ieee/cvf conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Avatar digitization from a single image for real-time rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nagano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fursund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Arch: Animatable reconstruction of clothed humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2020)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2020)</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dynamic 3d avatar creation from hand-held video input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Ichim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bouaziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pauly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learnable triangulation of human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Iskakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Burkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Malkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Modnet: Real-time trimap-free portrait matting via objective decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Lau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>AAAI</publisher>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep video portraits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zoll?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Neural human performer: Learning generalizable radiance fields for human performance rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fuchs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep appearance models for face rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lombardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lombardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural volumes: Learning dynamic renderable volumes from images</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lombardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhoefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.01954</idno>
		<title level="m">Mixture of volumetric primitives for efficient neural rendering</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Smpl: A skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Martin-Brualla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pidlypenskyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khamis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tkach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lincoln</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.05029</idno>
		<title level="m">Lookingood: Enhancing performance capture with real-time neural re-rendering</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Image-based visual hulls</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gortler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mcmillan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep relightable textures: volumetric performance capture with neural rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Meka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Haene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Orts-Escolano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barnum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>David-Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erickson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bouaziz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">COAP: Compositional articulated occupancy of people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mihajlovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhoefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022-06" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deepsurfels: Learning online appearance fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mihajlovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Weder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Oswald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">LEAP: Learning articulated occupancy of people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mihajlovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2021)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2021)</meeting>
		<imprint>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Local light field fusion: Practical view synthesis with prescriptive sampling guidelines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ortiz-Cayon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">K</forename><surname>Kalantari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Nerf: Representing scenes as neural radiance fields for view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="405" to="421" />
		</imprint>
	</monogr>
	<note>2020) 1, 4, 5, 6, 8</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Animatable neural radiance fields for modeling dynamic human bodies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2021) 2, 4, 8, 9</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2021) 2, 4, 8, 9</meeting>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Smplpix: Neural avatars from 3d human models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Prokudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Anr: Articulated neural rendering for virtual avatars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tanke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stoll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Pva: Pixel-aligned volumetric avatars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhoefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lombardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2021) 2, 4, 5</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2021) 2, 4, 5</meeting>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ramon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Triginer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Escur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pumarola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">G</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.12512</idno>
		<title level="m">H3d-net: Few-shot high-fidelity 3d head reconstruction</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Lolnerf: Learn from one look</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rebain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lagun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tagliasacchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Pifu: Pixel-aligned implicit function for high-resolution clothed human digitization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Natsume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Morishima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV) (2019) 2, 3, 4</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV) (2019) 2, 3, 4</meeting>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Pifuhd: Multi-level pixel-aligned implicit function for high-resolution 3d human digitization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2020) 3, 4</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2020) 3, 4</meeting>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Graf: Generative radiance fields for 3d-aware image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03798</idno>
		<title level="m">Doublefield: Bridging the neural surface and radiance fields for high-fidelity human rendering</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sitzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lombardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Martin-Brualla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">State of the art on neural rendering</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tretschk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sitzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Martin-Brualla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lombardi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.05849</idno>
		<title level="m">Advances in neural rendering</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Articulated mesh animation from multi-view silhouettes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vlasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Baran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Popovi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Dynamic shape capture using multi-view photometric stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vlasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Baran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Debevec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Popovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rusinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matusik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Faceverse: a fine-grained and detail-controllable 3d face morphable model from a hybrid dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Ibrnet: Learning multi-view image-based rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Genova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Martin-Brualla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Metaavatar: Learning animatable clothed human models from few depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mihajlovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">ARAH: Animatable volume rendering of articulated human sdfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning compositional radiance fields of dynamic human heads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bagautdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lombardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hodgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Humannerf: Free-viewpoint rendering of moving people from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.04127</idno>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">H-nerf: Neural radiance fields for rendering and temporal reconstruction of humans in motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Alldieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Ghum &amp; ghuml: Generative 3d human shape and articulated pose models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename><surname>Bazavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">pixelnerf: Neural radiance fields from one or few images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note>2021) 2, 3, 4, 5</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Doublefusion: Real-time capture of human performances with inner body shapes from a single depth sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">High-fidelity human avatars from a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">I M Avatar: Implicit morphable head avatars from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">F</forename><surname>Abrevaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>B?hler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022-06" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Structured local radiance fields for human avatar modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Pamir: Parametric model-conditioned implicit representation for image-based human reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
