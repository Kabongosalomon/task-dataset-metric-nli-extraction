<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-modal Motion Prediction with Transformer-based Neural Network for Autonomous Driving</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyu</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Mo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Lv</surname></persName>
						</author>
						<title level="a" type="main">Multi-modal Motion Prediction with Transformer-based Neural Network for Autonomous Driving</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Predicting the behaviors of other agents on the road is critical for autonomous driving to ensure safety and efficiency. However, the challenging part is how to represent the social interactions between agents and output different possible trajectories with interpretability. In this paper, we introduce a neural prediction framework based on the Transformer structure to model the relationship among the interacting agents and extract the attention of the target agent on the map waypoints. Specifically, we organize the interacting agents into a graph and utilize the multi-head attention Transformer encoder to extract the relations between them. To address the multi-modality of motion prediction, we propose a multimodal attention Transformer encoder, which modifies the multihead attention mechanism to multi-modal attention, and each predicted trajectory is conditioned on an independent attention mode. The proposed model is validated on the Argoverse motion forecasting dataset and shows state-of-the-art prediction accuracy while maintaining a small model size and a simple training process. We also demonstrate that the multi-modal attention module can automatically identify different modes of the target agent's attention on the map, which improves the interpretability of the model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Accurately predicting traffic participants' future trajectories is critical for autonomous vehicles to make safe, informed, and human-like decisions <ref type="bibr" target="#b0">[1]</ref>, especially in complex traffic scenarios. However, motion prediction is a remarkably challenging task due to the complicated dependencies of agents' behaviors on the road structure and interactions among agents in addition to their kinematics, as well as the inherent uncertainty and multi-modality of their intentions.</p><p>One major challenge is how to represent the driving environment in the prediction model, including encoding road structure and agent interactions, in addition to the target's physical states. The difficulty of representing interactions or map data in explicit rules or parametric models gives rise to deep neural network-based methods <ref type="bibr" target="#b1">[2]</ref>, which are able to handle high-dimensional map data and represent the agent interaction patterns by learning from human driving data. In this paper, we employ a graph-based structure to represent the relationship between the interacting agents, where all the agents are treated as nodes and each surrounding agent is connected to the target agent, and a Transformer-based Z. Huang, X. Mo, and C. Lv are with the School of Mechanical and Aerospace Engineering, Nanyang Technological University, 639798, Singapore. (E-mails: zhiyu001@e.ntu.edu.sg, xiaoyu006@e.ntu.edu.sg, lyuchen@ntu.edu.sg.)</p><p>This work was supported in part by A*STAR Grant (No. 1922500046), A*STAR AME Young Individual Research Grant (No. A2084c0156), and SUG-NAP Grant (No. M4082268.050) of Nanyang Technological University, Singapore. * Corresponding author: C. Lv encoder to model the relationship among the target agent and its surrounding agents. The multi-head attention in the Transformer layer can help extract different aspects of the agent interactions. As for extracting the relationship between the map and the target agent, we utilize the vectorized map representation <ref type="bibr" target="#b2">[3]</ref> and a lane set-based map structure consisting of a list of waypoints from different lanes, which provides a higher map resolution. To model the target agent's different aspects of attention on the different lane segments, we propose the multi-modal Transformer encoder that can extract the different modes of agent-map relations. The rationals and details are given below. The motion prediction model should be capable of outputting multiple possible trajectories, and we can notice that the uncertainty of agent behaviors predominantly comes from their different targets on the road <ref type="bibr" target="#b3">[4]</ref>. Therefore, in modeling the attention between the target agent and the map, we propose to modify the multi-head attention mechanism in the Transformer to multi-modal attention. The multi-head attention is designed to extract multiple possible relationships and richer interpretations between the inputs, and all of the attention heads are then merged to produce a final output. Instead of combining them to output a single result, we propose to directly output the result of each independent attention head, and each separate agent-map attention result is used for predicting a possible trajectory. The intuition behind this is that each attention head can represent a different relationship between the target agent and different segments of the map, which affects their future trajectories. Moreover, to ensure the diversity of the multiple attention heads, only the head that outputs the closest trajectory to the ground-truth one gets updated during training.</p><p>The proposed motion prediction model is succinct, which encompasses only two cross-attention Transformer layers for modeling agent interactions and map attention respectively, other than the trajectory and map encoders and prediction head, enabling it to be easy to train and deploy while maintaining satisfactory prediction accuracy. The main contributions of this paper are listed as follows.</p><p>1) We propose a Transformer-based network using the waypoint-based map structure for multi-modal trajectory prediction. The proposed multi-modal attention Transformer layer is shown to be able to capture the different modes of agent-map relations, thus bringing better accuracy and interpretability. 2) We validate the proposed method on a large-scale realworld driving dataset and the results reveal that the proposed method with a simple structure and training process can achieve competitive accuracy compared to other state-of-the-art methods. 3) We investigate the performance of using lane-based and waypoint-based map structure with the proposed prediction network, as well as the improvement of the proposed multi-modal attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Encoding Map and Interaction</head><p>The most common method of encoding map and agent interaction information is to rasterize the driving scene into bird-eye-view images, which contain the information of relationships among agents and road structure. Such environment representation can be effectively processed by convolutional neural networks (CNNs), which have been used in many motion prediction-related works <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[6]</ref>, <ref type="bibr" target="#b7">[7]</ref>. However, the drawback of using rasterized images and CNNs is that the image structure is overly complex for driving environment representation and thus it requires a larger network to process and more computation and data to train the network. More recently, a notably compact vector map representation <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b8">[8]</ref> has been proposed, which can significantly reduce the computation burden. VectorNet <ref type="bibr" target="#b2">[3]</ref> treats the lanes on the map and agent historical trajectories as a set of polylines, and models them as a global fully-connected interaction graph, which is proposed by a graph neural network (GNN) to encode the map and agent information. However, putting all information including lanes and agents in a single graph makes training hard and inefficient, and thus we utilize two separate Transformer-based layers to encode agent interaction and agent's attention on map, respectively. On the other hand, LaneGCN <ref type="bibr" target="#b8">[8]</ref> proposes to organize the lanes on the map into a lane graph considering the spatial connectivity, and then use the graph convolutional network (GCN) to encode the topology of the map. However, using lane-level features may decrease the map resolution, and thereby we propose to use lane set-based map representation, which is a set of waypoints from different lanes with minimal information loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multi-modal Prediction</head><p>To realize multi-modal prediction, i.e., predicting multiple possible future trajectories, some generative modeling approaches, such as conditional variational autoencoder (CVAE) <ref type="bibr" target="#b9">[9]</ref>, <ref type="bibr" target="#b10">[10]</ref> and generative adversarial network (GAN) <ref type="bibr" target="#b11">[11]</ref>, are employed. However, such generative methods are hard to train and infer because the model needs to be sampled many times to recover a plausible distribution over future behaviors. Some other works propose to predict a set of trajectories <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b8">[8]</ref>, <ref type="bibr" target="#b12">[12]</ref> using the variety loss <ref type="bibr" target="#b13">[13]</ref>. However, they use the same extracted feature vector to output multiple trajectories, which is not intuitive and lacks interpretability on the outputs. Anchor-based methods <ref type="bibr" target="#b14">[14]</ref>, <ref type="bibr" target="#b15">[15]</ref>, <ref type="bibr" target="#b16">[16]</ref> can provide better interpretability, feasibility, and diversity on the results, but their predictions are restricted to a predefined set, obtained by clustering from the data or generated by a model, which may impede the prediction accuracy. On the other hand, goal-based <ref type="bibr" target="#b3">[4]</ref> or proposal-based methods <ref type="bibr" target="#b17">[17]</ref>, <ref type="bibr" target="#b18">[18]</ref>, <ref type="bibr" target="#b19">[19]</ref> have been widely used due to their superior accuracy and interpretability. The trajectory predictions are conditioned on the possible long-term goals or proposals on the map, which brings diversity and also flexibility to the model outputs. Nevertheless, the goals or proposals are still manually selected, which is laborious in data processing and needs careful design. Different from these methods above, in this work, we propose to use the multi-modal Transformer layer to learn to attend to different segments of the map and produce diverse possible trajectories accordingly in an endto-end manner, which can simplify the training process and maintain accuracy, interpretability, and flexibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MULTI-MODAL MOTION PREDICTION FRAMEWORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem Formulation</head><p>The task of motion prediction is to predict the possible future trajectories of a target agent over a time horizon T f based on its historical states over a time period T h and environmental context information. The input X to the prediction model consists of the historical dynamic states of the target agent (S 0 ) and its surrounding agents (S 1 , . . . , S N ), as well as the current environment information M. Without loss of generality, we assume that there are N surrounding agents (e.g., vehicles, pedestrians, and cyclists) around the target agent, however, the number of surrounding agents can be varied in different situations. The output of the prediction model? is K trajectories, each consisting of a discrete sequence of 2D coordinates ? j , denoting the future positions of the target agent, as well as the corresponding probability p j . Mathematically, the problem is formulated as:</p><formula xml:id="formula_0">Y = f (X|?) , X = {S 0 , S 1 , . . . , S N , M} , Y = {? j , p j } K j=1 , ? j = (x t j , y t j )|t ? {t 0 + 1, . . . , t 0 + T f } ,<label>(1)</label></formula><p>where ? denotes the parameters of the prediction model f ,</p><formula xml:id="formula_1">S i = {s t i |t ? {t 0 ? T h , .</formula><p>. . , t 0 }} and s t i is the dynamic state of the agent i at timestep t, (x t j , y t j ) is the coordinate of the jth predicted trajectory at timestep t, and t 0 is the current time step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Prediction Framework</head><p>The proposed motion prediction framework is illustrated in <ref type="figure">Fig. 1</ref>, which encompasses four main parts. First, the map and agent encoders translate the low-dimensional raw state inputs to high-dimensional feature vectors. Then, the agentagent encoder is used to capture the relationship between the interacting agents, and the agent-map encoder models the target agent's attention on different segments of the map. Finally, the interaction feature, map attention feature, and dynamic feature of the target agent are concatenated and passed through the scored trajectory decoder to generate possible trajectories with associated probabilities. The detailed structures of the key components, i.e, the map encoder, agentagent encoder, and agent-map encoder are illustrated in Fig.  <ref type="figure">Fig. 1</ref>. An overview of our proposed motion prediction model. The agent encoder and map encoder are used to extract the features of agents and map waypoints, respectively. The agent-agent encoder is employed to model the relationship among interacting agents, and the map-agent encoder to model the relationship between the target agent with interaction feature and the waypoints on the map. Finally, the interaction feature, target's agent historical feature, and agent-map attention feature are concatenated and passed through the trajectory and score decoders, to output the predicted trajectories and their scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Agent-agent Encoder Agent-map Encoder</head><p>Multi-Head Attention (h=K)  1) Map and Agent Encoders: The dynamic state of the target agent and its surrounding agents s t i at timestep t is in the format of (x, y, v x , v y , ?), where (x, y) is the coordinate, (v x , v y ) the velocity, and ? the heading angle. Note that the coordinate system is centered on the target agent's position at the current timestep with its heading aligned with the x-axis. Therefore, the historical state of an agent S i can be represented by a tensor with shape (T h , 5). The agent encoder consists of two layers, i.e., one 1D convolutional layer and one long short-term memory (LSTM) layer to extract the temporal motion feature of the agent. We only consider up to ten surrounding agents within a radius of 30 meters to the target agent. All the agents including the target and surrounding agents share the same agent encoder.</p><p>The map information M is represented by a set of waypoints from different segments of the map. Each waypoint has a unique feature of (x, y, ?), where (x, y) is the coordinate relative to the target agent, and ? is the direction. The waypoints from the same lane have the same lane features, which are the turning direction, whether the lane is in an intersection, and whether the lane has traffic control measures.</p><p>These lane features are first one-hot encoded respectively and then concatenated with the waypoint features. All the waypoints share the same map encoder, which is shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. First of all, the waypoints features are feed into a fully connected layer and we use the max-pooling operation to aggregate the information from all the waypoints on the same lane; the lane features are processed by another fully connected layer. The waypoint feature, aggregated feature, and lane feature are concatenated and passed through a fully connected layer to get the final feature vector of the waypoint.</p><p>2) Agent-agent Encoder: We first represent the relationship between the agents as a graph, shown in <ref type="figure">Fig. 1</ref>. All the agents (nodes) are connected to the target agent (including the self-loop), and we ignore the edge attributes. We use a Transformer layer with the multi-head attention mechanism to encode the interactions between the agents, as seen in <ref type="figure" target="#fig_0">Fig. 2</ref>. In addition to the multi-head attention, the encoder also contains two position-wise fully connected feed-forward layers and two layer normalization layers following the Transformer architecture. The multi-head attention mechanism is illustrated as follows <ref type="bibr" target="#b20">[20]</ref>.</p><formula xml:id="formula_2">MultiHead(Q, K, V) = Concat (head 1 , ? ? ? , head h ) W O , (2) head i = Attention(QW Q i , KW K i , VW V i ),<label>(3)</label></formula><p>where h is the total number of attention heads, Q, K, V are the query, key, and value vectors, respectively, and W Q i , W K i , W V i , W O are the matrices for linear projection. The attention operation is called scaled dot-product attention, which is shown as</p><formula xml:id="formula_3">Attention(Q, K, V) = softmax QK T ? d k V,<label>(4)</label></formula><p>where d k is the dimension of the key vector.</p><p>In the agent-agent encoder, the query is the target agent's feature vector from the agent encoder and the key and value are the feature vectors of all the agents.</p><p>3) Agent-map Encoder: The relationship between the target agent and the map is represented as a graph, where the target agent can attend to all the elements in the map. We use another Transformer layer to model the agentmap relationship, as seen in <ref type="figure" target="#fig_0">Fig. 2</ref>, and we modify the multi-head attention to multi-modal attention as shown in Eq. 5. Specifically, we do not concatenate the results from individual heads and project the concatenated vector to a low-dimensional one, but instead, we directly output the results of individual heads, and the final trajectory outputs are conditioned on the individual heads.</p><p>MultiModal(Q, K, V) = (head 1 , ? ? ? , head h ) . <ref type="formula">(5)</ref> The output of the agent-map encoder is a mode-wise feature, which means each mode has a different feature, corresponding to a different relationship between the target agent and map. To ensure the diversity of these modes, i.e., attending to different parts on the map, we only backpropagate the loss through the individual head that is closest to the ground truth in terms of final displacement error. In the agent-map encoder, the query is the interaction feature from the agent-agent encoder and the key and value are the feature vectors from the map encoder.</p><p>4) Score and Trajectory Decoders: The predicted trajectories and their scores are conditioned on three features, i.e., the target agent's historical state, the interaction among agents, and the target agent's attention on the map. The interaction feature and target agent feature are first repeated along the mode axis to match with the shape of the multimodal agent-map feature, and then the three features are concatenated to form a final representation of the driving environment. The trajectory decoder is a mode-wise fourlayer MLP with the final layer outputting the coordinates of the trajectory at each timestep. The score decoder follows the same structure, except for the last layer that outputs the score of the predicted trajectories. The scores of all the predicted trajectories are grouped and passed through a softmax layer to yield a probability distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training Objectives</head><p>All the modules in the prediction model are differentiable, and thus we can train the model end-to-end. To predict the trajectories, we use the smooth L1 loss on all predicted time steps. For a data point, the trajectory regression loss is defined as:</p><formula xml:id="formula_4">L traj = t L 1 p t gt ? p t j * ,<label>(6)</label></formula><p>where p t gt is the ground truth position at time step t. Using the variety or Minimum over N (MoN) loss <ref type="bibr" target="#b13">[13]</ref>, we only calculate the loss between the ground truth and the closest output prediction, and j * is the index of the predicted trajectory that is closest to the ground truth in terms of L2 distance between the endpoint p </p><formula xml:id="formula_5">j * = argmin j?{1,...,K} p T f j ? p T f gt 2 .<label>(7)</label></formula><p>Since different predicted trajectories are conditioned on the different attention heads in the agent-map encoder, only the head that corresponds to the closet trajectory gets updated, making the heads attend to different parts of the map and ensuring the diversity of the heads.</p><p>The scoring loss is the cross entropy loss between the ground truth scores (probability distribution) and the predicted probability distribution p. For a data point, the scoring loss is defined as:</p><formula xml:id="formula_6">L score = L CE (p gt , p),<label>(8)</label></formula><p>where the ground truth distribution p gt is defined as:</p><formula xml:id="formula_7">p gt = exp ? p T f ? p T f gt 2 j exp ? p T f j ? p T f gt 2 . (9)</formula><p>The total loss is a weighted sum of the trajectory regression loss and the scoring loss:</p><formula xml:id="formula_8">L = L score + ?L traj ,<label>(10)</label></formula><p>where ? is a hyperparameter to balance the two learning objectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS A. Experimental Setup 1) Dataset:</head><p>The proposed method is validated on the Argoverse Motion Forecasting dataset <ref type="bibr" target="#b21">[21]</ref>, which contains 324,557 real-world driving scenarios for training and validation. For each scenario, five-second trajectory sequences of each tracked object sampled at 10 Hz are provided and the map information is represented as a set of lane centerlines, which is composed of a set of waypoints. The prediction task is to forecast the future possible trajectories of the target agent in a scenario over the next 3 seconds, given the 2-second historical trajectory of the target agent, and the trajectories of its neighboring agents, as well as the map context. The whole dataset is split into 205,942 training, 39,472 validation, and 78,143 testing sequences, respectively. We train the prediction model on the training set and test it on the standard testing set to benchmark the performance of the model.</p><p>2) Metrics: The performance of the prediction model is evaluated using some standard evaluation metrics, which are minimum average displacement error (minADE), minimum final displacement error (minFDE), brier-minFDE, and miss rate (MR). minADE and minFDE are two common distancebased metrics; minADE reports the average displacement error between the best-predicted trajectory and the ground truth over the entire time steps, and minFDE reports the displacement error at the endpoint. The best-predicted trajectory refers to the trajectory that has the minimum endpoint error. To evaluate the scoring function of the model, we employ anthor metric; brier-minFDE is defined as the sum of minFDE and the brier score (1 ? p) 2 , where p is the probability of the best-predicted trajectory. Additionally, the miss rate (MR) is reported, which is defined as the ratio of the scenarios in which none of the endpoints of predicted trajectories are within 2.0 meters of ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target agent</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Surrounding agent</head><p>Predicted trajectory Ground-truth trajectory B. Implementation Details 1) Input and Output: For the map information, we search for 40 lanes closest to the current position of the target agent, each lane with 10 waypoints. For the agent information, we search for 10 neighboring agents within 30 meters to the target agent and organize them into a tensor along with the target agent. The missing lanes or agents in the tensors are padded with zeros and masked out when calculating the attention. The historical horizon is 2 seconds (T h = 20) and the prediction horizon is 3 seconds (T f = 30). The output of the model is K possible trajectories (a sequence of (x, y) coordinates), with a matching probability for each trajectory. The number of prediction modes is set as K = 6.</p><p>2) Network Structure: The dimensions of the embedded agent and map waypoint features are both 256. In the agent-agent encoder, the number of heads in the multi-head attention is 6, and the feed-forward network first projects the feature vector to 1024-dimension and then reduces it to 256dimension. In the agent-agent encoder, the number of modes (heads) in the multi-modal attention is 6 and the remaining structure is the same as the agent-agent encoder. The final feature vector obtained from the environment encoders is with the shape of (6, 768), and then feed into the trajectory and score decoders to produce the trajectory and score per mode, respectively. All the activation functions in the dense layers are ELU, and all the fully connected layers are followed by dropout layers with a dropout rate of 0.1 to mitigate overfitting. The total number of parameters of the model is 6,328,125.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Training:</head><p>The hyperparameter ? in the loss function (Eq. 10) is 0.5 after some trials. We use Nadam optimizer with a learning rate that starts with 1e-4 and decays by 50% after every 20 epochs. To stabilize training, we use gradient clipping with a threshold of 5 (by norm). The number of training epochs is 100 and the batch size is 64. No data augmentation is used and training one epoch on the training set takes about 10 minutes using Tensorflow with an NVIDIA RTX 2080Ti GPU. <ref type="figure" target="#fig_2">Fig. 3</ref> shows some representative examples of the motion forecasting results given by our prediction model. The model is capable of outputting multiple possible trajectories that are diverse and compliant with the structure of the map. The best-predicted trajectory is very close to the ground-truth one while the model maintains the ability to predict other possible trajectories with varying speed profiles or directions. The qualitative results demonstrate the effectiveness of our proposed model on different complex urban driving scenarios including leftturn, right-turn, intersection, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Qualitative Results:</head><p>Here, we visualize the target agent's attention on the map waypoints, as seen in <ref type="figure">Fig. 4</ref>, to demonstrate the interpretability of our prediction model. The attention of each mode (head) to the map waypoints is represented as attention scores, and the waypoints with scores greater than 0.01 are displayed on the map according to different modes. In the given two cases of left-turn scenarios, we can notice that the more attention on the left-turn lane when the model predicts The results manifest that the proposed multi-modal attention can automatically learn to extract the possible goals on the map, and thus the predicted trajectories can be conditioned on the different goals.</p><p>(1)</p><p>(2) <ref type="figure">Fig. 4</ref>. The visualization of the attention scores on the map waypoints for different modes. The darker red means more attention on the waypoint.</p><p>2) Quantitative Results: The proposed method is evaluated with the quantitative metrics previously defined and compared against the state-of-the-art methods on the Argoverse benchmark (test set). We only report the results with six forecasted trajectories (K = 6), which are summarized in <ref type="table" target="#tab_2">Table I</ref>. Our proposed method can achieve the best prediction accuracy in terms of the two distance error-based metrics (minADE and minFDE). The brier-minFDE and miss rate of our model are slightly worse than that of HOME <ref type="bibr" target="#b24">[24]</ref>. This is because HOME is optimized for minimizing the miss rate, and the weight of the scoring term in the loss function needs further tuning in training our model to improve the scoring performance. It is also worth noting that our model is with a smaller size and simpler training process, which can ease the burden of pre-or post-processing and simplify the model training process, as well as bring fast inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Study</head><p>We conduct an ablation study to evaluate and analyze the influence of map structure and the contributions of our proposed multi-modal attention to the final prediction accuracy. For the map structure, we investigate two different levels of representations, i.e., lane and waypoint. To encode the feature of a map lane, we use global max-pooling to aggregate the features of all the waypoints in a lane. For multi-modal prediction, in addition to our proposed multimodal attention, we take the ensemble method trained with the variety loss as a comparison. It uses an ensemble of trajectory decoders to output different trajectories from the same extracted environment feature vector. All these methods are tested on the Argoverse standard test set with six predicted trajectories. From the results given in <ref type="table" target="#tab_2">Table II</ref>, we can conclude that our proposed method with map waypoints and multi-modal agent-map attention can delivery the best prediction accuracy. Using the map waypoints can bring a higher map resolution and reduce the loss of information. Moreover, using the proposed multi-modal attention can achieve not only better prediction accuracy but also better interpretability, as shown in the previous section. V. CONCLUSIONS In this paper, we propose a multi-modal trajectory prediction model based on the Transformer structure. We employ a multi-head attention Transformer layer to model the relationship among interacting agents and introduce a multimodal attention Transformer layer to extract the different relationships between the target agent and map waypoints, which determines the final trajectory outputs. Comprehensive experiments on the Argoverse motion dataset reveal the effectiveness of our model with competitive accuracy, better interpretability, yet a simple structure and training process.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>The detailed structures of the map encoder, agent-agent encoder, and agent-map encoder in the prediction model. 2, and the detailed explanations of the prediction model are given below.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>T f j and the ground truth endpoint p T f gt :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>The qualitative motion forecasting results of the proposed model on the Argoverse validation set. The historical trajectory of the target agent is in red, and the surrounding agents are in blue; the predicted trajectories in yellow and ground truth trajectory in green, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Model structure</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>Agent-agent Interaction</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Target Agent States</cell><cell>Agent Encoder</cell><cell>Surrounding agents</cell><cell>Agent-agent</cell><cell>repeat?K</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Target agent</cell><cell>Encoder</cell><cell></cell><cell></cell></row><row><cell>Surrounding Agents States</cell><cell>Agent Encoder</cell><cell></cell><cell></cell><cell>Score</cell><cell>Softmax</cell><cell>Scores</cell><cell>0.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Decoder</cell><cell></cell><cell>(K?1)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Target-agent</cell><cell>repeat?K</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Feature</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Target agent interaction</cell><cell></cell><cell>Trajectory</cell><cell>Reshape</cell><cell>Trajectories</cell><cell>0.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Decoder</cell><cell></cell><cell>(K?T f ?2)</cell></row><row><cell>Map Waypoints</cell><cell>Map Encoder</cell><cell></cell><cell>Agent-map Encoder</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Waypoints</cell><cell></cell><cell>Concatenate</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Agent-map Interaction</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I THE</head><label>I</label><figDesc>QUANTITATIVE RESULTS IN COMPARISON WITH EXISTING METHODS ON THE ARGOVERSE BENCHMARK(TEST SET)</figDesc><table><row><cell>Method</cell><cell cols="4">minADE (m) minFDE(m) brier-minFDE miss rate</cell></row><row><cell>PRIME [16]</cell><cell>1.2187</cell><cell>1.5582</cell><cell>2.0978</cell><cell>0.1150</cell></row><row><cell>LaneRCNN [22]</cell><cell>0.9038</cell><cell>1.4526</cell><cell>2.1470</cell><cell>0.1232</cell></row><row><cell>TNT [4]</cell><cell>0.9097</cell><cell>1.4457</cell><cell>2.1401</cell><cell>0.1656</cell></row><row><cell>Multi-head attention [23]</cell><cell>0.9973</cell><cell>1.4209</cell><cell>2.1154</cell><cell>0.1308</cell></row><row><cell>LaneGCN [8]</cell><cell>0.8679</cell><cell>1.3550</cell><cell>2.0495</cell><cell>0.1597</cell></row><row><cell>mmTransformer [18]</cell><cell>0.8436</cell><cell>1.3383</cell><cell>2.0328</cell><cell>0.1540</cell></row><row><cell>HOME [24]</cell><cell>0.8904</cell><cell>1.2919</cell><cell>1.8601</cell><cell>0.0846</cell></row><row><cell>Ours (Multi-modal Transformer)</cell><cell>0.8372</cell><cell>1.2905</cell><cell>1.9393</cell><cell>0.1429</cell></row><row><cell cols="2">a left-turn trajectory, and likewise, more attention on the</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">straight lane when the model predicts a go-straight trajectory.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II ABLATION</head><label>II</label><figDesc>STUDY OF THE MAP STRUCTURE AND MULTI-MODAL PREDICTION METHOD ON THE ARGOVERSE TEST SET</figDesc><table><row><cell>Map</cell><cell cols="2">Multi-modal</cell><cell>minADE (m) minFDE(m)</cell></row><row><cell>Lane Waypoint</cell><cell>Attention</cell><cell>Ensemble</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.8372</cell><cell>1.2905</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.8461</cell><cell>1.3097</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.8512</cell><cell>1.3199</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.8604</cell><cell>1.3373</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Driving behavior modeling using naturalistic human driving data with inverse reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep learning-based vehicle behavior prediction for autonomous driving applications: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mozaffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Y</forename><surname>Al-Jarrah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dianati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jennings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mouzakitis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vectornet: Encoding hd maps and agent dynamics from vectorized representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11" to="525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tnt: Target-driven trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning (CoRL)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Radosavljevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-C</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multimodal trajectory predictions for autonomous driving using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Djuric</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2090" to="2096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Tra-jectron++: Dynamically-feasible trajectory forecasting with heterogeneous data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ivanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chakravarty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pavone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="683" to="700" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XVIII 16</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multimodal trajectory prediction for autonomous driving with semantic map and dynamic graph attention network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Autonomous Driving Workshop at the 34th Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning lane graph representations for motion forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="541" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Desire: Distant future prediction in dynamic scenes with interacting agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vernaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="336" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multimodal deep generative models for trajectory prediction: A conditional variational autoencoder approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ivanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schmerling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pavone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="302" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Social gan: Socially acceptable trajectories with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2255" to="2264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Tpcn: Temporal point cloud networks for motion forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Analyzing the variety loss in the context of probabilistic trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Thiede</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Brahma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9954" to="9963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multipath: Multiple probabilistic anchor trajectory hypotheses for behavior prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Covernet: Multimodal behavior prediction using trajectory sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Phan-Minh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Grigore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Boulton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Wolff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">83</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning to predict vehicle trajectories with model-based planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.04027</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Map-adaptive goal-based trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">C</forename><surname>Haynes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchetti-Bowick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multimodal motion prediction with stacked transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7577" to="7586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Tpnet: Trajectory proposal network for motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6797" to="6806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Argoverse: 3d tracking and forecasting with rich maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sangkloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hartnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8748" to="8757" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Lanercnn: Distributed representations for graph-centric motion forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.06653</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-head attention for multi-modal joint vehicle motion forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mercat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gilles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">El</forename><surname>Zoghby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sandou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Beauvois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>Gil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9638" to="9644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Home: Heatmap output for future motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gilles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sabatini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsishkou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Stanciulescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moutarde</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.10968</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
