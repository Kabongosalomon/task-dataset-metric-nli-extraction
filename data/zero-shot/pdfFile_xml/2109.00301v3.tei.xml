<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">?-former: Infinite Memory Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">Henrique</forename><surname>Martins</surname></persName>
							<email>pedrohenriqueamartins@tecnico.ulisboa.pt</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Instituto de Telecomunica??es DeepMind Institute of Systems and Robotics LUMLIS (Lisbon ELLIS Unit)</orgName>
								<orgName type="institution">Instituto Superior T?cnico Unbabel Lisbon</orgName>
								<address>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zita</forename><surname>Marinho</surname></persName>
							<email>zmarinho@google.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Instituto de Telecomunica??es DeepMind Institute of Systems and Robotics LUMLIS (Lisbon ELLIS Unit)</orgName>
								<orgName type="institution">Instituto Superior T?cnico Unbabel Lisbon</orgName>
								<address>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr?</forename><forename type="middle">F T</forename><surname>Martins</surname></persName>
							<email>andre.t.martins@tecnico.ulisboa.pt.</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Instituto de Telecomunica??es DeepMind Institute of Systems and Robotics LUMLIS (Lisbon ELLIS Unit)</orgName>
								<orgName type="institution">Instituto Superior T?cnico Unbabel Lisbon</orgName>
								<address>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">?-former: Infinite Memory Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformers are unable to model long-term memories effectively, since the amount of computation they need to perform grows with the context length. While variations of efficient transformers have been proposed, they all have a finite memory capacity and are forced to drop old information. In this paper, we propose the ?-former, which extends the vanilla transformer with an unbounded longterm memory. By making use of a continuousspace attention mechanism to attend over the long-term memory, the ?-former's attention complexity becomes independent of the context length, trading off memory length with precision. In order to control where precision is more important, ?-former maintains "sticky memories," being able to model arbitrarily long contexts while keeping the computation budget fixed. Experiments on a synthetic sorting task, language modeling, and document grounded dialogue generation demonstrate the ?-former's ability to retain information from long sequences. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>When reading or writing a document, it is important to keep in memory the information previously read or written. Humans have a remarkable ability to remember long-term context, keeping in memory the relevant details <ref type="bibr">(Carroll, 2007;</ref><ref type="bibr" target="#b6">Kuhbandner, 2020)</ref>. Recently, transformer-based language models have achieved impressive results by increasing the context size <ref type="bibr" target="#b13">(Radford et al., 2018</ref><ref type="bibr" target="#b14">(Radford et al., , 2019</ref><ref type="bibr">Dai et al., 2019;</ref><ref type="bibr" target="#b16">Rae et al., 2019;</ref><ref type="bibr">Brown et al., 2020)</ref>. However, while humans process information sequentially, updating their memories continuously, and recurrent neural networks (RNNs) update a single memory vector during time, transformers do not -they exhaustively query every representation associated to the past events. Thus, the amount of computation they need to perform grows with the length of the context, and, consequently, transformers have computational limitations about how much information can fit into memory. For example, a vanilla transformer requires quadratic time to process an input sequence and linear time to attend to the context when generating every new word.</p><p>Several variations have been proposed to address this problem <ref type="bibr">(Tay et al., 2020b)</ref>. Some propose using sparse attention mechanisms, either with data-dependent patterns <ref type="bibr" target="#b5">(Kitaev et al., 2020;</ref><ref type="bibr">Vyas et al., 2020;</ref><ref type="bibr">Tay et al., 2020a;</ref><ref type="bibr">Wang et al., 2021)</ref> or data-independent patterns <ref type="bibr" target="#b14">(Child et al., 2019;</ref><ref type="bibr" target="#b3">Beltagy et al., 2020;</ref><ref type="bibr">Zaheer et al., 2020)</ref>, reducing the self-attention complexity <ref type="bibr">(Katharopoulos et al., 2020;</ref><ref type="bibr">Choromanski et al., 2021;</ref><ref type="bibr" target="#b11">Peng et al., 2021;</ref><ref type="bibr">Jaegle et al., 2021)</ref>, and caching past representations in a memory <ref type="bibr">(Dai et al., 2019;</ref><ref type="bibr" target="#b16">Rae et al., 2019)</ref>. These models are able to reduce the attention complexity, and, consequently, to scale up to longer contexts. However, as their complexity still depends on the context length, they cannot deal with unbounded context.</p><p>In this paper, we propose the ?-former (infinite former; <ref type="figure" target="#fig_0">Fig. 1</ref>): a transformer model extended with an unbounded long-term memory <ref type="bibr">(LTM)</ref>, which allows the model to attend to arbitrarily long contexts. The key for making the LTM unbounded is a continuous-space attention framework <ref type="bibr" target="#b8">(Martins et al., 2020)</ref> which trades off the number of information units that fit into memory (basis functions) with the granularity of their representations. In this framework, the input sequence is represented as a continuous signal, expressed as a linear combination of N radial basis functions (RBFs). By doing this, the ?-former's attention complexity is O(L 2 + L ? N ) while the vanilla transformer's is O(L ? (L + L LTM )), where L and L LTM correspond to the transformer input size and the long-term memory length, respectively (details in ?3.1.1). Thus, this representation comes with arXiv:2109.00301v3 [cs.CL] 25 Mar 2022 two significant advantages: (i) the context can be represented using a number of basis functions N smaller than the number of tokens, reducing the attention computational cost; and (ii) N can be fixed, making it possible to represent unbounded context in memory, as described in ?3.2 and <ref type="figure">Fig. 2</ref>, without increasing its attention complexity. The price, of course, is a loss in resolution: using a smaller number of basis functions leads to lower precision when representing the input sequence as a continuous signal, as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>.</p><p>To mitigate the problem of losing resolution, we introduce the concept of "sticky memories" ( ?3.3), in which we attribute a larger space in the LTM's signal to regions of the memory more frequently accessed. This creates a notion of "permanence" in the LTM, allowing the model to better capture long contexts without losing the relevant information, which takes inspiration from long-term potentiation and plasticity in the brain <ref type="bibr" target="#b10">(Mills et al., 2014;</ref><ref type="bibr" target="#b1">Bamji, 2005)</ref>.</p><p>To sum up, our contributions are the following:</p><p>? We propose the ?-former, in which we extend the transformer model with a continuous long-term memory ( ?3.1). Since the attention computational complexity is independent of the context length, the ?-former is able to model long contexts.</p><p>? We propose a procedure that allows the model to keep unbounded context in memory ( ?3.2).</p><p>? We introduce sticky memories, a procedure that enforces the persistence of important information in the LTM ( ?3.3).</p><p>? We perform empirical comparisons in a synthetic task ( ?4.1), which considers increasingly long sequences, in language modeling ( ?4.2), and in document grounded dialogue generation ( ?4.3). These experiments show the benefits of using an unbounded memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Transformer</head><p>A transformer <ref type="bibr">(Vaswani et al., 2017)</ref> is composed of several layers, which encompass a multi-head self-attention layer followed by a feed-forward layer, along with residual connections <ref type="bibr">(He et al., 2016)</ref> and layer normalization <ref type="bibr" target="#b0">(Ba et al., 2016)</ref>. Let us denote the input sequence as X = [x 1 , . . . , x L ] ? R L?e , where L is the input size and e is the embedding size of the attention layer. The queries Q, keys K, and values V , to be used in the multi-head self-attention computation are obtained by linearly projecting the input, or the output of the previous layer, X, for each attention head h:</p><formula xml:id="formula_0">Q h = X h W Q h , K h = X h W K h , V h = X h W V h ,<label>(1)</label></formula><p>where </p><formula xml:id="formula_1">W Q h , W K h , W V h ? R d?d are</formula><formula xml:id="formula_2">Z h = softmax Q h K h ? d V h ,<label>(2)</label></formula><p>where the softmax is performed row-wise. The head context representations are concatenated to obtain the final context representation Z ? R L?e :</p><formula xml:id="formula_3">Z = [Z 1 , . . . , Z H ]W R ,<label>(3)</label></formula><p>where W R ? R e?e is another projection matrix that aggregates all head's representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Continuous Attention</head><p>Continuous attention mechanisms <ref type="bibr" target="#b8">(Martins et al., 2020)</ref> have been proposed to handle arbitrary continuous signals, where the attention probability mass function over words is replaced by a probability density over a signal. This allows time intervals or compact segments to be selected. To perform continuous attention, the first step is to transform the discrete text sequence represented by X ? R L?e into a continuous signal. This is done by expressing it as a linear combination of basis functions. To do so, each x i , with i ? {1, . . . , L}, is first associated with a position in an interval, t i ? [0, 1], e.g., by setting t i = i/L. Then, we obtain a continuous-space representation X(t) ? R e , for any t ? [0, 1] as:</p><formula xml:id="formula_4">X(t) = B ?(t),<label>(4)</label></formula><p>where ?(t) ? R N is a vector of N RBFs, e.g., ? j (t) = N (t; ? j , ? j ), with ? j ? [0, 1], and B ? R N ?e is a coefficient matrix. B is obtained with multivariate ridge regression <ref type="bibr" target="#b4">(Brown et al., 1980)</ref> so thatX(t i ) ? x i for each i ? [L], which leads to the closed form (see App. A for details):</p><formula xml:id="formula_5">B = X F (F F + ?I) ?1 = X G,<label>(5)</label></formula><p>where F = [?(t 1 ), . . . , ?(t L )] ? R N ?L packs the basis vectors for the L locations. As G ? R L?N only depends of F , it can be computed offline.</p><p>Having converted the input sequence into a continuous signalX(t), the second step is to attend over this signal. To do so, instead of having a discrete probability distribution over the input sequence as in standard attention mechanisms (like in Eq. 2), we have a probability density p, which can be a Gaussian N (t; ?, ? 2 ), where ? and ? 2 are computed by a neural component. A unimodal Gaussian distribution encourages each attention head to attend to a single region, as opposed to scattering its attention through many places, and enables tractable computation. Finally, having p, we can compute the context vector c as:</p><formula xml:id="formula_6">c = E p X (t) .<label>(6)</label></formula><p>Martins et al. <ref type="formula" target="#formula_2">(2020)</ref> introduced the continuous attention framework for RNNs. In the following section ( ?3.1), we will explain how it can be used for transformer multi-head attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Infinite Memory Transformer</head><p>To allow the model to access long-range context, we propose extending the vanilla transformer with a continuous LTM, which stores the input embeddings and hidden states of the previous steps. We also consider the possibility of having two memories: the LTM and a short-term memory (STM), which consists in an extension of the transformer's hidden states and is attended to by the transformer's self-attention, as in the transformer-XL <ref type="bibr">(Dai et al., 2019)</ref>. A diagram of the model is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Long-term Memory</head><p>For simplicity, let us first assume that the longterm memory contains an explicit input discrete sequence X that consists of the past text sequence's input embeddings or hidden states, 2 depending on the layer 3 (we will later extend this idea to an unbounded memory in ?3.2).</p><p>First, we need to transform X into a continuous approximationX(t). We computeX(t) as:</p><formula xml:id="formula_7">X(t) = B ?(t),<label>(7)</label></formula><p>where ?(t) ? R N are basis functions and coefficients B ? R N ?e are computed as in Eq. 5, <ref type="bibr">2</ref> We stop the gradient with respect to the word embeddings or hidden states before storing them in the LTM.</p><p>3 Each layer of the transformer has a different LTM. B = X G. Then, we can compute the LTM keys, K h ? R N ?d , and values, V h ? R N ?d , for each attention head h, as:</p><formula xml:id="formula_8">K h = B h W K h , V h = B h W V h ,<label>(8)</label></formula><p>where W K h , W V h ? R d?d are learnable projection matrices. 4 For each query q h,i for i ? {1, . . . , L}, we use a parameterized network, which takes as input the attention scores, to compute ? h,i ?]0, 1[ and ? 2 h,i ? R &gt;0 :</p><formula xml:id="formula_9">? h,i = sigmoid affine K h q h,i ? d (9) ? 2 h,i = softplus affine K h q h,i ? d .<label>(10)</label></formula><p>Then, using the continuous softmax transformation <ref type="bibr" target="#b8">(Martins et al., 2020)</ref>, we obtain the probability density p h,i as N (t; ? h,i , ? 2 h,i ). Finally, having the value functionV h (t) given asV h (t) = V h ?(t), we compute the head-specific representation vectors as in Eq. 6:</p><formula xml:id="formula_10">z h,i = E p h,i [V h ] = V h E p h,i [?(t)]<label>(11)</label></formula><p>which form the rows of matrix Z LTM,h ? R L?d that goes through an affine transformation,</p><formula xml:id="formula_11">Z LTM = [Z LTM,1 , . . . , Z LTM,H ]W O .</formula><p>The long-term representation, Z LTM , is then summed to the transformer context vector, Z T , to obtain the final context representation Z ? R L?e :</p><formula xml:id="formula_12">Z = Z T + Z LTM ,<label>(12)</label></formula><p>which will be the input to the feed-forward layer.</p><p>contraction + function evaluation concatenation regression <ref type="figure">Figure 2</ref>: Diagram of the unbounded memory update procedure. This is performed in parallel for each embedding dimension, and repeated throughout the input sequence. We propose two alternatives to select the positions used for the function evaluation: linearly spaced or sticky memories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Attention Complexity</head><p>As the ?-former makes use of a continuousspace attention framework <ref type="bibr" target="#b8">(Martins et al., 2020)</ref> to attend over the LTM signal, its key matrix size K h ? R N ?d depends only on the number of basis functions N , but not on the length of the context being attended to. Thus, the ?-former's attention complexity is also independent of the context's length. It corresponds to O(L ? (L + L STM ) + L ? N ) when also using a short-term memory and O(L 2 + L ? N ) when only using the LTM, both O(L ? (L + L LTM )), which would be the complexity of a vanilla transformer attending to the same context. For this reason, the ?-former can attend to arbitrarily long contexts without increasing the amount of computation needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Unbounded Memory</head><p>When representing the memory as a discrete sequence, to extend it, we need to store the new hidden states in memory. In a vanilla transformer, this is not feasible for long contexts due to the high memory requirements. However, the ?-former can attend to unbounded context without increasing memory requirements by using continuous attention, as next described and shown in <ref type="figure">Fig. 2</ref>.</p><p>To be able to build an unbounded representation, we first sample M locations in [0, 1] and evaluat? X(t) at those locations. These locations can simply be linearly spaced, or sampled according to the region importance, as described in ?3.3.</p><p>Then, we concatenate the corresponding vectors with the new vectors coming from the short-term memory. For that, we first need to contract this function by a factor of ? ? ]0, 1[ to make room for the new vectors. We do this by defining:</p><formula xml:id="formula_13">X contracted (t) = X(t/? ) = B ?(t/? ). (13)</formula><p>Then, we can evaluateX(t) at the M locations 0 ? t 1 , t 2 , . . . , t M ? ? as:</p><formula xml:id="formula_14">x m = B ?(t m /? ), for m ? [M ],<label>(14)</label></formula><p>and define a matrix X past = [x 1 , x 2 , . . . , x M ] ? R M ?e with these vectors as rows. After that, we concatenate this matrix with the new vectors X new , obtaining:</p><formula xml:id="formula_15">X = X past , X new ? R (M +L)?e . (15)</formula><p>Finally, we simply need to perform multivariate ridge regression to compute the new coefficient matrix B ? R N ?e , via B = X G, as in Eq. 5. To do this, we need to associate the vectors in X past with positions in [0, ? ] and in X new with positions in ]?, 1] so that we obtain the matrix G ? R (M +L)?N . We consider the vectors positions to be linearly spaced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Sticky Memories</head><p>When extending the LTM, we evaluate its current signal at M locations in [0, 1], as shown in Eq. 14. These locations can be linearly spaced. However, some regions of the signal can be more relevant than others, and should consequently be given a larger "memory space" in the next step LTM's signal. To take this into account, we propose sampling the M locations according to the signal's relevance at each region (see <ref type="figure" target="#fig_5">Fig. 6</ref> in App. B). To do so, we construct a histogram based on the attention given to each interval of the signal on the previous step. For that, we first divide the signal into D linearly spaced bins {d 1 , . . . , d D }. Then, we compute the probability given to each bin, p(d j ) for j ? {1, . . . , D}, as:</p><formula xml:id="formula_16">p(d j ) ? H h=1 L i=1 d j N (t; ? h,i , ? 2 h,i ) dt, (16)</formula><p>where H is the number of attention heads and L is the sequence length. Note that Eq. 16's integral can be evaluated efficiently using the erf function:</p><formula xml:id="formula_17">b a N (t; ?, ? 2 ) = 1 2 erf b ? 2 ? erf a ? 2 .</formula><p>(17) Then, we sample the M locations at which the LTM's signal is evaluated at, according to p. By doing so, we evaluate the LTM's signal at the regions which were considered more relevant by the previous step's attention, and will, consequently attribute a larger space of the new LTM's signal to the memories stored in those regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Implementation and Learning Details</head><p>Discrete sequences can be highly irregular and, consequently, difficult to convert into a continuous signal through regression. Because of this, before applying multivariate ridge regression to convert the discrete sequence X into a continuous signal, we use a simple convolutional layer (with stride = 1 and width = 3) as a gate, to smooth the sequence:</p><formula xml:id="formula_18">X = sigmoid (CNN(X)) X.<label>(18)</label></formula><p>To train the model we use the cross entropy loss. Having a sequence of text X of length L as input, a language model outputs a probability distribution of the next word p(x t+1 | x t , . . . , x t?L ). Given a corpus of T tokens, we train the model to minimize its negative log likelihood:</p><formula xml:id="formula_19">L NLL = ? T ?1 t=0 log p(x t+1 | x t , . . . , x t?L ). (19)</formula><p>Additionally, in order to avoid having uniform distributions over the LTM, we regularize the continuous attention given to the LTM, by minimizing the Kullback-Leibler (KL) divergence, D KL , between the attention probability density, N (? h , ? h ), and a Gaussian prior, N (? 0 , ? 0 ). As different heads can attend to different regions, we set ? 0 = ? h , regularizing only the attention variance, and get:</p><formula xml:id="formula_20">L KL = T ?1 t=0 H h=1 D KL (N (? h , ? h ) || N (? h , ? 0 )) (20) = T ?1 t=0 H h=1 1 2 ? 2 h ? 2 0 ? log ? h ? 0 ? 1 . (21)</formula><p>Thus, the final loss that is minimized corresponds to:</p><formula xml:id="formula_21">L = L NLL + ? KL L KL ,<label>(22)</label></formula><p>where ? KL is a hyperparameter that controls the amount of KL regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To understand if the ?-former is able to model long contexts, we first performed experiments on a synthetic task, which consists of sorting tokens by their frequencies in a long sequence ( ?4.1). Then, we performed experiments on language modeling ( ?4.2) and document grounded dialogue generation ( ?4.3) by fine-tuning a pre-trained language model. 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Sorting</head><p>In this task, the input consists of a sequence of tokens sampled according to a token probability distribution (which is not known to the system).</p><p>The goal is to generate the tokens in the decreasing order of their frequencies in the sequence. One example can be:</p><p>1 2 1 3 1 0 3 1 3 2 1 occurs 4 times; 3 occurs 3 times, etc.</p><p>&lt;SEP&gt; 1 3 2 0</p><p>To understand if the long-term memory is being effectively used and the transformer is not only performing sorting by modeling the most recent tokens, we design the token probability distribution to change over time: namely, we set it as a mixture of two distributions, p = ?p 0 + (1 ? ?)p 1 , where the mixture coefficient ? ? [0, 1] is progressively increased from 0 to 1 as the sequence is generated. The vocabulary has 20 tokens and we experiment with sequences of length 4,000, 8,000, and 16,000. Baselines. We consider the transformer-XL 6 (Dai et al., 2019) and the compressive transformer 7 (Rae et al., 2019) as baselines. The transformer-XL consists of a vanilla transformer <ref type="bibr">(Vaswani et al., 2017)</ref> extended with a short-term memory which is composed of the hidden states of the previous steps. The compressive transformer is an extension of the transformer-XL: besides the short-term memory, it has a compressive long-term memory which is composed of the old vectors of the short-term memory, compressed using a CNN. Both the transformer-XL and the compressive transformer require relative positional encodings. In contrast, there is no need for positional encodings in the memory in our approach since the memory vectors represent basis coefficients in a predefined continuous space.</p><p>For all models we used a transformer with 3 layers and 6 attention heads, input size L = 1, 024 and memory size 2,048. For the compressive transformer, both memories have size 1,024. For the ?-former, we also consider a STM of size 1,024 and a LTM with N = 1, 024 basis functions, having the models the same computational cost. Further details are described in App. C.1.</p><p>Results. As can be seen in the left plot of <ref type="figure" target="#fig_1">Fig. 3</ref>, the transformer-XL achieves a slightly higher accuracy than the compressive transformer and ?-former for a short sequence length (4,000). This is because the transformer-XL is able to keep almost the entire sequence in memory. However, its accuracy degrades rapidly when the sequence length is increased. Both the compressive trans-former and ?-former also lead to smaller accuracies when increasing the sequence length, as expected. However, this decrease is not so significant for the ?-former, which indicates that it is better at modeling long sequences.</p><p>Regression error analysis. To better understand the trade-off between the ?-former's memory precision and its computational efficiency, we analyze how its regression error and sorting accuracy vary when varying the number of basis functions used, on the sorting task with input sequences of length 8,000. As can be seen in the right plot of <ref type="figure" target="#fig_1">Fig. 3</ref>, the sorting accuracy is negatively correlated with the regression error, which is positively correlated with the number of basis functions. It can also be observed, that when increasing substantially the number of basis functions the regression error reaches a plateau and the accuracy starts to drop. We posit that the latter is caused by the model having a harder task at selecting the locations it should attend to. This shows that, as expected, when increasing ?-former's efficiency or increasing the size of context being modeled, the memory loses precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Language Modeling</head><p>To understand if long-term memories can be used to extend a pre-trained language model, we fine-tune GPT-2 small <ref type="bibr" target="#b14">(Radford et al., 2019)</ref> on Wikitext-103 <ref type="bibr" target="#b9">(Merity et al., 2017)</ref> and a subset of <ref type="bibr">PG-19 (Rae et al., 2019)</ref> containing the first 2,000 books (? 200 million tokens) of the training set. To do so, we extend GPT-2 with a continuous long-term memory (?-former) and a compressed memory (compressive transformer) with a positional bias,  based on <ref type="bibr" target="#b12">Press et al. (2021)</ref>. <ref type="bibr">8</ref> For these experiments, we consider transformers with input size L = 512, for the compressive transformer we use a compressed memory of size 512, and for the ?-former we consider a LTM with N = 512 Gaussian RBFs and a memory threshold of 2,048 tokens, having the same computational budget for the two models. Further details and hyperparameters are described in App. C.2.</p><p>Results. The results reported in <ref type="table" target="#tab_2">Table 1</ref> show that the ?-former leads to perplexity improvements on both Wikitext-103 and PG19, while the compressive transformer only has a slight improvement on the latter. The improvements obtained by the ?-former are larger on the PG19 dataset, which can be justified by the nature of the datasets: books have more long range dependencies than Wikipedia articles (Rae et al., 2019).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Document Grounded Dialogue</head><p>In document grounded dialogue generation, besides the dialogue history, models have access to a document concerning the conversation's topic. In the CMU Document Grounded Conversation dataset (CMU-DoG) <ref type="bibr">(Zhou et al., 2018)</ref>, the dialogues are about movies and a summary of the movie is given as the auxiliary document; the auxiliary document is divided into parts that should be considered for the different utterances of the dialogue. In this paper, to evaluate the usefulness of the long-term memories, we make this task slightly more challenging by only giving the models access to the document before the start of the dialogue.</p><p>We fine-tune GPT-2 small <ref type="bibr" target="#b14">(Radford et al., 2019)</ref> using an approach based on Wolf et al. <ref type="bibr">(2019)</ref>. To allow the model to keep the whole document on memory, we extend GPT-2 with a continuous LTM (?-former) with N = 512 basis functions. As baselines, we use GPT-2, with and without ac- <ref type="bibr">8</ref> The compressive transformer requires relative positional encodings. When using only GPT-2's absolute positional encodings the model gives too much attention to the compressed memory, and, consequently, diverges. Thus, we adapted it by using positional biases on the attention mechanism.  cess (GPT-2 w/o doc) to the auxiliary document, with input size L = 512, and GPT-2 with a compressed memory with attention positional biases (compressive), of size 512. Further details and hyper-parameters are stated in App. C.3.</p><p>To evaluate the models we use the metrics: perplexity, F1 score, Rouge-1 and Rouge-L <ref type="bibr" target="#b7">(Lin, 2004)</ref>, and Meteor <ref type="bibr" target="#b2">(Banerjee and Lavie, 2005)</ref>. <ref type="table" target="#tab_4">Table 2</ref>, by keeping the whole auxiliary document in memory, the ?-former and the compressive transformer are able to generate better utterances, according to all metrics. While the compressive and ?-former achieve essentially the same perplexity in this task, the ?-former achieves consistently better scores on all other metrics. Also, using sticky memories leads to slightly better results on those metrics, which suggests that attributing a larger space in the LTM to the most relevant tokens can be beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results. As shown in</head><p>Analysis. In <ref type="figure" target="#fig_2">Fig. 4, we</ref> show examples of utterances generated by ?-former along with the excerpts from the LTM that receive higher attention throughout the utterances' generation. In these examples, we can clearly see that these excerpts are highly pertinent to the answers being generated. Also, in <ref type="figure" target="#fig_3">Fig. 5</ref>, we can see that the phrases which are attributed larger spaces in the LTM, when using sticky memories, are relevant to the conversations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Continuous attention. <ref type="bibr" target="#b8">Martins et al. (2020)</ref> introduced 1D and 2D continuous attention, using Gaussians and truncated parabolas as densities. They applied it to RNN-based document classification, machine translation, and visual question answering. Several other works have also proposed the use of (discretized) Gaussian attention for natural language processing tasks: Guo et al. <ref type="formula" target="#formula_0">(2019)</ref> proposed a Gaussian prior to the self-attention mechanism to bias the model to give higher attention to nearby words, and applied it to natural language inference; You et al. <ref type="formula" target="#formula_2">(2020)</ref>    of hard-coded Gaussian attention as input-agnostic self-attention layer for machine translation; Dubois et al. (2020) proposed using Gaussian attention as a location attention mechanism to improve the model generalization to longer sequences. However, these approaches still consider discrete sequences and compute the attention by evaluating the Gaussian density at the token positions. Farinhas et al. <ref type="bibr">(2021)</ref> extend continuous attention to multimodal densities, i.e., mixtures of Gaussians, and applied it to VQA. In this paper, we opt for the simpler case, an unimodal Gaussian, and leave sparse and multimodal continuous attention for future work.</p><p>Efficient transformers. Several methods have been proposed that reduce the transformer's attention complexity, and can, consequently, model longer contexts. Some of these do so by performing sparse attention, either by selecting pre-defined attention patterns <ref type="bibr" target="#b14">(Child et al., 2019;</ref><ref type="bibr" target="#b3">Beltagy et al., 2020;</ref><ref type="bibr">Zaheer et al., 2020)</ref>, or by learning these patterns from data <ref type="bibr" target="#b5">(Kitaev et al., 2020;</ref><ref type="bibr">Vyas et al., 2020;</ref><ref type="bibr">Tay et al., 2020a;</ref><ref type="bibr">Wang et al., 2021)</ref>. Other works focus on directly reducing the attention complexity by applying the  <ref type="formula" target="#formula_0">2021)</ref> proposed combining the information at the architecture level. These models have the disadvantage of needing to perform a retrieval step when generating each token / utterance, which can be computationally expensive. These approaches are orthogonal to the ?-former's LTM and in future work the two can be combined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we proposed the ?-former: a transformer extended with an unbounded long-term memory. By using a continuous-space attention framework, its attention complexity is independent of the context's length, which allows the model to attend to arbitrarily long contexts while keeping a fixed computation budget. By updating the memory taking into account past usage, the model keeps "sticky memories", enforcing the persistence of relevant information in memory over time. Experiments on a sorting synthetic task show that ?former scales up to long sequences, maintaining a high accuracy. Experiments on language modeling and document grounded dialogue generation by fine-tuning a pre-trained language model have shown improvements across several metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics Statement</head><p>Transformer models that attend to long contexts, to improve their generation quality, need large amounts of computation and memory to perform self-attention. In this paper, we propose an extension to a transformer model that makes the attention complexity independent of the length of the context being attended to. This can lead to a reduced number of parameters needed to model the same context, which can, consequently, lead to gains in efficiency and reduce energy consumption.</p><p>On the other hand, the ?-former, as well as the other transformer language models, can be used on questionable scenarios, such as the generation of fake news <ref type="bibr">(Zellers et al., 2019)</ref>, defamatory text <ref type="bibr">(Wallace et al., 2019)</ref>, or other undesired content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Multivariate ridge regression</head><p>The coefficient matrix B ? R N ?e is obtained with multivariate ridge regression criterion so that X(t i ) ? x i for each i ? [L], which leads to the closed form:</p><formula xml:id="formula_22">B = arg min B ||B F ? X || 2 F + ?||B|| 2 F (23) = X F (F F + ?I) ?1 = X G,</formula><p>where F = [?(t 1 ), . . . , ?(t L )] packs the basis vectors for L locations and || ? || F is the Frobenius norm. As G ? R L?N only depends of F , it can be computed offline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Sticky memories</head><p>We present in <ref type="figure" target="#fig_5">Fig. 6</ref> a scheme of the sticky memories procedure. First we sample M locations from the previous step LTM attention histogram (Eq. 16). Then, we evaluate the LTM's signal at the sampled locations (Eq. 14). Finally, we consider that the sampled vectors, X past , are linearly spaced in <ref type="bibr">[0, ? ]</ref>. This way, the model is able to attribute larger spaces of its memory to the relevant words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Experimental details C.1 Sorting</head><p>For the compressive transformer, we consider compression rates of size 2 for sequences of length 4,000, from 2 to 6 for sequences of length 8,000, and from 2 to 12 for sequences of length 16,000. We also experiment training the compressive transformer with and without the attention reconstruction auxiliary loss. For the ?-former we consider 1,024 Gaussian RBFs N (t;?,? 2 ) with? linearly spaced in [0, 1] and? ? {.01, .05}. We set ? = 0.75 and for the KL regularization we used ? KL = 1 ? 10 ?5 and ? 0 = 0.05. For this task, for each sequence length, we created a training set with 8,000 sequences and validation and test sets with 800 sequences. We trained all models with batches of size 8 for 20 epochs on 1 Nvidia GeForce RTX 2080 Ti or 1 Nvidia GeForce GTX 1080 Ti GPU with ? 11 Gb of memory, using the Adam optimizer (Kingma and <ref type="bibr">Ba, 2015)</ref>. For the sequences of length 4,000 and 8,000 we used a learning rate of 2.5 ? 10 ?4 while for sequences of length 16,000 we used a learning rate of 2 ? 10 ?4 . The learning rate was decayed to 0 until the end of training with a cosine schedule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Pre-trained Language Models</head><p>In these experiments, we fine-tune the GPT-2 small, which is composed of 12 layers with 12 attention heads, on the English dataset Wikitext-103 and on a subset of the English dataset PG19 9 containing the first 2,000 books. We consider an input size L = 512 and a long-term memory with N = 512 Gaussian RBFs N (t;?,? 2 ) with? linearly spaced in [0, 1] and? ? {.005, .01} and for the KL regularization we use ? KL = 1 ? 10 ?6 and ? 0 = 0.05. We set ? = 0.5. For the compressive transformer we also consider a compressed memory of size 512 with a compression rate of 4, and train the model with the auxiliary reconstruction loss.</p><p>We fine-tuned GPT-2 small with a batch size of 1 on 1 Nvidia GeForce RTX 2080 Ti or 1 Nvidia GeForce GTX 1080 Ti GPU with ? 11 Gb of memory, using the Adam optimizer (Kingma and <ref type="bibr">Ba, 2015)</ref> for 1 epoch with a learning rate of 5 ? 10 ?5 for the GPT-2 parameters and a learning rate of 2.5 ? 10 ?4 for the LTM parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Document Grounded Generation</head><p>In these experiments, we fine-tune the GPT-2 small, which is composed of 12 layers with 12 attention heads, on the English dataset CMU -Document Grounded Conversations 10 (CMU-DoG. CMU-DoG has 4112 conversations, being the proportion of train/validation/test split 0.8/0.05/0.15.</p><p>We consider an input size L = 512 and a longterm memory with N = 512 Gaussian RBFs N (t;?,? 2 ) with? linearly spaced in [0, 1] and ? ? {.005, .01} and for the KL regularization we use ? KL = 1 ? 10 ?6 and ? 0 = 0.05. We set ? = 0.5. For the compressive transformer we consider a compressed memory of size 512 with a compression rate of 3, and train the model with the auxiliary reconstruction loss. We fine-tuned GPT-2 small with a batch size of 1 on 1 Nvidia GeForce RTX 2080 Ti or 1 Nvidia GeForce GTX 1080 Ti GPU with ? 11 Gb of memory, using the Adam optimizer (Kingma and <ref type="bibr">Ba, 2015)</ref> with a linearly decayed learning rate of 5 ? 10 ?5 , for 5 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Additional experiments</head><p>We also perform language modeling experiments on the Wikitext-103 dataset 11 <ref type="bibr" target="#b9">(Merity et al., 2017)</ref>  which has a training set with 103 million tokens and validation and test sets with 217,646 and 245,569 tokens, respectively. For that, we follow the standard architecture of the transformer-XL <ref type="bibr">(Dai et al., 2019)</ref>, which consists of a transformer with 16 layers and 10 attention heads. For the transformer-XL, we experiment with a memory of size 150. For the compressive transformer, we consider that both memories have a size of 150 and a compression rate of 4. For the ?-former we consider a shortterm memory of size 150, a continuous long-term memory with 150 Gaussian RBFs, and a memory threshold of 900 tokens.</p><p>For this experiment, we use a transformer with 16 layers, 10 heads, embeddings of size 410, and a feed-forward hidden size of 2100. For the compressive transformer, we follow <ref type="bibr" target="#b16">Rae et al. (2019)</ref> and use a compression rate of 4 and the attention reconstruction auxiliary loss. For the ?-former we consider 150 Gaussian RBFs N (t;?,? 2 ) with? linearly spaced in [0, 1] and? ? {.01, .05}. We set ? = 0.5 and for the KL regularization we used ? KL = 1 ? 10 ?5 and ? 0 = 0.1.</p><p>We trained all models, from scratch, with batches of size 40 for 250,000 steps on 1 Nvidia Titan RTX or 1 Nvidia Quadro RTX 6000 with ? 24 GPU Gb of memory using the Adam optimizer (Kingma and <ref type="bibr">Ba, 2015)</ref> with a learning rate of 2.5 ? 10 ?4 . The learning rate was decayed to 0 until the end of training with a cosine schedule.  Results. As can be seen in <ref type="table" target="#tab_7">Table 3</ref>, extending the model with a long-term memory leads to a better perplexity, for both the compressive transformer and ?-former. Moreover, the ?-former slightly outperforms the compressive transformer. We can also see that using sticky memories leads to a somewhat lower perplexity, which shows that it helps the model to focus on the relevant memories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STM LTM Perplexity</head><p>Analysis. To better understand whether ?former is paying more attention to the older memories in the LTM or to the most recent ones, we plotted a histogram of the attention given to each region of the long-term memory when predicting the tokens on the validation set. As can be seen in <ref type="figure">Fig. 7</ref>, in the first and middle layers, the ?-former tends to focus more on the older memories, while in the last layer, the attention pattern is more uniform. In <ref type="figure">Figs. 8 and 9</ref>, we present examples of words for which the ?-former has lower perplexity than the transformer-XL along with the attention given by the ?-former to the last layer's LTM. We can see that the word being predicted is present sev-eral times in the long-term memory and ?-former gives higher attention to those regions. To know whether the sticky memories approach attributes a larger space of the LTM's signal to relevant phrases or words, we plotted the memory space given to each word 12 present in the longterm memory of the last layer when using and not using sticky memories. We present examples in Figs. 10 and 11 along with the phrases / words which receive the largest spaces when using sticky memories. We can see in these examples that this procedure does in fact attribute large spaces to old memories, creating memories that stick over time.</p><p>We can also see that these memories appear to be relevant as shown by the words / phrases in the examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Additional examples</head><p>In <ref type="figure" target="#fig_0">Fig. 12, we</ref> show additional examples of utterances generated by ?-former along with the excerpts from the LTM that receive higher attention throughout the utterances' generation.</p><p>Additionally, ground truth conversations concerning the movies "Toy Story" and "La La Land", for which the sticky memories are stated in <ref type="figure" target="#fig_3">Fig. 5</ref>, are shown in Tables 4 and 5, respectively.   <ref type="figure">Figure 9</ref>: Example of attention given by ?-former to the last layer's long-term memory, when predicting the ground truth word "Headlam". The words in the long-term memory which receive higher attention (bigger than 0.05) are shaded.</p><p>Phrases / words: "transport gasoline" | "American Civil Rigths" | "along with Michael" | "community center" | "residents began to move" | "Landmarks Comission" | "Meridian Main" | "projects" | "the historic train station" | "Weidmann's Restaurant" | "Arts" | "Meridian Main Street" | "in late 2007" | "effort" | "Alliance serves" | "Plans were underway" | "Building" | "Mayor Cheri" | "the Alliance" | "promote further development" | "assist businesses" | "Street program" <ref type="figure" target="#fig_0">Figure 10</ref>: Example of the memory space attributed to each word in the last layer's long-term memory (after 5 updates) without / with the sticky memories procedure, along with the words / phrases which have the largest memory spaces when using sticky memories (top peaks with space&gt; .005). Excerpt of the sequence being generated in this example: "Given Meridian's site as a railroad junction, its travelers have attracted the development of many hotels." The dashed vertical lines represent the limits of the memory segments for the various memory updates.</p><p>Phrases / words: "July 1936" | "Headlam continued to serve" | "27 March" | "in Frankston" | "daugther" | "four of its aircraft" | "in response to fears of Japanese" | "stationed at Darwin" | "attacked the Japanese" | "forced it aground" | "dispersed at Penfui" | "three Japanese floatplanes" | "attacked regularly" | "withdrawn to Darwin" | "his staff remained at Penfui" | "ordered to evacuate" | "assistance from Sparrow Force" | "Four of No. 2 Squadron's Hudsons were destroyed" | "relocated to Daly Waters" <ref type="figure" target="#fig_0">Figure 11</ref>: Example of the memory space attributed to each word in the last layer's long-term memory (after 5 updates) without / with the sticky memories procedure, along with the words / phrases which have the largest memory spaces when using sticky memories (top peaks with space&gt; .005) Excerpt of the sequence being generated in this example: ... <ref type="figure" target="#fig_0">Figure 12</ref>: Examples of answers generated by ?-former on a dialogue about the movie "The Social Network". The excerpts from the LTM that are more attended to throughout their generation are highlighted on each color correspondingly.</p><p>-Hi -Yo you really need to watch Toy Story. It has 100% on Rotten Tomatoes! -Really! 100% that's pretty good What's it about -It's an animated buddy-comedy where toys come to life -who stars in it -The main characters are voiced by Tom Hanks and Tim Allen -does it have any other critic ratings -Yep, metacritic gave it 95/100 and Cinemascore gave it an A -how old is it? -It's a 1995 film so 23 years -The old ones are always good :) I heard there were some sad parts in it is that true -Yeah actually, the movie starts off pretty sad as the toys fear that they might be replaced and that they have to move -is this a disney or dreamworks movie -Disney, pixar to be exact -Why do the toys think they will be replaced :( -they thought so because Andy was having a birthday party and might get new toys -What part does Tom Hanks play -Woody, the main character -How about Tim Allen -Buzz, the main antagonist at first then he becomes a friend -What kind of toy is Woody? -A cowboy doll -What is Buzz -A space ranger -so do all the toys talk -yep! but andy doesn't know that -Is andy a little kid or a teen -He's 6! -Sounds good. Thanks for the info. Have a great day <ref type="table">Table 4</ref>: Ground truth conversation about movie "Toy Story".</p><p>-hey -hey -i just watched la la land. It is a movie from 2016 starring ryan gosling and emma stone. they are too artists (one actress and one panist) and they fall in love and try to achieve their dreams. its a great movie -It's a wonderful movie and got a score of 92% on rotten tomatoes -yes, i think it also won an oscar -Yes but I thought it was a little dull -metacritics rating is 93/100 as well its pretty critically acclaimed -the two leads singing and dancing weren't exceptional -i suppose it is not for everyone -It also sags badly in the middle I like how Sebastian slipped into a passionate jazz despite warnings from the owner.</p><p>-what do you think of the cover of "i ran so far away?" in the movie, sebastian found the song an insult for a serious musician -I don't know, it is considered an insult for serious musicians not sure why -yeah -The idea of a one woman play was daring -it was interesting how sebastian joined a jazz fusion band he couldnt find real happiness in any of the bands he was in its hard -It is considering she didn't know of any of that until she attended one of his concerts -yeah, that is daring the movie kind of speaks to a lot of people. she accussed him of abandoning his dreams but sometimes thats what you have to do.</p><p>-Not nice that she leaves because he told her she liked him better when he was unsuccessful The play was a disaster so he didn't miss anything when he missed it.</p><p>-yeah, but i dont blame her for dumping him for that -She should didn't want to support him and she had to move back -id be pretty upset as well to boulder city nevada -yes she didn't want to forgive him, I didn't understand that -well because that was a big deal to her and he missed it -if she was with him when he was unsuccessful, she could have supported him to follow his dreams or other dreams -i suppose that is true -she wasn't successful either -yeah she wasnt nobody showed up to her play -so why the big hulabaloo about him -not sure -she was selfish I guess He missed her play because he had to go for a photo shoot with the band that he had previously missed -yeah but he should have kept better track and scheduled it better -this shows that he was trying to commit some and follow his dreams although not necessarily like them so she would be please if he didn't attend the photo shoot a second time, and came to her show -i definitely felt bad for both of them though in that scene -it's more of a do or don't he is still condemned I feel bad for him because he tried he tried to get her back by apologizing as well she didn't want any of it -yeah because she felt like he didnt care enough because he missed it he's the one that suggested the one woman play -They could have started all over again just like the beginning -maybe so -did she fail because of the one-woman play? she could have tried something else if she felt that -she wanted to give it a shot -she did and it failed, he did and it failed they just had to compromise so they could be together again, which was how the happiness was He signed up for the band after hearing her talking to her mom about how he is working -on his career I think he did a lot for her <ref type="table">Table 5</ref>: Ground truth conversation about movie "La La Land".</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>?-former's attention diagram with sequence of text, X t , of size L = 2 and STM of size L STM = 2. Circles represent input embeddings or hidden states (depending on the layer) for head h and query i. Both the self-attention and the attention over the LTM are performed in parallel for each head and query.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Left: Sorting task accuracy for sequences of length 4,000, 8,000, and 16,000. Right: Sorting task accuracy vs regression mean error, when varying the number of basis functions, for sequences of length 8,000.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Examples of answers generated by ?-former on a dialogue about the movie "Home Alone". The excerpts from the LTM that are more attended to throughout the utterances generation are highlighted on each color, correspondingly.Toy Story: Tom Hanks as Woody | animated buddy comedy | Toy Story was the first feature length computer animated film | produced by Pixar | toys pretend to be lifeless whenever humans are present | focuses on the relationship between Woody and Gold | fashioned pull string cowboy doll La La Land: Ryan Gosling | Emma Stone as Mia | Hollywood | the city of Los Angeles | Meta critics: 93/100 | 2016 | During a gig at a restaurant Sebastian slips into a passionate jazz | despite warning from the owner | Mia overhears the music as she passes by | for his disobedience</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Phrases that hold larger spaces of the LTM, when using sticky memories, for two dialogue examples (in App. E).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(reversed) kernel trick(Katharopoulos et al., 2020;  Choromanski et al., 2021;<ref type="bibr" target="#b11">Peng et al., 2021;</ref> Jaegle et al., 2021). Closer to our approach are the transformer-XL and compressive transformer models(Dai et al., 2019;<ref type="bibr" target="#b16">Rae et al., 2019)</ref>, which extend the vanilla transformer with a bounded memory.Memory-augmented language models. RNNs, LSTMs, and GRUs(Hochreiter et al., 1997; Cho  et al., 2014)  have the ability of keeping a memory state of the past. However, these require backpropagation through time which is impractical for long sequences. Because of this, Graves et al. (2014), Weston et al. (2014), Joulin and Mikolov (2015) and Grefenstette et al. (2015) proposed extending RNNs with an external memory, while Chandar et al. (2016) and Rae et al. (2016) proposed efficient procedures to read and write from these memories, using hierarchies and sparsity. Grave et al. (2016) and Merity et al. (2017) proposed the use of cache-based memories which store pairs of hidden states and output tokens from previous steps. The distribution over the words in the memory is then combined with the distribution given by the language model. More recently, Khandelwal et al. (2019) and Yogatama et al. (2021) proposed using nearest neighbors to retrieve words from a keybased memory constructed based on the training data. Similarly, Fan et al. (2021) proposed retrieving sentences from a memory based on the training data and auxiliary information. Khandelwal et al. (2019) proposed interpolating the retrieved words probability distributions with the probability over the vocabulary words when generating a new word, while Yogatama et al. (2021) and Fan et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Sticky memories procedure diagram. The dashed vertical lines correspond to the position of the words in the LTM signal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Histograms of attention given to the LTM by ?-former, for the first (on the left), middle (on the middle), and last (on the right) layers. The dashed vertical lines represent the limits of the memory segments (? ) for the various memory updates.the Pet Shop Boys' synthpop cover of the song (titled "Where the Streets Have No Name (I Can't Take My Eyes off You) "). Bono parodied this by occasionally adopting the deadpan vocal style used in the Pet Shop Boys' cover. Critics welcomed the song in the group's setlist: The Independent said the song "induces instant euphoria, as U2 do what they're best at, slipping into epic rock mode, playing music made for the arena". In two other local newspaper reviews, critics praised the song's inclusion in a sequence of greatest hits. For the PopMart Tour of 1997-1998, U2 returned to the electronic dance arrangement they occasionally played on the Zoo TV Tour. The set's massive video screen displayed a video that Hot Press described as an "astonishing 2001-style trip into the heart of a swirling, psychedelic tunnel that sucks the audience in towards a horizontal monolith". Near the end of the song, peace doves were shown on the screen and bright beams of light flanking the set's golden arch were projected upwards. Hot Press said the effect transformed the stadium into a "UFO landing site". Shortly before the third leg of the Elevation Tour, the September 11 attacks occurred in New York City and Washington D.C.. During the band's first show in New York City following the attacks, the band performed "Where the Streets Have No Name", and when the stage lights illuminated the audience, the band saw tears streaming down the faces of many fans. The experience was one inspiration for the song "City of Blinding Lights". The band paid tribute to the 9/11 victims during their performance of the song at the Super Bowl XXXVI halftime show on 3 February 2002. The performance featured the names of the September 11 victims projected onto a large white banner behind the band. U2's appearance was later ranked number 1 on Sports Illustrated's list of "Top 10 Super Bowl Halftime Shows". For the Vertigo Tour, the group originally considered dropping the song from their setlists, but Mullen and Clayton successfully argued against this. All 131 of the Vertigo Tour concerts featured a performance of the song, which were accompanied by the stage's LED video curtains displaying African flags. On the tour's opening night, this reminded Bono that he had GT: as the respective audio releases of the latter two concerts, Zoo TV Live and Hasta la Vista Baby! U2 Example of attention given by ?-former to the last layer's long-term memory, when predicting the ground truth word "U2". The words in the LTM which receive higher attention (&gt; 0.05) are shaded.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>learnable projection matrices, d = e /H, and H is the number of heads. Then, the context representation Z</figDesc><table /><note>h ? R L?d , that corresponds to each attention head h, is obtained as:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Perplexity on Wikitext-103 and PG19.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Results on CMU-DoG dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>proposed the use Cast: Macaulay Culkin as Kevin. Joe Pesci as Harry. Daniel Stern as Marv. John Heard as Peter. Roberts Blossom as Marley. ... The film stars Macaulay Culkin as Kevin McCallister, a boy who is mistakenly left behind when his family flies to Paris for their Christmas vacation. Kevin initially relishes being home alone, but soon has to contend with two would-be burglars played by Joe Pesci and Daniel Stern. The film also features Catherine O'Hara and John Heard as Kevin's parents. Or maybe rent, anything is reason to celebrate..I would like to talk about a movie called "Home Alone" Answer: Macaulay Culkin is the main actor and it is a comedy. That sounds like a great movie. Any more details? Answer: The screenplay came out in 1990 and has been on the air for quite a while. Movie Name: Home Alone. Rating: Rotten Tomatoes: 62% and average: 5.5/10, Metacritic Score: 63/100, CinemaScore: A. Year: 1990. The McCallister family is preparing to spend Christmas in Paris, gathering at Peter and Kate's home outside of Chicago on the night before their departure. Peter and Kate's youngest son, eightyear-old Kevin, is being ridiculed by his siblings and cousins. A fight with his older brother, Buzz, results in Kevin getting sent to the third floor of the house for punishment, where he wishes that his ...</figDesc><table><row><cell>Previous utterance: Previous utterance:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Perplexity on Wikitext-103.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>for the first time on 26 January 1942, and attacked regularly thereafter, damaging some aircraft. The intact Hudsons were withdrawn to Darwin but Headlam</figDesc><table><row><cell>GT:</cell><cell></cell><cell></cell></row><row><cell>and fixed defences, Australia may be made practically</cell><cell>three Avro Ansons, one of which was piloted by Headlam,</cell><cell>as the station navigation officer. On 27 March he was</cell></row><row><cell>invulnerable". According to Air Force historian Alan</cell><cell>in November. The following month, Headlam led the three</cell><cell>posted to the staff of RAAF Headquarters, Melbourne. He</cell></row><row><cell>Stephens this paper "in effect, defined the anti-lodgment</cell><cell>Ansons on a six-day journey back and forth over Central</cell><cell>was promoted to squadron leader on 1 June 1940. Two</cell></row><row><cell>concept which has been a persistent feature of RAAF</cell><cell>Australia. He subsequently passed the navigation course</cell><cell>weeks later he married Katherine Bridge at St Paul's</cell></row><row><cell>strategic thinking".</cell><cell>with a special distinction. On 27 January 1939 he was</cell><cell>Anglican Church in Frankston; the couple would have a son</cell></row><row><cell>Headlam, completed a flying instructors course in July 1936</cell><cell>posted to RAAF Station Laverton, Victoria, as a flight</cell><cell>and a daughter. Headlam was given command of No. 2</cell></row><row><cell>and joined the staff of No. 1 FTS. He was promoted to flight</cell><cell>commander. He served initially with No. 2 Squadron, before</cell><cell>Squadron at Laverton on 15 April 1941, and raised to wing</cell></row><row><cell>lieutenant on 1 March 1937. Commencing in July 1938, he</cell><cell>transferring to No. 1 Squadron on 29 August. Both units</cell><cell>commander on 1 July. Equipped with Lockheed Hudsons,</cell></row><row><cell>was one of six students to take part in the RAAF's first Long</cell><cell>operated Ansons.</cell><cell>the squadron mainly conducted maritime patrols in</cell></row><row><cell>Specialist Navigation Course, run by Flight Lieutenants Bill</cell><cell>World War II</cell><cell>southern waters until 5 December, when four of its aircraft</cell></row><row><cell>Garing and Alister Murdoch at Point Cook. The course</cell><cell>Following the outbreak of World War II, No. 1 Squadron</cell><cell>were ordered to Darwin, Northern Territory, in response to</cell></row><row><cell>involved several epic training flights that attracted</cell><cell>was engaged in convoy escort and maritime</cell><cell>fears of Japanese aggression in the Pacific. On 7</cell></row><row><cell>considerable media attention, including a twelve-day,</cell><cell>reconnaissance duties off south-eastern Australia.</cell><cell>December, this detachment established itself at Penfui,</cell></row><row><cell>10,800-kilometre (6,700 mi) round-Australia trip by</cell><cell>Headlam continued to serve with the squadron as a flight</cell><cell>near Koepang in Dutch Timor, while No. 2 Squadron's eight</cell></row><row><cell></cell><cell>commander until 15 January 1940, when he was assigned</cell><cell>remaining Hudsons</cell></row><row><cell></cell><cell>to Headquarters Laverton</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>"Headlam became Officer Commanding North-Western Area in January 1946. Posted to Britain at the end of the year, he attended the Royal Air Force Staff College, Andover, and served with RAAF Overseas Headquarters, London." The dashed vertical lines represent the limits of the memory segments for the various memory updates. : Jesse Eisenberg as Mark Zuckerberg. Andrew Garfield as Eduardo Saverin. Justin Timberlake as Sean Parker. Armie Hammer as Cameron and Tyler Winklevoss. Max Minghella as Divya Narendra. Critical Response: David Fincher's film has the rare quality of being not only as smart as its brilliant hero, but in the same way. It is cocksure, impatient, cold, exciting and instinctively perceptive. The Social Network is the movie of the year ...</figDesc><table><row><cell cols="2">CastPrevious utterance: So, what movie are we going to In October 2003, 19-year-old Harvard University</cell></row><row><cell>student Mark Zuckerberg is dumped by his girlfriend</cell><cell>chat about today? Right, the one about Zuckerberg?</cell></row><row><cell>Erica Albright. Returning to his dorm, Zuckerberg</cell><cell></cell></row><row><cell>writes an insulting entry about Albright on his</cell><cell>Answer: Yep, Jesse Eisenberg plays Zuckerberg.</cell></row><row><cell>LiveJournal blog and then creates a campus website</cell><cell></cell></row><row><cell>called Facemash by hacking into college databases to</cell><cell>Previous utterance: So, have you seen it?</cell></row><row><cell>steal photos of female students, then allowing site</cell><cell></cell></row><row><cell>visitors to rate their attractiveness. After traffic to the</cell><cell>Answer: Yeah. Its about the founder of Facebook,</cell></row><row><cell>site crashes parts of Harvard's computer network,</cell><cell>Mark Zuckerberg who was basically dumped by his</cell></row><row><cell></cell><cell>girlfriend, Erica, so he created "TheFacebook."</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The code is available at https://github.com/ deep-spin/infinite-former.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Parameter weights are not shared between layers.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">See App.D for further experiments on language modeling.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">We use the authors' implementation available at https: //github.com/kimiyoung/transformer-xl.7  We use our implementation of the model.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">Dataset available at https://github.com/deepmind/pg19. 10 Dataset available at https://github.com/festvox/datasets-CMU_DoG.11 Dataset available at https://blog.einstein.ai/the-wikitextlong-term-dependency-language-modeling-dataset/.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">The (Voronoi) memory space attributed to each word is half the distance from the previous word plus half the distance to the next word in the LTM's signal, being the word's location computed based on the sampled positions from which we evaluate the signal when receiving new memory vectors.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the European Research Council (ERC StG DeepSPIN 758969), by the P2020 project MAIA (contract 045909), by the Funda??o para a Ci?ncia e Tecnologia through project PTDC/CCI-INF/4703/2021 (PRELUNA, contract UIDB/50008/2020), by the EU H2020 SELMA project (grant agreement No 957017), and by contract PD/BD/150633/2020 in the scope of the Doctoral Program FCT -PD/00140/2013 NET-SyS. We thank Jack Rae, Tom Schaul, the SAR-DINE team members, and the reviewers for helpful discussion and feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cadherins: actin with the cytoskeleton to form synapses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shernaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bamji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">METEOR: An automatic metric for MT evaluation with improved correlation with human judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satanjeev</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</title>
		<meeting>Workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Adaptive multivariate ridge regression. The Annals of Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">V</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zidek</surname></persName>
		</author>
		<idno type="DOI">https:/projecteuclid.org/journals/annals-of-statistics/volume-8/issue-1/Adaptive-Multivariate-Ridge-Regression/10.1214/aos/1176344891.full</idno>
		<imprint>
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long-Lasting Verbatim Memory for the Words of Books After a Single Reading Without Any Learning Intention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Kuhbandner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Workshop on Automatic Summarization</title>
		<meeting>Workshop on Automatic Summarization</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sparse and Continuous Attention Mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>Andr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcos</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ant?nio</forename><surname>Treviso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Farinhas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niculae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>M?rio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro Mq</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aguiar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pointer Sentinel Mixture Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cognitive flexibility and long-term depression (LTD) are impaired following ?-catenin stabilization in vivo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fergil</forename><surname>Mills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><forename type="middle">B</forename><surname>Dissing-Olesen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacek</forename><surname>Wisniewska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuznicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><forename type="middle">Tian</forename><surname>Macvicar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shernaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bamji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the National Academy of Sciences</title>
		<meeting>of the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Random Feature Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Train short, test long: Attention with linear biases enables input length extrapolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lewis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Tim Salimans, and Ilya Sutskever</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scaling memoryaugmented neural networks with sparse reads and writes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">J</forename><surname>Jack W Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Compressive Transformers for Long-Range Sequence Modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Jack W Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potapenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient content-based sparse attention with routing transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="53" to="68" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
