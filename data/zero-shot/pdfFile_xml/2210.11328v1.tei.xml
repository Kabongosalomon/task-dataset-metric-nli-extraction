<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PLAY IT BACK: ITERATIVE ATTENTION FOR AUDIO RECOGNITION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Stergiou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronics and Informatics</orgName>
								<orgName type="institution">Vrije University of Brussels</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Bristol</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PLAY IT BACK: ITERATIVE ATTENTION FOR AUDIO RECOGNITION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Audio classification</term>
					<term>playback</term>
					<term>attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A key function of auditory cognition is the association of characteristic sounds with their corresponding semantics over time. Humans attempting to discriminate between finegrained audio categories, often replay the same discriminative sounds to increase their prediction confidence. We propose an end-to-end attention-based architecture that through selective repetition attends over the most discriminative sounds across the audio sequence. Our model initially uses the full audio sequence and iteratively refines the temporal segments replayed based on slot attention. At each playback, the selected segments are replayed using a smaller hop length which represents higher resolution features within these segments. We show that our method can consistently achieve state-of-theart performance across three audio-classification benchmarks: AudioSet, VGG-Sound, and EPIC-KITCHENS-100. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Audio recognition is the task of categorizing audio with discrete labels that semantically represent the emitted sounds. This includes significant challenges considering the similarity in object sounds (e.g. boat motors and road vehicles), musical instruments (e.g. guitar, banjo, and ukulele), human (e.g. wail and groan), or animal (e.g. yip and growl) sounds.</p><p>In everyday life, we repeat parts of songs or ask for someone to repeat themselves to better understand audio. This relates to the development of echoic memory which is responsible for the memorization of sounds <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Therefore, repeated listens and replays of sound stimulants <ref type="bibr" target="#b2">[3]</ref> are an essential part of learning and associating sound patterns.</p><p>Driven by the perception of sound through echoic memory and the recent success of Vision Transformers (ViT) <ref type="bibr" target="#b3">[4]</ref> at utilizing global context information, we propose an endto-end attention-based model that learns to recognize sounds through discovering and playing back the most informative sounds from the audio sequence, as shown in <ref type="figure">Figure 1</ref>. We <ref type="figure">Fig. 1</ref>: Playback of discriminative sounds. Given an audio sequence, the most relevant sounds are selected and played back with a reduced hop length. The generated playbacks attend solely to informative sounds at a higher temporal resolution, effectively slowing back the representation. use slots <ref type="bibr" target="#b4">[5]</ref> to attend to category-relevant sounds in the input sequence. These slots select time segments to be replayed. Coarser features from earlier playbacks are memorized alongside finer (i.e. higher-temporal resolution) features from later playbacks with the use of a transformer decoder.</p><p>Our contributions are as follows: i) We propose to select and replay relevant audio features with decreased hop lengths, slowing down relevant parts of the audio. ii) We propose an end-to-end transformer architecture for audio recognition that jointly selects and attends to multiple audio replays, and refines the final class predictions. iii) Our method achieves state-of-the-art performance on AudioSet <ref type="bibr" target="#b5">[6]</ref>, VGG-Sound <ref type="bibr" target="#b6">[7]</ref>, and EPIC-KITCHENS-100 <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Audio recognition. A popular approach for audio classification has been the use of convolutional networks, previously used for image-based object recognition <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref> or video classification <ref type="bibr" target="#b11">[12]</ref> tasks, to learn features from audio spectrograms. The recent introduction of Transformer-based architectures has further given rise to works that aim to adapt  chitectures <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>. Similar attempts have also built on image-pretrained Transformer models for attending audio spectrograms <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. <ref type="bibr" target="#b17">[18]</ref> incorporated an additional video modality to improve performance. Recently, <ref type="bibr" target="#b18">[19]</ref> have also studied the effects of different hop lengths on the temporal resolution of spectrograms.</p><p>In contrast to the majority of previous works, we focus on identifying relevant and irrelevant sounds in the audio. The irrelevant sounds are removed, while the relevant audio segments are slowed and replayed with predictions calculated across all playbacks. Selecting discriminative features. Modeling discriminative features has been a central focus of image recognition methods. <ref type="bibr" target="#b19">[20]</ref> generated features from multiple scales selecting the best-suited features per scale. <ref type="bibr" target="#b20">[21]</ref> proposed the aggregation of features from image regions cropped based on class saliencies. <ref type="bibr" target="#b21">[22]</ref> applied distortion grids on the cropped regions. Most similar to our work, <ref type="bibr" target="#b22">[23]</ref> used a recurrent CNN to select image regions that are attended to in follow-up scales. In their work, only a single region was selected per scale. Instead, we identify multiple discriminative sound segments which we combine to form the next playback.</p><p>We describe our PlayItBack method next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">METHOD</head><p>In this section, we describe the proposed PlayItBack architecture, depicted in <ref type="figure" target="#fig_1">Figure 2</ref>. We compute the mel-spectrogram for a given audio sequence resulting in an F?T representation of frequency F and time T , and extract k non-overlapping patches. We project the patches to feature tokens x i ? R D , where D = F T . A transformer encoder B is used to encode these into features z i . Slot attention G is applied to z i to select the discriminative regions which, at the next playback, will be slowed by decreasing the hop length in the spectrogram. The Decoder D then relates features across playbacks. Transformer Encoder. Given the linear projections x i , we use frequency and temporal patch positional encodings P . The encoder network B extracts feature representations for each playback,</p><formula xml:id="formula_0">z i = B(x i ) ? R d?C , where d &lt; F T .</formula><p>Slot attention. We use slot attention <ref type="bibr" target="#b4">[5]</ref> G to iteratively map the resulting feature vectors z i from each playback to two slot vectors s lj corresponding to the informative s 1j and uninformative s 2j temporal segments of the audio input respectively. We use j ? {1, ..., J} to denote slot iterations. The query</p><formula xml:id="formula_1">Q lj = M LP (LN (s lj?1 )), key K lj = M LP (LN (z i )) and value V lj = M LP (LN (z i ))</formula><p>are computed to map the features z i and slots s lj vectors to a common dimension d. We set the softmax temperature based on a fixed value</p><formula xml:id="formula_2">? d. h lj = GRU a lj V lj m?{1,2} a mj , where a lj = Attn K lj Q T lj ? d<label>(1)</label></formula><p>A Gated Recurrent Unit (GRU) with two hidden units is used at each slot iteration updating the slot hidden states h lj as in <ref type="bibr" target="#b4">[5]</ref>. A linear transformation alongside a residual connection is used for the slots s lj = s lj?1 + M LP (LN (h lj )).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Backbone</head><p>Train set mAP Audio-only models MAE-AST <ref type="bibr" target="#b23">[24]</ref> ViT-B <ref type="bibr" target="#b3">[4]</ref> mini-AS 30.6 Perceiver <ref type="bibr" target="#b24">[25]</ref> Perceiver AS-2M 38.4 Conformer <ref type="bibr" target="#b13">[14]</ref> Conformer AS-2M 41.1 PANN <ref type="bibr" target="#b9">[10]</ref> ResNet38 <ref type="bibr" target="#b25">[26]</ref> AS-2M 43.4 MBT <ref type="bibr" target="#b17">[18]</ref> ViT-B AS-500K 44.3 PSLA <ref type="bibr" target="#b8">[9]</ref> EffNet-B2 <ref type="bibr" target="#b26">[27]</ref> AS-2M 44.4 PaSST <ref type="bibr" target="#b16">[17]</ref> DeiT-B <ref type="bibr" target="#b27">[28]</ref> AS-2M 47.1 HTS-AT <ref type="bibr" target="#b15">[16]</ref> Swin-T <ref type="bibr" target="#b28">[29]</ref> AS-2M 47.1 MaskSpec <ref type="bibr" target="#b29">[30]</ref> ViT</p><formula xml:id="formula_3">-B AS-2M 47.1 Audio-MAE [31] ViT-B AS-2M 47.3 PlayItBackX3</formula><p>MViTv2-B <ref type="bibr" target="#b31">[32]</ref> AS-500K 47.7  We train the two slots so s 1 attends to informative audio while s 2 captures the remaining audio. This is achieved by combining G(z i ) 1 = s 1 and the inverse of the uninformative slot G(z i ) 2 = s 2 to create the attention matrix:</p><formula xml:id="formula_4">M = Attn(G(z i ) T 1 G(z i ) ?1 2 )</formula><p>. We normalize and rescale the main diagonal diag(M) by interpolation so that it matches the temporal dimension of x i . Activations above the normalized average (&gt; 0.5) are selected for the segments in x i+1 . Transformer Decoder. Given the extracted encoder features z i , the decoder transformer D relates information across playbacks. Positional encodings based on patches and the playback number are added to z i . Considering the iterative nature of the PlayItBack model, cross-attending <ref type="bibr" target="#b24">[25]</ref> information over playbacks enables the model to retain general features and associate patterns that are common. For the decoder, we define the query from the previous playback as Q i = M LP (LN (v i )), where v 1 is initialized with a latent vector then updated at each playback v i = D(z i?1 , v i?1 ), where i &gt; 1, key K i = M LP (LN (z i )) and value V i = M LP (LN (z i )) for the cross attention. This is followed by a self-attention block. The decoder features are then passed to a classifier shared across playbacks. Classification and rank loss. We use an inter-playback weighted ranking loss L rank(i) for forcing the network to attain more confident predictions in later playbacks. The ranking loss L rank(i) , uses the pair-wise class probabilities p(?) i and p(?) m ?m ? {1, ..., i ? 1} for the correct class label ?. We compute the probability difference L rank(m?i) between the ith playback and all previous playbacks.</p><formula xml:id="formula_5">L rank(i) = i?1 m=1 ? m max(0, ? ? p(?) i + p(?) m )<label>(2)</label></formula><p>The ranking loss thus uses predictions from the previous playbacks as a reference with the expectation that p(?) i &gt; p(?) m + ?, i.e. subsequent playbacks always increase confi-dence, where ? is the ranking loss's margin. For stability in training, we include a weight ? m = 1 i?m computed based on the difference between the playback indices.</p><p>We combine L rank(i) with an inter-playback classification loss (Cross-entropy) L CLS(i) and define our multi-task loss as:</p><formula xml:id="formula_6">L = L CLS(1) + N i=2 ? L CLS(i) + (1 ? ?) L rank(i)<label>(3)</label></formula><p>where ? is a weighting parameter for the aggregation of the cross-entropy and ranking losses. During inference, our model uses the average of all predictions across playbacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head><p>Datasets We evaluate our proposed PlayItBack architecture on three large-scale datasets. AudioSet <ref type="bibr" target="#b5">[6]</ref> is composed of 2M 10s audio clips from YouTube annotated with 527 classes (AS-2M). Because of the high imbalance of the dataset, we instead train with a proposed subset consisting of 500K (AS-500K) clips <ref type="bibr" target="#b17">[18]</ref>. VGG-Sound <ref type="bibr" target="#b6">[7]</ref> consists of 200k clips of 10s length with 309 labels corresponding to human actions, objects and interactions. EPIC-KITCHENS-100 <ref type="bibr" target="#b7">[8]</ref> includes 90k untrimmed clips of hand-object interactions labeled with 97 verb, 300 noun classes, and 4025 action classes. The average action length is 2.6s. Evaluation metrics. For AudioSet along the lines of previous works, we use the mean average precision (mAP). For VGG-Sound as <ref type="bibr" target="#b11">[12]</ref> we report the top-1/5 % accuracies as well as the mAP, AUC, and d-prime. For EPIC-KITCHENS-100 we report the top-1/5 % accuracies for the verb, noun, and action labels on the validation set. Implementation details. We use PlayItBackX3 with N=3 as our model for comparative evaluation, with ablations showcasing that this produces the best accuracy (top-1)/compute  <ref type="table">Table 3</ref>: Comparisons to state-of-the-art for EPIC-KITCHENS-100. We report the top-1 and top-5 accuracies (%) for the verb, noun, and action labels.</p><p>(GFLOPs) trade-off. We use the 24-layer MViTv2-B <ref type="bibr" target="#b31">[32]</ref> as our default encoder 2 . For all experiments, we set the ranking margin ? = 0.05, J = 3 slot iterations, and ? = 0.7. As in <ref type="bibr" target="#b11">[12]</ref> we use spectrograms with frequency dimension of 128 corresponding to inputs of size 128?100S for S seconds of audio. We train for 50 epochs with Mixup <ref type="bibr" target="#b35">[36]</ref> (? = 0.3) and base learning rate of 0.5 for AudioSet and 0.01 for VGG-Sound &amp; EPIC-KITCHENS-100. We use warm-up for the first 2.5 epochs, a decayed cosine schedule, batch size of 64 with SGD, momentum set to 0.9, and 1e ?4 weight decay.</p><p>Results. We compare PlayItBack to current state-of-theart models on AudioSet in <ref type="table" target="#tab_0">Table 1</ref>. Despite only using the smaller sub-set AS-500K for training, PlayItBackX3 achieves the best performance in comparison to other models. Additionally, compared to HTS-AT <ref type="bibr" target="#b15">[16]</ref> and PaSST <ref type="bibr" target="#b16">[17]</ref> which use a higher sampling frequency of 32kHz, PlayItBack uses a sampling frequency of 16kHz, demonstrating that choosing to replay audio segments of higher relevance can be a better performing strategy than increasing the number of samples per second over the entire audio sequence. We report results on VGG-Sound in <ref type="table" target="#tab_1">Table 2</ref>. Our proposed PlayItBackX3 performs favorably to previous indomain models trained and using solely audio data. In comparison to the previously top-performing SlowFast model <ref type="bibr" target="#b11">[12]</ref>, we observe a +1.3% improvement over the top-1 accuracy. Our model is only outperformed by the multi-modal (audiovisual) version of MBT (AV) <ref type="bibr" target="#b17">[18]</ref>. However, PlayItBackX3 outperforms MBT (A) in the audio-only setting.</p><p>In <ref type="table">Table 3</ref>, we compare to audio-classification methods on EPIC-KITCHENS-100. We observe that the relative improvement in performance varies across datasets (higher performance gains are observed in AudioSet and VGG-Sound, while somewhat smaller on EPIC-KITCHENS-100). We believe that this is due to the dataset annotations, as AudioSet and VGG-Sound include sequences of 10s and 5s on average, whilst EPIC-KITCHENS-100 uses variable-length and shortterm action annotations with an average length of 2.6s. Even in such settings, PlayItBackX3 demonstrates a moderate but consistent performance improvement. Ablations. <ref type="table" target="#tab_4">Table 4</ref> compares the performance achieved from different numbers of slot iterations J on VGG-Sound with <ref type="bibr" target="#b1">2</ref> The flatten vector size is d=50 and the number of features is C=768   PlayItBackX3 alongside the additional computations. In general, we observe that moderate performance improvements can be achieved with an increase in the number of slot attention iterations. The added computations also remain moderate with +2.6 GFLOPs from J = 1 to J = 3.</p><p>In <ref type="figure" target="#fig_2">Figure 3</ref>, we investigate the impact that the number of playbacks (N ) has on the model performance and the computational overhead. We use a decoder-only model (N = 0), alongside PlayItBackXN as N increases from 1 to 5 on VGG-Sound. We observe that there is a clear performance difference across the number of playbacks. The model shows a clear improvement in performance for 1 ? N ? 3. As N is further increased, the performance drops. This is due to the increase in the model's complexity as well as the challenge of discovering salient information in very deep playbackssmall hop lengths (and thus very low speeds of playback) can decrease the discriminative capability of the model. As showcased in this figure and all results, N = 3 offers the best performance. These results are consistent across datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>We propose an end-to-end attention-based architecture for audio recognition. Our PlayItBack model uses information from the full audio sequence to iteratively discover audio segments that are relevant to the sound. These audio segments are discovered through slot attention and amplified in the next iteration (playback). A transformer decoder is used to relate information across playbacks. We demonstrate the advantages of our proposed PlayItBack approach through extensive experiments on AudioSet, VGG-Sound, and EPIC-KITCHENS-100 as well as through ablation studies.</p><p>Future work can explore the selection strategy for the number of playbacks, which might vary per audio sample. We hope PlayItBack can trigger insights into similar approaches for other audio signals such as speech as well as audio-visual fine-grained understanding.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Transformers for audio recognition by relying on hybrid ar-arXiv:2210.11328v1 [cs.SD] 20 Oct 2022</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>PlayItBack architecture. The spectrogram of the full audio sequence (top) is replayed by focusing on discriminative features using spectrograms with a reduced hop length (bottom). During each playback, spectrogram patches are projected to tokens x i and appended with patch (frequency and temporal) positional encodings (P ). Several multi-head attention layers are used to encode features z i . Slot attention G(z i ) then discovers discriminative temporal segments. These are considered as input to the next playback, using a smaller hop length to capture finer temporal details. To combine decisions between playbacks, playback and patch encodings (P P ) are appended to the encoded features. A recurrent Transformer Decoder D(z i ) takes previously decoded features from i and the encoded features in i + 1 playback. PlayItBack is trained to classify the audio label (L CLSi ), regularized by the weighted sum of ranking losses between i and {1, ..., i ? 1} playbacks (L ranki ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>VGG-Sound top-1 accuracy over different playbacknumbers (N) with respect to the compute (in GFLOPs).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparisons to state-of-the-art audio-only models on AudioSet. We report the mean average precision (mAP) alongside the backbone and training set used.</figDesc><table><row><cell>Model</cell><cell cols="5">top-1 top-5 mAP AUC d-prime</cell></row><row><cell>Audio-only models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">McDonnell &amp; Gao [33] 39.7</cell><cell>71.6</cell><cell cols="2">40.3 0.963</cell><cell>2.532</cell></row><row><cell>Peng et al. (A) [34]</cell><cell>44.3</cell><cell>-</cell><cell>48.4</cell><cell>-</cell><cell>-</cell></row><row><cell>ResNet-101 [12]</cell><cell>45.6</cell><cell>72.3</cell><cell cols="2">47.6 0.968</cell><cell>2.615</cell></row><row><cell>Chen et al. [7]</cell><cell>51.0</cell><cell>76.4</cell><cell cols="2">53.2 0.973</cell><cell>2.735</cell></row><row><cell>MBT (A) [18]</cell><cell>52.3</cell><cell>78.1</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Slow-Fast [12]</cell><cell>52.4</cell><cell>78.1</cell><cell cols="2">54.4 0.974</cell><cell>2.761</cell></row><row><cell>PlayItBackX3</cell><cell>53.7</cell><cell>79.2</cell><cell cols="2">56.1 0.978</cell><cell>2.846</cell></row><row><cell cols="3">Models trained with additional modalities</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Peng et al. (AV) [34]</cell><cell>50.6</cell><cell>-</cell><cell>53.9</cell><cell>-</cell><cell>-</cell></row><row><cell>PolyViT [35]</cell><cell>51.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MBT (AV)</cell><cell>64.1</cell><cell>85.6</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Comparisons to state-of-the-art models on VGG- Sound. We report the top-1 and top-5 accuracies (%) along- side mAP, the AUC and d-prime.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>150 GFLOPs</cell></row><row><cell>100 GFLOPs</cell></row><row><cell>50 GFLOPs</cell></row><row><cell>: Number of</cell></row><row><cell>slot attention iterations</cell></row><row><cell>(J) with respect to the</cell></row><row><cell>top-1 accuracy (%) and</cell></row><row><cell>GFLOPs.</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Ack. Research funded by the United Nation's End Violence Fund (iCOP 2.0 project) and EPSRC UMPIRE (EP/T004991/1).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Echoic memory explored and applied</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Clark</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
	<note>Journal of services marketing</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Auditory sensory (&quot; echoic&quot;) memory dysfunction in schizophrenia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nelson</forename><surname>Strous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel C</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Javitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The American journal of psychiatry</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Radvansky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Memory</title>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Psychology Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Object-centric learning with slot attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravindh</forename><surname>Mahendran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<pubPlace>NeuIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Audio set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jort F Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">VGGSound: A large-scale audio-visual dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rescaling egocentric vision: collection, pipeline and challenges for EPIC-KITCHENS-100</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazel</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Psla: Improving audio tagging with pretraining, sampling, labeling, and aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-An</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM TASLP</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Panns: Large-scale pretrained audio neural networks for audio pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuqiang</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Turab</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM TASLP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A comparison of five multiple instance learning pooling functions for sound event detection with weak labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juncheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Slow-fast auditory streams for audio recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sound event detection of weakly labelled data with cnn-transformer and automatic threshold optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuqiang</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM TASLP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Conformer: Convolution-augmented transformer for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anmol</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Convolution augmented transformer for semi-supervised sound event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koichi</forename><surname>Miyazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Komatsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>DCASE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Htsat: A hierarchical token-semantic audio transformer for sound classification and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjian</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejun</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Efficient training of audio transformers with patchout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khaled</forename><surname>Koutini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Schl?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Eghbal-Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Widmer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attention bottlenecks for multimodal fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Jansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning the spectrogram temporal resolution for audio classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haohe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xubo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuqiang</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.01719</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Autoscaler: Scale-attention networks for visual correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visual concept recognition and localization via iterative introspection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to zoom: a saliencybased sampling layer for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adria</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Kellnhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Mae-ast: Masked autoencoding audio spectrogram transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Baade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puyuan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Harwath</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Perceiver: General perception with iterative attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Jaegle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Gimeno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Masked spectrogram prediction for self-supervised audio pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dading</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcheng</forename><surname>Zeng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Masked autoencoders that listen</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juncheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mvitv2: Improved multiscale vision transformers for classification and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karttikeya</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mangalam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Acoustic scene classification using deep residual networks with late fusion of separated high and low frequency paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Mcdonnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Balanced multimodal learning via on-the-fly gradient modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yake</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Polyvit: Co-training vision transformers on images, videos and audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerii</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
