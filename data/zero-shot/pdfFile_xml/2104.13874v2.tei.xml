<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploring Relational Context for Multi-Task Dense Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bruggemann</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menelaos</forename><surname>Kanakis</surname></persName>
							<email>kanakism@vision.ee.ethz.ch</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Obukhov</surname></persName>
							<email>obukhova@vision.ee.ethz.ch</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Luc</roleName><forename type="first">Stamatios</forename><surname>Georgoulis</surname></persName>
							<email>georgous@vision.ee.ethz.ch</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Van</forename><surname>Gool</surname></persName>
							<email>vangool@vision.ee.ethz.ch</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eth</forename><surname>Zurich</surname></persName>
						</author>
						<title level="a" type="main">Exploring Relational Context for Multi-Task Dense Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The timeline of computer vision research is marked with advances in learning and utilizing efficient contextual representations. Most of them, however, are targeted at improving model performance on a single downstream task. We consider a multi-task environment for dense prediction tasks, represented by a common backbone and independent task-specific heads. Our goal is to find the most efficient way to refine each task prediction by capturing cross-task contexts dependent on tasks' relations. We explore various attention-based contexts, such as global and local, in the multi-task setting and analyze their behavior when applied to refine each task independently. Empirical findings confirm that different source-target task pairs benefit from different context types. To automate the selection process, we propose an Adaptive Task-Relational Context (ATRC) module, which samples the pool of all available contexts for each task pair using neural architecture search and outputs the optimal configuration for deployment. Our method achieves state-of-the-art performance on two important multi-task benchmarks, namely NYUD-v2 and PASCAL-Context. The proposed ATRC has a low computational toll and can be used as a drop-in refinement module for any supervised multi-task architecture. arXiv:2104.13874v2 [cs.CV] 23 Aug 2021 attention-driven multi-modal distillation scheme, based on three key contributions:</p><p>1. Increase the expressivity of the cross-task gate by conditioning it on the interdependence of source and target task pixels. Our multi-modal distillation scheme is therefore relational.</p><p>2. Enable global cross-task message passing by enlarging the receptive field of the distillation scheme. We refer to each pixel's distillation receptive field as its distillation context.</p><p>3. Customize the distillation context type for each sourcetarget task pair. We formulate five context type candidates (global, local, T-label, S-label, none) and adapt the type automatically with respect to each sourcetarget task pair in a given architecture (see <ref type="figure">Fig. 1</ref>).</p><p>Contributions 1 and 2 are addressed by leveraging and adapting the scaled-dot product attention mechanism [51] for multi-modal distillation. For contribution 3, we repurpose modern Neural Architecture Search (NAS) methods to automatically find the optimal context type for each sourcetarget task connection. Overall, we present a novel Adaptive Task-Relational Context (ATRC) module which can be used as a drop-in module for CNNs to refine any dictionary of supervised dense prediction tasks. We show its effectiveness empirically with the architecture shown in <ref type="figure">Fig. 2</ref>: a single neural network for all tasks with a shared backbone of RGB input, multiple task-specific heads, and ATRC distillation modules to refine each task's predictions.</p><p>The paper is structured as follows: Sec. 2 provides an overview of related work; Sec. 3.1 introduces the architecture of ATRC; Sec. 3.2 explains the types of relational contexts in consideration; Sec. 3.3 covers the adaptation of the context type through NAS techniques; Sec. 4 provides the empirical study details and verifies the proposed method state-of-theart performance on several important benchmarks; Sec. 5 concludes the paper.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The role of context in computer vision is hard to overstate; most notable breakthroughs boil down to a clever extraction <ref type="bibr" target="#b35">[36]</ref>, learning <ref type="bibr" target="#b29">[30]</ref>, and utilization <ref type="bibr" target="#b28">[29]</ref> of contextual representations. The success of Convolutional Neural Networks (CNN) is largely due to their inherent ability to capture the local context and build very deep <ref type="bibr" target="#b46">[47]</ref> contextual hierarchies within the model. Recently, the progressive adoption of the attention mechanism in computer vision <ref type="bibr" target="#b56">[57]</ref> has brought forth more flexible context descriptions conditioned on the interdependence of individual pixels, while steadily replacing the traditional convolutional building blocks <ref type="bibr" target="#b12">[13]</ref>.</p><p>Multi-Task Learning (MTL) <ref type="bibr" target="#b5">[6]</ref> is concerned with sharing representations between tasks. Motivated by the obser- for the marked pixel (orange cross) of target task semantic segmentation. Our algorithm selects one distillation context type for each source task (dashed lines represent a switch). Alternatively, the connection can be severed by choosing none. The procedure is analogous for all other target tasks.</p><p>vation that representations of visual tasks are often highly correlated <ref type="bibr" target="#b62">[63]</ref>, recent works <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b49">50]</ref> focusing on multi-task dense prediction have extended context extraction across tasks through soft-gated message passing. Referred to as multi-modal distillation in the literature <ref type="bibr" target="#b55">[56]</ref>, the idea is to augment the high-level representations of downstream target tasks by selectively aggregating complementary features of a set of source tasks. The gating function in the distillation thereby learns to focus on useful cross-task information flow.</p><p>Despite their effectiveness, current multi-modal distillation schemes <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b49">50]</ref> suffer from two main limitations: <ref type="bibr" target="#b0">(1)</ref> The employed gates only regulate information flow based on the source task feature values. As such, the distillation module fails to capture task interactions fully. (2) Each target pixel exclusively receives information from its source counterpart, i.e., the message passing is restricted locally. Compelled by these drawbacks, we propose a new type of</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Multi-Task Learning (MTL) methods employ two main paradigms to learn shared representations: hard parameter sharing and soft parameter sharing. Hard parameter sharing characterizes architectures which typically share the first hidden representations among the tasks while branching to independent task-specific representations at a later stage. Most approaches split to task-specific heads at a single branch point <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b44">45]</ref>. However, such naive branching can be sub-optimal, raising interest in mechanisms that allow for finely branched architectures <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b4">5]</ref>. Our work is complementary to these hard parameters sharing methods, since we introduce a module which refines task-specific features. Soft parameter sharing, in contrast, marks architectures which induce knowledge transfer between separate task-specific networks through feature fusing mechanisms. Feature fusing can be introduced along the entire network depth <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b33">34]</ref>, whereby computational cost is often a limiting factor. Our proposed module can be interpreted as a sophisticated feature fusing mechanism, applied only at a single stage to refine high-level representations.</p><p>Several recent MTL works follow a similar strategy: PAP <ref type="bibr" target="#b62">[63]</ref> and PSD <ref type="bibr" target="#b63">[64]</ref> refine task-specific feature maps through global and local self-attention respectively. The employed attention masks are first refined by propagating affinity patterns across tasks and then applied iteratively on the target task feature maps. In contrast to <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b63">64]</ref>, our approach directly attends to source task features by explicitly modeling pairwise interactions between source and target tasks. More closely related to our work, PAD-Net <ref type="bibr" target="#b55">[56]</ref> uses multi-modal distillation to enhance task-specific predictions. Information flow from each source to target task is regulated with a sigmoid-activated gate function. MTI-Net <ref type="bibr" target="#b49">[50]</ref> combines the multi-modal distillation module of PAD-Net with a multi-scale refinement scheme to facilitate cross-task talk at multiple scales. However, the gates used in the distillation module of <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b49">50]</ref> are functions of the source task features only and operate per pixel. Our method, on the other hand, leverages pairwise task similarities to create more expressive gates through the attention mechanism, while also enabling global cross-task message passing.</p><p>Attention was originally developed to improve sentence alignment in neural machine translation <ref type="bibr" target="#b1">[2]</ref>. In computer vision, variants of scaled dot-product attention <ref type="bibr" target="#b50">[51]</ref> in particular have been used to capture global relationships over the entire pixel space <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b57">58]</ref>, locally <ref type="bibr" target="#b41">[42]</ref>, and even channel-wise <ref type="bibr" target="#b14">[15]</ref>. In these approaches, the representation of each target pixel is augmented by aggregating the representations of pixels within the specified context. Each context pixel thereby contributes according to its relation to the target, hence the term relational context. Relevant to our work, A 2 -Net <ref type="bibr" target="#b9">[10]</ref>, ACFNet <ref type="bibr" target="#b60">[61]</ref>, and OCR-Net <ref type="bibr" target="#b58">[59]</ref> define their own relational context types by grouping pixels into distinct regions (e.g., object class) and attending to prototypical representations of those regions instead. All of the above mentioned methods focus on attention for a single downstream task and utilize fixed context descriptions. Our work extends these concepts to a multi-task scenario while choosing the optimal relational context type from a pool of candidates for each source-target task pair.</p><p>Neural Architecture Search (NAS) automates the process of engineering problem-specific neural network architectures, with the goal of minimizing hand-crafted network design. To this end, seminal works use either reinforcement learning <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b65">66]</ref> or evolutionary <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b42">43]</ref> algorithms to sample promising candidate architectures from a large  <ref type="figure" target="#fig_10">Figure 2</ref>. (a) Overview of a multi-task network with the proposed Adaptive Task-Relational Context (ATRC) module. The main network can have any topology, provided that the head for each task n produces both the features (Fn) for ATRC to refine and the auxiliary prediction (An). In our experiments we predict Fn and An with the main and auxiliary independent heads respectively. Within ATRC each task is routed as target task to N Context Pooling (CP) blocks (n-th row of CP blocks) and as source task to N CP blocks (n-th column). The outputs of CP blocks are concatenated for each task independently and fed through a projection module ('Task n'). The predictions Pn are obtained after processing ATRC outputs with a final layer ('Pred n'). (b) Dissection of a CP block, refining target task (T ) features through source task (S) information. During the search stage, the CP block extracts all five contextual representations (white blocks, see Sec. 3.2) and returns a convex combination of them. After search convergence, a single context type is sampled via argmax, i.e., the ?i form a one-hot vector. Legend: Green blocks denote modules with learned weights, red blocks denote loss functions. Best viewed in color.</p><p>search space. Although effective, architecture search with these methods can be very compute-intensive, prompting researchers to explore differentiable NAS <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b19">20]</ref>. Instead of a single operation, differentiable NAS uses a convex combination of several operations at a given layer, enabling gradient-based optimization of the search space by training the operation mixing weights. The primary contribution of our work is a novel multi-modal distillation module; we thus utilize existing advances in differentiable NAS <ref type="bibr" target="#b54">[55]</ref> and a custom search space to automate the context selection for different source-target task pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Adaptive Task-Relational Context</head><p>In this section, we describe the proposed Adaptive Task-Relational Context (ATRC) module within a general multitask learning framework. First, we briefly outline the overall architecture, before dissecting the building blocks of the ATRC module. Finally, we discuss the employed adaptive context type search scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Architecture Design</head><p>Our ATRC module can be incorporated as a refinement stage in any multi-task neural network (e.g., across multiple scales). For transparency we intentionally keep the example configuration simple (see <ref type="figure" target="#fig_10">Fig. 2a</ref>): The backbone is shared among all tasks; shallow heads are used per task to generate task-specific features F n and auxiliary predictions A n , where n ? {1, ..., N } indexes the task. In our basic design, we predict F n and A n independently, using a 3?3 Conv-BN-ReLU and 1?1 Conv-BN-ReLU-1?1 Conv layer respectively. The role of the A n is further explained in Sec. 3.2.3.</p><p>The ATRC module refines the features F T of each target task T by attending to the features F n of every available task n ? {1, ..., N } within a separate Context Pooling (CP) block for each source-target task pair. Each row of the cartesian grid of CP blocks in <ref type="figure" target="#fig_10">Fig. 2a</ref> thus serves to refine one target task T , using information from a different source task S in each column. The self-attention performed in the CP blocks on the diagonal enables the distillation module to additionally capture intra-task relationships. The outputs of all CP blocks within a row are concatenated along the channel dimension, fused with a 1?1 Conv-BN layer, concatenated with the original target task features F T , and processed with 1?1 Conv-BN-ReLU. Lastly, the refined features are fed through a 1?1 Conv layer ('Pred T ' in <ref type="figure" target="#fig_10">Fig. 2a</ref>) to obtain the final predictions P T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Context Pooling Block</head><p>A CP block aims to extract useful features from one source task S to augment one target task T . To this end, each CP block performs at its core a version of scaled dotproduct attention, the main component of the widely successful Transformer <ref type="bibr" target="#b50">[51]</ref>. Accordingly, the target task feature map F T and the source task feature map F S are first transformed to queries q, keys k and values v using 1?1 Conv-</p><formula xml:id="formula_0">BN-ReLU layers f * . q = f q (F T ), k = f k (F S ), v = f v (F S )<label>(1)</label></formula><p>Throughout this paper, we assume that tensors are flattened along the spatial dimension (including q, k, v). A matrix of attention weights A is generated based on the pairwise similarity between q and k features. CP block outputs v are attention-weighted combinations of v features (d k is the channel dimension of k)</p><p>.</p><formula xml:id="formula_1">v = softmax qk ? d k A v<label>(2)</label></formula><p>In the multi-task setting, the attention weights can be interpreted as modeling the likelihood of feature cooccurrence <ref type="bibr" target="#b61">[62]</ref> in transformed target (q) and source (k) task maps. The contribution of each source task pixel within the context of the target task pixel is then gated according to the estimated co-occurrence likelihood. Intuitively, co-occurrence might improve the robustness of target task predictions in ambiguous cases, e.g., for T = 'semantic segmentation' and S = 'depth estimation', the context of a pixel of class 'sky' is more likely to consist of many pixels with large depth. The attention maps in Eq. 2 model pixel interactions globally ('all-to-all'), i.e., the distillation context of each pixel is unconstrained. Depending on the present source and target task combination, this might not be ideal. We therefore introduce four variants of the above attention mechanism in Sec. 3.2.1, 3.2.2, 3.2.3, each characterized by a different context definition. In Sec. 3.3 we describe how we adapt the CP block for different source-target task pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Global Context</head><p>In this case, the distillation context of a specific target pixel is simply every pixel of the source task. Naive implementations of this approach lead to a prohibitively large memory footprint, as the complexity of computing the attention weights scales with O(L 2 ), where L is the number of pixels.</p><p>To circumvent this issue, we utilize a linearization scheme similar to <ref type="bibr" target="#b23">[24]</ref>. In particular, we can calculate the attention map for a target pixel i using an arbitrary similarity function sim(?) with positive domain instead of softmax.</p><formula xml:id="formula_2">v i = L j=1 sim (q i , k j ) v j L j=1 sim (q i , k j )<label>(3)</label></formula><p>This includes all kernel functions sim(q i , k j ) = ?(q i )?(k j ), which allows us to shift the multiplication order: ?(k j ) and v j can be multiplied first and reused for every ?(q i ), which reduces the overall complexity to O(L). In this work, we simply choose a linear kernel ?(x) = x, corresponding to cosine similarity. To avoid numerical issues, we replace the ReLU activation functions in f q and f k of Eq. 1 with the smooth approximation softplus(x) = log (1 + exp (x)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Local Context</head><p>We can constrain the context to encompass only source pixels spatially close to the target pixel <ref type="bibr" target="#b41">[42]</ref>, mimicking the receptive field of a convolution. With N b (i) denoting the 2D spatial neighborhood of target pixel i with extent b (we use b = 9 ? 9), the attention formula analogously to Eq. 2 is:</p><formula xml:id="formula_3">v i = j?N b (i) softmax N b (i) q i k j ? d k v j<label>(4)</label></formula><p>This operation resembles a convolution with a spatiallyadaptive filter <ref type="bibr" target="#b47">[48]</ref>-the attention map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Label Context</head><p>Both the global and local relational contexts are spatially defined, i.e., distillation is conducted through a spatial attention mask. Label context, on the other hand, is defined in label space, meaning that we (1) partition the label space into a set of disjoint label regions, (2) find a prototypical representation for each region, and (3) relate each pixel to each region prototype. This concept has been applied to semantic segmentation in <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b58">59]</ref>. In this section we generalize it to any dense prediction task and explore its potential for MTL. Partitioning the label space is straightforward for classification tasks, i.e., the label regions can be equivalent to the classes. For regression tasks, however, we need to discretize the continuous label space. Consequently, we bin the values on a logarithmic scale for depth prediction and cluster predictions on the unit sphere using k-means for surface normal estimation (see Appendix D for details).</p><p>We follow the approach of OCR-Net <ref type="bibr" target="#b58">[59]</ref> for supervised learning of the region prototypes for each task n: Specifically, auxiliary prediction heads calculate the spatial maps A n ? R L?Rn (see <ref type="figure" target="#fig_10">Fig. 2a</ref>), where each entry indicates the degree to which a pixel l ? {1, ..., L} belongs to a label region r ? {1, ..., R n }. During training, these maps are learned with ground truth supervision using a cross-entropy loss. The resulting maps A n are normalized using spatial softmax to obtain? n , representing the spatial probability density of each label region r. In a multi-task setup, we can then choose to define the label regions in either target or source task label space:</p><p>T -label. In this approach, label regions are defined in target task (T ) space. Source task features are spatially aggregated using target task spatial maps? T , yielding the region prototypes p S ? R R T ?C , where C is the source task channel dimension.</p><formula xml:id="formula_4">p S =? T F S<label>(5)</label></formula><p>p S is then substituted for F S in Eq. 1 to obtain k and v. S-label. Alternatively, source task features can also be aggregated via source task (S) spatial maps? S , by substi-tuting? S for? T in Eq. 5.</p><p>The key difference between the two approaches is best illustrated with an example: Assuming target task semantic segmentation and source task depth estimation, the T -label context groups depth features according to object class and makes each target pixel attend to the prototypical depth features for each object class (e.g., the representative depth feature of all 'car' pixels). Conversely, the S-label context simply groups depth features according to their depth, enabling semantic features to interact with entire depth regions.</p><p>We visualize example self-attention maps for a single target pixel (white cross) of a trained label context distillation model in <ref type="figure" target="#fig_2">Fig. 3</ref>. The maps illustrate that the model learns to focus on context pixels within distinct label regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Automated Context Type Selection</head><p>While all presented context types could help improve target task features, some might be more effective than others in specific scenarios. Therefore, CP blocks are designed to tailor their context type (attention mechanism) to the present source-target task pair. In this paper we opt for differentiable NAS techniques to automatically select a single context type for each CP block, by optimizing a supergraph encompassing all options (see <ref type="figure" target="#fig_10">Fig. 2b</ref>). However, a CP block is not limited to a single context type per se and could instead refine predictions given a combination of context types in a static <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b53">54]</ref> or even dynamic <ref type="bibr" target="#b20">[21]</ref> fashion.</p><p>Our search space consists of five candidates in each CP block: global, local, T-label, S-label, and a none operation. The none operation simply severs the information flow between two tasks, which can prevent task interference, a common problem in MTL <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b22">23]</ref>. Operation selection in a CP block j can be formulated as a multiplication of all candidates O j with a one-hot vector Z j sampled from the categorical distribution p ?j (Z j ).</p><formula xml:id="formula_5">O j = Z j O j (6)</formula><p>Continuous relaxation of the search space (while maintaining this sampling process) is achieved through the Gumbel-Softmax gradient estimator <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b21">22]</ref>, yielding a softened one-hot random variable? j .</p><formula xml:id="formula_6">Z (i) j = exp log ? (i) j + G (i) j /? 5 u=1 exp log ? (u) j + G (u) j /? (7) G (u) j ? Gumbel(0, 1)</formula><p>is a Gumbel random variable, and ? is the softmax temperature. In our case, the architecture parameters ? are updated in the same round of backpropagation as the network weights (single-level optimization). A more detailed discussion of Gumbel-Softmax for differentiable NAS is provided in <ref type="bibr" target="#b54">[55]</ref>.</p><p>Empirically, samples from ?-distributions trained with Gumbel-Softmax exhibit large variance after convergence, leading to unstable evaluation of sampled subgraphs. We thus use a two-pronged strategy to counteract this problem: (1) Similarly to <ref type="bibr" target="#b15">[16]</ref>, we adopt entropy regularization on p ?j (Z j ) to explicitly control the sampling variance. Instead of the commonly employed candidate operation pretraining, we can simply start the architecture search from scratch with a negative regularization weight to enforce a uniform ?distribution. The weight is gradually increased to a positive value during training to ultimately incentivize low-entropy solutions, which imply a low variance as the architecture is sampled from the supergraph. <ref type="formula" target="#formula_1">(2)</ref> We stop the architecture sampling process in CP block j completely once p ?j has reached a low-entropy solution. After a defined threshold is surpassed, we fix the block selection procedure in j using argmax. Using this strategy, we obtain highperforming architectures directly during the search stage (see <ref type="figure" target="#fig_4">Fig. 4</ref>), demonstrating that our search objective is well defined. Nevertheless, for a fair comparison, we still retrain the discovered architectures from scratch-as is common practice <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b54">55]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We briefly review the experimental setup, before presenting empirical studies.</p><p>Training details are provided in Appendix A and reference code is available at https://github.com/brdav/atrc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Setup</head><p>Datasets. Experiments are conducted on two widely-used dense prediction datasets: (1) NYUD-v2 <ref type="bibr" target="#b45">[46]</ref>, which consists  <ref type="table" target="#tab_9">Table 1</ref>. Controlled distillation module comparison on NYUD-v2 with a HRNet18 backbone. For all models except the single task baseline, a shared encoder and small task-specific heads are used (Sec. 3.1). We insert the different distillation modules before the final prediction layer.  , a split of the larger PASCAL dataset <ref type="bibr" target="#b13">[14]</ref>, providing 4998 training and 5105 testing images, labeled for semantic segmentation, human parts segmentation ('PartSeg'), saliency estimation ('Sal'), surface normal estimation, and boundary detection. We use the distilled saliency and surface normal labels of <ref type="bibr" target="#b38">[39]</ref>.</p><p>Backbones. We test our framework using several backbones: HRNetV2-W18-small (HRNet18), HRNetV2-W48 (HRNet48) <ref type="bibr" target="#b51">[52]</ref>, and ResNet-50 <ref type="bibr" target="#b18">[19]</ref>.</p><p>Metrics. We evaluate 'Semseg' and 'PartSeg' with mean intersection over union, 'Depth' with root mean square error, 'Normal' with mean angular error, 'Sal' with maximum F-measure as in <ref type="bibr" target="#b0">[1]</ref>, and 'Bound' with the optimaldataset-scale F-measure of <ref type="bibr" target="#b39">[40]</ref>. All experiments in this paper are repeated five times; the mean is reported for every metric (in <ref type="table" target="#tab_9">Table 1</ref> also the standard deviation). To quantify overall multi-task performance for N tasks, we adopt the average per-task performance drop (? m ) with re-spect to single task baselines b for model m <ref type="bibr" target="#b38">[39]</ref>:</p><formula xml:id="formula_7">? m = 1 N N i=1 (?1) ?i (M m,i ? M b,i )/M b,i . ? i = 1 if lower is better for metric M i and ? i = 0 otherwise.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Distillation Module Benchmarking</head><p>In <ref type="table" target="#tab_9">Table 1</ref> we conduct a series of controlled experiments to assess the effectiveness of different distillation modules fairly. Using a HRNet18 backbone, we alter the MTL architecture design described in Sec. 3.1 only by replacing the ATRC module with other distillation modules. For the baselines, no distillation module is used.</p><p>As expected, all investigated distillation modules outperform the trivial multi-task baseline in terms of multitask performance ? m . Furthermore, most relational context modules fare significantly better than their alternatives. Excepting local relational context, augmenting the multi-task network with relational context beats the single task baseline while maintaining a far lower computational footprint. <ref type="table" target="#tab_9">Table 1</ref> also reveals that no single relational context type dominates for every task. This suggests that a more finegrained context customization for each individual sourcetarget task pair could improve overall performance. Indeed, applying our automated context type selection (Sec. 3.3), ATRC, produces the best result in multi-task performance. <ref type="figure">Fig. 5</ref> visualizes the resource cost of the various distillation modules by plotting the multi-task performance vs. number of parameters and multiply-add operations (MAdds). The computational overhead of the relational context modules-and most other distillation modules-remains low compared to single task networks. Our ATRC combines the benefits of all the relational context modules by maximizing performance while remaining bounded in terms of resource cost.  <ref type="figure">Figure 5</ref>. Distillation module resource analysis using an HRNet18 backbone on NYUD-v2. We plot multi-task performance ?m vs. number of parameters (left) and MAdds (right) for multi-task models with different distillation modules inserted before the final prediction layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with State-of-the-Art</head><p>To validate the proposed ATRC module, we present experimental comparisons with the following baselines across a number of scenarios: separate single task networks, multitask network (shared backbone; task-specific heads; no distillation) and the state-of-the-art MTI-Net <ref type="bibr" target="#b49">[50]</ref>. <ref type="table" target="#tab_2">Tables 2  and 3</ref> display the results obtained on the NYUD-v2 dataset, using HRNet18 and HRNet48 backbones respectively, while <ref type="table" target="#tab_4">Table 4</ref> shows PASCAL-Context results using HRNet18. MTI-Net uses a large-scale decoder head consisting of two separate stages: the Feature Propagation Module (FPM) and a multi-scale multi-modal distillation module (the analog of our ATRC module). To ensure a fair comparison, we apply our method on both the basic architecture described in Sec. 3.1, as well as the backbone complemented with the FPM (+174% and +79% in number of parameters for HRNet18 and HRNet48 respectively).</p><p>In all investigated cases, ATRC enhances performance significantly compared to the multi-task baseline. Furthermore, our method combined with the FPM consistently outperforms MTI-Net, even though MTI-Net applies multi-modal distillation on four scales, while we only distill on the largest scale (causing our model to be more parameter efficient, e.g., -22% in <ref type="table" target="#tab_2">Table 2</ref>). This implies that task interactions can be adequately captured at a single scale for distillation, provided that the backbone is able to extract and fuse multi-scale information effectively (like HRNet).</p><p>Overall, the multi-task approaches are less effective compared to single task baselines on the PASCAL-Context dataset. This finding is in agreement with other works <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b49">50]</ref> and could be attributed to the larger and more diverse task dictionary. Nevertheless, the ranking order of the multitask approaches in terms of multi-task performance remains consistent with the results obtained for NYUD-v2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Source Task Importance</head><p>The simple design of the proposed ATRC module allows us to investigate the importance of each source-target task connection ( = CP block) for the final predictions of fitted    models. To this end, we adapt permutation feature importance <ref type="bibr" target="#b3">[4]</ref> to our setting. We can determine the importance of a CP block by recording the drop in multi-task performance ? m when the output of that block is randomly shuffled over the dataset. To get a more reliable estimate, this procedure is repeated multiple times with different permutations. Neglecting feature multicollinearity, the average drop in ? m provides an estimate of how strongly the fitted model depends on the inspected source task for the corresponding target task prediction. We use held-out data in this experiment to assess the importance for generalization power. <ref type="figure">Fig. 6</ref> visualizes the results for NYUD-v2. The inspection reveals that self-attention remains the most important distillation connection for three out of four tasks. However, depth  <ref type="figure">Figure 6</ref>. Source task importance; measured by permutation testing of fitted ATRC models on NYUD-v2. The contribution of a source task in the distillation is gauged by the drop in multi-task performance ?m as the output of the corresponding source-target task distillation is randomly permuted. The values shown in the matrix are mean percentage drops in ?m. estimation seems to rely more strongly on semantic segmentation source features, corroborating empirical evidence in the literature that depth estimation can be improved significantly using semantic predictions <ref type="bibr" target="#b55">[56]</ref>. Overall, boundary detection profits little from multi-modal distillation according to this analysis, which is consistent with the lack of noteworthy performance gain for this task in <ref type="table" target="#tab_9">Table 1</ref>. We hypothesize that this could be due to the large discrepancy between the loss (we follow others <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b22">23]</ref> and use balanced cross entropy) and metric for this task. A more tailored loss function such as <ref type="bibr" target="#b26">[27]</ref> might help in this case.</p><p>Source task importance scores are linearly correlated with the search algorithm reliability-albeit weakly (Pearson correlation coefficient of 0.43). Notably, we observe 100% reliability for the three most important source-target task connections of <ref type="figure">Fig. 6</ref>. This suggests that the search algorithm is more consistent for important decisions. We quantify search algorithm reliability using percentage agreement in candidate selection between all search run pairs (does not account for chance agreement, see Appendix F).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Complementary Methods</head><p>To demonstrate its flexibility, we combine our ATRC module with (1) the contextual Atrous Spatial Pyramid Pooling (ASPP) module of <ref type="bibr" target="#b7">[8]</ref> and (2) automatic backbone branching via Branched Multi Task Architecture Search (BMTAS) <ref type="bibr" target="#b4">[5]</ref>.</p><p>For these experiments, we use a dilated ResNet-50 backbone (output stride 16) with a skip connection at stride 4, and fully convolutional task-specific heads.</p><p>ASPP is a popular multi-scale context aggregation module leveraging dilated convolutions. We insert a separate ASPP module before each task-specific head. <ref type="table" target="#tab_9">Table 5</ref> shows that ATRC also improves the performance of the ASPPaugmented network, indicating that the two context aggregation stages are complementary to some extent. Interestingly, the proportions of selected relational context types in the ATRC search change drastically with ASPP, as illustrated in <ref type="figure" target="#fig_6">Fig. 7</ref>: The proportion of local context rises from 0% (w/o ASPP) to 41.6% (w/ ASPP), demonstrating that ATRC adapts the context types given the nature of different back-  <ref type="table" target="#tab_9">Table 5</ref>. PASCAL-Context performance of ASPP <ref type="bibr" target="#b7">[8]</ref> and BM-TAS <ref type="bibr" target="#b4">[5]</ref> when supplemented with our ATRC. For ASPP, we insert an ASPP module at the beginning of each task-specific head. For BMTAS, we use their method to find a branched backbone (instead of fully shared). ATRC is complementary to both approaches. Experiments are based on a dilated ResNet-50 backbone. bones (e.g., the enhanced receptive field of ASPP is better complemented with local information). Branched networks are a hard parameter sharing MTL strategy and, as such, complementary to multi-modal distillation (see Sec. 2). We show this by applying our method in combination with a branched backbone configuration, determined through the NAS-based BMTAS. The results in <ref type="table" target="#tab_9">Table 5</ref> demonstrate that ATRC improves performance also for branched multi-task networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We presented ATRC, a novel multi-modal distillation module which exploits inter-and intra-task relationships to refine pixel-wise predictions. The proposed approach leverages scaled dot-product attention to enrich the features of a target task through contextual source task features, while explicitly factoring in tasks' relations. We formulate four relational context types for multi-modal distillation (global, local, T-label, and S-label context) and detail an algorithm which customizes the context type for every given sourcetarget task pair. Experimental analyses on NYUD-v2 and PASCAL-Context benchmarks indicate that our ATRC module outperforms comparable multi-modal distillation modules established in the literature. Overall, the presented framework shows great promise for multi-task dense prediction and opens the door for future research in customized task-relational context descriptions.</p><p>In this section, we describe the training setup. For consistency, all experiments in the paper were repeated five times using the pipeline detailed below.</p><p>Data augmentation. We augment input images during training by random scaling with values between 0.5 and 2.0, random cropping to input size (425 ? 560 for NYUD-v2-we use the cropped version of <ref type="bibr" target="#b17">[18]</ref>-and padded to 512?512 for PASCAL-Context), random horizontal flipping and random color jitter. Image intensities are standardized. Depth labels are corrected for scaling and surface normal labels are corrected for horizontal flipping.</p><p>Task losses. The total loss of the multi-task network with parameters ? is a weighted sum of losses (for tasks n ? {1, ..., N }):</p><formula xml:id="formula_8">L total (?) = N n=1 ? n L n (?)<label>(8)</label></formula><p>For semantic segmentation and human parts segmentation we use a cross-entropy loss (loss weights ? n = 1 and ? n = 2 respectively), for saliency estimation a balanced cross-entropy loss (? n = 5), for depth estimation a L 1 loss (? n = 1), for surface normal estimation a L 1 loss with unit vector normalization (? n = 10) and for boundary detection a weighted cross-entropy loss (? n = 50). For boundary detection, the positive pixels are weighted with 0.8 and the negative pixels with 0.2 on NYUD-v2, while on PASCAL-Context the weights are 0.95 and 0.05. ? n for each task was determined through a logarithmic grid search over candidate values with single-task networks. The auxiliary predictions A n are trained with a crossentropy loss using the same loss weights as above. However, the auxiliary head backpropagation is stopped from updating parameters of the main network.</p><p>Optimization hyperparameters. All backbones are initialized with ImageNet pretrained weights. We use Stochastic Gradient Descent (SGD) with momentum of 0.9 and weight decay of 0.0005 to optimize the model parameters. The initial learning rate is determined through a logarithmic grid search (..., 0.002, 0.005, 0.01, 0.02, ...), with the option of having a 10 times higher learning rate for the heads vs. the backbone. The initial value is decayed during training according to a 'poly' learning rate schedule <ref type="bibr" target="#b6">[7]</ref>. For all experiments, we use a minibatch size of 8 and train for 40000 iterations.</p><p>Context type search. The architecture distribution parameters ? are initialized with zeros. We use an Adam optimizer <ref type="bibr" target="#b25">[26]</ref> to update them, with learning rate 0.0005 (no weight decay, no learning rate scheduler). The update occurs in the same round of backpropagation as the regular model parameters (single-level optimization). Over the course of training, the Gumbel-Softmax temperature ? is annealed linearly from 1.0 to 0.05 (following <ref type="bibr" target="#b54">[55]</ref>). Also, to ensure a fair candidate context type selection, we disable learnable affine parameters of the last batch normalization of every context type attention mechanism.</p><p>As discussed in Sec. 3.3, we use entropy (H) regularization to control the sampling variance during the architecture search. Specifically, we calculate the mean entropy of the architecture parameter (?)-distributions over all Context Pooling (CP) blocks, scale it with a weight ? H , and add it to the total loss.</p><formula xml:id="formula_9">L search (?, ?) = N n=1 ? n L n (?, ?) + ? H N 2 N 2 j=1 H(? j ) (9)</formula><p>j indexes the CP blocks. The scaling factor ? H follows a linear schedule during the search, from -0.02 to 0.06. We found that this provides an adequate balance between candidate exploration and exploitation. For a given CP block j, architecture search is terminated prematurely if the difference between the two largest values of ? j exceeds 0.3. One candidate is then sampled using argmax (i.e., ? j becomes a one-hot vector).  <ref type="table" target="#tab_9">Table B</ref>-1. NYUD-v2 single task performances of HRNetV2-W18small (HRNet18) and HRNetV2-W48 (HRNet48) models <ref type="bibr" target="#b51">[52]</ref>. We compare the performances obtained using our implementation with the numbers published in <ref type="bibr" target="#b49">[50]</ref>.</p><p>After concluding five runs of the architecture search, we determine the final configuration by choosing the context type receiving the most votes over the five runs in each CP block. Ultimately, this final configuration is retrained five times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Verification</head><p>We verify the implementation of our pipeline by comparing HRNet single task performances with the numbers published in <ref type="bibr" target="#b49">[50]</ref>. <ref type="table" target="#tab_9">Table B</ref>-1 shows that the baselines trained with our pipeline outperform those of <ref type="bibr" target="#b49">[50]</ref>.</p><p>For implementing the various distillation modules in Table 1, we used the code provided by the authors whenever possible, and otherwise followed the information provided in the papers closely. For MTI-Net <ref type="bibr" target="#b49">[50]</ref>, we used the authors' model code within our pipeline.</p><p>Finally, we attempted to reimplement the full PSD [64] network based on a ResNet-50 backbone (as suggested in the original paper), but were unable to obtain competitive results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Relational Context Schematics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Label Context: Regression Tasks</head><p>In this section, we discuss how the label space of regression tasks can be partitioned into distinct regions for label context formation, as mentioned in Sec. 3.2.3. Regression tasks can be easily reformulated as classification tasks by binning the continuous ground truth values. However, the discretization scheme has to be tailored towards each task separately to obtain satisfactory performance. For depth prediction, our approach is inspired by <ref type="bibr" target="#b30">[31]</ref>. Specifically, we divide the range of depth values into 40 logarithmic bins, accounting for the fact that the estimation error for larger depth values is naturally larger. During training, we learn a classifier to assign the pixels to the bins. During evaluation, we use a soft-weighted-sum inference: Every bin is represented by its mean depth in log space. A weighted sum of bins (weight = prediction score) is used as the final prediction.</p><p>For surface normal estimation, we use the triangular coding technique of <ref type="bibr" target="#b59">[60]</ref>. First, a codebook is learned with k-means. The codewords form a Delaunay triangulation cover on the unit sphere. Any surface normal can thus be expressed as a weighted combination of the three codewords marking its triangle. During training, we learn a classifier to predict those codeword weights. Following <ref type="bibr" target="#b59">[60]</ref>, we choose 40 codewords ( = 40 classes). Evaluation consists of two steps: (1) Find the triangle with maximum total probability. (2) Use the probabilities of the three codewords of that triangle to reconstruct the surface normal.</p><p>To verify the above discretization schemes, we trained single task models accordingly, and compare them to the regression models in <ref type="figure" target="#fig_0">Fig. D-1</ref>. The figure shows that the performance of classification-while slightly worse than regression-is satisfactory for both depth and surface normal estimation. The same conclusion can be drawn from a qualitative comparison, shown in <ref type="figure" target="#fig_10">Fig. D-2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Label Context: Performance Upper Bound</head><p>To estimate the potential of label context for multi-modal distillation, we conduct experiments using ground truth label regions. Instead of predicting the spatial maps A n from the input image (see Sec Figure D-1. Performance comparison of single task depth and surface normal estimation models, using either a regression or classification framework. Their similar performance confirms that we can exploit the classification scheme to form high-quality label regions for the label context.  data A (GT ) n to partition the label space into distinct regions. This provides an upper bound for the performance of label context distillation. Table E-1 shows the results for both T -label and S-label context: The performance increases greatly (with the exception of T -label context for boundary detection), confirming that label region grouping is highly effective for multi-modal distillation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Context Type Search Reliability</head><p>We consider context type selection during architecture search as a rater decision. Since we repeat each run five times, we can evaluate the intra-rater reliability: The agreement among the five selected context types in all CP blocks.</p><p>The most intuitive way to quantify agreement is through percentage agreement (i.e., counting the fraction of times a pair of runs agree on a decision). However, this measure does not take into account that agreement may happen purely due to chance. We thus report also Light's kappa <ref type="bibr" target="#b31">[32]</ref>, which is an agreement score calculated by averaging Cohen's kappa <ref type="bibr" target="#b11">[12]</ref> over all pairs of runs. Kappa statistics are corrected for chance agreement, with the drawback that their interpretation is less intuitive. A value of 0 indicates no agreement, -1 indicates perfect disagreement, and 1 perfect agreement. We obtain an overall percentage agreement of 71.2% and a Light's kappa of 0.48 for the search on NYUD-v2 with a HRNet18 backbone.</p><p>We emphasize that reliability is not strictly necessary for an effective search algorithm. If there is no dominating choice of context type (e.g., none of the options lead to significant performance gain), then even a valid search algorithm is expected to be unreliable. In such cases, introducing a tie-breaking auxiliary objective could help promote convergence (e.g., a resource loss).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Schematic of the task relational context (orange overlay)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Heatmaps showing label context attention maps relating to the pixel marked with a white cross in the left image, i.e. we visualize the corresponding row of A in Eq. 2. For each target task we visualize the self-attention maps only.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Performance comparison of the models sampled from the supergraph at the end of the context type search vs. after retraining. The chart shows mean and std. of the relative performance improvement w.r.t. single task (ST) models: (Mm ? MST )/MST for model m and 'higher = better' metric M , and vice versa for 'lower = better'. of 795 training and 654 testing images of indoor scenes, with annotations for semantic segmentation ('SemSeg'), depth estimation ('Depth'), surface normal estimation ('Normal'), and boundary detection ('Bound'). (2) PASCAL-Context [9]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Proportions of selected context types over five search runs, for architectures without and with an ASPP module<ref type="bibr" target="#b7">[8]</ref> inserted before the ATRC module. The change in proportion of the local context indicates that ATRC adapts to better complement the new backbone. This experiment was conducted on the PASCAL-Context dataset using a ResNet-50 based architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. C- 1</head><label>1</label><figDesc>depicts the different relational context types used in this work schematically. We use a 1?1 Conv-BN-ReLU layer as the learned non-linear transform. For all contexts except global, the similarity function is sim(q i , k j ) = exp( qik j d k ) (which corresponds to softmax). For the global context, it is simply sim(q i , k j ) = q i k j .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure C- 1 .</head><label>1</label><figDesc>Schematics of the different relational context types. The grids represent individual pixels (channels not shown), the attention mechanism is shown for one target pixel (framed in red box) respectively. Normalization is applied over all pixels of the attention map. A * are the auxiliary predictions, as depicted inFig. 2a.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>. 3.2.3), we directly use the ground truth D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure D- 2 .</head><label>2</label><figDesc>Qualitative NYUD-v2 comparison of regression and classification schemes for depth (top two rows) and surface normal (bottom two rows) estimation. Classification achieves satisfactory results on both tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>NYUD-v2 performance comparison, using a HRNet18 backbone. FPM = Feature Propagation Module<ref type="bibr" target="#b49">[50]</ref>.</figDesc><table><row><cell>Model</cell><cell>FPM</cell><cell>SemSeg ?</cell><cell>Depth ?</cell><cell>Normal ?</cell><cell>Bound ?</cell><cell>?m [%] ?</cell></row><row><cell>Single task</cell><cell></cell><cell>45.87</cell><cell>0.5397</cell><cell>20.09</cell><cell>77.34</cell><cell>0.00</cell></row><row><cell>Multi-task</cell><cell></cell><cell>41.96</cell><cell>0.5543</cell><cell>20.36</cell><cell>77.62</cell><cell>-3.05</cell></row><row><cell>MTI-Net [50]</cell><cell></cell><cell>45.97</cell><cell>0.5365</cell><cell>20.27</cell><cell>77.86</cell><cell>0.15</cell></row><row><cell>ATRC (ours)</cell><cell></cell><cell>46.27 46.33</cell><cell>0.5495 0.5363</cell><cell>20.20 20.18</cell><cell>77.60 77.94</cell><cell>-0.28 0.49</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>NYUD-v2 performance comparison, using a HRNet48 backbone. FPM = Feature Propagation Module<ref type="bibr" target="#b49">[50]</ref>.</figDesc><table><row><cell>Model</cell><cell>FPM SemSeg ?</cell><cell>PartSeg ?</cell><cell>Sal ?</cell><cell>Normal ?</cell><cell>Bound ?</cell><cell>?m [%] ?</cell></row><row><cell>Single task</cell><cell>62.23</cell><cell>61.66</cell><cell>85.08</cell><cell>13.69</cell><cell>73.06</cell><cell>0.00</cell></row><row><cell>Multi-task</cell><cell>51.48</cell><cell>57.23</cell><cell>83.43</cell><cell>14.10</cell><cell>69.76</cell><cell>-6.77</cell></row><row><cell>MTI-Net [50]</cell><cell>61.70</cell><cell>60.18</cell><cell>84.78</cell><cell>14.23</cell><cell>70.80</cell><cell>-2.12</cell></row><row><cell>ATRC (ours)</cell><cell>57.89 62.69</cell><cell>57.33 59.42</cell><cell>83.77 84.70</cell><cell>13.99 14.20</cell><cell>69.74 70.96</cell><cell>-4.45 -1.98</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table /><note>PASCAL-Context performance comparison, using a HR- Net18 backbone. FPM = Feature Propagation Module [50].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table E -</head><label>E</label><figDesc>1. NYUD-v2 comparison of the performance upper bound of T -label and S-label context, using ground truth (GT) spatial region maps A</figDesc><table><row><cell>(GT ) n</cell><cell>(see Sec. 3.2.3).</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>A. Training Details G. How Important is Self-Attention in ATRC?</p><p>The permutation testing results of Sec. 4.4 can be utilized to partly address this question. We conclude there that selfattention constitutes the most important distillation module for 3 out of 4 investigated tasks. However, other cross-task connections contribute significantly too. To investigate further, we provide in Table G-1 the performance of ATRC without self-attention. The multi-task performance ? m for this model is 0.87% (vs. 1.56% for the full ATRC), outperforming the single task configuration. This confirms that, even though self-attention is vital according to permutation testing, the cross-task distillation modules are able to provide a substantial performance boost on their own.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radhakrishna</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheila</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Attention augmented convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Random forests. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Breiman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automated search for resourceefficient branched multi-task networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bruggemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menelaos</forename><surname>Kanakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multitask learning: A knowledge-based source of inductive bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Detect what you can: Detecting and representing objects using holistic models and body parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">a 2 -nets: Double attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A coefficient of agreement for nominal scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Educational and psychological measurement</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="46" />
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mtl-nas: Task-agnostic neural architecture search towards general-purpose multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoping</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Nddr-cnn: Layerwise feature fusing in multi-task cnns by neural discriminative dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning rich features from rgb-d images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dsnas: Direct neural architecture search without parameter retraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoukang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sirui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hehui</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adaptive mixtures of local experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Nowlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="87" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menelaos</forename><surname>Kanakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bruggemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Obukhov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<title level="m">Reparameterizing convolutions for incremental multi-task learning without task interference. In ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Nikolaos Pappas, and Fran?ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelos</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pushing the boundaries of boundary detection using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Monocular depth estimation with hierarchical fusion of dilated cnns and softweighted-sum inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyi</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="328" to="339" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Measures of response agreement for qualitative data: some generalizations and alternatives. Psychological bulletin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Light</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971" />
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page">365</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Darts: Differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">End-toend multi-task learning with attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Ensemble learning via negative correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1399" to="1404" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fully-adaptive feature sharing in multi-task networks with applications in person attribute classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangfei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><surname>Javidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Chris J Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Teh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Ilija Radosavovic, and Iasonas Kokkinos. Attentive single-tasking of multiple tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevis-Kokitsi</forename><surname>Maninis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning to detect natural image boundaries using local brightness, color, and texture cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David R Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="530" to="549" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Cross-stitch networks for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Stand-alone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Large-scale evolution of image classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Selle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><forename type="middle">Leon</forename><surname>Suematsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kurakin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multi-task learning as multiobjective optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orazio</forename><surname>Gallo</surname></persName>
		</author>
		<title level="m">Erik Learned-Miller, and Jan Kautz. Pixel-adaptive convolutional neural networks. In CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Bert De Brabandere, and Luc Van Gool. Branched multi-task networks: deciding what layers to share</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Georgoulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Mti-net: Multi-scale task interaction networks for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Abhinav Gupta, and Kaiming He. Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Snas: Stochastic neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sirui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hehui</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Padnet: Multi-tasks guided prediction-and-distillation network for simultaneous depth estimation and scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Disentangled non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Objectcontextual representations for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Discriminatively trained dense surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Zeisl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Acfnet: Attentional class feature network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibin</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingtuo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Co-occurrent features in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Pattern-affinitive propagation across depth, surface normal and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Pattern-structure diffusion for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoqun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
