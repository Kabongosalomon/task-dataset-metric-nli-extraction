<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DSGN++: Exploiting Visual-Spatial Relation for Stereo-based 3D Detectors</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Yilun</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Shijia</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Shu</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Bei</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
						</author>
						<title level="a" type="main">DSGN++: Exploiting Visual-Spatial Relation for Stereo-based 3D Detectors</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-3D Object Detection</term>
					<term>Stereo Matching</term>
					<term>Autonomous Driving</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Camera-based 3D object detectors are welcome due to their wider deployment and lower price than LiDAR sensors. We first revisit the prior stereo detector DSGN for its stereo volume construction ways for representing both 3D geometry and semantics. We polish the stereo modeling and propose the advanced version, DSGN++, aiming to enhance effective information flow throughout the 2D-to-3D pipeline in three main aspects. First, to effectively lift the 2D information to stereo volume, we propose depth-wise plane sweeping (DPS) that allows denser connections and extracts depth-guided features. Second, for grasping differently spaced features, we present a novel stereo volume -Dual-view Stereo Volume (DSV) that integrates front-view and top-view features and reconstructs sub-voxel depth in the camera frustum. Third, as the foreground region becomes less dominant in 3D space, we propose a multi-modal data editing strategy -Stereo-LiDAR Copy-Paste, which ensures cross-modal alignment and improves data efficiency. Without bells and whistles, extensive experiments in various modality setups on the popular KITTI benchmark show that our method consistently outperforms other camera-based 3D detectors for all categories. Code is available at https://github.com/chenyilun95/DSGN2.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>C AMERA-based 3D visual perception is a fundamental and challenging task in 3D computer vision, which serves as the essential component for autonomous driving and robotics.</p><p>The main difficulty of camera-based 3D detectors lies in the fact that cameras provide front-view information but generally lack top-view cues or depth for accurate 3D object localization. A common choice for camera-based 3D detectors is to leverage the successful 2D object detectors <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> and depth estimators <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. A series of approaches <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> design complicated strategies to predict 3D boxes with explicit projective geometry of keypoints or boxes.</p><p>In contrast, the 2D-to-3D transformation converts the problem on 3D representation <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, which sidesteps the dimensionality loss in solving 3D problems. Particularly, the problem of 3D detection can be solved innately by predicting objects over every 3D spatial location. For instance, Pseudo-LiDARs <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref> generate explicit 3D representation followed by direct application of 3D detectors <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>. The explicit 3D form via, e.g., depth maps, occupancy grids, or pseudo point clouds, removes the uncertainty and decouples the tasks of depth estimation and object recognition.</p><p>However, predicting depth from images is ill-posed. Thus, the generated depth cost volume depicts the uncertainty of voxel occupancy. To preserve the knowledge of depth uncertainty, implicit modeling of geometry-encoded ? Y. <ref type="bibr">Chen</ref> feature volume becomes popular recently <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>. Stereo geometry (or epipolar geometry) is encoded into the concrete 3D voxel grids and guides the following 3D prediction. In essence, feature transformation from 2D to 3D representation avoids the loss of geometric uncertainty, which is proved influential for following 3D prediction <ref type="bibr" target="#b18">[19]</ref>. Therefore, valid information flow from 2D semantics to stereo volume determines the efficacy of the following 3D geometric representation for both geometric and semantic cues. However, current 3D modeling remains as an approximation of realistic 3D representation and poses three vital challenges for creating effective stereo feature volumes as follows:</p><p>(I) Direct 2D-to-3D information propagation along the ray constrains volumetric representation power. In geometric modeling, plane sweeping (PS) <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref> is the dominant way to lift 2D information to 3D volume. Specifically, per-view features are directly propagated by tracing the ray in the volumetric space <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> for matching the pixel differences. As 3D volume consumes one more order of magnitude of computation cost than 2D one, raw 2D features are required to be compressed to a small channel to reduce the amount of calculation. Accordingly, the 3D representation power is limited by the compressed 2D features. Our finding reveals that alleviating this bottleneck unleashes the power of volumetric representation for the following prediction tasks.</p><p>(II) Perception of differently shaped objects. By plane sweeping, we can produce two views of stereo volumes: Planesweep volume (PSV) in camera frustum and 3D-geometry volume (3DGV) in regular 3D space. However, real-world 3D objects are non-rigid and irregular-shaped. Some categories like Pedestrian occupy fewer voxels in the bird's eye view albeit being clearly visible in the front view. In our study, these single-view stereo volumes show varied properties. Plane-sweep volume extracts more voxel features for front-view objects (such as Pedestrian and Cyclist ) while 3D-geometry volume obtains same gradients for same object at different distances.</p><p>(III) Biased modeling. The proportion of the foregrounds is usually small in the aerial view for outdoor scenes, which curbs data efficiency. Additionally, imbalanced class distribution also gives the biased gradient flow towards frequent objects and suppresses the generalization ability of stereo modeling. These two difficulties restrict the model's capacity to generate unbiased estimation.</p><p>In this paper, we provide the following three solutions for addressing the above challenges in 2D-to-3D modeling. By polishing the overall stereo modeling, we present a simple yet effective stereo-based 3D detection framework DSGN++.</p><p>First, we present a generic operator for 2D-to-3D transformation -depth-wise plane sweeping (D-PS) to relieve the bottleneck of 2D-to-3D information propagation. With D-PS, the transformation allows the input of wider 2D features, that encodes depth-guided features within its expanded channels. And the generated volume yields continuously changing features that slice the 2D features via the sliding window technique. A key component called "cyclic slicing" is employed to realize local feature continuity for nearby depth planes. Experiments demonstrate its notable improvement for both monocular and binocular camera-based 3D object detection.</p><p>Second, we provide a new form of stereo volumetric representation -Dual-view Stereo Volume (DSV) to build more extensive connections to different views. We aggregate features of differently-shaped voxels from the front view (plane-sweep volume) and top view (3D-geometry volume). Notably, with a front-surface depth head, the final cost volume is generated by transforming volumetric representation to camera frustum space because we found that the geometric supervision of sub-voxel depth values in the front view provides stronger supervision than discretized voxel occupancy learning.</p><p>Last, to overcome the limited foreground regions in 3D modeling and make unbiased predictions towards categories, we seek to apply the copy-paste strategy <ref type="bibr" target="#b31">[32]</ref>. However, the requirement of precise cross-modal alignment restricts the freedom of data editing. To overcome the limitation, we propose Stereo-LiDAR Copy-Paste (SLCP) that allows joint stereo and 3D data editing and meets the constraint of cross-modal projection. We validate this flexible data editing improves modeling efficiency and generalization ability to various categories.</p><p>Our total contribution is fourfold.</p><p>? Without additional computation, we propose a novel volume construction way of depth-wise plane sweeping (D-PS) to expand the capacity of information flow and extract depth-relevant 2D features.</p><p>? We propose Dual-view Stereo Volume with the frontsurface depth head to extract the features from two differently spaced stereo volumes and investigates its effectiveness over prior constructions.</p><p>? For the first time, the method augments multi-modal data pair by Stereo-LiDAR Copy-Paste strategy that ensures the stereo alignments at the sub-pixel level and improves the data efficiency. We prove that the strategy greatly mitigates the class imbalanced problem.</p><p>? Without bells and whistles, our proposed DSGN++ achieves the first place for all categories among all camera-based approaches on the challenging KITTI benchmark <ref type="bibr" target="#b32">[33]</ref> on Nov 20, 2021, and even surpasses some LiDAR detectors in AP 3D , such as AVOD <ref type="bibr" target="#b33">[34]</ref> for the first time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Stereo Matching and Multi-View Stereo. With the development of neural networks in stereo matching, methods of <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref> process the left and right images by a Siamese network and construct a 3D cost volume to compute the matching cost. Correlation-based cost volume is applied in recent work <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>. Methods of <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> form a concatenation-based cost volume and apply 3D convolution to regress disparity estimates. For multi-view scene reconstruction, prior work <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref> even achieves fewer depth errors than RGB-D sensors, which shows great potential to be an alternative of expensive depth sensors. MVSMachine <ref type="bibr" target="#b43">[44]</ref> proposes the differentiable projection and unprojection for better extracting 3D to manipulate the volume construction from multi-view images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LiDAR-based 3D Detection.</head><p>LiDAR sensors are very powerful to produce data for 3D detectors. The target of LiDAR-based detectors is to extract discriminative features from point clouds for 3D object recognition. There are generally two types of 3D representations, i.e., voxel-based representation <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref> and point-based representation <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref>. Albeit depth sensors (e.g., LiDAR sensors and RGB-D cameras) can retrieve accurate depth cues, they are generally more expensive and are with sparser sensing resolution than the common off-the-shelf RGB cameras. We prove that with a simple fusion strategy, our stereo modeling can further promote the performance of LiDAR-based 3D detectors. Camera-based 3D Detection. In contrast to the high cost and sparse resolution of depth sensors, cameras are readily available and applied on a wide scale. The dense imaging resolutions (&gt; 720P) provide human-readable semantics that is easy to distinguish. The accessibility and dominance as the basic perception sensor in the real world make it attractive to perceive and understand 3D scenes. We classify methods into two types according to their intermediate 2D or 3D representation. The key difference is that 2D representation extracts features in the front view while 3D or bird's eye view (BEV) type extracts features in top views or 3D space.</p><p>For 2D representation-based 3D detectors, an intuitive solution is to leverage a 2D object detector <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. Similar to 2D object detectors, prior work <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b59">[60]</ref> directly estimates 3D bounding boxes from camera images and relies on perspective modeling of the 2D projected object and its 3D objects. On the other hand, depth supervision via point clouds or depth maps is accessible during training. Explicit learning of depth cues improves the accuracy of 3D object detection <ref type="bibr" target="#b60">[61]</ref>. Methods of <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b61">[62]</ref> also jointly aggregate the learned depth cues and semantic cues.</p><p>Generally, due to the consistency with 3D spaces, the 3D form provides an elegant and effective representation with no complicated post-processing steps. 3DOP <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b63">[64]</ref> generates point clouds by stereo and encodes the prior knowledge and depth in an energy function. Several methods <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref> transform the depth map to Pseudo-LiDAR with point cloud followed by another independent network. Pseudo-LiDAR <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b64">[65]</ref> introduces the pseudo point clouds as the intermediate 3D representation followed by a LiDAR-based 3D detector. This pipeline yields much improvement over previous 2D representation-based approaches. E2E-PL <ref type="bibr" target="#b25">[26]</ref> further enables back-propagation to depth coordinates by introducing radial basis functions. We note that these methods are limited to explicit modeling of depth maps. They compress the abundant information from pixel-level feature projection and correspondence. Recent end-to-end pipelines <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b66">[67]</ref> utilize 3D feature volumes as intermediate representation. Recently DSGN <ref type="bibr" target="#b18">[19]</ref> performs remarkably by implicitly encoding 3D geometry into neural networks. CDN <ref type="bibr" target="#b66">[67]</ref> further refine depth prediction near object boundarys via a Wasserstein distance-based loss. PLUME <ref type="bibr" target="#b24">[25]</ref> designs the efficient 3D-BEV network to achieve proper trade-off between speed and accuracy. LIGA-stereo <ref type="bibr" target="#b27">[28]</ref> further leverages the welllearned LiDAR-detector to transfer the knowledge to DSGN and demonstrates the effectiveness of cross-modal distillation <ref type="bibr" target="#b67">[68]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OUR APPROACH</head><p>In Sec. 3.1, as a prerequisite, we revisit the stereo volume generation and transformation in DSGN <ref type="bibr" target="#b18">[19]</ref> for encoding implicit cues of geometry and semantics. We introduce our DSGN++ model (shown in <ref type="figure" target="#fig_0">Fig. 1</ref>) to increase the capacity of stereo modeling in the following three aspects.</p><p>First, we identify the network bottleneck that limits the quantity of information flow and introduce a generic operator -depth-wise plane sweeping (D-PS) (Sec. 3.2) for allowing denser connections between 2D and stereo volumes.</p><p>Second, in Sec. 3.3, we compare the volume effectiveness for differently shaped objects between camera front-view and top-view. For aggregating more view-specific features, we introduce Dual-view Stereo Volume (DSV), which includes volumes integration and front-surface depth head.</p><p>Finally, in Sec. 3.4, we introduce a multi-modal data augmentation strategy -joint Stereo-LiDAR copy-paste for increasing the positive ratios and balancing the category distribution in each training sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Stereo Volumes Generation Revisit</head><p>Given a binocular image pair (I L , I R ), the objective is to detect and localize objects in 3D world space. To avoid information loss of depth uncertainty in explicit data structures such as point clouds, recent approaches <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b24">[25]</ref> create stereo volumes and encode the geometry cues into the 3D feature volume. With accurate depth cues, the 3D detection head can detect and regress 3D objects efficiently, especially for faraway objects. In this section, we briefly revisit several structures of stereo feature volumes -plane-sweep volume in camera frustum space and 3D-geometry volume (or BEV volume <ref type="bibr" target="#b24">[25]</ref>) in 3D regular space.</p><p>For simplicity, we denote the voxel coordinate u = (u, v, d) in the camera frustum and the voxel coordinate x = (x, y, d) in pre-defined voxel space, where d denotes the depth dimension. proj : R 3 ? R 2 represents the projection. Binocular Images to Plane-Sweep Volume. The binocular features are generated by feeding a binocular image pair (I L , I R ) into a Siamese network. In stereo matching <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b68">[69]</ref> and MVS <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b43">[44]</ref>, a set of evenly-spaced depth (or disparity) planes are generated towards the target view. By the classic sweeping planes towards the camera view, multiplane images (MPI) <ref type="bibr" target="#b69">[70]</ref> are generated by gathering image features at each depth plane. The per-view mapping function can be formulated as</p><formula xml:id="formula_0">P SV : R H I ?W I ?C I ? R H I ?W I ?D V ?C V , P SV (I) = V proj where V proj (u, c) = I (proj(u), c) .<label>(1)</label></formula><p>where the size of (H I , W I ) is linearly related to (H I , W I ) . Voxels with coordinates {x = (u, v, d)} P SV are uniformly spaced in the camera frustum with C V (C V = C I channels.</p><p>By comparing feature similarity of each voxel, the following neural network infers the underlying 3D geometry at the target view. Plane-Sweep Volume to 3D-Geometry Volume. As the final objective is to detect 3D objects in 3D world space, one way to encode the scene in the 3D world is to transform PSV to 3DGV. Specifically, a detection area of size (</p><formula xml:id="formula_1">H V , W V , D V )</formula><p>can be discretized into voxel gird. 3D-geometry volume is computed by reversing 3D projection from camera frustum space to 3D world space. Binocular Images to 3D-Geometry Volume. Another way to construct 3D-geometry volume (or BEV volume <ref type="bibr" target="#b24">[25]</ref>) is introduced by the operation of differentiable unprojection <ref type="bibr" target="#b43">[44]</ref>. The discrete grid {(x, y, z)} 3DGV obtains the projected 2D image features at (u, v) by differentiable bilinear sampling. Image features at different views are aggregated inside each voxel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Depth-wise Plane Sweeping for 2D-to-3D Transformation</head><p>3D volume construction from the images is critical to represent a 3D scene for either monocular or multi-view settings. It facilitates a series of downstream 3D applications, e.g., stereo matching, novel view synthesis, and 3D object detection. Without loss of generality, for a predefined voxel grid of size (H V , W V , D V ) in arbitrarily voxelized space, we retrieve multi-view features by the per-view projection of each voxel coordinate p = (x, y, z). The general formulation from a camera view to a specific volume V is expressed as</p><formula xml:id="formula_2">V : R H I ?W I ?C I ? R H V ?W V ?D V ?C V , V(I) = V proj where V proj (p, c) = I(proj(p), c).<label>(2)</label></formula><p>where the 2D feature has the shape of (H I , W I , C I ).  orthogonal to camera planes is generated. Normally, the feature grids are filled by replication of image features through viewing rays. The size of generated 4D tensor V is normally far larger than the source 2D feature tensor, expressed as</p><formula xml:id="formula_3">(f) Front-Surface Depth Head BEV Detector Plane Sweep Volume SPP module 3D Geometry Volume 3D Hourglass Upsample (b) Plane Sweeping (a) Siamese Network (c) D-PSV (e) Dual-View Stereo Volume 3D Detection Mask Add 4X 3D Upsample 3D Warp Information Flow BEV Downsample ? th-wise Volume C Dual-View Stereo Volume C Concat (d) D-3DGV Conv3Ds</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D/2D Convolutions 3D/2D Hourglass</head><formula xml:id="formula_4">H V ? W V ? D V ? C V H I ? W I ? C I .<label>(3)</label></formula><p>In prior stereo networks <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, the number of depth planes (D V ) is usually large, e.g., 192-D and the feature resolution is maintained at least a quarter of the full resolution for matching at the pixel level. In other words, the representation power (a.k.a. effective degree of freedom <ref type="bibr" target="#b70">[71]</ref>, <ref type="bibr" target="#b71">[72]</ref>) of the constructed 3D volume is constrained by its narrow 2D feature (small feature channels). An ideal way for building a denser connection is to expand the channel size C I of 2D features, which reduces the tensor dimension gap. However, expanding C V is not straightforward as expanding volume channels C V leads to more calculations in 3D. More, the transformation needs to maintain feature locality for matching the left-right correspondence cost.</p><p>Accordingly, we introduce depth-wise (disparity-wise) plane sweeping (D-PS) to build denser connections between 2D feature maps and 3D feature volumes. Instead of compressing 2D channels to a small number, we preserve the number of channels C I at a relatively large number (e.g., 96) and slice the feature via a sliding window (C V channels) along the channel axis. The shift on the channel axis depends on pixel disparity (inverse depth) as that distant object recognition is sensitive to sub-pixel differences. We empirically show in Sec. 4.3.1 that overcoming this challenge leads to considerable performance gain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D Geo Volu</head><p>Plane Sweep Volum</p><formula xml:id="formula_5">(a) Sia (c) D- Inf Depth Channels Cyclic Slicing ? ? Depth-wise Volume Per-View 2D Feature ( , ) (d) D-3 Channel Reordering</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2:</head><p>Depth-wise plane sweeping. Assume the constructed volume is a 4D tensor in the space along (x, y, depth, channel) axes. We visualize the depth-channel plane of 3D featured volume where the graduated color indicates channel orders. Depth-wise volume is constructed by jointly sweeping the depth planes and slicing the features along the channel dimension. Cyclic slicing reorders the channels to ensure channel consistency across nearby depth planes.</p><p>However, directly slicing the feature by shift produces unstable features as the order of feature channels is unchangeable. Therefore, we propose Cyclic Slicing to ensure local feature similarity for adjacent objects, i.e., reordering the channels of the sliced features to maintain channel consistency. Cyclic Slicing. Given a voxel coordinate x = (x, y, d), with its image feature channel C I , we obtain the feature slice with C V (C V ? C I ) channels as the voxel feature. As shown in <ref type="figure">Fig. 2</ref>, we divide the C I channels into several parts. Each region contains C V channels except the last part. The sliced channels are an ordered union of two channel intervals of</p><formula xml:id="formula_6">disp/C V ? C V , disp + C V ? disp, disp/C V ? C V where</formula><p>is the ceiling function. The reordering of the selected channels ensures feature continuity around nearby depth planes. For simplicity, we ignore cyclic slicing in Eqs. <ref type="bibr" target="#b3">(4)</ref> and <ref type="bibr" target="#b4">(5)</ref>. The way to generate depth-wise plane-sweep volume (D-PSV) is expressed as</p><formula xml:id="formula_7">D-P SV : R H I ?W I ?C I ? R H I ?W I ?D V ?C V D-P SV (I) = V proj (4) where V proj (u, c) = I proj(u), f u ? baseline d ? s + c .</formula><p>s is the ratio between 2D feature channels C I and the number of depth planes D. u denotes the coordinate (u, v, d) in the camera frustum. ? controls the smoothness of channel shifting rate. f u denotes horizontal focal length and baseline denotes stereo camera baseline. Similarly, construction of depth-wise 3D-geometry volume (D-3DGV) is formulated as</p><formula xml:id="formula_8">D-3DGV : R H I ?W I ?C I ? R H V ?W V ?D V ?C V D-3DGV (I) = V proj (5) where V proj (x, c) = I proj(x), f u ? baseline d ? s + c .</formula><p>where x denotes the 3D coordinate (x, y, z). The computation complexity is exactly same as classic plane sweeping despite the growth of memory usage (the expansion of 2D feature map size). Experiments show that the simple solution to reduce the bottleneck leads to a considerable performance boost without extra techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Dual-view Stereo Volume for Building Effective 3D Representation</head><p>In this section, we compare the information flows of two pipelines in recent works <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b26">[27]</ref> as shown in <ref type="figure">Fig. 3</ref> (a, b) and analyze the difference between their voxel shapes. Further, to effectively represent depth and semantics, we introduce a new stereo volume -Dual-view Stereo Volume (DSV), that is susceptible to both views. This volume construction contains two key steps: Volume Integration and Front-Surface Depth Head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Front-view Representation vs. Top-view Representation.</head><p>In geometric learning, the front-view (FV) pipeline adopts plane-sweep volume for front-view depth learning in the camera frustum. Differently, the top-view (TV) pipeline constructs 3D structures within 3D-geometry volume (3DGV) in 3D regular space. The essential difference between the stereo volumes lies in their different shaped voxels or spaces, which directly leads to diverse receptive fields and voxel occupancy densities. Hence, taking KITTI dataset <ref type="bibr" target="#b32">[33]</ref> as an example, we visualize average voxel occupancy counts for all categories and their performance in <ref type="figure">Fig. 4</ref>. Visually, nearby objects in PSV occupy much more voxels than faraway objects while the 3DGV curve is smoother. However, the average voxel occupancy counts </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 4: Comparison of average voxel occupancy per category within the plane-sweep and 3D-geometry volumes.</head><p>We set the maximum voxel numbers to 600 for visualization of distant regions.</p><p>for Pedestrian and Cyclist (&lt; 20m) are less than 100 voxels. The limited voxel occupancy impedes effective gradients towards the smaller objects, resulting in a performance reduction in the top-view pipeline. On the other hand, the distribution in PSV volume can deteriorate the learning of faraway objects. Stereo Volumes Integration. After construction of D-P SV and D-3DGV , we aggregate both information flows in our framework. Integration of both volumes allows the voxel to aggregate differently spaced 3D structure information and further expand the 2D-to-3D information flow. Specifically, we transform D-P SV to 3D space and concatenate both volumes followed by a 3D Hourglass module <ref type="bibr" target="#b5">[6]</ref>. The diversely spread voxel features are accessible within the combined feature volume. Experiments demonstrate the joint flows perform better than each independent volume under strong augmentation. Front-Surface Depth Head. Geometric learning determines the perception accuracy of distant scenes. DSGN <ref type="bibr" target="#b18">[19]</ref> intermediately supervises the depth inside plane-sweep volume followed by the transformation to 3D-geometry volume that cannot benefit from the following computations. PLUME <ref type="bibr" target="#b24">[25]</ref> adopts occupancy loss in the voxel grid that discretizes the depths, which is inferior to reason the geometry as shown in experiments (Sec. 4.3.1).</p><p>To perceive accurate front-surface depths, the depth head on stereo volume in 3D space (e.g., DSV) is first transformed to the frustum space followed by front-view depth supervision. Meanwhile, the semantic supervision (e.g. 3D bounding boxes) jointly acts on the same feature volume. In detail, the transformation starts by building the 3D coordinate mapping from camera frustum space to 3D space. With the coordinate mapping, voxels at (u, v, d) of the front-view volume obtain stereo volume features at (x, y, z):</p><formula xml:id="formula_9">? ? u v 1 ? ? d = ? ? f u 0 c u 0 f v c v 0 0 1 ? ? ? ? x y z ? ? ,<label>(6)</label></formula><p>where f u , f v are the horizontal and vertical focal lengths. We ignored the extrinsics for simplicity. This generated front-view volume has the identical shape of PSV and is followed by a upsampling head network. The head network includes one hidden 3D convolution and a convolution that squeeze the channels to 1. The generated cost volume is then upsampled to original image size and supervised with depth loss <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b27">[28]</ref>. The construction enables monopeak front-view depth that coincides with the real depth sensor data. Note that the transformation-based depth head is pluggable and can be inserted to 3D-geometry volume for both monocular and stereo settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Stereo-LiDAR Copy-Paste for Improving Data Efficiency</head><p>We illustrate the necessity to augment more and balanced foreground objects into the training scene as follows: Limited Foreground Area Ratio in Top View. The 2Dto-3D transformation reduces the problem of front-view 3D detection to BEV detection. However, the foreground region ratio is also reduced in the bird's eye view. The imbalance decreases the magnitude of foreground gradients back to the 2D network, leading to biased model learning. Imbalanced Class Distribution. Long-tailed distribution commonly exists <ref type="bibr" target="#b72">[73]</ref> in real scenes. For example, pedestrian and cyclist exists in less than 1/3 of full data, direct training of the imbalanced data could bias the gradient flow. Unlike point clouds, multi-modal data augmentation is constrained by the tight correspondence between image and point clouds. For the localization of 3D objects, the sub-pixel misalignment affects the estimation of stereo disparity, leading to the large localization error for distant objects. Common copy-paste <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b73">[74]</ref> randomly pastes object patches onto 2D images, which makes it hard to satisfy the projective constraint. Moreover, the segmentation mask is unavailable in binocular images and it is hard to guarantee the left-right alignment at pixel level without human annotations. For binocular training samples, the source object patches are cropped with their calibrated projections P S lef t , P S right . The augmented scene uses projections P T lef t , P T right . Object patches are pasted bilinearly to binocular images.</p><p>To effectively augment the stereo data at the instance level, for the first time in literature, we propose a multimodal data editing strategy -Stereo-LiDAR copy-paste (SLCP) that maintains precise cross-modal alignments at the sub-pixel level. Specifically, we preserve the 3D location of the source objects and project the 3D boxes onto images with the target camera's internal parameters. As shown in <ref type="figure" target="#fig_1">Fig. 5</ref>, suppose we sample several objects from their source scene for a target training scene. The cropped point clouds within objects can be put into the training scene. For binocular training images, we compute the projected 2D bounding box B S for each 3D object box B 3D by the source projection matrices {P S lef t , P S right }. By projecting the same 3D box by target projection P T lef t , P T right , we obtain the target bounding box B T . We crop and warp the source object image patches to the respective target boxes as B S ? B T . Note stereo alignment still holds under horizontal flipping. In this way, the alignment between LiDAR and both stereo images is guaranteed at the sub-pixel level. Also, to ensure uni-peak depth, the overlapped 3D points, whose projections are within B T , are removed.</p><p>For a training scene, we sample a sufficient and balanced number of objects per category (5 objects per category in our experiments) from other data and paste objects into the multi-modal data. Experiments demonstrate that our strategy effectively improves data efficiency and largely mitigates the imbalanced class distributions. Comparison with geometry-preserved copy-paste. Lian et al. <ref type="bibr" target="#b74">[75]</ref> introduce geometry-preserved way to paste segmented objects <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b73">[74]</ref> for monocular 3D object detection. Instead of further position and size change under geometric constraints, our copy-paste simply maintains the 3D object locations for keeping both constraints of projection and stereo alignment. And our method also augments the corresponding point clouds within 3D boxes for LiDAR supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we conduct extensive experiments to validate the effectiveness of our proposed framework with the settings of various modalities. In Sec. 4.1, we briefly illustrate the datasets and experimental setups for 3D object detection. The quantitative and qualitative results are provided in Sec. 4.2 and Sec. 4.4, respectively. We further illustrate the usefulness of each component in the ablation study (Sec. 4.3.1). Lastly, the efficiency of our approach is discussed in Sec. 4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Datasets. We evaluate methods on the popular KITTI 3D object detection dataset <ref type="bibr" target="#b32">[33]</ref>, which is a union of 7, 481 stereo image-pairs and point clouds for training and 7, 518 for testing. The training data has annotations for Car, Pedestrian and Cyclist. The ground-truth depth maps are generated from point clouds following <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b20">[21]</ref>. Following the protocol in <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b49">[50]</ref>, the training data is divided into a training set (3,712 images) and a validation set (3,769 images). As KITTI leaderboard limits the access to submission to the server for evaluating the test set, the ablation studies are conducted on the KITTI train-val split. Evaluation Metric. KITTI divides the evaluation metric into three regimes (Easy, Moderate, and Hard) according to their recognition difficulty, which considers object occlusion/truncation and the size of an object in the 2D image. The AP evaluations for 2D, BEV and 3D have diverse IoU criteria per class , i.e., IoU ? 0.7 for Car, IoU ? 0.5 for Pedestrian and Cyclist. All experiments and ablation studies adopt AP | R40 by default as the KITTI benchmark altered AP calculation that utilizes 40 recall positions (AP | R40 ) instead of the earlier 11 recall positions (AP | R11 ). Experimental Setups. Our models are trained with the respectively best-performing parameters on four NVIDIA V100 GPUs, each GPU holding one pair of stereo images of size 384 ? 1248. We apply ADAM <ref type="bibr" target="#b79">[80]</ref> optimizer with initial learning rate 0.001. Data augmentation strategy <ref type="bibr" target="#b18">[19]</ref> includes horizontal flipping and Stereo-LiDAR copy-paste. All models are trained for 60 epochs and the learning rate is decreased by 10 at the 50-th epoch. Baseline Methods. We build our framework based on the official author-released codes -DSGN <ref type="bibr" target="#b18">[19]</ref> and LIGA <ref type="bibr" target="#b27">[28]</ref>). LIGA reproduced and improved DSGN in the code framework of OpenPCDet <ref type="bibr" target="#b80">[81]</ref> with several technical modifications for a stronger baseline. More implementation details are referred to the paper. We discard the strong cross-modal distillation technique throughout our experiments. Based on the LIGA 's reproduced DSGN, we make several modifications for efficient implementation and adopt it (called L-DSGN) as the baseline approach unless otherwise specified. Particularly, we set the kernel size to 1 ? 1 ? 1 for all the first 3D convolution after stereo volumes and move the 3D hourglass network after Dual-view Stereo Volume as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Also, as depth-wise plane sweeping supports wider feature inputs, L-DSGN aggregates the multi-scale features by concatenation for binocular feature extraction instead of feature addition. The last 2D convolutional layer uses the number of filters C I according to the type of plane sweeping. During testing, the 2D detection head and depth prediction head are dropped. Synchronized batch normalization is applied throughout the network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Quantitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Official Results on the KITTI test benchmark</head><p>We report experimental results with comparison on the KITTI test set as shown in <ref type="table" target="#tab_4">TABLE 1 and TABLE 2</ref>. Without cross-modal distillation <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b67">[68]</ref>, the simple solution DSGN++ outperforms all other stereo-based approaches over all difficulties and evaluation metrics (Car: +2.71 AP 3D , +2.16 AP BEV , and +1.88 AP 2D in moderate difficulty regime). In terms of AP 2D , our approaches achieve the impressively high 95. <ref type="bibr" target="#b69">70</ref> AP, surpassing all the strong 3D object detectors.</p><p>As detecting smaller and non-rigid 3D objects pose the greater challenge for the regime of camera-based 3D detectors, only several approaches report the results for Pedestrian and Cyclist. To verify the generalization ability of our approach, we provide the results of Pedestrian and Cyclist in TABLE 2. Our approach achieves noticeable improvements over prior methods (Cyc.: +7.04 AP 3D , Ped.: +2.74 AP 3D ).</p><p>Compared with LiDAR-based approaches, in terms of AP 3D , our method even completely beats some LiDAR detectors including AVOD for all categories (for the first time in literature). Concretely, DSGN++ exceeds AVOD by 5.82 AP 2D in the front view while scoring 6.01 AP BEV lower than AVOD in the bird's eye view. This comparison indicates that the performance between camera-based approaches mainly lies in the foreground depth estimation error.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Method Performance in Stereo Setup</head><p>As shown in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Method Performance in Multi-Modal Setup</head><p>We further validate the effectiveness of our approaches in the multi-modal setting -binocular cameras and LiDAR. The adopted LiDAR baseline network is SECOND4x <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b27">[28]</ref>, which downsamples the sparse grid by four times on the bird's eye view and has the same grid size with stereo volume. Without special design, we simply fuse the learned Dual-view Stereo Volume by feature addition with LiDAR feature volume generated by SECOND4x. This multi-modal modeling uses the same training setup as in Sec. 4.1.</p><p>We conduct a set of experiments that input LiDAR signals from sparse (4 beams) to dense (complete 64 beams) to validate the complementary effects of stereo cameras. The low-beams simulation of LiDAR signals follows <ref type="bibr" target="#b20">[21]</ref>. As shown in  beams performs better in primary metric AP 3D and AP 2D while getting worse results in AP BEV . The results reveal the LiDAR model benefits more from accurate front-view detection performance than the top-view one. As a result, the multi-modal 3D detector still performs better in 3D metrics.</p><p>In particular for the complete 64-beams LiDAR setup, direct multi-modal modeling by fusing L-DSGN even deteriorates the detection performance. In contrast, multi-modal modeling with DSGN++ yet improves the LiDAR network significantly (Car: +4.09 AP 3D , Ped.: +3.63 AP 3D , Cyc.: +1.89 AP 3D ). This comparison reveals the fact that stereo representation can provide strong complementary cues over the vanilla LiDAR signals. The fusion of multi-sensor is promising and improves the robustness of 3D perception system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>In this section, we investigate the effectiveness of the major adaptations. For fair comparisons, we conduct ablation studies on the KITTI val set mainly in TABLE 4. Note that we primarily adopt the accuracy of Car category for the ablation study by default, as the dataset contains fewer annotations for Pedestrian and Cyclist that causes greater result variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Ablation Study for Depth-wise Plane Sweeping</head><p>As shown in TABLE 4 (a. vs. b.; d. vs. e.; h. vs. i.), with similar computations, models with depth-wise plane sweeping obtains extra 1.5?3.2 AP improvements. In general, performance gains are consistent for all three categories, which indicates that traditional plane sweeping is unsuited for representing complicated predictions, and D-PS much eases it. TABLE 5 ablates the expanded channels C I and smoothness factor ? for plane sweeping. We observe that the wider 2D features contribute to the learning of 2D semantic features. We compare D-PS with another possible choice (called Group-PS) to input the features with expanded channels: equally splitting the channels/depths into several groups; extracting the depth-wise features in respective groups. However, Group-PS cannot guarantee feature's channelwise similarity between groups for stereo matching and yields 65.48 AP 3D . D-PS is designed to preserve the local feature continuity and the degree of sharing can be adjusted w.r.t disparity. The smoothing factors ? = 0.1 (66.42 AP 3D ) for FV and ? = 0.5 (64.59 AP 3D ) for TV yield best performance. Dual-view stereo volume adopts the respective best-performing parameters for both volumes.</p><p>In addition, to illustrate the generality of D-PS, we also provide the monocular experiments. We simply adapt the top-view pipeline for monocular 3D detectors (remove right   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Ablation Study for Stereo Volumetric Representation</head><p>We conduct the comparison of several stereo pipelines (See information flows in <ref type="figure">Fig. 3)</ref>). Effects of Front-Surface Depth Head.  In terms of depth estimation quality, as visualized in <ref type="figure" target="#fig_4">Fig. 7</ref>, the ratio of foreground localization error w.r.t object distances gets a smoother slope. Overall, our model reduces the foreground depth estimation error from 0.63 to 0.57 (m). TABLE 6 ablates several hyper-parameters used in Stereo-LiDAR copy-paste. TABLE 6 (a., b., f., i.) ablates the number of pasted samples into the training scenes and 5 samples yield the best performance. Compared with raw data pair inputs (a.), the sufficient pasted objects greatly alleviate the imbalanced problems across categories (f.). TA-BLE 6 (a., c., d.) ablates the effect of more positive numbers, where the improvement of +2.53 AP 3D for Car (TABLE 6 (a. vs. c.)) indicates that the current positive instances limit   the modeling efficiency even for the most frequent class. (e. -h.) ablates the probability of applying copy-paste. As the image-level copy-paste produces object patch artifacts and occlusion, the larger probability can lead to worse results. TABLE 6(f. vs. j.) ablates whether to remove the background point clouds and the results reveal the removal of background points facilitates the model to better learn from copy-paste augmentation (Car: +2.60 AP 3D ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Qualitative Results</head><p>We present some representative results in <ref type="figure" target="#fig_3">Fig. 6</ref>, especially for the occlusion situations. From the visualization of BEV results, our approach robustly predicts most objects and estimates their accurate 3D bounding boxes even for the scene 50 meters away (red line in the bird's eye view). The visualization shows the great potential for the lowcost outdoor perception system based on stereo cameras. Noticeably, some extremely occluded cases in the top row could be still detected. The bottom row in <ref type="figure" target="#fig_3">Fig. 6</ref> also visualizes some failure cases including missing occluded objects, missing distant objects, and wrong orientation/dimension predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Efficiency Study</head><p>As the large performance gap between LiDAR-based approaches and camera-based approaches <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, most works focus on the improvement of detection accuracy. Efficiency comparison of different algorithms is less investigated due to various experimental setups.  For a fair comparison, we conduct the efficiency comparisons on an NVIDIA RTX 2080TI GPU as shown in TABLE 7. Generally, the complete DSGN++ with ResNet-34 runs takes 0.273s on average, where binocular feature extraction takes 2 ? 0.058 = 0.116s, PSV takes 0.036s, 3DGV takes 0.045s, DSV and 3D network take 0.044s and last BEV detector costs 0.012s. The inference time of single-view pipelines TV-DSGN++ and FV-DSGN++ are 0.198s and 0.202s, respectively. Despite their lower performance compared with DSGN++, the simple single-view pipelines can also serve as simple baselines for stereo 3D object detection for faster speed.</p><p>For accelerating our pipeline, we provide another efficient implementation (R18-DSGN++) to demonstrate the speed-accuracy trade-off of our work. We replace ResNet-34 with ResNet-18 and adopt the same 2D upsampling head for both stereo volumes. The efficient model still achieves 68.12 AP 3D despite the backbone network removing about half of the parameters. This fact also indicates that the original 2D backbone network is not fully exploited for the construction of the following stereo volumes as described in Sec. 3.2.</p><p>We note that the code is not yet fully optimized and affects the speed of the 3D detector. For example, despite the same computation, the CUDA implementation of D-PS costs extra 4ms than PyTorch built-in implementation of F.grid sample for plane sweeping. We leave the further code optimization to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We have renewed several key components that build an end-to-end stereo detection pipeline and provided a new stereo modeling -DSGN++ -for 3D object detection. Without bells and whistles, we conducted a set of comprehensive experiments to illustrate the effectiveness of the proposed modules. Specifically, the proposed depth-wise plane sweeping allows inputs of wider 2D features and improves modeling efficiency in 2D-to-3D transformation. Dual-view stereo volumes provide better 3D representations that grasp differently spaced features. And Stereo-LiDAR copy-paste strategy largely improves data efficiency and enhances modeling generalization ability for all categories. We expect the framework provides a strong baseline for the future application of camera-based 3D perception systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Overview of the proposed DSGN++ framework. The whole framework consists of six components. (a) 2D image extraction network for extracting stereo features. (b) Volume construction process by Depth-wise Plane Sweeping. (c) 3D CNN for front-view and top-view feature extraction. (d) Dual-view flow integration followed by 3D CNN. (e) Front-surface depth head for supervising depth signals in the front-view. (f) 3D detection head that detects objects in bird's eye view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 :</head><label>5</label><figDesc>Joint Stereo-LiDAR copy-paste strategy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Implementation of Stereo Volumes. PSV is pre-defined with shape (W I /4, H I /4, D I /4, 64), where the image size is (W I = 1248, H I = 384). Both left and right image features have 32 channels. The number of depth D I is set to 192 (DSGN) and 288 (L-DSGN). Extra 3D convolutions are applied to squeeze the channel dimension to 32-D. 3DGV contains a 3D voxel occupancy grid of size (W V = 300, H V = 20, D V = 288) along the respective directions in KITTI camera's view with each voxel of size (0.2, 0.2, 0.2) (meters). Extra 3D convolutions with 32 filters for compressing the features when generating 3DGV (H V ? W V ? D V ? 64) directly from binocular images. Both stereo volumes adapt the 2D semantic features by planesweeping. We set the shifting ratio ? of Depth-wise plane sweeping to 0.1 and its input channels C = 96 by default.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 :</head><label>6</label><figDesc>Qualitative results on the KITTI val set. Green boxes represents ground-truth and red boxes denotes our predictions. The left-view images are shown in the left column and the BEV point clouds images are shown on the right side. Some failure cases are shown at the bottom of the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 :</head><label>7</label><figDesc>Comparison of foreground object localization error. Red dotted line computes the regressed line. The localization error computes the average depth estimation error within the 3D object boxes at the respective depth ranges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, S. Huang, B. Yu and J. Jia are with the Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong, China.</figDesc><table /><note>E-mail: {ylchen,sjhuang,byu,leojia}@cse.cuhk.edu.hk; liushuhust@gmail.com? J. Jia and S. Liu are with the SmartMore.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Front-View Pipeline Top-View Pipeline 3D Detector 3D Detector Depth 3D Detector Dual-View Pipeline Plane Sweeping 3D Warp Information Flow</head><label></label><figDesc></figDesc><table><row><cell>&amp;DU</cell><cell>3HGHVWULDQ</cell><cell>&amp;\FOLVW</cell></row><row><cell>Front-View 67.63 AP Top-View 67.85 AP</cell><cell>Front-View 42.98 AP Top-View 36.20 36.20 AP</cell><cell>Front-View 41.63 AP Top-View 29.17 AP</cell></row><row><cell>9R[HO&amp;RXQW</cell><cell></cell><cell>Pipelines TV FV</cell></row><row><cell>'LVWDQFH</cell><cell>'LVWDQFH</cell><cell>'LVWDQFH</cell></row></table><note>Fig. 3: Comparison of stereo information flows. Prior stereo detectors adapt 2D features to plane-sweep volume (green cube) or 3D-geometry volume (golden cube). Differently, dual-view stereo volume (DSV) aggregates both spaced features in 3D space and enforces geometric learning in the front view that is fit for visual sensors.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>Sensor</cell><cell>Methods</cell><cell>Source</cell><cell></cell><cell>Ped. AP 3D</cell><cell></cell><cell cols="3">Ped. AP BEV</cell><cell></cell><cell>Cyc. AP 3D</cell><cell></cell><cell>Cyc. AP BEV</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Easy</cell><cell>Mod</cell><cell>Hard</cell><cell>Easy</cell><cell>Mod</cell><cell>Hard</cell><cell>Easy</cell><cell>Mod</cell><cell>Hard</cell><cell>Easy</cell><cell>Mod</cell><cell>Hard</cell></row><row><cell>LiDAR</cell><cell>Point R-CNN [22] AVOD [34]</cell><cell>CVPR2018 IROS2018</cell><cell cols="2">47.98 39.37 36.10 27.86</cell><cell>36.01 25.76</cell><cell cols="2">54.77 46.13 42.58 33.57</cell><cell>42.84 30.14</cell><cell cols="2">74.96 58.82 57.19 42.08</cell><cell>52.53 38.29</cell><cell>82.56 67.24 64.11 48.15</cell><cell>60.28 42.37</cell></row><row><cell></cell><cell>OC-Stereo [65]</cell><cell>ICRA2020</cell><cell cols="2">24.48 17.58</cell><cell>15.60</cell><cell cols="2">29.79 20.80</cell><cell>18.62</cell><cell cols="2">29.40 16.63</cell><cell>14.72</cell><cell>32.47 19.23</cell><cell>17.11</cell></row><row><cell></cell><cell>Disp R-CNN [77]</cell><cell cols="3">TPAMI2021 37.12 25.80</cell><cell>22.04</cell><cell cols="2">40.21 28.34</cell><cell>24.46</cell><cell cols="2">40.05 24.40</cell><cell>21.12</cell><cell>44.19 27.04</cell><cell>23.58</cell></row><row><cell></cell><cell>DSGN [19]</cell><cell>CVPR2020</cell><cell cols="2">20.53 15.55</cell><cell>14.15</cell><cell cols="2">26.61 20.75</cell><cell>18.86</cell><cell cols="2">27.76 18.17</cell><cell>16.21</cell><cell>31.23 21.04</cell><cell>18.93</cell></row><row><cell>Stereo</cell><cell>CG-stereo [78]</cell><cell>IROS2020</cell><cell cols="2">33.22 24.31</cell><cell>20.95</cell><cell cols="2">39.24 29.56</cell><cell>25.87</cell><cell cols="2">47.40 30.89</cell><cell>27.73</cell><cell>55.33 36.25</cell><cell>32.17</cell></row><row><cell></cell><cell cols="2">YoLoStereo3D [79] AAAI2021</cell><cell cols="2">28.49 19.75</cell><cell>16.48</cell><cell cols="2">31.01 20.76</cell><cell>18.41</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>LIGA [28]</cell><cell>ICCV2021</cell><cell cols="2">40.46 30.00</cell><cell>27.07</cell><cell cols="2">44.71 34.13</cell><cell>30.42</cell><cell cols="2">54.44 36.86</cell><cell>32.06</cell><cell>58.95 40.60</cell><cell>35.27</cell></row><row><cell></cell><cell>DSGN++ (Ours)</cell><cell>-</cell><cell cols="2">43.05 32.74</cell><cell>29.54</cell><cell cols="2">50.26 38.92</cell><cell>35.12</cell><cell cols="2">62.82 43.90</cell><cell>39.21</cell><cell>68.29 49.37</cell><cell>43.79</cell></row></table><note>Performance comparison on the official KITTI test server (Car). * means refining the pseudo point clouds by additional 4-beam LiDAR. Best results are highlighted in bold. LiDAR supervision (L Sup.) represents whether to apply LiDAR depth supervision.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 2 :</head><label>2</label><figDesc></figDesc><table /><note>Performance comparison on the official KITTI test server (Pedestrian and Cyclist ). Best results are highlighted in bold.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 3</head><label>3</label><figDesc>Car: +5.53 AP 3D , Ped.: +8.78 AP 3D , Cyc.: +11.67 AP 3D ) and L-DSGN (Car: +5.54 AP 3D , Ped.: +8.32 AP 3D , Cyc.: +16.19 AP 3D ) regarding the moderate evaluation difficulty. The more noticeable improvements for Pedestrian and Cyclist reveal that the imbalanced class learning is greatly alleviated. Even without LiDAR signals supervision, our method still achieves 66+ AP 3D for Car category and demonstrates significant improvements (Car: +10.86 AP 3D , Ped.: +6.75 AP 3D , Cyc.: +17.15 AP 3D ) compared with the baseline.</figDesc><table><row><cell>, we build our model based on two</cell></row><row><cell>networks DSGN [19] (PSMNet [6] backbone) and L-DSGN</cell></row><row><cell>[28] (ResNet-34 backbones). We adopt the same experi-</cell></row><row><cell>mental settings for the respective experiments. By incor-</cell></row><row><cell>porating the proposed techniques, our method significantly</cell></row><row><cell>surpasses the baselines: DSGN (</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 3 ,</head><label>3</label><figDesc>SECOND4x cannot handle well with the low-beams LiDAR and gets only &lt; 50 AP 3D with inputs of ? 8-beams LiDAR. With the simple fusion above with stereo features, all the LiDAR networks obtain noticeable accuracy gain consistently. The sparser the LiDAR signal is, the better the acquired improvements are. For example, an 8-beams LiDAR model gets an accuracy boost of 29.15 AP 3D to 78.15 AP, which is even comparable to the reported results of SECOND with inputs of 64-beams LiDAR. Interestingly, DSGN++ with 4 beams even performs worse than DSGN++, which indicates direct fusion of extremely sparse LiDAR (4-beams) is potentially harmful to stereo 3D detectors. Compared with PV-RCNN, DSGN++ with 64</figDesc><table><row><cell></cell><cell></cell><cell>Car</cell><cell></cell><cell></cell><cell>Pedestrian</cell><cell></cell><cell></cell><cell>Cyclist</cell><cell></cell></row><row><cell>Methods</cell><cell>AP 3D</cell><cell>AP BEV</cell><cell>AP 2D</cell><cell>AP 3D</cell><cell>AP BEV</cell><cell>AP 2D</cell><cell>AP 3D</cell><cell>AP BEV</cell><cell>AP 2D</cell></row><row><cell>LiDAR Sensor</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SECOND [23], [81]</cell><cell>78.62</cell><cell>87.93</cell><cell>89.90</cell><cell>52.98</cell><cell>56.66</cell><cell>66.33</cell><cell>67.15</cell><cell>70.70</cell><cell>77.09</cell></row><row><cell>PV-RCNN [82]</cell><cell>84.43</cell><cell>94.03</cell><cell>89.44</cell><cell>54.89</cell><cell>58.14</cell><cell>65.37</cell><cell>71.52</cell><cell>75.31</cell><cell>83.04</cell></row><row><cell>Stereo Camera Sensor</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DSGN [19]  ?</cell><cell>56.09</cell><cell>65.24</cell><cell>85.03</cell><cell>35.39</cell><cell>42.58</cell><cell>55.22</cell><cell>25.37</cell><cell>27.43</cell><cell>35.3</cell></row><row><cell>ours, DSGN++ on DSGN  ?</cell><cell>61.62</cell><cell>70.61</cell><cell>89.47</cell><cell>44.17</cell><cell>48.51</cell><cell>62.35</cell><cell>36.04</cell><cell>39.05</cell><cell>43.56</cell></row><row><cell>Improvement</cell><cell>+5.53</cell><cell>+5.37</cell><cell>+4.44</cell><cell>+8.78</cell><cell>+5.93</cell><cell>+7.13</cell><cell>+10.67</cell><cell>+11.62</cell><cell>+8.26</cell></row><row><cell>L-DSGN</cell><cell>63.58</cell><cell>73.53</cell><cell>93.59</cell><cell>33.12</cell><cell>40.50</cell><cell>59.16</cell><cell>28.09</cell><cell>29.58</cell><cell>36.95</cell></row><row><cell>ours, DSGN++ on L-DSGN</cell><cell>69.12</cell><cell>78.93</cell><cell>95.85</cell><cell>42.44</cell><cell>50.06</cell><cell>68.92</cell><cell>42.48</cell><cell>45.77</cell><cell>53.81</cell></row><row><cell>Improvement</cell><cell>+5.54</cell><cell>+5.40</cell><cell>+2.26</cell><cell>+8.32</cell><cell>+8.56</cell><cell>+9.76</cell><cell>+14.39</cell><cell>+16.19</cell><cell>+16.86</cell></row><row><cell>L-DSGN w/o LiDAR sup.</cell><cell>55.22</cell><cell>65.36</cell><cell>90.11</cell><cell>24.74</cell><cell>31.90</cell><cell>49.49</cell><cell>21.67</cell><cell>23.04</cell><cell>34.27</cell></row><row><cell>ours, DSGN++ on L-DSGN w/o L Sup.</cell><cell>66.08</cell><cell>75.92</cell><cell>95.52</cell><cell>31.49</cell><cell>39.30</cell><cell>63.95</cell><cell>38.82</cell><cell>40.54</cell><cell>54.18</cell></row><row><cell>Improvement</cell><cell>+10.86</cell><cell>+10.56</cell><cell>+5.41</cell><cell>+6.75</cell><cell>+7.40</cell><cell>+14.46</cell><cell>+17.15</cell><cell>+17.50</cell><cell>+19.91</cell></row><row><cell>Multi-Modal Sensors</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4-LiDAR SECOND4x</cell><cell>23.82</cell><cell>30.85</cell><cell>32.27</cell><cell>16.70</cell><cell>21.79</cell><cell>24.85</cell><cell>12.18</cell><cell>13.45</cell><cell>14.05</cell></row><row><cell>Fusion with DSGN++</cell><cell>67.41</cell><cell>76.30</cell><cell>95.17</cell><cell>40.85</cell><cell>49.51</cell><cell>62.65</cell><cell>32.31</cell><cell>33.69</cell><cell>46.17</cell></row><row><cell>Improvement</cell><cell>+43.59</cell><cell>+45.45</cell><cell cols="2">+62.90 +24.15</cell><cell>+27.72</cell><cell>+37.80</cell><cell>+20.13</cell><cell>+20.24</cell><cell>+32.12</cell></row><row><cell>8-LiDAR SECOND4x</cell><cell>49.00</cell><cell>66.83</cell><cell>66.97</cell><cell>38.19</cell><cell>44.34</cell><cell>44.13</cell><cell>25.88</cell><cell>27.56</cell><cell>31.17</cell></row><row><cell>Fusion with DSGN++</cell><cell>78.15</cell><cell>85.49</cell><cell>95.48</cell><cell>51.03</cell><cell>59.48</cell><cell>71.58</cell><cell>46.83</cell><cell>48.25</cell><cell>53.23</cell></row><row><cell>Improvement</cell><cell>+29.15</cell><cell>+18.66</cell><cell cols="2">+28.51 +12.84</cell><cell>+15.14</cell><cell>+27.45</cell><cell>+20.95</cell><cell>+20.69</cell><cell>+22.06</cell></row><row><cell>16-LiDAR SECOND4x</cell><cell>65.31</cell><cell>78.28</cell><cell>80.18</cell><cell>52.97</cell><cell>58.62</cell><cell>59.40</cell><cell>43.86</cell><cell>47.37</cell><cell>49.35</cell></row><row><cell>Fusion with DSGN++</cell><cell>79.41</cell><cell>87.55</cell><cell>95.17</cell><cell>58.07</cell><cell>66.18</cell><cell>74.55</cell><cell>54.08</cell><cell>55.95</cell><cell>59.63</cell></row><row><cell>Improvement</cell><cell>+14.10</cell><cell>+9.27</cell><cell>+14.99</cell><cell>+5.10</cell><cell>+7.56</cell><cell>+15.15</cell><cell>+10.22</cell><cell>+8.58</cell><cell>+10.28</cell></row><row><cell>64-LiDAR SECOND4x</cell><cell>81.23</cell><cell>89.52</cell><cell>94.37</cell><cell>59.40</cell><cell>62.71</cell><cell>68.50</cell><cell>61.90</cell><cell>62.45</cell><cell>68.87</cell></row><row><cell>Fusion with L-DSGN</cell><cell>81.13</cell><cell>88.36</cell><cell>94.50</cell><cell>60.05</cell><cell>62.65</cell><cell>68.42</cell><cell>56.94</cell><cell>57.05</cell><cell>65.84</cell></row><row><cell>Fusion with DSGN++</cell><cell>85.32</cell><cell>91.37</cell><cell>95.79</cell><cell>63.03</cell><cell>68.87</cell><cell>76.72</cell><cell>63.79</cell><cell>66.18</cell><cell>73.87</cell></row><row><cell>Improvement</cell><cell>+4.09</cell><cell>+1.85</cell><cell>+1.42</cell><cell>+3.63</cell><cell>+6.16</cell><cell>+8.22</cell><cell>+1.89</cell><cell>+3.73</cell><cell>+5.00</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 3 :</head><label>3</label><figDesc>Results in moderate difficulty regime for all categories are provided as the main metric. ? means training another model for Pedestrian and Cyclist. Best results are highlighted in bold for each sensor setup. LiDAR Sup. (L Sup.) represents whether to apply LiDAR depth supervision.</figDesc><table /><note>Performance comparison on the KITTI val set in various modality settings.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 4 :</head><label>4</label><figDesc>Main ablation studies on the KITTI val set. As illustrated inFig. 3, we separate the pipelines according to the information flow types -front-view (FV), top-view (TV), and dual-view (DV). FSD Head denotes the application of the front-surface depth head (Sec. 3.3) for geometric learning. SLCP represents Stereo-LiDAR copy-paste. Originally FV applies depth head and TV is supervised by voxel occupancy loss. "-" denotes the component that is not applicable in the respective pipeline.</figDesc><table><row><cell>Pipelines</cell><cell>Sampling</cell><cell>#Chn</cell><cell>?</cell><cell></cell><cell>Car</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>AP 3D</cell><cell>AP BEV</cell></row><row><cell cols="2">Stereo Camera Sensor</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>PS</cell><cell>32</cell><cell>-</cell><cell>63.58</cell><cell>73.53</cell></row><row><cell></cell><cell>Group-PS</cell><cell>96</cell><cell>32-sep</cell><cell>64.98</cell><cell>76.02</cell></row><row><cell>FV</cell><cell>D-PS</cell><cell>48 96 96</cell><cell>0.1 1. 0.1</cell><cell>65.48 65.95 66.42</cell><cell>76.28 76.71 77.13</cell></row><row><cell></cell><cell></cell><cell>96</cell><cell>0.01</cell><cell>66.28</cell><cell>76.82</cell></row><row><cell></cell><cell>PS</cell><cell>32</cell><cell>-</cell><cell>61.33</cell><cell>71.45</cell></row><row><cell></cell><cell>Group-PS</cell><cell>96</cell><cell>32-sep</cell><cell>62.50</cell><cell>72.60</cell></row><row><cell>TV</cell><cell>D-PS</cell><cell>48 96 96</cell><cell>0.1 1 0.5</cell><cell>62.59 63.85 64.59</cell><cell>73.22 73.59 74.14</cell></row><row><cell></cell><cell></cell><cell>96</cell><cell>0.1</cell><cell>64.15</cell><cell>73.40</cell></row><row><cell cols="3">Monocular Camera Sensor</cell><cell></cell><cell></cell><cell></cell></row><row><cell>TV</cell><cell>PS D-PS</cell><cell>32 96</cell><cell>-0.1</cell><cell>15.36 17.41</cell><cell>21.23 24.04</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 5 : Effects of expanded channels and smoothness factor ? for Depth-wise Plane Sweeping. #Chn</head><label>5</label><figDesc>denotes the channel number C I of 2D feature for building volumes. "Group-PS" (32-sep) represents sweeping planes by propagates features from the evenly-spaced channel groups (each with 32 channels) to the depth planes of the respective group. image input and right feature) and keep other training setups the same. As shown at the bottom of TABLE 5, the method with D-PS surpasses the baseline by 2.05 AP 3D and 2.81 AP BEV .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 4</head><label>4</label><figDesc>+4.64 AP 3D and +2.97 AP 3D . The improvement demonstrates that FSD head can take in more precise depth signals than discretized occupancy classification. We conjecture the consistent shape between cost volume and input views assists sparse depth supervision with front-view context features between sparse beams. For fair comparisons of both single-view representations, we ensure the front-view pipeline and top-view one share similar volume sizes: PSV is of shape 72 ? 80 ? 32 = 1797120 and 3DGV has the shape of 20?304?288 = 1751040, and there is only 2.6% calculation counts difference. As shown in TABLE 4 (c. vs. g.; b. vs. f.), despite the input of balanced foreground categories, TV branch cannot achieve the same performance for Ped. and Cyc., which shows top-view representation may not be well-suited for smaller objects in bird's eye view. The volume integration (k. in TABLE 4) boosts the performance to (69.12 AP 3D , 78.93 AP BEV ) for Car, and (42.48 AP 3D , 45.77 AP BEV ) for Cyc.. In comparison of c., g. and k., topview representation provides more complementary cues for Car and Cyc..</figDesc><table><row><cell>4.3.3 Ablation Study for Stereo-LiDAR Copy-Paste</cell></row></table><note>(d. vs. e., h. vs. i.) compares the effects of different depth supervision signals (voxel occupancy head vs. front-surface depth head), where FSD head yields the respective performance gains of Car:Effects of Volume Integration.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 4 (</head><label>4</label><figDesc>j. vs. k.) shows that SLCP augmentation improves data efficiency (Car: +3.96 AP 3D ) and greatly mitigates imbalanced class learning (Ped.: +5.25 AP 3D , Cyc. +12.47 AP 3D .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>table .</head><label>.</label><figDesc>Please zoom in to observe the prediction details. Redline shown in the bird's eye view is 50 meters away from the sensor.</figDesc><table><row><cell>id.</cell><cell>Samples</cell><cell>Prob</cell><cell>Car</cell><cell>Ped.</cell><cell>Cyc.</cell></row><row><cell>a.</cell><cell>-</cell><cell>0.</cell><cell cols="3">66.21 37.19 30.01</cell></row><row><cell>b.</cell><cell>{3, 3, 3}</cell><cell>0.6</cell><cell cols="3">67.79 39.66 39.56</cell></row><row><cell>c.</cell><cell>{5, 0, 0}</cell><cell>0.6</cell><cell cols="3">68.74 32.50 28.81</cell></row><row><cell>d.</cell><cell>{0, 5, 5}</cell><cell>0.6</cell><cell cols="3">66.18 39.42 43.22</cell></row><row><cell>e.</cell><cell></cell><cell>0.4</cell><cell cols="3">68.79 39.20 43.51</cell></row><row><cell>f. g.</cell><cell>{5, 5, 5}</cell><cell>0.6 0.8</cell><cell cols="3">69.12 42.44 42.48 68.41 41.74 39.47</cell></row><row><cell>h.</cell><cell></cell><cell>1.</cell><cell cols="3">68.55 40.18 38.58</cell></row><row><cell>i.</cell><cell>{7, 7, 7}</cell><cell>0.6</cell><cell cols="3">68.92 41.03 42.93</cell></row><row><cell>j.</cell><cell cols="2">w/o Occ Removal</cell><cell cols="3">66.52 38.79 36.27</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE 6 : Hyper-parameter choices for Stereo-LiDAR copy- paste.</head><label>6</label><figDesc>Models are evaluated using AP 3D (Moderate) on the KITTI val set. "Samples" denotes the augmented object counts for Car, Pedestrian, and Cyclist, respectively. "Prob" indicates the apply SLCP for each training scene. "w/o Occ Removal" denotes cancelling the removal of occluded point clouds. "Paste by Distance" represents pasting objects from near to far.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>TABLE 7 :</head><label>7</label><figDesc>Inference time comparison and the efficient implementation. The experiments are conducted on a NVIDIA RTX 2080Ti GPU with batch size of 1.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2002" to="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pyramid stereo matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-R</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="5410" to="5418" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">End-to-end learning of geometry and context for deep stereo regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Martirosyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="66" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Stereo r-cnn based 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7644" to="7652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Monogrnet: A geometric reasoning network for monocular 3d object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8851" to="8858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-level fusion based 3d object detection from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="2345" to="2353" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Objects are different: Flexible monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3289" to="3298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning depth-guided convolutions for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops, 2020</title>
		<imprint>
			<biblScope unit="page" from="1000" to="1001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Triangulation learning network: from monocular to stereo 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Disentangling monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Simonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lopez-Antequera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">M3d-rpn: Monocular 3d region proposal network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06038</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">in arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Fcos3d: Fully convolutional one-stage monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10956</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Orthographic feature transform for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Roddick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="8445" to="8453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dsgn: Deep stereo geometry network for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Accurate monocular 3d object detection via color-embedded 3d reconstruction for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.11444</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">in arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Pseudo-lidar++: Accurate depth for 3d object detection in autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="770" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Plume: Efficient 3d object detection from stereo images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">End-to-end pseudo-lidar for image-based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="5881" to="5890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Categorical depth distributionnetwork for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Reading</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Liga-stereo: Learning lidar geometry aware representations for stereo-based 3d detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3153" to="3163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A space-sweep approach to true multi-image matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="358" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deepstereo: Learning to predict new views from the world&apos;s imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Neulander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5515" to="5524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mvsnet: Depth inference for unstructured multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="767" to="783" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Simple copy-paste is a strong data augmentation method for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="2918" to="2928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Joint 3d proposal generation and object detection from view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in IROS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Ga-net: Guided aggregation net for end-to-end stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Prisacariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="185" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Group-wise correlation stereo network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="8934" to="8943" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Anytime stereo image depth estimation on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<biblScope unit="page" from="5893" to="5900" />
			<date type="published" when="2019" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="4040" to="4048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Hierarchical discrete distribution decomposition for match density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6044" to="6053" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Segstereo: Exploiting semantic information for disparity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="636" to="651" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning for disparity estimation through feature constancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="2811" to="2820" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Edgestereo: A context integrated residual pyramid network for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ACCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning a multi-view stereo machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>H?ne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="365" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Recurrent mvsnet for high-resolution multi-view stereo depth inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Point-based multi-view stereo network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Surfacenet: An endto-end 3d neural network for multiview stereopsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2307" to="2315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deepmvs: Learning multi-view stereopsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Matzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2821" to="2830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Cascade cost volume for high-resolution multi-view stereo and stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.05784</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Fast point r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9775" to="9784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Std: Sparse-to-dense 3d object detector for point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Pointfusion: Deep sensor fusion for 3d bounding box estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Monopair: Monocular 3d object detection using pairwise spatial relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="93" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Geometry-based distance decomposition for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.03775</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Distance-normalized unified representation for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="91" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Delving into localization errors for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4721" to="4730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Depth-conditioned dynamic message propagation for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="454" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">3d object proposals for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="424" to="432" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">3d object proposals using stereo imagery for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1259" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Object-centric stereo matching for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Pon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="8383" to="8389" />
		</imprint>
	</monogr>
	<note>in ICRA. IEEE, 2020</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Lift, splat, shoot: Encoding images from arbitrary camera rigs by implicitly unprojecting to 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Wasserstein distances for stereo disparity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Cross modal distillation for supervision transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2827" to="2836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>H?usser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Haz?rba?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.06852</idno>
		<title level="m">Flownet: Learning optical flow with convolutional networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Stereo magnification: Learning view synthesis using multiplane images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fyffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09817</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">On the expressive power of deep architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Delalleau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on algorithmic learning theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="18" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">On the expressive power of deep polynomial neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kileel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="10" to="310" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Lvis: A dataset for large vocabulary instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5356" to="5364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Cut, paste and learn: Surprisingly easy synthesis for instance detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dwibedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1301" to="1310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Geometry-aware data augmentation for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.05858</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Zoomnet: Part-aware adaptive zooming neural network for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Disp r-cnn: Stereo 3d object detection via shape prior guided instance disparity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Confidence guided stereo 3d object detection with split depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<biblScope unit="page" from="5776" to="5783" />
			<date type="published" when="2020" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Yolostereo3d: A step back to 2d for efficient stereo 3d detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<editor>ICRA. IEEE</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Openpcdet: An open-source toolbox for 3d object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">D</forename><surname>Team</surname></persName>
		</author>
		<ptr target="https://github.com/open-mmlab/OpenPCDet" />
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Pvrcnn: Point-voxel feature set abstraction for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10" to="529" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
