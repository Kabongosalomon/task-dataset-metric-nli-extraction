<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pre-Training with Whole Word Masking for Chinese BERT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Cui</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">? iFLYTEK AI Research (Hebei)</orgName>
								<orgName type="laboratory">State Key Laboratory of Cognitive Intelligence</orgName>
								<orgName type="institution">iFLYTEK Research</orgName>
								<address>
									<settlement>Beijing, Langfang</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
							<email>qinb@ir.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqing</forename><surname>Yang</surname></persName>
							<email>zqyang5@iflytek.com</email>
							<affiliation key="aff1">
								<orgName type="department">? iFLYTEK AI Research (Hebei)</orgName>
								<orgName type="laboratory">State Key Laboratory of Cognitive Intelligence</orgName>
								<orgName type="institution">iFLYTEK Research</orgName>
								<address>
									<settlement>Beijing, Langfang</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pre-Training with Whole Word Masking for Chinese BERT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, NOVEMBER 2021 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-pre-trained language model</term>
					<term>representation learning</term>
					<term>natural language processing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Bidirectional Encoder Representations from Transformers (BERT) has shown marvelous improvements across various NLP tasks, and its consecutive variants have been proposed to further improve the performance of the pre-trained language models. In this paper, we aim to first introduce the whole word masking (wwm) strategy for Chinese BERT, along with a series of Chinese pre-trained language models. Then we also propose a simple but effective model called MacBERT, which improves upon RoBERTa in several ways. Especially, we propose a new masking strategy called MLM as correction (Mac). To demonstrate the effectiveness of these models, we create a series of Chinese pre-trained language models as our baselines, including BERT, RoBERTa, ELECTRA, RBT, etc. We carried out extensive experiments on ten Chinese NLP tasks to evaluate the created Chinese pre-trained language models as well as the proposed MacBERT. Experimental results show that MacBERT could achieve state-of-the-art performances on many NLP tasks, and we also ablate details with several findings that may help future research. We open-source our pre-trained language models for further facilitating our research community. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>B ERT <ref type="bibr" target="#b1">[2]</ref> has become enormously popular and has proven to be effective in recent natural language processing studies, which utilizes large-scale unlabeled training data and generates enriched contextual representations. As we traverse several popular machine reading comprehension benchmarks, such as SQuAD <ref type="bibr" target="#b2">[3]</ref>, CoQA <ref type="bibr" target="#b3">[4]</ref>, QuAC <ref type="bibr" target="#b4">[5]</ref>, NaturalQuestions <ref type="bibr" target="#b5">[6]</ref>, RACE <ref type="bibr" target="#b6">[7]</ref>, we can see that most of the top-performing models are based on BERT and its variants <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, demonstrating that the pre-trained language models have become new fundamental components in natural language processing field.</p><p>Starting from BERT, the community members have made great and rapid progress on optimizing the pre-trained language models, such as ERNIE <ref type="bibr" target="#b10">[11]</ref>, XLNet <ref type="bibr" target="#b11">[12]</ref>, RoBERTa <ref type="bibr" target="#b12">[13]</ref>, SpanBERT <ref type="bibr" target="#b13">[14]</ref>, ALBERT <ref type="bibr" target="#b14">[15]</ref>, ELECTRA <ref type="bibr" target="#b15">[16]</ref>, etc. However, training Transformer-based <ref type="bibr" target="#b16">[17]</ref> pre-trained language models are not as easy as we used to train word embeddings or other traditional neural networks for learning representations. Typically, training a powerful BERT-large model with a 24-layer Transformer and 330 million parameters, to convergence needs high-memory computing devices, <ref type="bibr" target="#b0">1</ref> Resources are available: https://github.com/ymcui/Chinese-BERT-wwm such as TPU or TPU Pod, which are very expensive. On the other hand, though various pre-trained language models have been released, most of them are based on English, and there are few efforts on building powerful pre-trained language models in other languages.</p><p>To minimize the repetitive work and build baselines for future studies, in this paper, we aim to build Chinese pretrained language model series and release them to the public for facilitating the research community, as Chinese and English are among the most spoken languages in the world. We revisit the existing popular pre-trained language models and adjust them to the Chinese language to see whether these models could generalize and perform well in a language other than English. Besides, we also propose a new pre-trained language model called MacBERT, which replaces the original MLM task into MLM as correction (Mac) task. MacBERT mainly aims to mitigate the discrepancy of the pre-training and fine-tuning stage in original BERT. Extensive experiments are conducted on ten popular Chinese NLP datasets, ranging from sentence-level to document-level tasks, such as machine reading comprehension, text classification, etc. The results show that the proposed MacBERT could give significant gains in most of the tasks against other pre-trained language models, and detailed ablations are given to better examine the composition of the improvements. The contributions of this paper are listed as follows.</p><p>? To further accelerate future research on Chinese NLP, we create and release the Chinese pre-trained language model series to our community. Extensive empirical studies are carried out to revisit the performance of these pre-trained language models on various tasks with careful analyses. <ref type="bibr">?</ref> We propose a new pre-trained language model called MacBERT that mitigates the gap between the pre-training and fine-tuning stage by masking the word with its similar word, which has proven to be effective on various downstream tasks. <ref type="bibr">?</ref> We also create a series of small models, called RBT, to demonstrate how small models perform compared to regular pre-trained language models, which could help utilize them in real-life applications.</p><p>II. RELATED WORK In this section, we revisit the techniques of the representative pre-trained language models in the recent natural language arXiv:1906.08101v3 [cs.CL] <ref type="bibr" target="#b24">25</ref> Nov 2021 processing field. The overall comparisons of these models, as well as the proposed MacBERT, are depicted in <ref type="table">Table  I</ref>. We elaborate on their key components in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. BERT</head><p>BERT (Bidirectional Encoder Representations from Transformers) <ref type="bibr" target="#b1">[2]</ref> has demonstrated its effectiveness in a wide range of natural language processing tasks. BERT is designed to pretrain deep bidirectional representations by jointly conditioning on both left and right context in all Transformer layers. Primarily, BERT consists of two pre-training tasks: Masked Language Model (MLM) and Next Sentence Prediction (NSP).</p><p>? MLM: Randomly masks some of the tokens from the input, and the objective is to predict the original word based only on its context. ? NSP: To predict whether sentence B is the next sentence of sentence A. Later, they further propose a technique called whole word masking (wwm) for optimizing the original masking in the MLM task. In this setting, instead of randomly selecting WordPiece <ref type="bibr" target="#b17">[18]</ref> tokens to mask, we always mask all of the tokens corresponding to a whole word at once. This explicitly forces the model to recover the whole word in the MLM pre-training task instead of just recovering WordPiece tokens <ref type="bibr" target="#b0">[1]</ref>, which is much more challenging. As the whole word masking only affects the masking strategy of the pretraining process, it would not bring additional burdens on downstream tasks. Moreover, as training pre-trained language models are computationally expensive, they also release all the pre-trained models as well as the source codes, which significantly stimulates the community to have great interests in the research of pre-trained language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. ERNIE</head><p>ERNIE (Enhanced Representation through kNowledge IntEgration) <ref type="bibr" target="#b10">[11]</ref> is designed to optimize the masking process of BERT, which includes entity-level masking and phrase-level masking. Different from selecting random words in the input, entity-level masking masks the named entities, which are often formed by several words. Phrase-level masking is to mask consecutive words, which is similar to the N-gram masking strategy <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b18">[19]</ref>. <ref type="bibr" target="#b1">2</ref> . C. XLNet <ref type="bibr" target="#b11">[12]</ref> argues that the existing pre-trained language models that are based on auto-encoding, such as BERT, which suffer from the discrepancy of the pre-training and fine-tuning stage because the masking token [MASK] never appears in the fine-tuning stage. To alleviate this problem, XLNet is proposed, which is based on Transformer-XL <ref type="bibr" target="#b7">[8]</ref>. XLNet mainly modifies in two ways. The first is to maximize the expected likelihood over all permutations of the factorization order of the input, where they call the Permutation Language Model. To achieve this goal, they propose a novel two-stream self-attention mechanism. Another one is to change the autoencoding language model into an auto-regressive one, which is similar to the traditional statistical language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. RoBERTa</head><p>RoBERTa (Robustly Optimized BERT Pretraining Approach) <ref type="bibr" target="#b12">[13]</ref> aims to adopt original BERT architecture but make much more precise modifications to fully release the power of BERT, which is underestimated in <ref type="bibr" target="#b1">[2]</ref>. They carry out careful comparisons of various components in BERT, including the masking strategies, input format, training steps, etc. After thorough evaluations, they come up with several useful conclusions to make BERT more powerful, mainly including 1) training longer with bigger batches and longer sequences over more data; 2) removing the next sentence prediction task and using dynamic masking in MLM task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. ALBERT</head><p>ALBERT (A Lite BERT) <ref type="bibr" target="#b14">[15]</ref> primarily tackles the problems of higher memory consumption and slow training speed of BERT. ALBERT introduces two techniques for parameter reduction. The first one is the factorized embedding parameterization, which decomposes the embedding matrix into two small matrices. The second one is the cross-layer parameter sharing that the Transformer weights are shared across each layer of ALBERT, which significantly reduces the overall parameters. Besides, they also propose the sentenceorder prediction (SOP) task to replace the traditional NSP pretraining task and yield better performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. ELECTRA</head><p>ELECTRA (Efficiently Learning an Encoder that Classifiers Token Replacements Accurately) <ref type="bibr" target="#b15">[16]</ref> employs a new generator-discriminator framework that is similar to generative adversarial net (GAN) <ref type="bibr" target="#b19">[20]</ref>. The generator is typically a small MLM that learns to predict the original words of the masked tokens. The discriminator is trained to discriminate whether the input token is replaced by the generator, which they call Replaced Token Detection (RTD). Note that, to achieve efficient training, the discriminator is only required to predict a binary label to indicate "replacement", unlike the way of MLM that should predict the exact masked word. After the pre-training stage, we discard the generator and only use the discriminator for fine-tuning downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. CHINESE PRE-TRAINED LANGUAGE MODELS</head><p>While BERT and its variants have achieved significant improvements in various English tasks, we wonder if these models and techniques could generalize well in other languages. In this section, we illustrate how the existing pretrained language models are adapted for the Chinese language. We adopt BERT, RoBERTa, and ELECTRA as well as their variants to create Chinese pre-trained model series, and their effectiveness is shown in Section VI. Note that, as these   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chinese English</head><p>Original Sentence ????????????????? we use a language model to predict the probability of the next word.</p><formula xml:id="formula_0">+ CWS ?? ?? ? ?? ? ?? ? ? ?? ? - + BERT Tokenizer ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</formula><p>we use a language model to pre ##di ##ct the pro ##ba ##bility of the next word .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original Masking</head><formula xml:id="formula_1">? ? [M] ? ? [M] ? ? ? ? ? ? ? ? ? we use a language [M] to [M] ##di ##ct the pro [M] ##bility of the next word . + WWM ? ? [M] [M] ? [M] [M] ? ? ? ? ? ? ? ? we use a language [M] to [M] [M] [M] the [M] [M] [M] of the next word . ++ N-gram Masking [M] [M] [M] [M] ? [M] [M] ? ? ? ? ? ? ? ? we use a [M] [M] to [M] [M] [M] the [M] [M] [M] [M] [M] next word . +++ Mac Masking ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</formula><p>we use a text system to ca ##lc ##ulate the po ##si ##bility of the next word .</p><p>models are all originated from BERT or ELECTRA without changing the nature of the input, no modification should be made to adapt to these models in the fine-tuning stage, which is very flexible for replacing one another.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. BERT-wwm &amp; RoBERTa-wwm</head><p>In the original BERT, a WordPiece tokenizer <ref type="bibr" target="#b17">[18]</ref> is used to split the text into WordPiece tokens, where some words are split into several small fragments. The whole word masking (wwm) mitigates the drawback of masking only a part of the whole word, which is easier for the model to predict. In Chinese condition, WordPiece tokenizer no longer splits the word into small fragments, as Chinese characters are not formed by alphabet-like symbols. We use the traditional Chinese Word Segmentation (CWS) tool to split the text into several words. In this way, we could adopt the whole word masking in Chinese to mask the word instead of individual Chinese characters. For implementation, we strictly follow the original whole word masking codes and do not change other components, such as the percentage of word masking, etc. We use LTP <ref type="bibr" target="#b20">[21]</ref> for Chinese word segmentation to identify the word boundaries. Note that the whole word masking only affects the selection of the masking tokens in the pre-training stage. We still uses WordPiece tokenizer to split the text, which is identical to the original BERT.</p><p>Similarly, whole word masking can also be applied on RoBERTa, where the NSP task is not adopted. However, we still use a paired input for pre-training, which could be beneficial to the sentence pair classification and reading comprehension tasks. An example of the whole word masking is depicted in <ref type="table" target="#tab_1">Table II</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. ELECTRA</head><p>Besides BERT and RoBERTa series, we also explore the ELECTRA model, which adopts a new pre-training framework that consists of a generator and discriminator. We strictly follow the original implementation as in <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. RBT Series</head><p>Though the aforementioned pre-trained language models are powerful, they are computationally ineffective and hard to adopt in real-life applications. To make pre-trained models more accessible by community researchers, besides the regular pre-trained language models, we also pre-train several small models, where we call RBT. Specifically, we use exactly the same training strategy as in training RoBERTa, but we use fewer Transformer layers. We train 3-layer, 4-layer, 6layer RoBERTa-base, denoted as RBT3, RBT4, and RBT6, respectively. We also train a 3-layer RoBERTa-large, denoted as RBTL3, which has a similar parameter size as RBT6. This is designed to compare a wider and shorter model (RBTL3) with a thinner and taller model (RBT6) under a comparable parameter size, which could be useful in the design of future pre-trained language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. MACBERT</head><p>In the previous section, we propose a series of Chinese pretrained language models. In this section, we make the best use of them and propose a novel model called MacBERT (MLM as correction BERT). MacBERT shares the similar types of pre-training tasks as BERT with several modifications. MacBERT consists of two pre-training tasks: MLM as correction, and sentence order prediction. The overall architecture of MacBERT is depicted in <ref type="figure" target="#fig_1">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. MLM as correction</head><p>Masked Language Model (MLM) is the most important pre-training task in BERT and its variants, which models bidirectional contextual inference ability. However, as shown</p><formula xml:id="formula_2">Input Sequence MacBERT [CLS] A 1 A n [SEP] B 1 B m [SEP]</formula><p>A B in the previous section, the MLM suffers from the 'pre-training and fine-tuning' discrepancy, where the artificial tokens in the pre-training stage, such as [MASK], never appear in the real downstream fine-tuning tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MLM as correction (Mac) Sentence Order Prediction (SOP)</head><p>To address this issue, we propose a novel pre-training task called MLM as correction (Mac). In this pre-training task, we do not adopt any pre-defined tokens for masking purposes. Instead, we transform the original MLM as a text correction task, where the model should correct the wrong word into the correct one, which is much more natural than MLM. Specifically, in the Mac task, we perform the following modifications on the original MLM.</p><p>? We use the whole word masking as well as N-gram masking strategies to select candidate tokens for masking, with a percentage of 40%, 30%, 20%, 10% for wordlevel unigram to 4-gram. We also notice that a recent work PMI-masking <ref type="bibr" target="#b21">[22]</ref> is proposed, which optimizes the masking strategy. In this paper, we resort to vanilla Ngram masking and will try PMI-masking in the future. ? Instead of masking with [MASK] token, which never appears in the fine-tuning stage, we propose to use similar words for the masking purpose. A similar word is obtained by using Synonyms toolkit <ref type="bibr" target="#b22">[23]</ref>, which is based on word2vec <ref type="bibr" target="#b23">[24]</ref> similarity calculations. If an N-gram is selected to mask, we find similar words individually. In rare cases, when there is no similar word, we degrade to use random word replacement. Such replacements are restricted to no more than 10% of all tokens to be masked. ? Following previous works, we use a percentage of 15% input words for masking, where 80% tokens are replaced with similar words, 10% tokens are replaced with random words, and keep with original words for the rest of 10%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Sentence Order Prediction</head><p>The original next sentence prediction (NSP) task in BERT is considered to be too easy for the model and proved to be not that effective <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>. In this paper, we adopt the sentence order prediction (SOP) task as introduced by ALBERT <ref type="bibr" target="#b14">[15]</ref>, which is shown to be much more effective than NSP. The positive samples are created by using two consecutive texts, while the negative ones are created by switching the original order of them. We ablate these modifications in Section VII-A to better demonstrate the contributions of each component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Neural Architecture</head><p>Formally, given a pair of sequences A = {A 1 , . . . , A n } and B = {B 1 , . . . , B m }, we first construct the input sequence X by concatenating two sequences. Then, MacBERT converts X into a contextualized representation H (L) ? R N ?d through an embedding layer (which consists of word embedding, positional embedding, and token type embedding), and a consecutive L-layer transformer, where N is the maximum sequence length, and d is the dimension of hidden layers.</p><formula xml:id="formula_3">X = [CLS] A 1 . . . A n [SEP] B 1 . . . B m [SEP]</formula><p>(1)</p><formula xml:id="formula_4">H (0) = Embedding(X)<label>(2)</label></formula><formula xml:id="formula_5">H (i) = Transformer(H (i?1) ), i ? {1, . . . , L} (3)</formula><p>As we only need to predict the positions that are replaced by the Mac task, after getting the contextual representation H L , we collect a subset with respect to the replaced positions, forming the replaced representation H m ? R k?d , where k is the number of the replaced tokens. According to the definition of Mac task, k = N ? 15% .</p><p>Then we project H m into the vocabulary space to predict the probability distributions p over the whole vocabulary V. Following original BERT implementation, we also use word embedding matrix W e ? R |V|?d to perform the projection, as the embedding and hidden size are identical.</p><formula xml:id="formula_6">p i = H m i W e + b<label>(4)</label></formula><p>Then we use the standard cross-entropy loss to optimize the pre-training task.</p><formula xml:id="formula_7">L = ? 1 M M i=1 y i log p i<label>(5)</label></formula><p>For the SOP task, we directly use the contextual representation of the [CLS] token, which is the first component of H, and project it into the label prediction layer.</p><formula xml:id="formula_8">p = softmax(h 0 W s + b s )<label>(6)</label></formula><p>where the W s ? R d?2 and b s ? R 2 are the weight matrix and bias. Then we also use the cross-entropy loss to optimize the pre-training task (similar to <ref type="bibr">Equation 5</ref>). Finally, the overall training loss is the combination of the Mac and SOP task.</p><formula xml:id="formula_9">L = L mac + L sop<label>(7)</label></formula><p>V. EXPERIMENTAL SETUPS</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Processing</head><p>We use Wikipedia dump 3 (as of March 25, 2019), and pre-process with WikiExtractor.py as suggested by <ref type="bibr" target="#b1">[2]</ref>, resulting in 1,307 extracted files. We use both Simplified and Traditional Chinese in this dump and do not convert the Traditional Chinese portion into Simplified one. We demonstrate the effectiveness in the Traditional Chinese task in Section VI-A. After cleaning the raw text, such as removing html tags and separating the document, we obtain about 0.4B words. As Chinese Wikipedia data is relatively small, besides Chinese Wikipedia, we also use extended training data for training these pre-trained language models (mark with ext in the model name). The in-house collected extended data contains encyclopedia, news, and question answering web, which has 5.4B words and is over ten times bigger than the Chinese Wikipedia. Note that we always use extended data for MacBERT and omit the ext mark. In order to identify the boundary of Chinese words for whole word masking, we use LTP <ref type="bibr" target="#b20">[21]</ref> for Chinese word segmentation. We use official create_pretraining_data.py provided by <ref type="bibr" target="#b1">[2]</ref> to convert the raw input text to the pre-training examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Setups for Pre-Trained Language Models</head><p>To better acquire the knowledge from the existing pretrained language model, we did NOT train our base-level model from scratch but the official Chinese BERT-base, inheriting its vocabulary and weight. However, for the large-level model, we have to train from scratch but still using the same vocabulary provided by the base-level model. The base-level model is a 12-layer transformer with a hidden dimension of 768, while the large-level model is a 24-layer transformer with a hidden dimension of 1024.</p><p>For training BERT series, we adopt the scheme of training on a maximum sequence length of 128 tokens then on 512, suggested by <ref type="bibr" target="#b1">[2]</ref>. However, we empirically found that this results in insufficient adaptation for the long-sequence tasks, such as reading comprehension. In this context, for models other than BERT, we directly use a maximum length of 512 throughout the pre-training process, which is adopted in <ref type="bibr" target="#b12">[13]</ref>. For smaller batch sizes, we adopt the original ADAM <ref type="bibr" target="#b24">[25]</ref> with weight decay optimizer in BERT for optimization, and use LAMB optimizer <ref type="bibr" target="#b25">[26]</ref> for better scalability in larger batch size. The pre-training was either done on a single Google Cloud TPU 4 v3-8 (equals to a single TPU) or TPU Pod v3-32 (equals to 4 TPUs), depending on the magnitude of the model. Specifically, for MacBERT-large, we trained for 2M steps with a batch size of 512 and an initial learning rate of 1e-4.</p><p>The training details are shown in <ref type="table" target="#tab_1">Table III</ref>. For clarity, we do not list 'ext' models, where the other parameters are the same as the one that is not trained on extended data. <ref type="bibr" target="#b3">4</ref> https://cloud.google.com/tpu/ </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Setups for Fine-tuning Tasks</head><p>To thoroughly test these pre-trained language models, we carry out extensive experiments on various natural language processing tasks, covering a wide spectrum of text length, i.e., from sentence-level to document-level. Task details are shown in <ref type="table" target="#tab_3">Table IV</ref>. Specifically, we choose the following ten popular Chinese datasets.</p><p>? Machine Reading Comprehension (MRC): CMRC 2018 <ref type="bibr" target="#b26">[27]</ref>, DRCD <ref type="bibr" target="#b27">[28]</ref>, CJRC <ref type="bibr" target="#b28">[29]</ref>. ? Single Sentence Classification (SSC): ChnSentiCorp <ref type="bibr" target="#b29">[30]</ref>, THUCNews <ref type="bibr" target="#b30">[31]</ref>, TNEWS <ref type="bibr" target="#b31">[32]</ref>. ? Sentence Pair Classification (SPC): XNLI <ref type="bibr" target="#b32">[33]</ref>, LCQMC <ref type="bibr" target="#b33">[34]</ref>, BQ Corpus <ref type="bibr" target="#b34">[35]</ref>, OCNLI <ref type="bibr" target="#b35">[36]</ref>.</p><p>In order to make a fair comparison, for each dataset, we keep the same hyper-parameters (such as maximum length, warm-up steps, etc.) and only tune the initial learning rate from 1e-5 to 5e-5 for each task. Note that the initial learning rates are tuned on the original Chinese BERT, and it would be possible to achieve another gain by tuning the learning rate individually. We run the same experiment ten times to ensure the reliability of the results. The best initial learning rate is determined by selecting the best average development set performance. We report the maximum and average scores to both evaluate the peak and average performance. Except for TNEWS and OCNLI, where the test sets are not publicly available, we report both development and test set results.</p><p>For all models except for ELECTRA, we use the same initial learning rate setting for each task, as depicted in <ref type="table" target="#tab_3">Table IV</ref>. For ELECTRA models, we use a universal initial learning rate of 1e-4 for base-level models and 5e-5 for large-level models as suggested in <ref type="bibr" target="#b15">[16]</ref>. As the pre-training data are quite different among various existing Chinese pre-trained language models, such as ERNIE <ref type="bibr" target="#b10">[11]</ref>, ERNIE 2.0 <ref type="bibr" target="#b36">[37]</ref>, NEZHA <ref type="bibr" target="#b37">[38]</ref>, we only compare BERT <ref type="bibr" target="#b1">[2]</ref>, BERT-wwm, BERT-wwm-ext, RoBERTa-wwmext, RoBERTa-wwm-ext-large, ELECTRA, along with our MacBERT to ensure relatively fair comparisons among different models, where all models are trained by ourselves except for the original Chinese BERT <ref type="bibr" target="#b1">[2]</ref>. We carried out experiments under TensorFlow framework <ref type="bibr" target="#b38">[39]</ref> with slight modifications to the fine-tuning scripts 5 provided by <ref type="bibr" target="#b1">[2]</ref> to better adapt to Chinese tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Machine Reading Comprehension</head><p>Machine Reading Comprehension (MRC) is a representative document-level modeling task that requires to answer the questions based on the given passages. We mainly test these models on three datasets: CMRC 2018, DRCD, and CJRC.</p><p>? CMRC 2018: A span-extraction machine reading comprehension dataset, which is similar to SQuAD [40] that extract a passage span for the given question. ? DRCD: This is also a span-extraction MRC dataset but in Traditional Chinese. ? CJRC: Similar to CoQA <ref type="bibr" target="#b3">[4]</ref>, which has yes/no questions, no-answer questions, and span-extraction questions. The data is collected from Chinese law judgment documents. Note that we only use small-train-data.json for training. The results are depicted in <ref type="table" target="#tab_4">Table V</ref> and VI. Using additional pre-training data results in further improvement, as shown in the comparison between BERT-wwm and BERT-wwm-ext. This is why we use extended data for RoBERTa, ELECTRA, and MacBERT. Moreover, the proposed MacBERT yields significant improvements on all reading comprehension datasets. It is worth mentioning that our MacBERT-large could achieve a state-of-the-art F1 of 60% on the challenge set of CMRC 2018, which requires deeper text understanding.</p><p>Also, it should be noted that though DRCD is a traditional Chinese dataset, training with additional large-scale simplified 5 https://github.com/google-research/bert Chinese could also have a great positive effect. As simplified and traditional Chinese share many identical characters, using a powerful pre-trained language model with only a few traditional Chinese data could also bring improvements without converting traditional Chinese characters into simplified ones. Regarding CJRC, where the text is written in professional ways regarding Chinese laws, BERT-wwm shows moderate improvement over BERT but not that salient, indicating that further domain adaptation is needed for the fine-tuning tasks on non-general domains. However, increasing general pretraining data results in improvement, suggesting that when there is not enough domain data, we could also use large-scale general data as a remedy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Single Sentence Classification</head><p>For the single sentence classification tasks, we select ChnSentiCorp, THUCNews, and TNEWS datasets. We use the ChnSentiCorp for evaluating sentiment classification, where the text should be classified into either a positive or negative label. THUCNews is a dataset that contains news in different genres, where the text is typically very long. In this paper, we use a version that contains 50K news in 10 domains (evenly distributed), including sports, finance, technology, etc. <ref type="bibr" target="#b5">6</ref> TNEWS is a short text classification task consisting of news titles and keywords. TNEWS requires to classify into one of 15 In TNEWS, we can see that our MacBERT yields consistent improvements across base-level and large-level PLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Sentence Pair Classification</head><p>For sentence pair classification tasks, we use XNLI data (Chinese portion), Large-scale Chinese Question Matching Corpus (LCQMC), BQ Corpus, and OCNLI, which require to input two sequences and predict their relations.</p><p>In XNLI and OCNLI, we can see that MacBERT yields relatively consistent and significant improvements over baselines. However, MacBERT only shows moderate improvements on LCQMC and BQ Corpus, with a slight improvement on the average score, but the peak performance is not as good as RoBERTa-wwm-ext-large. We suspect that these tasks are less sensitive to the subtle difference of the input than the reading comprehension tasks. As sentence pair classification only needs to generate a unified representation of the whole input and thus results in a moderate improvement.</p><p>We also noticed that the improvements are bigger in MRC tasks than classification tasks, while it might attribute to the masking strategy. In MRC tasks, the models should identify the exact answer span in the passage. In MacBERT, each word of N-gram is either replaced by its synonym or a random word, and thus each word can be easily identified, which forces the model to learn the word boundaries.</p><p>Another observation is that MacBERT-base generally yields larger improvements than MacBERT-large. This might be caused by two reasons. Firstly, MacBERT-base is initialized by BERT-base, which could benefit from the knowledge in BERT-base and avoid the cold-starting issue. Secondly, the results of large-level PLMs are generally higher than those of base-level PLMs, and thus getting a higher score is much difficult than base-level PLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Results on Small Models</head><p>We also build a series of small models, namely RBT, built on either RoBERTa-base or RoBERTa-large models. The experimental results are shown in <ref type="table" target="#tab_8">Table IX</ref>. Small models perform worse than the general models (base-level, largelevel), because they use fewer parameters. As we can see that the performance drops in classification tasks are smaller than the reading comprehension tasks, indicating that it is possible to sacrifice minor performance to obtain a faster and smaller model, which could be beneficial for real-life applications. Also, by comparing RBTL3 and RBT6, which have similar parameter sizes, we can see that RBT6 substantially outperforms RBTL3, which indicates that a thin-and-tall model usually outperforms a wide-and-short model. These observations could be helpful in future model design for real-life applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. DISCUSSION</head><p>Based on the experimental results, we can see that these pre-trained language models also yield significant improvements over traditional BERT in Chinese tasks, indicating their effectiveness and generalizability. While our models achieve significant improvements on various Chinese tasks, we wonder where the essential components of the improvements from. To this end, we carry out detailed ablations on MacBERT to demonstrate its effectiveness, and we also compare the claims of the existing pre-trained language models in English to see if their modification still holds true in another language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Effectiveness of MacBERT</head><p>We carry out detailed ablations to examine the contributions of each component in MacBERT. The results are shown in <ref type="table">Table X</ref>.</p><p>The overall average scores are obtained by averaging the test scores of each task (EM and F1 metrics are averaged before the overall averaging). From a general view, removing any component in MacBERT results in a decline in the average performance, suggesting that all modifications contribute to the overall improvements. Specifically, the most effective modifications are the N-gram masking and similar word replacement, which are the modifications on the masked language model task. When we compare N-gram masking and similar word replacement, we could see clear pros and cons, where Ngram masking seems to be more effective in text classification tasks, and the performance of reading comprehension tasks seems to benefit more from the similar word replacement task. Combining these two tasks could compensate for each other and have a better performance on both genres.</p><p>The NSP task does not show as much importance as the MLM task, demonstrating that it is much more important to design a better MLM task to fully unleash the text modeling power. Also, we compared the next sentence prediction <ref type="bibr" target="#b1">[2]</ref> and sentence order prediction <ref type="bibr" target="#b14">[15]</ref> task to better judge which one is much powerful. The results show that the sentence order prediction task indeed shows better performance than the original NSP, though it is not that salient. The SOP task requires identifying the correct order of the two sentences rather than using a random sentence, which is much easy for the machine to identify. Removing the SOP task results in noticeable declines in reading comprehension tasks compared to the text classification tasks, which suggests that it is necessary to design an NSP-like task to learn the relations  between two segments (for example, passage and question in reading comprehension task).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Investigation on MLM Task</head><p>As illustrated in the previous section, the dominant pretraining task is the masked language model and its variants. The masked language model task relies on two sides: 1) the selection of the tokens to be masked, and 2) the replacement of the selected tokens. In the previous section, we have demonstrated the effectiveness of the selection of the masking tokens, such as the whole word masking or N-gram masking, etc. Now we are going to investigate how the replacement of the selected tokens affects the performance of the pre-trained language models. In order to investigate this problem, we plot the CMRC 2018 and DRCD performance at different pretraining steps. Specifically, we follow the original masking percentage 15% of the input sequence, in which 10% masked tokens remain the same. In terms of the remaining 90% masked tokens, we classify them into four categories.</p><p>? MacBERT: 80% tokens replaced into their similar words, and 10% replaced into random words.</p><p>? Random Replace: 90% tokens replaced into random words. ? Partial Mask: original BERT implementation, with 80% tokens replaced into [MASK] tokens, and 10% replaced into random words. ? All Mask: 90% tokens replaced with [MASK] tokens. We only plot the steps from 1M to 2M to show stabler results than the first 1M steps. The results are depicted in <ref type="figure">Figure 2</ref>.</p><p>The pre-training models that rely on mostly using [MASK] for masking purposes (i.e., partial mask and all mask) result in worse performances, indicating that the discrepancy of the pretraining and fine-tuning is an actual problem that affects the overall performance. Among which, we also noticed that if we do not leave 10% as original tokens (i.e., identity projection), there is also a consistent decline, indicating that masking with [MASK] token is less robust and vulnerable to the absence of identity projection for negative sample training.</p><p>To dependent masking strategies. This also strengthens the claims that the original masking method that relies on the [MASK] token, which never appears in the fine-tuning task, resulting in a discrepancy and worse performance. Also, using random words rather than the artificial token [MASK] could improve the de-noising ability of the pre-trained model, which might also be a possible reason. To make this more delicate, in this paper, we propose to use similar words for masking purposes, instead of randomly pick a word from the vocabulary, as random words are not fit in the context and may break the naturalness of the language model learning, as traditional Ngram language model is based on natural sentence rather than a manipulated influent sentence. However, if we use similar words for masking purposes, the fluency of the sentence is much better than using random words, and the whole task transforms into a grammar correction task, which is much more natural and without the discrepancy of the pretraining and fine-tuning stage. From the figure, we can see that the MacBERT yields the best performance among the four variants, which verifies our assumptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Analyses on Chinese Spell Check</head><p>MacBERT introduces 'MLM as correction' tasks, which is similar to the actual grammar or spell error correction tasks. We perform additional experiments on Chinese Spell Check tasks. We use SIGHAN-15 <ref type="bibr" target="#b40">[41]</ref> dataset to explore the effect of different pre-trained language models when using different percentages of training data. SIGHAN-15 consists of a training set of 3.1K instances and a test set of 1.1K instances. We compare BERT-wwm-ext, RoBERTa-wwm-ext, ELECTRA-base, and MacBERT-base in this experiment, as they share the same pre-training data. We fine-tune each model five times and plot the figures with averaged F1 (sentencelevel). We use a universal learning rate of 5e-5 and train 5 epochs with a batch size of 64. The results are shown in <ref type="figure" target="#fig_2">Figure  3</ref>, including detection-level and correction-level scores. As we can see that our MacBERT yields consistent improvements over others when using different percentages of the training data, indicating that our approach is effective and scalable. We notice that ELECTRA does not perform well on this task. Especially, the gap between ELECTRA and others on the correction-level results are relatively larger than that in the detection-level. ELECTRA uses replaced token detection (RTD) task for training the discriminator (which will be used for fine-tuning). However, the RTD task only needs to identify whether the input tokens are altered without predicting the original token, which we think is quite simple. On the contrary, MLM and Mac objectives require identifyand-correction at the same time. By comparing MLM and Mac, our MacBERT alleviates the discrepancy of pre-training and fine-tuning issues, which yields another significant gain.</p><p>We note that though the Mac task is similar to the spell check task, we only use synonyms for replacement, which is only a small proportion in real spell check tasks. This could explain why our model does not yield larger improvement over others when there is fewer training data available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSION</head><p>In this paper, we revisit pre-trained language models in Chinese to see if the techniques in these state-of-the-art models generalize well in a different language other than English only. We created Chinese pre-trained language model series and proposed a new model called MacBERT, which modifies the masked language model (MLM) task as a language correction manner and mitigates the discrepancy of the pre-training and fine-tuning stage. Extensive experiments are conducted on various Chinese NLP datasets, and the results show that the proposed MacBERT could give significant gains in most of the tasks, and detailed ablations show that more focus should be made on the MLM task rather than the NSP task and its variants, as we found that NSP-like task does not show a landslide advantage over one another. With the release of the Chinese pre-trained language model series, we hope it will further accelerate the natural language processing in our research community.</p><p>In the future, we would like to investigate an effective way to determine the masking ratios instead of heuristic ones to further improve the performance of the pre-trained language models. Also, we would like to design more effective language modeling approaches to further exploit large-scale unsupervised data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>I COMPARISONS OF THE PRE-TRAINED LANGUAGE MODELS. (AE: AUTO-ENCODING, AR: AUTO-REGRESSIVE, T: TOKEN, S: SEGMENT, P: POSITION, E: ENTITY, PH: PHRASE, WWM: WHOLE WORD MASKING, NM: N-GRAM MASKING, NSP: NEXT SENTENCE PREDICTION, SOP: SENTENCE ORDER PREDICTION, MLM: MASKED LM, PLM: PERMUTATION LM, GEN-DIS: GENERATOR-DISCRIMINATOR, MAC: MLM AS CORRECTION)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Neural architecture of MacBERT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Results of using different percentage of SIGHAN-15 training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II EXAMPLES</head><label>II</label><figDesc>OF DIFFERENT MASKING STRATEGIES. WE ALSO INCLUDE AN ENGLISH EXAMPLE FOR CLARITY. MASKED TOKENS ARE IN BOLDFACE.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III TRAINING</head><label>III</label><figDesc>DETAILS OF CHINESE PRE-TRAINED LANGUAGE MODELS.</figDesc><table><row><cell></cell><cell>BERT</cell><cell cols="2">BERT-wwm RoBERTa-wwm</cell><cell>RBT</cell><cell cols="2">ELECTRA MacBERT</cell></row><row><cell>Word #</cell><cell>0.4B</cell><cell>5.4B</cell><cell>5.4B</cell><cell>5.4B</cell><cell>5.4B</cell><cell>5.4B</cell></row><row><cell>Vocab #</cell><cell>21,128</cell><cell>21,128</cell><cell>21,128</cell><cell>21,128</cell><cell>21,128</cell><cell>21,128</cell></row><row><cell>Hidden Activation</cell><cell>GeLU</cell><cell>GeLU</cell><cell>GeLU</cell><cell>GeLU</cell><cell>GeLU</cell><cell>GeLU</cell></row><row><cell>Optimizer</cell><cell>AdamW</cell><cell>LAMB</cell><cell>AdamW</cell><cell>AdamW</cell><cell>AdamW</cell><cell>LAMB</cell></row><row><cell>Training Steps (base/large)</cell><cell>?</cell><cell>2M</cell><cell>1M / 2M</cell><cell>1M</cell><cell>1M / 2M</cell><cell>1M / 2M</cell></row><row><cell>Initial Checkpoint (base)</cell><cell>random</cell><cell>BERT</cell><cell>BERT</cell><cell>RoBERTa</cell><cell>random</cell><cell>BERT</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV DATA</head><label>IV</label><figDesc>STATISTICS AND HYPER-PARAMETER SETTINGS FOR DIFFERENT FINE-TUNING TASKS.</figDesc><table><row><cell>Dataset</cell><cell cols="2">MaxLen Epoch</cell><cell>LR</cell><cell>Train</cell><cell>Dev</cell><cell>Test</cell></row><row><cell>CMRC 2018</cell><cell>512</cell><cell>2</cell><cell>3e-5</cell><cell>10K</cell><cell>3.2K</cell><cell>4.9K</cell></row><row><cell>DRCD</cell><cell>512</cell><cell>2</cell><cell>3e-5</cell><cell>27K</cell><cell>3.5K</cell><cell>3.5K</cell></row><row><cell>CJRC</cell><cell>512</cell><cell>2</cell><cell>4e-5</cell><cell>10K</cell><cell>3.2K</cell><cell>3.2K</cell></row><row><cell>ChnSentiCorp</cell><cell>256</cell><cell>3</cell><cell>2e-5</cell><cell>9.6K</cell><cell>1.2K</cell><cell>1.2K</cell></row><row><cell>THUCNews</cell><cell>512</cell><cell>3</cell><cell>2e-5</cell><cell>50K</cell><cell>5K</cell><cell>10K</cell></row><row><cell>TNEWS</cell><cell>128</cell><cell>3</cell><cell cols="2">2e-5 53.3K</cell><cell>10K</cell><cell>10K</cell></row><row><cell>XNLI</cell><cell>128</cell><cell>2</cell><cell>3e-5</cell><cell>392K</cell><cell>2.5K</cell><cell>5K</cell></row><row><cell>LCQMC</cell><cell>128</cell><cell>3</cell><cell>2e-5</cell><cell>240K</cell><cell cols="2">8.8K 12.5K</cell></row><row><cell>BQ Corpus</cell><cell>128</cell><cell>3</cell><cell>3e-5</cell><cell>100K</cell><cell>10K</cell><cell>10K</cell></row><row><cell>OCNLI</cell><cell>128</cell><cell>3</cell><cell>2e-5</cell><cell>56K</cell><cell>3K</cell><cell>3K</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V RESULTS</head><label>V</label><figDesc>ON CMRC 2018 (SIMPLIFIED CHINESE) AND DRCD. THE AVERAGE SCORES OF 10 INDEPENDENT RUNS ARE DEPICTED IN BRACKETS. OVERALL BEST PERFORMANCES ARE DEPICTED IN BOLDFACE (BASE-LEVEL AND LARGE-LEVEL ARE MARKED INDIVIDUALLY).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">CMRC 2018</cell><cell></cell><cell></cell><cell></cell><cell cols="2">DRCD</cell><cell></cell></row><row><cell></cell><cell cols="2">Dev</cell><cell cols="2">Test</cell><cell cols="2">Challenge</cell><cell cols="2">Dev</cell><cell cols="2">Test</cell></row><row><cell></cell><cell>EM</cell><cell>F1</cell><cell>EM</cell><cell>F1</cell><cell>EM</cell><cell>F1</cell><cell>EM</cell><cell>F1</cell><cell>EM</cell><cell>F1</cell></row><row><cell>BERT</cell><cell>65.5 (64.4)</cell><cell>84.5 (84.0)</cell><cell>70.0 (68.7)</cell><cell>87.0 (86.3)</cell><cell>18.6 (17.0)</cell><cell>43.3 (41.3)</cell><cell>83.1 (82.7)</cell><cell>89.9 (89.6)</cell><cell>82.2 (81.6)</cell><cell>89.2 (88.8)</cell></row><row><cell>BERT-wwm</cell><cell>66.3 (65.0)</cell><cell>85.6 (84.7)</cell><cell>70.5 (69.1)</cell><cell>87.4 (86.7)</cell><cell>21.0 (19.3)</cell><cell>47.0 (43.9)</cell><cell>84.3 (83.4)</cell><cell>90.5 (90.2)</cell><cell>82.8 (81.8)</cell><cell>89.7 (89.0)</cell></row><row><cell>BERT-wwm-ext</cell><cell>67.1 (65.6)</cell><cell>85.7 (85.0)</cell><cell>71.4 (70.0)</cell><cell>87.7 (87.0)</cell><cell>24.0 (20.0)</cell><cell>47.3 (44.6)</cell><cell>85.0 (84.5)</cell><cell>91.2 (90.9)</cell><cell>83.6 (83.0)</cell><cell>90.4 (89.9)</cell></row><row><cell>RoBERTa-wwm-ext</cell><cell>67.4 (66.5)</cell><cell>87.2 (86.5)</cell><cell>72.6 (71.4)</cell><cell>89.4 (88.8)</cell><cell>26.2 (24.6)</cell><cell>51.0 (49.1)</cell><cell>86.6 (85.9)</cell><cell>92.5 (92.2)</cell><cell>85.6 (85.2)</cell><cell>92.0 (91.7)</cell></row><row><cell>ELECTRA-base</cell><cell>68.4 (68.0)</cell><cell>84.8 (84.6)</cell><cell>73.1 (72.7)</cell><cell>87.1 (86.9)</cell><cell>22.6 (21.7)</cell><cell>45.0 (43.8)</cell><cell>87.5 (87.0)</cell><cell>92.5 (92.3)</cell><cell>86.9 (86.6)</cell><cell>91.8 (91.7)</cell></row><row><cell>MacBERT-base</cell><cell>68.5 (67.3)</cell><cell>87.9 (87.1)</cell><cell>73.2 (72.4)</cell><cell>89.5 (89.2)</cell><cell>30.2 (26.4)</cell><cell>54.0 (52.2)</cell><cell>89.4 (89.2)</cell><cell>94.3 (94.1)</cell><cell>89.5 (88.7)</cell><cell>93.8 (93.5)</cell></row><row><cell>ELECTRA-large</cell><cell>69.1 (68.2)</cell><cell>85.2 (84.5)</cell><cell>73.9 (72.8)</cell><cell>87.1 (86.6)</cell><cell>23.0 (21.6)</cell><cell>44.2 (43.2)</cell><cell>88.8 (88.7)</cell><cell>93.3 (93.2)</cell><cell>88.8 (88.2)</cell><cell>93.6 (93.2)</cell></row><row><cell>RoBERTa-wwm-ext-large</cell><cell>68.5 (67.6)</cell><cell>88.4 (87.9)</cell><cell>74.2 (72.4)</cell><cell>90.6 (90.0)</cell><cell>31.5 (30.1)</cell><cell>60.1 (57.5)</cell><cell>89.6 (89.1)</cell><cell>94.8 (94.4)</cell><cell>89.6 (88.9)</cell><cell>94.5 (94.1)</cell></row><row><cell>MacBERT-large</cell><cell>70.7 (68.6)</cell><cell>88.9 (88.2)</cell><cell>74.8 (73.2)</cell><cell>90.7 (90.1)</cell><cell>31.9 (29.6)</cell><cell>60.2 (57.6)</cell><cell>91.2 (90.8)</cell><cell>95.6 (95.3)</cell><cell>91.7 (90.9)</cell><cell>95.6 (95.3)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI RESULTS</head><label>VI</label><figDesc>ON CJRC.</figDesc><table><row><cell>CJRC</cell><cell>EM</cell><cell cols="2">Dev</cell><cell>F1</cell><cell>EM</cell><cell cols="2">Test</cell><cell>F1</cell></row><row><cell>BERT</cell><cell cols="2">54.6 (54.0)</cell><cell cols="2">75.4 (74.5)</cell><cell cols="2">55.1 (54.1)</cell><cell>75.2 (74.3)</cell></row><row><cell>BERT-wwm</cell><cell cols="2">54.7 (54.0)</cell><cell cols="2">75.2 (74.8)</cell><cell cols="2">55.1 (54.1)</cell><cell>75.4 (74.4)</cell></row><row><cell>BERT-wwm-ext</cell><cell cols="2">55.6 (54.8)</cell><cell cols="2">76.0 (75.3)</cell><cell cols="2">55.6 (54.9)</cell><cell>75.8 (75.0)</cell></row><row><cell>RoBERTa-wwm-ext</cell><cell cols="2">58.7 (57.6)</cell><cell cols="2">79.1 (78.3)</cell><cell cols="2">59.0 (57.8)</cell><cell>79.0 (78.0)</cell></row><row><cell>ELECTRA-base</cell><cell cols="2">59.0 (58.1)</cell><cell cols="2">79.4 (78.5)</cell><cell cols="2">59.3 (58.2)</cell><cell>79.4 (78.3)</cell></row><row><cell>MacBERT-base</cell><cell cols="2">60.4 (59.5)</cell><cell cols="2">80.3 (79.2)</cell><cell cols="2">60.3 (59.3)</cell><cell>79.8 (79.0)</cell></row><row><cell>ELECTRA-large</cell><cell cols="2">61.9 (60.8)</cell><cell cols="2">82.1 (81.2)</cell><cell cols="2">62.3 (61.2)</cell><cell>82.0 (80.7)</cell></row><row><cell>RoBERTa-wwm-ext-L</cell><cell cols="2">62.1 (61.1)</cell><cell cols="2">82.4 (81.6)</cell><cell cols="2">62.4 (61.4)</cell><cell>82.2 (81.0)</cell></row><row><cell>MacBERT-large</cell><cell cols="2">62.4 (61.3)</cell><cell cols="2">82.3 (81.4)</cell><cell cols="2">62.9 (61.6)</cell><cell>82.5 (81.1)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VII RESULTS</head><label>VII</label><figDesc>ON SINGLE SENTENCE CLASSIFICATION TASKS: CHNSENTICORP, THUCNEWS AND TNEWS. 'R' STANDS FOR ROBERTA, 'E' STANDS FOR ELECTRA, 'M' STANDS FOR 'MACBERT'.</figDesc><table><row><cell></cell><cell cols="2">ChnSentiCorp</cell><cell cols="2">THUCNews</cell><cell>TNEWS</cell></row><row><cell></cell><cell>Dev</cell><cell>Test</cell><cell>Dev</cell><cell>Test</cell><cell>Dev</cell></row><row><cell>BERT</cell><cell>94.7 (94.3)</cell><cell>95.0 (94.7)</cell><cell>97.7 (97.4)</cell><cell>97.8 (97.6)</cell><cell>56.3 (56.1)</cell></row><row><cell>BERT-w</cell><cell>95.1 (94.5)</cell><cell>95.4 (95.0)</cell><cell>98.0 (97.6)</cell><cell>97.8 (97.6)</cell><cell>56.5 (56.3)</cell></row><row><cell cols="2">BERT-w-e 95.4 (94.6)</cell><cell>95.3 (94.8)</cell><cell>97.7 (97.5)</cell><cell>97.7 (97.5)</cell><cell>57.0 (56.6)</cell></row><row><cell>R-base</cell><cell>94.9 (94.6)</cell><cell>95.6 (94.9)</cell><cell>98.3 (97.9)</cell><cell>97.8 (97.5)</cell><cell>57.4 (56.9)</cell></row><row><cell>E-base</cell><cell>93.8 (93.0)</cell><cell>94.5 (93.5)</cell><cell>98.1 (97.9)</cell><cell>97.8 (97.5)</cell><cell>56.1 (55.7)</cell></row><row><cell>M-base</cell><cell>95.2 (94.8)</cell><cell>95.6 (94.9)</cell><cell>98.2 (98.0)</cell><cell>97.7 (97.5)</cell><cell>57.4 (57.1)</cell></row><row><cell>E-large</cell><cell>95.2 (94.6)</cell><cell>95.3 (94.8)</cell><cell>98.2 (97.8)</cell><cell>97.8 (97.6)</cell><cell>57.2 (56.9)</cell></row><row><cell>R-large</cell><cell>95.8 (94.9)</cell><cell>95.8 (94.9)</cell><cell>98.3 (97.7)</cell><cell>97.8 (97.6)</cell><cell>58.8 (58.4)</cell></row><row><cell>M-large</cell><cell>95.7 (95.0)</cell><cell>95.9 (95.1)</cell><cell>98.1 (97.8)</cell><cell>97.9 (97.7)</cell><cell>59.0 (58.8)</cell></row><row><cell cols="6">classes. The results show that MacBERT could give moderate</cell></row><row><cell cols="6">improvements over baselines in ChnSentiCorp and THUC-</cell></row><row><cell cols="6">News, as these datasets have already reached high accuracies.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VIII RESULTS</head><label>VIII</label><figDesc>ON SENTENCE PAIR CLASSIFICATION TASKS: XNLI, LCQMC, BQ CORPUS, AND OCNLI.</figDesc><table><row><cell>XNLI</cell><cell></cell><cell cols="2">LCQMC</cell><cell cols="2">BQ Corpus</cell><cell>OCNLI</cell></row><row><cell>Dev</cell><cell>Test</cell><cell>Dev</cell><cell>Test</cell><cell>Dev</cell><cell>Test</cell><cell>Dev</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE IX RESULTS</head><label>IX</label><figDesc>ON RBT SERIES, WHICH ARE BUILT ON ROBERTA-LARGE (ROBERTA-WWM-EXT-LARGE) AND ROBERTA-BASE (ROBERTA-WWM-EXT).</figDesc><table><row><cell>System</cell><cell cols="2">Params</cell><cell cols="2">CMRC 2018 EM F1</cell><cell cols="2">DRCD EM F1</cell><cell cols="2">CJRC EM F1</cell><cell>CSC ACC</cell><cell cols="3">THUC XNLI ACC ACC</cell><cell>LC ACC ACC BQ</cell><cell>AVG</cell></row><row><cell>RoBERTa-large</cell><cell cols="2">324M</cell><cell>74.2</cell><cell>90.6</cell><cell cols="2">89.6 94.5</cell><cell cols="2">62.4 82.2</cell><cell>95.8</cell><cell>97.8</cell><cell cols="2">81.2</cell><cell>87.0</cell><cell>85.8</cell><cell>86.79</cell></row><row><cell>RoBERTa-base</cell><cell cols="2">102M</cell><cell>72.6</cell><cell>89.4</cell><cell cols="2">85.6 92.0</cell><cell cols="2">59.0 79.0</cell><cell>95.6</cell><cell>97.8</cell><cell cols="2">78.8</cell><cell>86.4</cell><cell>85.0</cell><cell>85.30</cell></row><row><cell>RBTL3</cell><cell cols="2">61M</cell><cell>63.3</cell><cell>83.4</cell><cell cols="2">77.2 85.6</cell><cell cols="2">64.6 74.9</cell><cell>94.2</cell><cell>97.8</cell><cell cols="2">74.0</cell><cell>85.1</cell><cell>83.6</cell><cell>82.40</cell></row><row><cell>RBT3</cell><cell cols="2">38M</cell><cell>62.2</cell><cell>81.8</cell><cell cols="2">75.0 83.9</cell><cell cols="2">63.5 73.7</cell><cell>92.8</cell><cell>97.5</cell><cell cols="2">72.3</cell><cell>85.1</cell><cell>83.3</cell><cell>81.38</cell></row><row><cell>RBT4</cell><cell cols="2">45M</cell><cell>65.0</cell><cell>83.9</cell><cell cols="2">78.7 86.7</cell><cell cols="2">65.5 75.3</cell><cell>93.8</cell><cell>97.7</cell><cell cols="2">74.2</cell><cell>85.7</cell><cell>83.7</cell><cell>82.83</cell></row><row><cell>RBT6</cell><cell cols="2">60M</cell><cell>68.3</cell><cell>84.4</cell><cell cols="2">83.9 90.2</cell><cell cols="2">69.1 78.8</cell><cell>95.3</cell><cell>97.8</cell><cell cols="2">76.2</cell><cell>86.6</cell><cell>84.2</cell><cell>84.68</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>TABLE X</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="10">ABLATIONS OF MACBERT-LARGE ON DIFFERENT FINE-TUNING TASKS.</cell></row><row><cell>System</cell><cell></cell><cell cols="2">CMRC 2018 EM F1</cell><cell cols="2">DRCD EM F1</cell><cell cols="2">CJRC EM F1</cell><cell>CSC ACC</cell><cell cols="3">THUC XNLI ACC ACC</cell><cell>LC ACC ACC BQ</cell><cell>AVG</cell></row><row><cell cols="3">MacBERT-large 74.8</cell><cell>90.7</cell><cell cols="2">91.7 95.6</cell><cell cols="2">62.9 82.5</cell><cell>95.9</cell><cell cols="2">97.9</cell><cell>81.3</cell><cell>87.6</cell><cell>85.6</cell><cell>87.18</cell></row><row><cell>SOP ? NSP</cell><cell></cell><cell>74.5</cell><cell>90.6</cell><cell cols="2">91.5 95.5</cell><cell cols="2">62.4 82.3</cell><cell>96.0</cell><cell cols="2">97.8</cell><cell>81.2</cell><cell>87.4</cell><cell>85.2</cell><cell>87.00</cell></row><row><cell>w/o SOP</cell><cell></cell><cell>74.4</cell><cell>90.6</cell><cell cols="2">91.0 95.4</cell><cell cols="2">62.2 82.1</cell><cell>95.8</cell><cell cols="2">97.8</cell><cell>81.1</cell><cell>87.4</cell><cell>85.2</cell><cell>86.89</cell></row><row><cell>w/o Mac</cell><cell></cell><cell>74.2</cell><cell>90.1</cell><cell cols="2">91.2 95.4</cell><cell cols="2">62.2 82.3</cell><cell>95.7</cell><cell cols="2">97.8</cell><cell>81.2</cell><cell>87.4</cell><cell>85.3</cell><cell>86.88</cell></row><row><cell>w/o NM</cell><cell></cell><cell>74.0</cell><cell>89.8</cell><cell cols="2">90.9 95.1</cell><cell cols="2">62.1 82.0</cell><cell>95.9</cell><cell cols="2">97.9</cell><cell>81.3</cell><cell>87.5</cell><cell>85.6</cell><cell>86.89</cell></row><row><cell cols="2">RoBERTa-large</cell><cell>74.2</cell><cell>90.6</cell><cell cols="2">89.6 94.5</cell><cell cols="2">62.4 82.2</cell><cell>95.8</cell><cell cols="2">97.8</cell><cell>81.2</cell><cell>87.0</cell><cell>85.8</cell><cell>86.79</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>our surprise, a quick fix, that is to abandon the [MASK] token completely and replace all 90% masked tokens into random words, yields consistent improvements over [MASK]-Fig. 2. Results of different MLM tasks on CMRC 2018 and DRCD.</figDesc><table><row><cell></cell><cell>74.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>74.0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>73.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>EM</cell><cell>73.0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>72.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>71.5 72.0</cell><cell>1M</cell><cell>1.1M</cell><cell>1.2M</cell><cell>1.5M</cell><cell>1.8M MacBERT Random Replace 2M Partial [MASK] All [MASK]</cell></row><row><cell></cell><cell>91.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>91.0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>90.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>EM</cell><cell>90.0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>89.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>88.5 89.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MacBERT Random Replace Partial [MASK] All [MASK]</cell></row><row><cell></cell><cell></cell><cell>1M</cell><cell>1.1M</cell><cell>1.2M</cell><cell>1.5M</cell><cell>1.8M</cell><cell>2M</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Though N-gram masking was not included in<ref type="bibr" target="#b1">[2]</ref>, according to their model name in SQuAD leaderboard, we often admit their credit towards this method.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://dumps.wikimedia.org/zhwiki/latest/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/gaussic/text-classification-cnn-rnn</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>We would like to thank all anonymous reviewers and editors for their thorough reviewing and providing constructive comments to improve our paper. The first author was partially supported by the Google TPU Research Cloud (TRC) program for Cloud TPU access.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Ziqing Yang is a researcher at Joint Laboratory of HIT and iFLYTEK Research (HFL). He received his B.S degree from Wuhan University in 2010 and received his Ph.D. degree in Physics from University of Chinese Academy of Sciences in 2017. He has a broad interest in machine learning and natural language processing, including machine reading comprehension, knowledge distillation for NLP and general machine learning method for NLP. He has published several top-tier conference papers, including ACL, COLING, etc.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Pretraining with whole word masking for chinese bert</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08101</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/N19-1423" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Know what you don&apos;t know: Unanswerable questions for SQuAD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/P18-2124" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="784" to="789" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Coqa: A conversational question answering challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="249" to="266" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">QuAC: Question answering in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11" />
			<biblScope unit="page" from="2174" to="2184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Natural questions: a benchmark for question answering research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="453" to="466" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Race: Large-scale reading comprehension dataset from examinations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D17-1083" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="796" to="805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Transformer-XL: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07" />
			<biblScope unit="page" from="2978" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Dual co-matching network for multi-choice reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09381</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Option comparison network for multiple-choice reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.03033</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Ernie: Enhanced representation through knowledge integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09223</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Spanbert: Improving pre-training by representing and predicting spans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10529</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ELECTRA: Pretraining text encoders as discriminators rather than generators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="https://openreview.net/pdf?id=r1xMH1BtvB" />
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A mutual information maximization perspective of language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>De Masson D&amp;apos;autume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yogatama</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Syx79eBKwr" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ltp: A chinese language technology platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics: Demonstrations. Association for Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics: Demonstrations. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="13" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">{PMI}-masking: Principled masking of correlated spans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Abend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Leyton-Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tennenholtz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shoham</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=3Aoft6NWFej" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Synonyms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<ptr target="https://github.com/huyingxi/Synonyms" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Reducing bert pre-training time from 3 days to 76 minutes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hseu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00962</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A Span-Extraction Dataset for Chinese Machine Reading Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11" />
			<biblScope unit="page" from="5886" to="5891" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Drcd: a chinese machine reading comprehension dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tsai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00920</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cjrc: A reliable human-annotated benchmark dataset for chinese judicial reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">China National Conference on Chinese Computational Linguistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="439" to="451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An empirical study of sentiment analysis for chinese documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with applications</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2622" to="2629" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Scalable term selection for text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">CLUE: A Chinese language understanding evaluation benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2020.coling-main.419" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12" />
			<biblScope unit="page" from="4762" to="4772" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Xnli: Evaluating cross-lingual sentence representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Lcqmc: A large-scale chinese question matching corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1952" to="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The BQ corpus: A large-scale domain-specific Chinese corpus for sentence semantic equivalence identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tang</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D18-1536" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11" />
			<biblScope unit="page" from="4946" to="4951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Ocnli: Original chinese natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kuebler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Moss</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2010.05444" />
	</analytic>
	<monogr>
		<title level="m">Findings of EMNLP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Ernie 2.0: A continual pre-training framework for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.12412</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">NEZHA: Neural Contextualized Representation for Chinese Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.00204</idno>
		<imprint>
			<date type="published" when="2019-08" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th USENIX Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D16-1264" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Introduction to SIGHAN 2015 bake-off for Chinese spelling check</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-H</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/W15-3106" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth SIGHAN Workshop on Chinese Language Processing</title>
		<meeting>the Eighth SIGHAN Workshop on Chinese Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="page" from="32" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">He received M.S. and B.S. degrees and is currently pursuing a doctoral degree at Harbin Institute of Technology. His main research interests include Machine Reading Comprehension, Question Answering, and Pre-trained Language Model, etc. He has published more than 20 papers in top conferences</title>
	</analytic>
	<monogr>
		<title level="m">such as in ACL, EMNLP, AAAI, COLING, NAACL, etc. He serves as a senior member of China Computer Federation (CCF)</title>
		<imprint/>
	</monogr>
	<note>Yiming Cui is a principal researcher at Joint Laboratory of HIT and iFLYTEK Research (HFL)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
