<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DKN: Deep Knowledge-Aware Network for News Recommendation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
							<email>wanghongwei55@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
							<email>xingx@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyi</forename><surname>Guo</surname></persName>
							<email>guo-my@cs.sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DKN: Deep Knowledge-Aware Network for News Recommendation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/nnnnnnn.nnnnnnn</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>News recommendation</term>
					<term>knowledge graph representation</term>
					<term>deep neu- ral networks</term>
					<term>attention model</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Online news recommender systems aim to address the information explosion of news and make personalized recommendation for users. In general, news language is highly condensed, full of knowledge entities and common sense. However, existing methods are unaware of such external knowledge and cannot fully discover latent knowledge-level connections among news. The recommended results for a user are consequently limited to simple patterns and cannot be extended reasonably. To solve the above problem, in this paper, we propose a deep knowledge-aware network (DKN) that incorporates knowledge graph representation into news recommendation. DKN is a content-based deep recommendation framework for click-through rate prediction. The key component of DKN is a multi-channel and word-entity-aligned knowledge-aware convolutional neural network (KCNN) that fuses semantic-level and knowledge-level representations of news. KCNN treats words and entities as multiple channels, and explicitly keeps their alignment relationship during convolution. In addition, to address users' diverse interests, we also design an attention module in DKN to dynamically aggregate a user's history with respect to current candidate news. Through extensive experiments on a real online news platform, we demonstrate that DKN achieves substantial gains over state-of-the-art deep recommendation models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>With the advance of the World Wide Web, people's news reading habits have gradually shifted from traditional media such as newspapers and TV to the Internet. Online news websites, such as Google News 1 and Bing News 2 , collect news from various sources and provide an aggregate view of news for readers. A notorious problem with online news platforms is that the volume of articles can be overwhelming to users. To alleviate the impact of information overloading, it is critical to help users target their reading interests and make personalized recommendations <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b39">39]</ref>.</p><p>Generally, news recommendation is quite difficult as it poses three major challenges. First, unlike other items such as movies <ref type="bibr" target="#b9">[9]</ref> Figure 1: Illustration of two pieces of news connected through knowledge entities. and restaurants <ref type="bibr" target="#b12">[12]</ref>, news articles are highly time-sensitive and their relevance expires quickly within a short period (see Section 5.1). Out-of-date news are substituted by newer ones frequently, which makes traditional ID-based methods such as collaborative filtering (CF) <ref type="bibr" target="#b41">[41]</ref> less effective. Second, people are topic-sensitive in news reading as they are usually interested in multiple specific news categories (see <ref type="bibr">Section 5.5)</ref>. How to dynamically measure a user's interest based on his diversified reading history for current candidate news is key to news recommender systems. Third, news language is usually highly condensed and comprised of a large amount of knowledge entities and common sense. For example, as shown in <ref type="figure">Figure 1</ref>, a user clicks a piece of news with title "Boris Johnson Has Warned Donald Trump To Stick To The Iran Nuclear Deal" that contains four knowledge entities: "Boris Johnson", "Donald Trump", "Iran" and "Nuclear". In fact, the user may also be interested in another piece of news with title "North Korean EMP Attack Would Cause Mass U.S. Starvation, Says Congressional Report" with high probability, which shares a great deal of contextual knowledge and is strongly connected with the previous one in terms of commonsense reasoning. However, traditional semantic models <ref type="bibr" target="#b30">[30]</ref> or topic models <ref type="bibr" target="#b2">[3]</ref> can only find their relatedness based on co-occurrence or clustering structure of words, but are hardly able to discover their latent knowledge-level connection. As a result, a user's reading pattern will be narrowed down to a limited circle and cannot be reasonably extended based on existing recommendation methods.</p><p>To extract deep logical connections among news, it is necessary to introduce additional knowledge graph information into news recommendations. A knowledge graph is a type of directed heterogeneous graph in which nodes correspond to entities and edges correspond to relations. Recently, researchers have proposed several academic knowledge graphs such as NELL <ref type="bibr" target="#b2">3</ref> and DBpedia 4 , as well as commercial ones such as Google Knowledge Graph <ref type="bibr" target="#b4">5</ref> and Microsoft Satori <ref type="bibr" target="#b5">6</ref> . These knowledge graphs are successfully applied in scenarios of machine reading <ref type="bibr" target="#b51">[51]</ref>, text classification <ref type="bibr" target="#b46">[46]</ref>, and word embedding <ref type="bibr" target="#b49">[49]</ref>.</p><p>Considering the above challenges in news recommendation and inspired by the wide success of leveraging knowledge graphs, in this paper, we propose a novel framework that takes advantage of external knowledge for news recommendation, namely the deep knowledge-aware network (DKN). DKN is a content-based model for click-through rate (CTR) prediction, which takes one piece of candidate news and one user's click history as input, and outputs the probability of the user clicking the news. Specifically, for a piece of input news, we first enrich its information by associating each word in the news content with a relevant entity in the knowledge graph. We also search and use the set of contextual entities of each entity (i.e., its immediate neighbors in the knowledge graph) to provide more complementary and distinguishable information. Then we design a key component in DKN, namely knowledge-aware convolutional neural networks (KCNN), to fuse the word-level and knowledge-level representations of news and generate a knowledgeaware embedding vector. Distinct from existing work <ref type="bibr" target="#b46">[46]</ref>, KCNN is: 1) multi-channel, as it treats word embedding, entity embedding, and contextual entity embedding of news as multiple stacked channels just like colored images; 2) word-entity-aligned, as it aligns a word and its associated entity in multiple channels and applies a transformation function to eliminate the heterogeneity of the word embedding and entity embedding spaces.</p><p>Using KCNN, we obtain a knowledge-aware representation vector for each piece of news. To get a dynamic representation of a user with respect to current candidate news, we use an attention module to automatically match candidate news to each piece of clicked news, and aggregate the user's history with different weights. The user's embedding and the candidate news' embedding are finally processed by a deep neural network (DNN) for CTR prediction.</p><p>Empirically, we apply DKN to a real-world dataset from Bing News with extensive experiments. The results show that DKN achieves substantial gains over state-of-the-art deep-learning-based methods for recommendation. Specifically, DKN significantly outperforms baselines by 2.8% to 17.0% on F1 and 2.6% to 16.1% on AUC with a significance level of 0.1. The results also prove that the usage of knowledge and an attention module can bring additional 3.5% and 1.4% in improvement, respectively, in the DKN framework. Moreover, we present a visualization result of attention values to intuitively demonstrate the efficacy of the usage of the knowledge graph in Section 5.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>In this section, we present several concepts and models related to this work, including knowledge graph embedding and convolutional neural networks for sentence representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Knowledge Graph Embedding</head><p>A typical knowledge graph consists of millions of entity-relationentity triples (h, r , t), in which h, r and t represent the head, the relation, and the tail of a triple, respectively. Given all the triples in a knowledge graph, the goal of knowledge graph embedding is to 6 https://searchengineland.com/library/bing/bing-satori  <ref type="figure">Figure 2</ref>: A typical architecture of CNN for sentence representation learning <ref type="bibr" target="#b20">[20]</ref>.</p><p>learn a low-dimensional representation vector for each entity and relation that preserves the structural information of the original knowledge graph. Recently, translation-based knowledge graph embedding methods have received great attention due to their concise models and superior performance. To be self-contained, we briefly review these translation-based methods in the following.</p><p>? TransE <ref type="bibr" target="#b3">[4]</ref> wants h + r ? t when (h, r , t) holds, where h, r and t are the corresponding representation vector of h, r and t. Therefore, TransE assumes the score function</p><formula xml:id="formula_0">f r (h, t) = ?h + r ? t? 2 2<label>(1)</label></formula><p>is low if (h, r, t) holds, and high otherwise.</p><p>? TransH <ref type="bibr" target="#b48">[48]</ref> allows entities to have different representations when involved in different relations by projecting the entity embeddings into relation hyperplanes:</p><formula xml:id="formula_1">f r (h, t) = ?h ? + r ? t ? ? 2 2 ,<label>(2)</label></formula><p>where h ? = h ? w ? r hw r and t ? = t ? w ? r tw r are the projections of h and t to the hyperplane w r , respectively, and ?w r ? 2 = 1.</p><p>? TransR <ref type="bibr" target="#b26">[26]</ref> introduces a projection matrix M r for each relation r to map entity embeddings to the corresponding relation space. The score function in TransR is defined as</p><formula xml:id="formula_2">f r (h, t) = ?h r + r ? t r ? 2 2 ,<label>(3)</label></formula><p>where h r = hM r and t r = tM r . ? TransD <ref type="bibr" target="#b18">[18]</ref> replaces the projection matrix in TransR by the product of two projection vectors of an entity-relation pair:</p><formula xml:id="formula_3">f r (h, t) = ?h ? + r ? t ? ? 2 2 ,<label>(4)</label></formula><p>where h ? = (r p h ? p + I)h, t ? = (r p t ? p + I)t, h p , r p and t p are another set of vectors for entities and relations, and I is the identity matrix.</p><p>To encourage the discrimination between correct triples and incorrect triples, for all the methods above, the following marginbased ranking loss is used for training:</p><formula xml:id="formula_4">L = (h,r,t )?? (h ? ,r,t ? )?? ? max 0, f r (h, t) + ? ? f r (h ? , t ? ) ,<label>(5)</label></formula><p>where ? is the margin, ? and ? ? are the set of correct triples and incorrect triples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">CNN for Sentence Representation Learning</head><p>Traditional methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b43">43]</ref> usually represent sentences using the bag-of-words (BOW) technique, i.e., taking word counting statistics as the feature of sentences. However, BOW-based methods ignore word orders in sentences and are vulnerable to the sparsity problem, which leads to poor generalization performance. A more effective way to model sentences is to represent each sentence in a given corpus as a distributed low-dimensional vector. Recently, inspired by the success of applying convolutional neural networks (CNN) in the filed of computer vision <ref type="bibr" target="#b23">[23]</ref>, researchers have proposed many CNN-based models for sentence representation learning <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b53">53]</ref>  <ref type="bibr" target="#b7">7</ref> . In this subsection, we introduce a typical type of CNN architecture, namely Kim CNN <ref type="bibr" target="#b20">[20]</ref>. <ref type="figure">Figure 2</ref> illustrates the architecture of Kim CNN. Let w 1:n be the raw input of a sentence of length n, and w 1:n = [w 1 w 2 ... w n ] ? R d ?n be the word embedding matrix of the input sentence, where w i ? R d ?1 is the embedding of the i-th word in the sentence and d is the dimension of word embeddings. A convolution operation with filter h ? R d ?l is then applied to the word embedding matrix w 1:n , where l (l ? n) is the window size of the filter. Specifically, a feature c i is generated from a sub-matrix w i:i+l ?1 by</p><formula xml:id="formula_5">c i = f (h * w i:i+l ?1 + b),<label>(6)</label></formula><p>where f is a non-linear function, * is the convolution operator, and b ? R is a bias. After applying the filter to every possible position in the word embedding matrix, a feature map</p><formula xml:id="formula_6">c = [c 1 , c 2 , ..., c n?l +1 ]<label>(7)</label></formula><p>is obtained, then a max-over-time pooling operation is used on feature map c to identify the most significant feature:</p><formula xml:id="formula_7">c = max{c} = max{c 1 , c 2 , ..., c n?l +1 }.<label>(8)</label></formula><p>One can use multiple filters (with varying window sizes) to obtain multiple features, and these features are concatenated together to form the final sentence representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM FORMULATION</head><p>We formulate the news recommendation problem in this paper as follows. For a given user i in the online news platform, we denote his click history as {t i 1 , t i 2 , ..., t i N i }, where t i j (j = 1, ..., N i ) is the title 8 of the j-th news clicked by user i, and N i is the total number of user i's clicked news. Each news title t is composed of a sequence of words, i.e., t = [w 1 , w 2 , ...], where each word w may be associated with an entity e in the knowledge graph. For example, in the title "Trump praises Las Vegas medical team", "Trump" is linked with the entity "Donald Trump", while "Las" and "Vegas" are linked with the entity "Las Vegas". Given users' click history as well as the connection between words in news titles and entities in the <ref type="bibr" target="#b7">7</ref> Researchers have also proposed other types of neural network models for sentence modeling such as recurrent neural networks <ref type="bibr" target="#b40">[40]</ref>, recursive neural networks <ref type="bibr" target="#b38">[38]</ref>, and hybrid models <ref type="bibr" target="#b24">[24]</ref>. However, CNN-based models are empirically proven to be superior than others <ref type="bibr" target="#b15">[15]</ref>, since they can detect and extract specific local patterns from sentences due to the convolution operation. To keep our presentation focused, we only discuss CNN-based models in this paper. <ref type="bibr" target="#b8">8</ref> In addition to title, it is also viable to use abstracts or snippets of news. In this paper, we only take news titles as input, since a title is a decisive factor affecting users' choice of reading. But note that our approach can be easily generalized to any sort of news-related texts. knowledge graph, we aim to predict whether user i will click a candidate news t j that he has not seen before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DEEP KNOWLEDGE-AWARE NETWORK</head><p>In this section, we present the proposed DKN model in detail. We first introduce the overall framework of DKN, then discuss the process of knowledge distillation from a knowledge graph, the design of knowledge-aware convolutional neural networks (KCNN), and the attention-based user interest extraction, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">DKN Framework</head><p>The framework of DKN is illustrated in <ref type="figure">Figure 3</ref>. We introduce the architecture of DKN from the bottom up. As shown in <ref type="figure">Figure 3</ref>, DKN takes one piece of candidate news and one piece of a user's clicked news as input. For each piece of news, a specially designed KCNN is used to process its title and generate an embedding vector. KCNN is an extension of traditional CNN that allows flexibility in incorporating symbolic knowledge from a knowledge graph into sentence representation learning. We will detail the process of knowledge distillation in Section 4.2 and the KCNN module in Section 4.3, respectively. By KCNN, we obtain a set of embedding vectors for a user's clicked history. To get final embedding of the user with respect to the current candidate news, we use an attention-based method to automatically match the candidate news to each piece of his clicked news, and aggregate the user's historical interests with different weights. The details of attention-based user interest extraction are presented in Section 4.4. The candidate news embedding and the user embedding are concatenated and fed into a deep neural network (DNN) to calculate the predicted probability that the user will click the candidate news.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Knowledge Distillation</head><p>The process of knowledge distillation is illustrated in <ref type="figure" target="#fig_0">Figure 4</ref>, which consists of four steps. First, to distinguish knowledge entities in news content, we utilize the technique of entity linking <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b36">36]</ref> to disambiguate mentions in texts by associating them with predefined entities in a knowledge graph. Based on these identified entities, we construct a sub-graph and extract all relational links among them from the original knowledge graph. Note that the relations among identified entities only may be sparse and lack diversity. Therefore, we expand the knowledge sub-graph to all entities within one hop of identified ones. Given the extracted knowledge graph, a great many knowledge graph embedding methods, such as TransE <ref type="bibr" target="#b3">[4]</ref>, TransH <ref type="bibr" target="#b48">[48]</ref>, TransR <ref type="bibr" target="#b26">[26]</ref>, and TransD <ref type="bibr" target="#b18">[18]</ref>, can be utilized for entity representation learning. Learned entity embeddings are taken as the input for KCNN in the DKN framework.</p><p>It should be noted that though state-of-the-art knowledge graph embedding methods could generally preserve the structural information in the original graph, we find that the information of learned embedding for a single entity is still limited when used in subsequent recommendations. To help identify the position of entities in the knowledge graph, we propose extracting additional contextual information for each entity. The "context" of entity e is defined as the set of its immediate neighbors in the knowledge graph, i.e., context(e) = {e i | (e, r , e i ) ? G or (e i , r , e) ? G},  where r is a relation and G is the knowledge graph. Since the contextual entities are usually closely related to the current entity with respect to semantics and logic, the usage of context could provide more complementary information and assist in improving the identifiability of entities. <ref type="figure" target="#fig_1">Figure 5</ref> illustrates an example of context. In addition to use the embedding of "Fight Club" itself to represent the entity, we also include its contexts, such as "Suspense" (genre), "Brad Pitt" (actor), "United States" (country) and "Oscars" (award), as its identifiers. Given the context of entity e, the context embedding is calculated as the average of its contextual entities:</p><formula xml:id="formula_9">e = 1 |context(e)| e i ?cont ex t (e) e i ,<label>(10)</label></formula><p>where e i is the entity embedding of e i learned by knowledge graph embedding. We empirically demonstrate the efficacy of context embedding in the experiment section.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Knowledge-aware CNN</head><p>Following the notations used in Section 2.2, we use t = w 1:n = [w 1 , w 2 , ..., w n ] to denote the raw input sequence of a news title t of length n, and w 1:n = [w 1 w 2 ... w n ] ? R d ?n to denote the word embedding matrix of the title, which can be pre-learned from a large corpus or randomly initialized. After the knowledge distillation introduced in Section 4.2, each word w i may also be associated with an entity embedding e i ? R k ?1 and the corresponding context embedding e i ? R k ?1 , where k is the dimension of entity embedding. Given the input above, a straightforward way to combine words and associated entities is to treat the entities as "pseudo words" and concatenate them to the word sequence <ref type="bibr" target="#b46">[46]</ref>, i.e.,</p><formula xml:id="formula_10">W = [w 1 w 2 ... w n e t 1 e t 2 ...],<label>(11)</label></formula><p>where {e t j } is the set of entity embeddings associated with this news title. The obtained new sentence W is fed into CNN <ref type="bibr" target="#b20">[20]</ref> for further processing. However, we argue that this simple concatenating strategy has the following limitations: 1) The concatenating strategy breaks up the connection between words and associated entities and is unaware of their alignment. 2) Word embeddings and entity embeddings are learned by different methods, meaning it is not suitable to convolute them together in a single vector space.</p><p>3) The concatenating strategy implicitly forces word embeddings and entity embeddings to have the same dimension, which may not be optimal in practical settings since the optimal dimensions for word and entity embeddings may differ from each other. Being aware of the above limitations, we propose a multi-channel and word-entity-aligned KCNN for combining word semantics and knowledge information. The architecture of KCNN is illustrated in the left lower part of <ref type="figure">Figure 3</ref>. For each news title t = [w 1 , w 2 , ..., w n ], in addition to use its word embeddings w 1:n = [w 1 w 2 ... w n ] as input, we also introduce the transformed entity embeddings</p><formula xml:id="formula_11">?(e 1:n ) = [?(e 1 ) ?(e 2 ) ... ?(e n )]<label>(12)</label></formula><p>and transformed context embeddings</p><formula xml:id="formula_12">?(e 1:n ) = [?(e 1 ) ?(e 2 ) ... ?(e n )]<label>(13)</label></formula><p>as source of input <ref type="bibr" target="#b9">9</ref> , where ? is the transformation function. In KCNN, ? can be either linear</p><formula xml:id="formula_13">?(e) = Me<label>(14)</label></formula><p>or non-linear</p><formula xml:id="formula_14">?(e) = tanh(Me + b),<label>(15)</label></formula><p>where M ? R d ?k is the trainable transformation matrix and b ? R d ?1 is the trainable bias. Since the transformation function is continuous, it can map the entity embeddings and context embeddings from the entity space to the word space while preserving their original spatial relationship. Note that word embeddings w 1:n , transformed entity embeddings ?(e 1:n ) and transformed context embeddings ?(e 1:n ) are the same size and serve as the multiple channels analogous to colored images. We therefore align and stack the three embedding matrices as</p><formula xml:id="formula_15">W = [w 1 ?(e 1 ) ?(e 1 )] [w 2 ?(e 2 ) ?(e 2 )] ... [e n ?(e n ) ?(e n )] ? R d ?n?3 .<label>(16)</label></formula><p>After getting the multi-channel input W, similar to Kim CNN <ref type="bibr" target="#b20">[20]</ref>, we apply multiple filters h ? R d ?l ?3 with varying window sizes l to extract specific local patterns in the news title. The local activation of sub-matrix W i:i+l ?1 with respect to h can be written as</p><formula xml:id="formula_16">c h i = f (h * W i:i+l ?1 + b),<label>(17)</label></formula><p>and we use a max-over-time pooling operation on the output feature map to choose the largest feature:</p><formula xml:id="formula_17">c h = max{c h 1 , c h 2 , ..., c h n?l +1 }.<label>(18)</label></formula><p>All featuresc h i are concatenated together and taken as the final representation e(t) of the input news title t, i.e.,</p><formula xml:id="formula_18">e(t) = [c h 1c h 2 ...c h m ],<label>(19)</label></formula><p>where m is the number of filters. <ref type="bibr" target="#b9">9</ref> e i and e i are set as zero if w i has no corresponding entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Attention-based User Interest Extraction</head><p>Given user i with clicked history {t i 1 , t i 2 , ..., t i N i }, the embeddings of his clicked news can be written as e(t i 1 ), e(t i 2 ), ..., e(t i N i ). To represent user i for the current candidate news t j , one can simply average all the embeddings of his clicked news titles:</p><formula xml:id="formula_19">e(i) = 1 N i N i k =1 e(t i k ).<label>(20)</label></formula><p>However, as discussed in the introduction, a user's interest in news topics may be various, and user i's clicked items are supposed to have different impacts on the candidate news t j when considering whether user i will click t j . To characterize user's diverse interests, we use an attention network <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b54">54]</ref> to model the different impacts of the user's clicked news on the candidate news. The attention network is illustrated in the left upper part of <ref type="figure">Figure 3</ref>. Specifically, for user i's clicked news t i k and candidate news t j , we first concatenate their embeddings, then apply a DNN H as the attention network and the softmax function to calculate the normalized impact weight:</p><formula xml:id="formula_20">s t i k ,t j = softmax H e(t i k ), e(t j ) = exp H e(t i k ), e(t j ) N i k =1 exp H e(t i k ), e(t j )</formula><p>.</p><p>(21) The attention network H receives embeddings of two news titles as input and outputs the impact weight. The embedding of user i with respect to the candidate news t j can thus be calculated as the weighted sum of his clicked news title embeddings:</p><formula xml:id="formula_21">e(i) = N i k =1 s t i k ,t j e(t i k ).<label>(22)</label></formula><p>Finally, given user i's embedding e(i) and candidate news t j 's embedding e(t j ), the probability of user i clicking news t j is predicted by another DNN G:</p><formula xml:id="formula_22">p i,t j = G e(i), e(t j ) .<label>(23)</label></formula><p>We will demonstrate the efficacy of the attention network in the experiment section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we present our experiments and the corresponding results, including dataset analysis and comparison of models. We also give a case study about user's reading interests and make discussions on tuning hyper-parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset Description</head><p>Our dataset comes from the server logs of Bing News. Each piece of log mainly contains the timestamp, user id, news url, news title, and click count (0 for no click and 1 for click). We collect a randomly sampled and balanced dataset from October 16, 2016 to June 11, 2017 as the training set, and from June 12, 2017 to August 11, 2017 as the test set. Additionally, we search all occurred entities in the dataset as well as the ones within their one hop in the Microsoft Satori knowledge graph, and extract all edges (triples) among them with confidence greater than 0.8. The basic statistics and distributions of the news dataset and the extracted knowledge graph are shown in <ref type="table" target="#tab_3">Table 1</ref> and <ref type="figure" target="#fig_3">Figure 6</ref>, respectively. "#" denotes "the number of". <ref type="figure" target="#fig_3">Figure 6a</ref> illustrates the distribution of the length of the news life cycle, where we define the life cycle of a piece of news as the period from its publication date to the date of its last received click. We observe that about 90% of news are clicked within two days, which proves that online news is extremely time-sensitive and are substituted by newer ones with high frequency. <ref type="figure" target="#fig_3">Figure 6b</ref> illustrates the distribution of the number of clicked pieces of news for a user. 77.9% of users clicked no more than five pieces of news, which demonstrates the data sparsity in the news recommendation scenario. <ref type="figure" target="#fig_3">Figures 6c and 6d</ref> illustrate the distributions of the number of words (without stop words) and entities in a news title, respectively. The average number per title is 7.9 for words and 3.7 for entities, showing that there is one entity in almost every two words in news titles on average. The high density of the occurrence of entities also empirically justifies the design of KCNN. <ref type="figure" target="#fig_3">Figures 6e and 6f</ref> present the distribution of occurrence times of an entity in the news dataset and the distribution of the number of contextual entities of an entity in extracted knowledge graph, respectively. We can conclude from the two figures that the occurrence pattern of entities in online news is sparse and has a long tail (80.4% of entities occur no more than ten times), but entities generally have abundant contexts in the knowledge graph: the average number of context entities per entity is 42.5 and the maximum is 140, 737. Therefore, contextual entities can greatly enrich the representations for a single entity in news recommendation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baselines</head><p>We use the following state-of-the-art methods as baselines in our experiments:</p><p>? LibFM [35] is a state-of-the-art feature-based factorization model and widely used in CTR scenarios. In this paper, the input feature of each piece of news for LibFM is comprised of two parts: TF-IDF features and averaged entity embeddings. We concatenate the feature of a user and candidate news to feed into LibFM.   features and averaged entity embeddings as input to feed both channels.</p><p>? DeepFM <ref type="bibr" target="#b13">[13]</ref> is also a general deep model for recommendation, which combines a component of factorization machines and a component of deep neural networks that share the input. We use the same input as in LibFM for DeepFM. ? YouTubeNet <ref type="bibr" target="#b8">[8]</ref> is proposed to recommend videos from a large-scale candidate set in YouTube using a deep candidate generation network and a deep ranking network. In this paper, we adapt the deep raking network to the news recommendation scenario. ? DMF [50] is a deep matrix factorization model for recommender systems which uses multiple non-linear layers to process raw rating vectors of users and items. We ignore the content of news and take the implicit feedback as input for DMF.</p><p>Note that except for LibFM, other baselines are all based on deep neural networks since we aim to compare our approach with state-of-the-art deep learning models. Additionally, except for DMF which is based on collaborative filtering, other baselines are all content-based or hybrid methods.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experiment Setup</head><p>We choose TransD <ref type="bibr" target="#b18">[18]</ref> to process the knowledge graph and learn entity embeddings, and use the non-linear transformation function in Eq. (15) in KCNN. The dimension of both word embeddings and entity embeddings are set as 100. The number of filters are set as 100 for each of the window sizes 1, 2, 3, 4. We use Adam <ref type="bibr" target="#b21">[21]</ref> to train DKN by optimizing the log loss. We will further study the variants of DKN and the sensitivity of key parameters in Sections 5.4 and 5.6, respectively. To compare DKN with baselines, we use F1 and AUC value as the evaluation metrics.</p><p>The key parameter settings for baselines are as follows. For KPCNN, the dimensions of word embeddings and entity embeddings are both set as 100. For DSSM, the dimension of semantic feature is set as 100. For DeepWide, the final representations for deep and wide components are both set as 100. For YouTubeNet, the dimension of final layer is set as 100. For LibFM and DeepFM, the dimensionality of the factorization machine is set as {1, 1, 0}. For DMF, the dimension of latent representation for users and items is set as 100. The above settings are for fair consideration. Other parameters in the baselines are set as default. Each experiment is repeated five times, and we report the average and maximum deviation as results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results</head><p>In this subsection, we present the results of comparison of different models and the comparison among variants of DKN.  <ref type="table" target="#tab_5">Table 2</ref>. For each baseline in which the input contains entity embedding, we also remove the entity embedding from input to see how its performance changes (denoted by "(-)"). Additionally, we list the improvements of baselines compared with DKN in brackets and calculate the p-value of statistical significance by t-test. Several observations stand out from <ref type="table" target="#tab_5">Table 2</ref>:</p><p>? The usage of entity embedding could boost the performance of most baselines. For example, the AUC of KPCNN, DeepWide, and YouTubeNet increases by 1.1%, 1.8% and 1.1%, respectively. However, the improvement for DeepFM is less obvious. We try different parameter settings for DeepFM and find that if the AUC drops to about 0.6, the improvement brought by the usage of knowledge could be up to 0.5%. The results show that FM-based method cannot take advantage of entity embedding stably in news recommendation. ? DMF performs worst among all methods. This is because DMF is a CF-based method, but news is generally highly timesensitive with a short life cycle. The result proves our aforementioned claim that CF methods cannot work well in the news recommendation scenario. ? Except for DMF, other deep-learning-based baselines outperform LibFM by 2.0% to 5.2% on F1 and by 1.5% to 4.5% on AUC, which suggests that deep models are effective in capturing the non-linear relations and dependencies in news data. ? The architecture of DeepWide and YouTubeNet is similar in the news recommendation scenario, thus we can observe comparable performance of the two methods. DSSM outperforms DeepWide and YouTubeNet, the reason for which might be that DSSM models raw texts directly with word hashing. ? KPCNN performs best in all baselines. This is because KPCNN uses CNN to process input texts and can better extract the specific local patterns in sentences. ? Finally, compared with KPCNN, DKN can still have a 1.7% AUC increase. We attribute the superiority of DKN to its two properties: 1) DKN uses word-entity-aligned KCNN for sentence representation learning, which could better preserve the relatedness between words and entities; 2) DKN uses an attention network to treat users' click history discriminatively, which better captures users' diverse reading interests.  <ref type="figure" target="#fig_4">Figure 7</ref> presents the AUC score of DKN and baselines for additional ten test days. We can observe that the curve of DKN is consistently above baselines over ten days, which strongly proves the competitiveness of DKN. Moreover, the performance of DKN is also with low variance compared with baselines, which suggests that DKN is also robust and stable in practical application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Comparison among DKN variants.</head><p>Further, we compare among the variants of DKN with respect to the following four aspects to demonstrate the efficacy of the design of the DKN framework: the usage of knowledge, the choice of knowledge graph embedding method, the choice of transformation function, and the usage of an attention network. The results are shown in <ref type="table" target="#tab_6">Table 3</ref>, from which we can conclude that:</p><p>? The usage of entity embedding and contextual embedding can improve AUC by 1.3% and 0.7%, respectively, and we can achieve even better performance by combining them together. This finding confirms the efficacy of using a knowledge graph in the DKN model. ? DKN+TransD outperforms other combinations. This is probably because, as presented in Section 2.1, TransD is the most complicated model among the four embedding methods, which is able to better capture non-linear relationships among the knowledge graph for news recommendation. ? DKN with mapping is better than DKN without mapping, and the non-linear function is superior to the linear one. The results prove that the transformation function can alleviate the heterogeneity between word and entity spaces by self learning, and the non-linear function can achieve better performance. ? The attention network brings a 1.7% gain on F1 and 0.9% gain on AUC for the DKN model. We will give a more intuitive demonstration on the attention network in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Case Study</head><p>To intuitively demonstrate the efficacy of the usage of the knowledge graph as well as the the attention network, we randomly sample a user and extract all his logs from the training set and the test set (training logs with label 0 are omitted for simplicity). As shown in <ref type="table" target="#tab_7">Table 4</ref>, the clicked news clearly exhibits his points of interest: No. 1-3 concern cars and No. 4-6 concern politics (categories are not contained in the original dataset but manually tagged by us).</p><p>We use the whole training data to train DKN with full features and DKN without entity nor context embedding, then feed each possible pair of training logs and test logs of this user to the two trained models and obtain the output value of their attention networks.</p><p>The results are visualized in <ref type="figure" target="#fig_5">Figure 8</ref>, in which the darker shade of blue indicates larger attention values. From <ref type="figure" target="#fig_5">Figure 8a</ref> we observe that, the first title in test logs gets high attention values with "Cars" in the training logs since they share the same word "Tesla", but the results for the second title are less satisfactory, since the second title shares no explicit word-similarity with any title in the training set, including No. 1-3. The case is similar for the third title in test logs. In contrast, in <ref type="figure" target="#fig_5">Figure 8b</ref> we see that the attention network precisely captures the relatedness within the two categories "Cars" and "Politics". This is because in the knowledge graph, "General Motors" and "Ford Inc." share a large amount of context with "Tesla Inc." and "Elon Musk", moreover, "Jeh Johnson" and "Russian" are also highly connected to "Donald Trump". The difference in the response of the attention network also affects the final predicted results: DKN with knowledge graph <ref type="figure" target="#fig_5">(Figure 8b)</ref> accurately predicts all the test logs, while DKN without knowledge graph <ref type="figure" target="#fig_5">(Figure 8a</ref>) fails on the third one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Parameter Sensitivity</head><p>DKN involves a number of hyper-parameters. In this subsection, we examine how different choices of hyper-parameters affect the performance of DKN. In the following experiments, expect for the parameter being tested, all other parameters are set as introduced in Section 5.3.</p><p>5.6.1 Dimension of word embedding d and dimension of entity embedding k. We first investigate how the dimension of word embedding d and dimension of entity embedding k affect performance by testing all combinations of d and k in set {20, 50, 100, 200}. The results are shown in <ref type="figure" target="#fig_6">Figure 9a</ref>, from which we can observe that, given dimension of entity embedding k, performance initially improves with the increase of dimension of word embedding d. This is because more bits in word embedding can encode more useful information of word semantics. However, the performance drops when d further increases, as a too large d (e.g., d = 200) may introduce noises which mislead the subsequent prediction. The case is similar for k when d is given.  5.6.2 Window sizes of filters and the number of filters m. We further investigate the choice of windows sizes of filters and the number of filters for KCNN in the DKN model. As shown in <ref type="figure" target="#fig_6">Figure  9b</ref>, given windows sizes, the AUC score generally increases as the number of filters m gets larger, since more filters are able to capture more local patterns in input sentences and enhance model capability. However, the trend changes when m is too large (m = 200) due to probable overfitting. Likewise, we can observe similar rules for window sizes given m: a small window size cannot capture longdistance patterns in sentences, while a too large window size may easily suffer from overfitting the noisy patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK 6.1 News Recommendation</head><p>News recommendation has previously been widely studied. Nonpersonalized news recommendation aims to model relatedness among news <ref type="bibr" target="#b29">[29]</ref> or learn human editors' demonstration <ref type="bibr" target="#b47">[47]</ref>. In personalized news recommendation, CF-based methods <ref type="bibr" target="#b41">[41]</ref> often suffer from the cold-start problem since news items are substituted frequently. Therefore, a large amount of content-based or hybrid methods have been proposed <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b39">39]</ref>. For example, <ref type="bibr" target="#b34">[34]</ref> proposes a Bayesian method for predicting users' current news interests based on their click behavior, and <ref type="bibr" target="#b39">[39]</ref> proposes an explicit localized sentiment analysis method for location-based news recommendation. Recently, researchers have also tried to combine other features into news recommendation, for example, contextualbandit <ref type="bibr" target="#b25">[25]</ref>, topic models <ref type="bibr" target="#b28">[28]</ref>, and recurrent neural networks <ref type="bibr" target="#b32">[32]</ref>. The major difference between prior work and ours is that we use a knowledge graph to extract latent knowledge-level connections among news for better exploration in news recommendation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Knowledge Graph</head><p>Knowledge graph representation aims to learn a low-dimensional vector for each entity and relation in the knowledge graph, while preserving the original graph structure. In addition to translationbased methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b48">48]</ref> used in DKN, researchers have also proposed many other models such as Structured Embedding <ref type="bibr" target="#b4">[5]</ref>, Latent Factor Model <ref type="bibr" target="#b17">[17]</ref>, Neural Tensor Network <ref type="bibr" target="#b37">[37]</ref> and GraphGAN <ref type="bibr" target="#b42">[42]</ref>. Recently, the knowledge graph has also been used in many applications, such as movie recommendation <ref type="bibr" target="#b52">[52]</ref>, top-N recommendation <ref type="bibr" target="#b33">[33]</ref>, machine reading <ref type="bibr" target="#b51">[51]</ref>, text classification <ref type="bibr" target="#b46">[46]</ref>  embedding <ref type="bibr" target="#b49">[49]</ref>, and question answering <ref type="bibr" target="#b10">[10]</ref>. To the best of our knowledge, this paper is the first work that proposes leveraging knowledge graph embedding in news recommendation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Deep Recommender Systems</head><p>Recently, deep learning has been revolutionizing recommender systems and achieves better performance in many recommendation scenarios. Roughly speaking, deep recommender systems can be classified into two categories: using deep neural networks to process the raw features of users or items, or using deep neural networks to model the interaction among users and items. In addition to the aforementioned DSSM <ref type="bibr" target="#b16">[16]</ref>, DeepWide <ref type="bibr" target="#b5">[6]</ref>, DeepFM <ref type="bibr" target="#b13">[13]</ref>, YouTubeNet <ref type="bibr" target="#b8">[8]</ref> and DMF <ref type="bibr" target="#b50">[50]</ref>, other popular deep-learning-based recommender systems include Collaborative Deep Learning <ref type="bibr" target="#b44">[44]</ref>, SHINE <ref type="bibr" target="#b45">[45]</ref>, Multi-view Deep Learning <ref type="bibr" target="#b11">[11]</ref>, and Neural Collaborative Filtering <ref type="bibr" target="#b14">[14]</ref>. The major difference between these methods and ours is that DKN specializes in news recommendation and could achieve better performance than other generic deep recommender systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>In this paper, we propose DKN, a deep knowledge-aware network that takes advantage of knowledge graph representation in news recommendation. DKN addresses three major challenges in news recommendation: 1) Different from ID-based methods such as collaborative filtering, DKN is a content-based deep model for clickthrough rate prediction that are suitable for highly time-sensitive news. 2) To make use of knowledge entities and common sense in news content, we design a KCNN module in DKN to jointly learn from semantic-level and knowledge-level representations of news. The multiple channels and alignment of words and entities enable KCNN to combine information from heterogeneous sources and maintain the correspondence of different embeddings for each word. 3) To model the different impacts of a user's diverse historical interests on current candidate news, DKN uses an attention module to dynamically calculate a user's aggregated historical representation. We conduct extensive experiments on a dataset from Bing News. The results demonstrate the significant superiority of DKN compared with strong baselines, as well as the efficacy of the usage of knowledge entity embedding and the attention module.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 :</head><label>4</label><figDesc>Donald Trump: (0.32, 0.48) Las Vegas: (0.71, -0.49) Apple Inc.: (-0.48, -0.41) CEO: (-0.57, 0.06) Tim Cook: (-0.61, -0.59) iPhone 8: (-0.46, -0.75) Illustration of knowledge distillation process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Illustration of context of an entity in a knowledge graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>20 # 4 #</head><label>204</label><figDesc>words in a news title Proportion of news title (c) Distribution of the number of words in a news title entities in a news title Proportion of news title (d) Distribution of the number of entities in a news title of an entity Proportion of entities (f) Distribution of the number of contextual entities of an entity in the knowledge graph</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Illustration of statistical distributions in news dataset and extracted knowledge graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>AUC score of DKN and baselines over ten days (Sep. 01-10, 2017).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Attention visualization for training logs and test logs for a randomly sampled user.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>AUC score w.r.t dimension of entity embedding k and dimension of word embedding d AUC score w.r.t window sizes of filters and the number of filters m Parameter sensitivity of DKN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Basic statistics of the news dataset and the extracted knowledge graph.</figDesc><table><row><cell># users</cell><cell>141,487</cell><cell># triples</cell><cell>7,145,776</cell></row><row><cell># news</cell><cell>535,145</cell><cell>avg. # words per title</cell><cell>7.9</cell></row><row><cell># logs</cell><cell>1,025,192</cell><cell>avg. # entities per title</cell><cell>3.7</cell></row><row><cell># entities # relations</cell><cell>336,350 4,668</cell><cell>avg. # contextual entities per entity</cell><cell>42.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>?</head><label></label><figDesc>KPCNN [46]  attaches the contained entities to the word sequence of a news title and uses Kim CNN to learn representations of news, as introduced in Section 4.3.? DSSM<ref type="bibr" target="#b16">[16]</ref> is a deep structured semantic model for document ranking using word hashing and multiple fully-connected layers. In this paper, the user's clicked news is treated as the query and the candidate news are treated as the documents.</figDesc><table /><note>? DeepWide [6] is a general deep model for recommendation, combining a (wide) linear channel with a (deep) non-linear channel. Similar to LibFM, we use the concatenated TF-IDF</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Comparison of different models.</figDesc><table><row><cell>Models  *</cell><cell>F1</cell><cell>AUC</cell><cell>p-value  *  *</cell></row><row><cell>DKN</cell><cell>68.9 ? 1.5</cell><cell>65.9 ? 1.2</cell><cell>?</cell></row><row><cell>LibFM</cell><cell cols="2">61.8 ? 2.1 (-10.3%) 59.7 ? 1.8 (-9.4%)</cell><cell>&lt; 10 ?3</cell></row><row><cell>LibFM(-)</cell><cell cols="2">61.1 ? 1.9 (-11.3%) 58.9 ? 1.7 (-10.6%)</cell><cell>&lt; 10 ?3</cell></row><row><cell>KPCNN</cell><cell>67.0 ? 1.6 (-2.8%)</cell><cell>64.2 ? 1.4 (-2.6%)</cell><cell>0.098</cell></row><row><cell>KPCNN(-)</cell><cell>65.8 ? 1.4 (-4.5%)</cell><cell>63.1 ? 1.5 (-4.2%)</cell><cell>0.036</cell></row><row><cell>DSSM</cell><cell>66.7 ? 1.8 (-3.2%)</cell><cell>63.6 ? 2.0 (-3.5%)</cell><cell>0.063</cell></row><row><cell>DSSM(-)</cell><cell>66.1 ? 1.6 (-4.1%)</cell><cell>63.2 ? 1.8 (-4.1%)</cell><cell>0.045</cell></row><row><cell>DeepWide</cell><cell>66.0 ?1.2 (-4.2%)</cell><cell>63.3 ? 1.5 (-3.9%)</cell><cell>0.039</cell></row><row><cell>DeepWide(-)</cell><cell>63.7 ? 0.9 (-7.5%)</cell><cell>61.5 ? 1.1 (-6.7%)</cell><cell>0.004</cell></row><row><cell>DeepFM</cell><cell>63.8 ? 1.5 (-7.4%)</cell><cell>61.2 ? 2.3 (-7.1%)</cell><cell>0.014</cell></row><row><cell>DeepFM(-)</cell><cell>64.0 ? 1.9 (-7.1%)</cell><cell>61.1 ? 1.8 (-7.3%)</cell><cell>0.007</cell></row><row><cell>YouTubeNet</cell><cell>65.5 ? 1.2 (-4.9%)</cell><cell>63.0 ? 1.4 (-4.4%)</cell><cell>0.025</cell></row><row><cell cols="2">YouTubeNet(-) 65.1 ? 0.7 (-5.5%)</cell><cell>62.1 ? 1.3 (-5.8%)</cell><cell>0.011</cell></row><row><cell>DMF</cell><cell cols="2">57.2 ? 1.2 (-17.0%) 55.3 ? 1.0 (-16.1%)</cell><cell>&lt; 10 ?3</cell></row><row><cell cols="2">* "(-)" denotes "without input of entity embeddings".</cell><cell></cell><cell></cell></row></table><note>** p-value is the probability of no significant difference with DKN on AUC by t-test.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Comparison among DKN variants.</figDesc><table><row><cell>Variants</cell><cell>F1</cell><cell>AUC</cell></row><row><cell>DKN with entity and context emd.</cell><cell cols="2">68.8 ? 1.4 65.7 ? 1.1</cell></row><row><cell>DKN with entity emd. only</cell><cell>67.2 ? 1.2</cell><cell>64.8 ? 1.0</cell></row><row><cell>DKN with context emd. only</cell><cell>66.5 ? 1.5</cell><cell>64.2 ? 1.3</cell></row><row><cell cols="2">DKN without entity nor context emd. 66.1 ?1.4</cell><cell>63.5 ? 1.1</cell></row><row><cell>DKN + TransE</cell><cell>67.6 ? 1.6</cell><cell>65.0 ? 1.3</cell></row><row><cell>DKN + TransH</cell><cell>67.3 ? 1.3</cell><cell>64.7 ? 1.2</cell></row><row><cell>DKN + TransR</cell><cell>67.9 ? 1.5</cell><cell>65.1 ? 1.5</cell></row><row><cell>DKN + TransD</cell><cell cols="2">68.8 ? 1.3 65.8 ? 1.4</cell></row><row><cell>DKN with non-linear mapping</cell><cell cols="2">69.0 ? 1.7 66.1 ? 1.4</cell></row><row><cell>DKN with linear mapping</cell><cell>67.1 ? 1.5</cell><cell>64.9 ? 1.3</cell></row><row><cell>DKN without mapping</cell><cell>66.7 ? 1.6</cell><cell>63.7 ? 1.6</cell></row><row><cell>DKN with attention</cell><cell cols="2">68.7 ? 1.3 65.7 ? 1.2</cell></row><row><cell>DKN without attention</cell><cell>67.0 ? 1.0</cell><cell>64.8 ? 0.8</cell></row><row><cell cols="3">5.4.1 Comparison of different models. The results of compari-</cell></row><row><cell>son of different models are shown in</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Illustration of training and test logs for a randomly sampled user (training logs with label 0 are omitted).</figDesc><table><row><cell></cell><cell>No.</cell><cell>Date</cell><cell>News title</cell><cell>Entities</cell><cell>Label Category</cell></row><row><cell></cell><cell>1</cell><cell cols="2">12/25/2016 Elon Musk teases huge upgrades for Tesla's supercharger network</cell><cell>Elon Musk; Tesla Inc.</cell><cell>1</cell><cell>Cars</cell></row><row><cell></cell><cell>2</cell><cell cols="2">03/25/2017 Elon Musk offers Tesla Model 3 sneak peek</cell><cell>Elon Musk; Tesla Model 3</cell><cell>1</cell><cell>Cars</cell></row><row><cell></cell><cell>3</cell><cell cols="2">12/14/2016 Google fumbles while Tesla sprints toward a driverless future</cell><cell>Google Inc.; Tesla Inc.</cell><cell>1</cell><cell>Cars</cell></row><row><cell>training</cell><cell>4 5 6</cell><cell cols="3">12/15/2016 Trump pledges aid to Silicon Valley during tech meeting 03/26/2017 Donald Trump is a big reason why the GOP kept the Montana House seat Donald Trump; GOP; Montana Donald Trump; Silicon Valley 05/03/2017 North Korea threat: Kim could use nuclear weapons as "blackmail" North Korea; Kim Jong-un</cell><cell>1 1 1</cell><cell>Politics Politics Politics</cell></row><row><cell></cell><cell>7</cell><cell cols="2">12/22/2016 Microsoft sells out of unlocked Lumia 950 and Lumia 950 XL in the US</cell><cell>Microsoft; Lumia; United States</cell><cell>1</cell><cell>Other</cell></row><row><cell></cell><cell>8</cell><cell cols="2">12/08/2017 6.5 magnitude earthquake recorded off the coast of California</cell><cell>earthquake; California</cell><cell>1</cell><cell>Other</cell></row><row><cell></cell><cell></cell><cell>......</cell><cell></cell><cell></cell></row><row><cell></cell><cell>1</cell><cell>07/08/2017 Tesla makes its first Model 3</cell><cell></cell><cell>Tesla Inc; Tesla Model 3</cell><cell>1</cell><cell>Cars</cell></row><row><cell>test</cell><cell>2 3</cell><cell cols="3">08/13/2017 General Motors is ramping up its self-driving car: Ford should be nervous General Motors; Ford Inc. 06/21/2017 Jeh Johnson testifies on Russian interference in 2016 election Jeh Johnson; Russian</cell><cell>1 1</cell><cell>Cars Politics</cell></row><row><cell></cell><cell>4</cell><cell cols="2">07/16/2017 "Game of Thrones" season 7 premiere: how you can watch</cell><cell>Game of Thrones</cell><cell>0</cell><cell>Other</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://rtw.ml.cmu.edu/rtw/ 4 http://wiki.dbpedia.org/ 5 https://www.google.com/intl/bn/insidesearch/features/search/knowledge.html arXiv:1801.08284v2 [stat.ML] 30 Jan 2018</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Regression-based latent factor models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bee-Chung</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="19" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Content driven user profiling for comment-worthy recommendations of news and blog articles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trapit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrinal</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiranjib</forename><surname>Bhattacharyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th ACM Conference on Recommender Systems</title>
		<meeting>the 9th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning Structured Embeddings of Knowledge Bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heng-Tze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremiah</forename><surname>Koc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrishi</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glen</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ispir</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Wide &amp; deep learning for recommender systems</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Deep Learning for Recommender Systems</title>
		<meeting>the 1st Workshop on Deep Learning for Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="7" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo?c</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01781</idno>
		<title level="m">Very deep convolutional networks for natural language processing</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep neural networks for youtube recommendations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Sargin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Conference on Recommender Systems</title>
		<meeting>the 10th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="191" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Jointly modeling aspects, ratings and sentiments for movie recommendation (jmars)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiming</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<editor>KDD. ACM</editor>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="193" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Question Answering over Freebase with Multi-Column Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A multi-view deep learning approach for cross domain user modeling in recommendation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Mamdouh Elkahky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee</title>
		<meeting>the 24th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="278" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">User preference learning with multiple information fusion for restaurant recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 SIAM International Conference on Data Mining. SIAM</title>
		<meeting>the 2014 SIAM International Conference on Data Mining. SIAM</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">DeepFM: A Factorization-Machine based Neural Network for CTR Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 26th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW. International World Wide Web Conferences Steering Committee</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Sentiment analysis with deeply learned distributed representations of variable length texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM. ACM</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2333" to="2338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A latent factor model for highly multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodolphe</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nicolas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><forename type="middle">R</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3167" to="3175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Knowledge Graph Embedding via Dynamic Mapping Matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="687" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.2188</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Content-Based News Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Kompan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M?ria</forename><surname>Bielikov?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EC-Web</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="61" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recurrent Convolutional Neural Networks for Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">333</biblScope>
			<biblScope unit="page" from="2267" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A contextualbandit approach to personalized news article recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on World wide web</title>
		<meeting>the 19th international conference on World wide web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="661" to="670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning Entity and Relation Embeddings for Knowledge Graph Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Personalized news recommendation based on click behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elin</forename><forename type="middle">R?nby</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th international conference on Intelligent user interfaces</title>
		<meeting>the 15th international conference on Intelligent user interfaces</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="31" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Using topic models in contentbased news recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapio</forename><surname>Luostarinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oskar</forename><surname>Kohonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th Nordic Conference of Computational Linguistics</title>
		<meeting>the 19th Nordic Conference of Computational Linguistics</meeting>
		<imprint>
			<publisher>Link?ping University Electronic Press</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="239" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to model relatedness for news recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhua</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesup</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranam</forename><surname>Kolari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohui</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on World wide web</title>
		<meeting>the 20th international conference on World wide web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to link with wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Milne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM. ACM</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="509" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Embedding-based News Recommendation for Millions of Users</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumpei</forename><surname>Okura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukihiro</forename><surname>Tagami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shingo</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Tajima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD. ACM</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1933" to="1942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">entity2rec: Learning User-Item Relatedness from Knowledge Graphs for Top-N Item Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Palumbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Rizzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rapha?l</forename><surname>Troncy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Using twitter to recommend real-time topical news</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Phelan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Smyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the third ACM conference on Recommender systems</title>
		<meeting>the third ACM conference on Recommender systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="385" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Factorization machines with libfm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steffen Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">57</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Re-ranking for joint named-entity recognition and linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avirup</forename><surname>Sil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Conference on information &amp; knowledge management</title>
		<meeting>the 22nd ACM international conference on Conference on information &amp; knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2369" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 conference on empirical methods in natural language processing</title>
		<meeting>the 2013 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A location-based news article recommendation with explicit localized semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeong-Woo</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Bae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 36th international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="293" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00075</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Collaborative topic modeling for recommending scientific articles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 17th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">GraphGAN: Graph Representation Learning with Generative Adversarial Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyi</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Joint-Topic-Semantic-aware Social Recommendation for Online Voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiannong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyi</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM International Conference on Conference on Information and Knowledge Management</title>
		<meeting>the 26th ACM International Conference on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="347" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Collaborative deep learning for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1235" to="1244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Shine: Signed heterogeneous information network embedding for sentiment link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Combining Knowledge with Deep Convolutional Neural Networks for Short Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence</title>
		<meeting>the International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Dynamic Attention Deep Model for Article Recommendation by Learning Human Editors&apos; Demonstration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuejian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lantao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanyu</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<editor>KDD. ACM</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Knowledge Graph Embedding by Translating on Hyperplanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1112" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Rc-net: A general framework for incorporating knowledge into word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management</title>
		<meeting>the 23rd ACM International Conference on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1219" to="1228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep Matrix Factorization Models for Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin-Yu</forename><surname>Hong-Jian Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 26th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Leveraging knowledge bases in lstms for improving machine reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Collaborative knowledge base embedding for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><forename type="middle">Jing</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Defu</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD. ACM</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="353" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guorui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengru</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingya</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqi</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06978</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Deep Interest Network for Click-Through Rate Prediction. arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
