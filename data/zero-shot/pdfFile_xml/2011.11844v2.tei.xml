<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Densely connected multidilated convolutional networks for dense prediction tasks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoya</forename><surname>Takahashi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sony Corporation</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Mitsufuji</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sony Corporation</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Densely connected multidilated convolutional networks for dense prediction tasks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Tasks that involve high-resolution dense prediction require a modeling of both local and global patterns in a large input field. Although the local and global structures often depend on each other and their simultaneous modeling is important, many convolutional neural network (CNN)based approaches interchange representations in different resolutions only a few times. In this paper, we claim the importance of a dense simultaneous modeling of multiresolution representation and propose a novel CNN architecture called densely connected multidilated DenseNet (D3Net). D3Net involves a novel multidilated convolution that has different dilation factors in a single layer to model different resolutions simultaneously. By combining the multidilated convolution with the DenseNet architecture, D3Net incorporates multiresolution learning with an exponentially growing receptive field in almost all layers, while avoiding the aliasing problem that occurs when we naively incorporate the dilated convolution in DenseNet. Experiments on the image semantic segmentation task using Cityscapes and the audio source separation task using MUSDB18 show that the proposed method has superior performance over stateof-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Dense prediction tasks such as semantic segmentation and audio source separation typically accept highdimensional input data and produce predictions with the same (or similar) dimensions. To efficiently handle highdimensional data and model the context that lies in a large field, various neural network architectures have been proposed <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b43">44]</ref>. In particular, convolutional neural networks (CNNs) have become an essential component, and a variety of advanced CNN architectures have been proposed to improve performance on the basis of motivations such as making the networks deeper while improving a gradient flow <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18]</ref>, multibranch convolution <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b38">39]</ref> and explicitly modeling interchannel dependences of convolutional features <ref type="bibr" target="#b13">[14]</ref>. One key component of these architectures is a skip connection that creates short paths from early layers to later layers. In <ref type="bibr" target="#b15">[16]</ref>, a simple yet powerful skip connectivity pattern that connects all preceding layers, called DenseNet, is proposed. Such dense connectivity allows maximum information flow, making CNNs deeper while keeping the model size small by efficiently reusing intermediate representations of preceding layers.</p><p>One of the benefits of a deeper CNN is its larger receptive field that allows a large context to be modeled, which is important for tasks that require the utilization of a widearea or long-term dependence in a high-resolution input. For example, sufficiently large parts of objects have to be modeled for semantic segmentation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b56">57]</ref>, whereas modeling a long-term dependence is shown to be important for various audio tasks such as audio event recognition and source separation <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b40">41]</ref>. Although the receptive field grows linearly with respect to the number of layers stacked, the simple stacking of convolution layers is not the optimal way to increase it, as too many layers are required to cover a sufficiently large input, which makes the network training difficult. A popular approach to incorporate a large context with a reasonable model size is to repeatedly downsample intermediate network outputs and apply operations in lower resolution representations. In dense prediction tasks, the low-resolution representations are again upsampled to recover the resolution lost while carrying over the global perspective from downsampled layers <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b40">41]</ref>. Another approach is dilated convolution, where dilation factors are set to grow exponentially as layers are stacked; and therefore, the networks cover a large receptive field with a small number of layers. Dilated convolution is shown to be effective for many tasks that require high-resolution dense predictions <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b45">46]</ref>. Most previously proposed CNN architectures interchange information in different resolutions only a few times, e.g., once or a few times at the end of the network <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56]</ref>, or once at the beginning or end of each module <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b46">47]</ref>. However, since the local and global patterns can depend on each other, i.e., a local structure can be more accurately estimated by knowing a global structure and vice versa, a more frequent (dense) interchange of information among representations in multiple resolutions could be beneficial.</p><p>In this work, we propose a novel CNN architecture for arXiv:2011.11844v2 [cs.CV] 9 Jun 2021</p><formula xml:id="formula_0">(b) Multi-dilated convolution d=1 d=2 d=4</formula><p>(a) Dilated dense block <ref type="figure">Figure 1</ref>. Illustration of D2 block. (a) The connectivity pattern is the same as that in DenseNet except that the D2 block involves the multidilated convolution. (b) Illustration of the multidilated convolution at the third layer. The production of a single feature map involves multiple dilation factors depending on the input channel. For clarity, we omit the normalization and nonlinearity from the illustration. densely incorporating representations in multiple resolutions. We combine advantages of the dense skip connections and dilated convolution, and propose a novel network architecture called the multidilated dense block (D2 block). To appropriately combine them, we propose a multidilated convolution layer that has multiple dilation factors within a single layer. A dilation factor depends on which skip connection the feature maps come from, as shown in <ref type="figure">Fig.1</ref>. Multidilated convolution can prevent the occurrence of aliasing that occurs when a standard dilated convolution is applied to feature maps whose receptive field is smaller than the dilation factor. Furthermore, we propose a nested architecture of multidilated dense blocks to effectively repeat dilation factors multiple times with dense connections that ensure sufficient depth, which is required for modeling each resolution. We call the nested architecture densely connected multidilated DenseNet (D3Net) <ref type="bibr" target="#b0">1</ref> .</p><p>Although neural network architecture search (NAS) has been actively investigated to automatically find a suitable network architecture <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b25">26]</ref>, it is often difficult to identify the key element for achieving good performance from the learnt architecture. We believe that this work provides another insight into the design of CNN architectures for dense prediction tasks, namely, the frequent interchange of information in multiple resolutions.</p><p>The contributions of this work are as follows: (i) We claim the importance of the dense multiresolution representation learning and propose the D2 block that combines dense skip connections with dilated convolution. The D2 block incorporates a novel multidilated convolution that enables multiresolution information interchange in most of the layers while avoiding the aliasing problem that occurs in a naive way of incorporating dilation in DenseNet. (ii) We further introduce a nested architecture of multidilated dense blocks called the D3 block to effectively apply different dilation factors multiple times to provide a sufficient modeling capacity in each resolution. (iii) We conduct intensive experiments on two dense prediction tasks in different domains (image semantic segmentation and audio source separation) and show the effectiveness of the proposed methods. The proposed architecture exhibits superior performance over state-of-the-art baselines in both tasks, demonstrating its generality against the task type and data domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>The motivation of our work is to combine the advantages of dense skip connectivity and dilated convolution to enable multiresolution modeling with an exponentially growing receptive field while appropriately avoiding the aliasing problem. Here, we review related works on these aspects.</p><p>Dense skip connection Dense skip connections from early layers promote the reuse of feature maps, efficient parameter usage, and gradient information flow. DenseNet has the most dense connectivity pattern (i.e., all layers with same feature-map size are connected to each other) and shows excellent performance in image classification tasks <ref type="bibr" target="#b15">[16]</ref>. Larsson et al. proposed another simple connectivity pattern called FractalNet, in which layers are connected in fractal manner <ref type="bibr" target="#b17">[18]</ref>. Dual path networks combine DenseNet and ResNet to enjoy the advantage of the dense connectivity with the concatenation of feature maps and residual blocks, which involve the addition of feature maps <ref type="bibr" target="#b4">[5]</ref>.</p><p>Large receptive field The importance of a large receptive field was addressed in many tasks that involve highdimensional data including image super-resolution <ref type="bibr" target="#b33">[34]</ref>, semantic segmentation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b49">50]</ref>, and audio source separation <ref type="bibr" target="#b40">[41]</ref>. The theoretical receptive field size of CNNs does not directly represent the context size that CNNs use. Zhou et al. showed that the empirical receptive field of CNNs is much smaller than the theoretical one, especially in deeper layers <ref type="bibr" target="#b57">[58]</ref>. Therefore, network architectures that efficiently incorporate context information in a large field attract great interest and many approaches have been proposed including the incorporation of the dilated convolution <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b2">3]</ref>, the aggregating of downsized feature maps <ref type="bibr" target="#b54">[55]</ref>, and (a) (b) (c) <ref type="figure">Figure 2</ref>. Strategies for multiscale representation integration. The yellow box indicates a composition of convolution layers, which operates in a single resolution. The green box depicts a layer that integrates feature maps from different resolutions. (a) Feature maps in multiple scales are integrated at the end <ref type="bibr" target="#b23">[24]</ref>. (b) Feature maps in the lower scale are sequentially recover a higher scale by integrating the feature maps from the higher scale in the early layer <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b26">27]</ref>. (c) Features in different resolutions are first processed in parallel and integrated at the end of each stage <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b46">47]</ref>. the use of the attention mechanism <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b53">54]</ref>.</p><p>Dilated convolution and aliasing Aliasing is a wellknown effect in signal processing, in which the signal over the Nyquist frequency becomes indistinguishable with lower frequency after (sub-)sampling. The aliasing causes artifacts such as the Moir? pattern in the image domain or audible noise in the audio domain. Therefore, a low-pass filter for anti-aliasing is typically applied before sampling to remove the signal with a frequency higher than the Nyquist frequency. The effect of pooling-based subsampling in CNN-based speech recognition was studied and a performance drop caused by aliasing was observed <ref type="bibr" target="#b11">[12]</ref>. The dilated convolution involves the subsampling of input feature maps and can cause aliasing <ref type="bibr" target="#b47">[48]</ref>. To avoid this problem, most CNN architectures that involve dilated convolution are carefully designed to allow earlier layers to learn appropriate anti-aliasing filter if necessary, i.e., standard convolutions are applied before dilated convolutions with fixed dilation factor <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b18">19]</ref>, or the dilation factors is gradually increased as the layer goes deeper <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b45">46]</ref>. A naive combination of DenseNet with dilation has already been proposed <ref type="bibr" target="#b9">[10]</ref>, where dilated convolutions are used and the dilation factor was set to one at the initial layer and doubled as the layer goes deeper. However, this approach has significant aliasing due to skip connections, as discussed in Sec. 3.</p><p>Multiresolution modeling Fusing local and global information is important especially for dense prediction tasks, since both local and global structures have to be recovered. In the fully convolutional network (FCN) <ref type="bibr" target="#b23">[24]</ref>, feature maps in different resolutions from early layers are aggregated at the end of the network <ref type="figure">(Fig. 2(a)</ref>). Another common strategy used in, for instance, UNet <ref type="bibr" target="#b28">[29]</ref> and Hourglass <ref type="bibr" target="#b26">[27]</ref>, is the sequential upsampling of feature maps while combining the feature maps from early downsampling paths with skip connection, as shown in <ref type="figure">Fig. 2(b)</ref>, which aggregates multiresolution information at few concatenation points. HR-Net <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b46">47]</ref> involves another strategy for the aggregation of feature maps <ref type="figure">(Fig. 2(c)</ref>). It is composed of several stages: in each stage, feature maps in different resolutions are first processed by CNNs individually and then aggregated by matching the resolution with other resolutions with up-or downsampling at the end of each stage. In these approaches, feature maps in different resolutions are fused only a few times. Another stage-wise aggregation was proposed in <ref type="bibr" target="#b50">[51]</ref>, where the feature maps in different resolutions are aggregated iteratively and hierarchically. In contrast, our method fuses feature maps with multiple resolutions in almost all layers (except the first layer of D2 blocks and few other layers such as 1 ? 1 convolution layers). Multibranch convolution can also be considered as multiresolution modeling when the convolution in each branch operates in a different resolution. In <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b31">32]</ref>, dilated convolutions with different dilation factors are applied in parallel to the same feature maps and combined in a multibranch convolution module called the dilated inception (DI) or SDC layer. The set of dilation factors is the same for all modules. In <ref type="bibr" target="#b18">[19]</ref>, Poly-Scale convolution (PSConv) arranges multiple dilation factors periodically along with input channels. In contrast, the dilation factors in multidilated convolution depends on the skip connection as shown in <ref type="figure">Fig. 1</ref>, and their range grows exponentially as the layer goes deeper. Moreover, DI, SDC, and PSConv themselves cannot solve the aliasing problem and they require several layers before these modules to circumvent it. Therefore, they cannot fuse very local information in the first few layers, and multiresolution modeling can be performed only on the feature maps that are anti-aliased by the first several layers that possibly remove high-frequency components. In contrast, proposed D3 block can be directly applied to the input, which enables to fuse very local and global information. MS-DenseNet <ref type="bibr" target="#b14">[15]</ref> also involves a frequent two resolution fusion. However, the architecture is not suitable for dense prediction tasks as there is no information flow from low-to high-resolution feature maps. In Res2Net <ref type="bibr" target="#b10">[11]</ref>, multi-scale feature maps in a single resolution are aggregated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Multidilated convolution for DenseNet</head><p>In DenseNet, the outputs of the lth convolutional layer x l are computed using 3 ? 3 convolution filters w l and outputs  of all preceding layers as</p><formula xml:id="formula_1">x l = ?([x 0 , x 1 , ? ? ? , x l?1 ]) w l ,<label>(1)</label></formula><p>where ?() denotes the composite operation of batch normalization and ReLU nonlinearity, [x 0 , x 1 , ? ? ? , x l?1 ] the concatenation of feature maps from 0, ? ? ? , l ? 1 layers (x 0 is the input), and the convolution. x l&gt;0 has k feature maps and k is the growth rate. A naive way of incorporating dilated convolution is to replace the convolution with the dilated convolution d with the dilation factor d = 2 l?1 . However, this causes a severe aliasing problem; for instance, at the third layer, input is subsampled at four sample intervals without any anti-aliasing filtering because of the skip connections. Only the path that passes through all convolution operations without any skip connection covers the input field without omission, and all other paths from skip connections have blind spots in their receptive fields that inherently make it impossible for appropriate ant-aliasing filters to be learned in the preceding layers ( <ref type="figure" target="#fig_1">Fig. 3(a)</ref>). To overcome this problem, we propose the multidilated convolution m l defined as</p><formula xml:id="formula_2">Y l m l k l = l?1 i=0 y i di w i l ,<label>(2)</label></formula><p>where Y l = [y 0 , ? ? ? , y l?1 ] = ?([x 0 , ? ? ? , x l?1 ]) is the composite layer output, w i l the subset of filters that corresponds to the ith skip connection, and d i = 2 i . As depicted in <ref type="figure" target="#fig_1">Fig. 3(b)</ref>, DenseNet with the proposed multidilated convolution has different dilation factors depending on which layer the channel comes from. This allows the receptive field to cover the input field without the loss of coverage  between the samples to which the filters are to be applied and, hence, to learn proper filters to prevent aliasing. One advantage of the multidilated convolution is its capability to integrate information from the very local to global information of an exponentially large receptive field within a single layer. Combined with the dense skip connection topology, D2 blocks can perform multiresolution modeling in all layers (except the first layer). This fast information flow with dense skip connections and the dense (frequent) information interchange among representations in a wide range of resolutions provide a more flexible capability of modeling a relationship between local and global structures.</p><p>Note that the multidilation convolution is not equivalent to the multibranch convolution, where convolutions with different dilation factors are applied to the same input feature maps, similar to the Inception block <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b38">39]</ref>, as it again causes the aliasing problem when combined with the dense skip connection topology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">D3Net</head><p>Although the D2 block provides an exponentially large receptive field as the number of layers increases, it is also worthwhile to provide sufficient flexibility to transform feature maps in each resolution. In WaveNet <ref type="bibr" target="#b45">[46]</ref>, dilation factors are reset to one after several layers are stacked and repeated; that is, the dilation factor in the lth layer is given by d l = 2 l?1 mod M , where mod is the modulo operation and M is the number of layers at which the dilation factor is doubled. Inspired by this work, we propose a nested architecture of D2 blocks, as shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. D2 blocks are considered single composite layers and are densely connected in the same way as within the D2 block itself. With the M D2 blocks nested, the multidilated convolution operates at each resolution at least M times, providing a flexible modeling capability at each resolution. We refer to this nested architecture as the D3 block.</p><p>Inspired by the DenseNet-BC architecture <ref type="bibr" target="#b15">[16]</ref>, we also employ two channel-reduction mechanisms to mitigate the excessive increase in the number of channels and thus improve computational efficiency. First, we adopt bottleneck layers that reduce the number of input channels using 1 ? 1 convolution at the beginning of each D2 block. In our experiment, bottleneck layers were set to produce 4k feature maps, where k is the growth rate, and such layers are placed only when the input channel to the D2 block is greater than 4k. Second, we compress the output channels at the end of each D2 block by 1 ? 1 convolution to produce cm channels, where 0 &lt; c &lt; 1 is the compression rate and m is the number of channels before the compression. Alternatively, we can simply pass the outputs of the last N layers to the next D2 block. In our experiment, we used the former approach for semantic segmentation and the latter approach for audio source separation. Note that without the channel reduction layer, the D3 architecture is reduced to standard dense connections with repeated multidilation factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Implementation details</head><p>Our proposed D3 block can be integrated with CNN architectures commonly used in image classification (e.g., VGG <ref type="bibr" target="#b35">[36]</ref>, ResNet <ref type="bibr" target="#b12">[13]</ref>), image segmentation (e.g., FCN <ref type="bibr" target="#b23">[24]</ref> and deconvolution-based approaches <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b8">9]</ref>), and audio tasks <ref type="bibr" target="#b43">[44]</ref> by replacing the series of convolution layers in the same resolution with a D3 block. We call a CNN architecture that uses D3 blocks as D3Net. When D3Net involves downsampling between D3 blocks, we adopt a transition layer which is composed of a 1 ? 1 convolution layer followed by 2 ? 2 average pooling. In the transition layer, the number of output channels is compressed to half of the input channels, as performed in DenseNet <ref type="bibr" target="#b15">[16]</ref>. In summary, a D3 block is characterized with a set of parameters (M, L, k, B, c), where M denotes the number of D2 blocks in a D3 block ( <ref type="figure" target="#fig_2">Fig. 4)</ref>, L the number of layers in each D2 block, k the growth rate, B the number of bottleneck layer channels (which is set to 4k in our experiments), and c the compression rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>We evaluate the proposed method on two dense prediction tasks, namely, image semantic segmentation and audio source separation, to show the generality of the proposed approach against the task and data domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Semantic segmentation</head><p>The goal of semantic segmentation is to assign a class label to each pixel, as shown in <ref type="figure" target="#fig_4">Fig. 5</ref>. Since our contribution is the CNN architecture, we mainly focus on the evaluation of backbone networks. To this end, unless otherwise noted, all experiments including baselines are conducted under the same training/testing setup using the MM-Segmentation 2 framework.</p><p>Dataset. We use the Cityscapes dataset <ref type="bibr" target="#b5">[6]</ref>, which con-  tains 5,000 images collected from street scenes in 50 different cities with high quality pixel-level annotation. The images are divided into 2,975, 500, 1,525 for training, validation, and testing, respectively. We did not use coarsely annotated images. Following the evaluation protocol in <ref type="bibr" target="#b5">[6]</ref>, 19 categories are used for evaluation and we report the mean of class-wise intersection over union (mIoU).</p><p>Model architecture D3Net consists of two 3 ? 3 convolution layers followed by four D3 blocks with transition layers in between. Here, we refer to the downsample ratio as "scale"; therefore D3 blocks operate in four different scales. Outputs of D3 blocks in each scale are combined and passed to a decode head in the same way as in <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b46">47]</ref>, i.e., feature extraction layers formed by 1 ? 1 convolution are applied to the outputs of each D3 block to collect features from all scales, and the features in a lower scale are rescaled by bilinear upsampling to match the highest scale. Finally, another 1 ? 1 convolution is performed on the concatenation of the rescaled features to mix the information in four representations. We consider two D3Nets. The smaller architecture, denoted as D3Net-S, employs D3 blocks of (M, L, k, c) = (4, 8, 36, 0.2), while the larger architecture, D3Net-L, uses D3 blocks of (M, L, k, c) = (4, 10, 64, 0.2). The number of feature maps extracted from each scale using the feature extraction layers are (32, 40, 64, 128) for D3Net-S, and <ref type="bibr" target="#b31">(32,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr">96,</ref><ref type="bibr">192)</ref> for D3Net-L.</p><p>Training We follow the same training protocol as in <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b55">56]</ref>. The data augmentation of random horizontal flip, random cropping (from 1024 ? 2048 to 512 ? 1024), and random scaling in the range of [0.5, 2] are performed. The stochastic gradient descent with a momentum of 0.9 and a weight decay of 0.0005 is used for optimization. The "poly" learning rate policy with a base learning rate of 0.01 and a power of 0.9 is used for dropping the learning rate. All the <ref type="table">Table 1</ref>. Ablation study on Cityscapes val set. D-ResNet stands for Dilated-ResNet Backbone #param. mIoU D-ResNet-50 <ref type="bibr" target="#b12">[13]</ref> 49.5M 59.7 D-ResNet-101 <ref type="bibr" target="#b12">[13]</ref> 68.5M 62.4 HRNetV2-W18 <ref type="bibr" target="#b46">[47]</ref> 9.6M 62.7 HRNetV2-W48 <ref type="bibr" target="#b46">[47]</ref> 65 models are trained on the training set with a batch size of 8 and the synchronized batch normalization <ref type="bibr" target="#b52">[53]</ref>.</p><p>Ablation study First, we focus on the evaluation of the proposed multidilated convolution with dense connections (D2 block) and the nested architecture (D3 block). To this end, we consider four baselines. To highlight the effect of the multidilated convolution, we consider models with the same architecture as D3Net-S but replace the multidilated convolution with a standard convolution (without dilation) and a standard dilated convolution, whose dilation factors d are equal to the maximum dilation factor in the corresponding multidilated convolution layer in D3Net, e.g., d = (1, 2, 4, 8, 1, 2, 4, ? ? ? ). For the evaluation of the nested architecture, we consider a model that replaces the D3 block with a standard dense block (with BC layers) <ref type="bibr" target="#b15">[16]</ref>. For a fair comparison, we design the dense block to have a similar parameter size to D3Net-S by either keeping the growth rate and fitting the number of layers, or keeping the number of layers nearly the same and fitting the growth rate. This results in two DenseNet baselines, DenseNet-133 that has 16 layers for each Dense block with the growth rate of 36, and DenseNet-189 that has 23 layers for each dense block with the growth rate of 23 (the number after DenseNet-indicates the total number of layers). For reference, we also evaluate commonly used backbone networks. All networks are trained from scratch for 40,000 iterations. <ref type="table">Table 1</ref> shows the mIoU scores on the validation set. D3Net-S (with the proposed multidilated convolution) performs significantly better than D3Net-S without dilation and D3Net-S with the standard dilation, improving mIoU by 2.8 points. This highlights the effectiveness of the multidilated convolution in dense connections. Interestingly, D3Net-S with the standard dilation performs significantly worse than the model without dilation. This is probably due to the aliasing problem since a large dilation factor is applied directly to the initial feature map, as discussed in Sec. 3. D3Net-S without dilation outperforms DenseNet-133 by 4.7 points, where both models have the same growth rate and no dilation. This could be because the receptive field of DenseNet-133 covers the entire input only in the last few layers, which did not provide a sufficient capacity to model global information. On the other hand, D3Net-S without dilation still largely outperforms DensNet-189, which has almost the same number of layers as D3Net-S. This is probably due to followings: the growth rate in DenseNet-189 had to be a smaller to have the similar parameter size and the receptive field of DenseNet-189 is still smaller than D3Net-S without dilation as DenseNet involves more 1?1 convolutions, which does not increase the receptive field. These results highlight the efficiency of the proposed nested architecture, the D3 block. D3Nets-L exhibits the best performance among all baselines with a much smaller number of parameters than current state-of-the-art backbone networks, such as HRNetV2-W48. D3Net-S outperforms dilated ResNet101 with nearly a seven times smaller parameter size, showing the parameter efficiency of the proposed architecture.</p><p>Comparison with state-of-the-art approaches Next, we compare D3Net with state-of-the-art approaches in <ref type="table" target="#tab_2">Table  2</ref>. Again, our focus is on the evaluation of D3Net as a backbone, and we train all models in the same setup (expect methods denoted with ?) to eliminate the effect of hyperparameter difference that mainly comes from computational resources such as the batch size. We initialize all backbone networks with weights pretrained on ImageNet <ref type="bibr" target="#b29">[30]</ref> and trained 80K iterations. Among backbone networks in the FCN approach, D3Net-L shows superior performance over all baselines with a much smaller number of parameters than HRNetV2p-W48 <ref type="bibr" target="#b46">[47]</ref>, D-ResNet101, or D-ResNet50. By combining with the object-contextual representation (OCR) scheme <ref type="bibr" target="#b51">[52]</ref>, D3Net further improves the performance, obtaining the best result of 81.2% in this experiment. In <ref type="table">Table 3</ref>, we also show the results for the test set. All results are with six scales and flipping. For this experiment, we train D3Net-L for 160K iterations with a batch size of 12. All other settings are the same as those in previous experiments. Baseline results are from the original papers. The proposed method again outperforms all baselines that were trained on the train set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Audio source separation</head><p>To show the generality of the proposed method in a different domain, we conduct experiments on an audio source separation task, where the goal is to separate source signals from their mixture. Recently, CNN-based methods have been intensively studied <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b40">41]</ref>. In most methods, a time domain signal is transformed by shorttime Fourier transform (STFT) and source separation is performed in the magnitude STFT domain. In this case, the audio source separation problem is similar to an image segmentation problem, i.e., a model accepts two-dimensional magnitude STFT maps and predicts the source magnitude for each time-frequency bin (cf. pixels in an image), as shown in <ref type="figure" target="#fig_5">Fig. 6</ref>. However, there are three major differences. First, source separation is a regression problem rather than a pixel-wise classification problem, as the model is trained to estimate the source magnitude STFT. Second, when multiple sources are in the same time-frequency bin, they are summed in a complex STFT domain, unlike objects in an image, where a front object can hide an object at the back (occlusion). Since only magnitude is considered in complex STFT, the mixing behavior becomes more complex. Third, in the STFT domain, the translation invariant property is not globally satisfied along with the frequency axis, although local translation along with frequency and translation along the time axis are invariant.</p><p>Dataset We use the MUSDB18 dataset prepared for the SiSEC 2018 challenge <ref type="bibr" target="#b22">[23]</ref>. In this dataset, approximately 10 hours of professionally recorded 150 songs in the stereo format at 44.1kHz are available. For each song, a mixture and its four sources, bass, drums, other, and vocals, are provided; thus, the task is to separate the four sources from the mixture. We adopted the official split of 100 and 50 songs for the Dev and Test sets, respectively. STFT magnitude frames of the mixture, windowed at 4096 samples with 75% overlap, with data augmentation <ref type="bibr" target="#b44">[45]</ref> are used as inputs.  Training The four networks for each source instrument are trained to estimate the source spectrogram by minimizing the mean square error with the Adam optimizer for 50 epochs. The patch length is set to 256 frames; thus, the dimensions of input were 2?256?2049. The batch size is set to 6. The learning rate is initially set to 0.001 and annealed to 0.0001 at 40 epochs.</p><p>Model architecture Following <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b40">41]</ref>, in which the best results obtained in SiSEC 2018 were reported, we use the multiscale multiband architecture in which band-dedicated modules and a full band module, each with a bottleneck encoder-decoder architecture with skip connections, are placed. The network configuration is shown in <ref type="table" target="#tab_4">Table 5</ref>.</p><p>The network outputs are used to calculate the multichannel Wiener filter (MWF) to obtain the final separations, as commonly performed in frequency domain audio source separation methods <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b30">31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The signal-to-distortion ratios (SDRs) of the proposed method and existing state-of-the-art methods are shown in <ref type="table" target="#tab_3">Table 4</ref>. The SDRs are computed using the museval package <ref type="bibr" target="#b22">[23]</ref> and median SDRs are reported as in the SiSEC 2018 challenge <ref type="bibr" target="#b22">[23]</ref>. TAK1 <ref type="bibr" target="#b40">[41]</ref> and UHL2 <ref type="bibr" target="#b44">[45]</ref> are the two best performing methods in SiSEC 2018 (among submissions that do not use external data). The proposed D3Net exhibited the best performance for vocals, drums and accompaniment (the summation of drums, bass, and other) and performed comparably to the best method for other. The average SDR of four instruments is 6.01dB, which is significantly better than all baselines. To the best of our knowledge, this is the best result reported to date. The primaly difference between MMDenseLSTM (TAK1) and the proposed method is that MMDenseLSTM incorporates LSTM units to further expand the receptive field, whereas the proposed method uses the multidilated convolution and the nested architecture. A comparison of these methods indicates the effectiveness of the D3 block. On the other hand, GRU dilation 1 <ref type="bibr" target="#b21">[22]</ref> consists of dilated convolution and dilated GRU units without a down-up-sampling path. This also highlights the effectiveness of the dense multiresolution modeling of D3Net. For bass, approaches that operate in the time domain perform better, as they are capable of recovering the target phase, which is easier in the low frequency range. Among the frequency domain approaches, D3Net performs the best.</p><p>We also conduct an ablation study to validate the effectiveness of the multidilated convolution. By replacing the multidilated convolution with the standard convolution, we obtain comparable results to the best performing model in SiSEC2018, TAK1. When we replace the multidilated convolution with the standard dilated convolution, we obtain a decent improvement over D3Net without dilation even though the aliasing problem arises. However, the proposed multidilated convolution clearly outperforms the standard dilated convolution, showing the importance of handling the aliasing problem in order to incorporate dilation in DenseNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we showed the importance of a dense multiresolution representation learning in dense prediction tasks and proposed a novel CNN architecture called D3Net. A novel multidiated convolution is introduced to enable the dense multiresolution modeling by combining with a dense skip connection topology while avoiding the aliasing problem that occurs when a standard dilated convolution is applied. We further propose a nested architecture of the densely connected multidilated convolution block to improve the parameter efficiency and provide a sufficient capacity to learn representation in each resolution. Extensive experiments in image semantic segmentation and audio source separation tasks confirm the effectiveness and generality of the proposed method in different types of task and domain. D3Net outperforms state-of-the-art backbones on Cityscapes with a much smaller number of parameters. In audio source separation on MUSDB18, D3Net achieved state-of-the-art performance. We believe that this work provides an insight into another important property for designing CNNs: the frequency of interchanging local and global information in multiple resolutions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Visualization of receptive fields at the third layer of (a) naive integration of dilated convolution and (b) proposed multidilated convolution (in the case of one dimension). The filter size is 3. Red dots denote the points on which filters are applied, and the colored background shows the receptive field covered by the red dots. In (a), convolution kernels for skip connections have blind spots in their receptive fields, while the multidilated convolution (b) appropriately changes the dilation factor to avoid them.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>D3 block densely connects D2 blocks with repeated dilation patterns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>2 https://github.com/open-mmlab/mmsegmentation (a) Image (b) Ground truth (c) D3Net</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative examples of Cityscapes results on val set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Illustration of audio source separation in STFT domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Cityscapes val set results. No test-time augmentation (multiscale, flipping) is applied. ? denotes results reported in reference papers.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="2">#param. mIoU</cell></row><row><cell>DeepLabV3 [1]</cell><cell>D-ResNet-50</cell><cell>68.1M</cell><cell>79.3</cell></row><row><cell>DeepLabV3 [1]</cell><cell>D-ResNet-101</cell><cell>87.1M</cell><cell>80.2</cell></row><row><cell>DeepLabV3 [1]</cell><cell>ResNeSt-101 [54]</cell><cell>90.8M</cell><cell>79.7</cell></row><row><cell>DeepLabV3+ ? [4]</cell><cell>Xception-71</cell><cell>43.5M</cell><cell>79.6</cell></row><row><cell>PSPNet [55]</cell><cell>D-ResNet-101</cell><cell>68.0M</cell><cell>79.8</cell></row><row><cell>PSANet [56]</cell><cell>D-ResNet-101</cell><cell>78.1M</cell><cell>79.3</cell></row><row><cell>Auto-DeepLab-L ? [21]</cell><cell>-</cell><cell>44.4M</cell><cell>80.3</cell></row><row><cell>FCN</cell><cell>D-ResNet-50</cell><cell>49.5M</cell><cell>73.6</cell></row><row><cell>FCN</cell><cell>D-ResNet-101</cell><cell>68.5M</cell><cell>75.1</cell></row><row><cell>FCN</cell><cell>HRNetV2-W18 [47]</cell><cell>9.6M</cell><cell>78.7</cell></row><row><cell>FCN</cell><cell>HRNetV2-W48 [47]</cell><cell>65.9M</cell><cell>79.9</cell></row><row><cell>OCRNet</cell><cell>HRNetV2-W48 [47]</cell><cell>70.3M</cell><cell>80.7</cell></row><row><cell>FCN</cell><cell>D3Net-S</cell><cell>9.7M</cell><cell>79.5</cell></row><row><cell>FCN</cell><cell>D3Net-L</cell><cell>38.7M</cell><cell>80.6</cell></row><row><cell>OCRNet</cell><cell>D3Net-L</cell><cell>42.3M</cell><cell>81.2</cell></row><row><cell cols="4">Table 3. Results on Cityscapes test set. Baseline results are from</cell></row><row><cell cols="4">original papers. All models are trained on the train set without</cell></row><row><cell>using coarse data.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Backbone</cell><cell>mIoU</cell><cell></cell></row><row><cell>PSPNet [55]</cell><cell>D-ResNet-101</cell><cell>78.4</cell><cell></cell></row><row><cell>PSANet [56]</cell><cell>D-ResNet-101</cell><cell>78.6</cell><cell></cell></row><row><cell>PAN [20]</cell><cell>D-ResNet-101</cell><cell>78.6</cell><cell></cell></row><row><cell>AAF [17]</cell><cell>D-ResNet-101</cell><cell>79.1</cell><cell></cell></row><row><cell cols="2">HRNetV2 [47] HRNetV2-W48</cell><cell>80.4</cell><cell></cell></row><row><cell>D3Net (FCN)</cell><cell>D3Net-L</cell><cell>80.8</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>SDRs for MUSDB18 dataset. '*' denotes the method operating in the time domain.</figDesc><table><row><cell>SDR in dB</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Proposed architectures. All D3 blocks have 3?3 kernels with growth rate k, L layers, and M D2 blocks.</figDesc><table><row><cell>Layer</cell><cell>scale</cell><cell>low</cell><cell>Vocals, Other high</cell><cell>full</cell><cell>low</cell><cell>Drums high</cell><cell>full</cell><cell>low</cell><cell>Bass high</cell><cell>full</cell></row><row><cell>band split index</cell><cell></cell><cell>1-256</cell><cell>257-1600</cell><cell>-</cell><cell>1-128</cell><cell>128-1600</cell><cell>-</cell><cell>1-192</cell><cell>192-1600</cell><cell>-</cell></row><row><cell>conv (t?f,ch)</cell><cell>1</cell><cell>3?3, 32</cell><cell>3?3, 8</cell><cell>3?3, 32</cell><cell>3?3, 32</cell><cell>3?3, 8</cell><cell>3?3, 32</cell><cell>3?3, 32</cell><cell>3?3, 8</cell><cell>3?3, 32</cell></row><row><cell>D3 block 1 (k,L,M)</cell><cell></cell><cell>16, 5, 2</cell><cell>2, 1, 1</cell><cell>13, 4, 2</cell><cell>16, 5, 2</cell><cell>2, 1, 1</cell><cell>13, 4, 2</cell><cell>16, 5, 2</cell><cell>2, 1, 1</cell><cell>10, 4, 2</cell></row><row><cell>down sample</cell><cell>1</cell><cell></cell><cell cols="2">avg. pool 2 ? 2</cell><cell></cell><cell cols="2">avg. pool 2 ? 2</cell><cell></cell><cell cols="2">avg. pool 2 ? 2</cell></row><row><cell>D3 block 2 (k,L,M)</cell><cell>2</cell><cell>18, 5, 2</cell><cell>2, 1, 1</cell><cell>14, 5, 2</cell><cell>18, 5, 2</cell><cell>2, 1, 1</cell><cell>14, 5, 2</cell><cell>18, 5, 2</cell><cell>2, 1, 1</cell><cell>10, 5, 2</cell></row><row><cell>down sample</cell><cell>1</cell><cell></cell><cell cols="2">avg. pool 2 ? 2</cell><cell></cell><cell cols="2">avg. pool 2 ? 2</cell><cell></cell><cell cols="2">avg. pool 2 ? 2</cell></row><row><cell>D3 block 3 (k,L,M)</cell><cell>4</cell><cell>20, 5, 2</cell><cell>2, 1, 1</cell><cell>15, 6, 2</cell><cell>20, 5, 2</cell><cell>2, 1, 1</cell><cell>15, 6, 2</cell><cell>18, 5, 2</cell><cell>2, 1, 1</cell><cell>12, 6, 2</cell></row><row><cell>down sample</cell><cell>1</cell><cell></cell><cell cols="2">avg. pool 2 ? 2</cell><cell></cell><cell cols="2">avg. pool 2 ? 2</cell><cell></cell><cell cols="2">avg. pool 2 ? 2</cell></row><row><cell>D3 block 4 (k,L,M)</cell><cell>8</cell><cell>22, 5, 2</cell><cell>2, 1, 1</cell><cell>16, 7, 2</cell><cell>22, 4, 2</cell><cell>2, 1, 1</cell><cell>16, 7, 2</cell><cell>20, 5, 2</cell><cell>2, 1, 1</cell><cell>14, 7, 2</cell></row><row><cell>down sample</cell><cell>1</cell><cell></cell><cell cols="2">avg. pool 2 ? 2</cell><cell></cell><cell cols="2">avg. pool 2 ? 2</cell><cell></cell><cell cols="2">avg. pool 2 ? 2</cell></row><row><cell>D3 block 5 (k,L,M)</cell><cell>16</cell><cell>-</cell><cell>-</cell><cell>17, 8, 2</cell><cell>-</cell><cell>-</cell><cell>16, 8, 2</cell><cell>-</cell><cell>-</cell><cell>16, 8, 2</cell></row><row><cell>up sample</cell><cell></cell><cell></cell><cell>t.conv 2 ? 2</cell><cell></cell><cell></cell><cell>t.conv 2 ? 2</cell><cell></cell><cell></cell><cell>t.conv 2 ? 2</cell><cell></cell></row><row><cell>concat. D3 block 6 (k,L,M)</cell><cell>1 8</cell><cell>--</cell><cell>--</cell><cell>D3 block 4 16, 6, 2</cell><cell>--</cell><cell>--</cell><cell>D3 block 4 16, 6, 2</cell><cell>--</cell><cell>--</cell><cell>D3 block 4 14, 6, 2</cell></row><row><cell>up sample</cell><cell></cell><cell></cell><cell>t.conv 2 ? 2</cell><cell></cell><cell></cell><cell>t.conv 2 ? 2</cell><cell></cell><cell></cell><cell>t.conv 2 ? 2</cell><cell></cell></row><row><cell>concat. D3 block 7 (k,L,M)</cell><cell>1 4</cell><cell cols="9">D3 block 3 D3 block 3 D3 block 3 D3 block 3 D3 block 3 D3 block 3 D3 block 3 D3 block 3 D3 block 3 20, 4, 2 2, 1, 1 14, 5, 2 20, 4, 2 2, 1, 1 14, 6, 2 18, 4, 2 2, 1, 1 12, 6, 2</cell></row><row><cell>up sample</cell><cell></cell><cell></cell><cell>t.conv 2 ? 2</cell><cell></cell><cell></cell><cell>t.conv 2 ? 2</cell><cell></cell><cell></cell><cell>t.conv 2 ? 2</cell><cell></cell></row><row><cell>concat. D3 block 8 (k,L,M)</cell><cell>1 2</cell><cell cols="9">D3 block 2 D3 block 2 D3 block 2 D3 block 2 D3 block 2 D3 block 2 D3 block 2 D3 block 2 D3 block 2 18, 4, 2 2, 1, 1 12, 4, 2 18, 4, 2 2, 1, 1 12, 4, 2 16, 4, 2 2, 1, 1 8, 4, 2</cell></row><row><cell>up sample</cell><cell></cell><cell></cell><cell>t.conv 2 ? 2</cell><cell></cell><cell></cell><cell>t.conv 2 ? 2</cell><cell></cell><cell></cell><cell>t.conv 2 ? 2</cell><cell></cell></row><row><cell>concat.</cell><cell>1</cell><cell cols="9">D3 block 1 D3 block 1 D3 block 1 D3 block 1 D3 block 1 D3 block 1 D3 block 1 D3 block 1 D3 block 1</cell></row><row><cell>D3 block 9 (k,L,M)</cell><cell></cell><cell>16, 4, 2</cell><cell>2, 1, 1</cell><cell>11, 4, 2</cell><cell>16, 4, 2</cell><cell>2, 1, 1</cell><cell>11, 4, 2</cell><cell>16, 4, 2</cell><cell>2, 1, 1</cell><cell>8, 4, 2</cell></row><row><cell>concat. (axis)</cell><cell></cell><cell></cell><cell>freq</cell><cell>-</cell><cell></cell><cell>freq</cell><cell>-</cell><cell></cell><cell>freq</cell><cell>-</cell></row><row><cell>concat. (axis) d2 block (k,L)</cell><cell>1</cell><cell></cell><cell>channel 12, 3</cell><cell></cell><cell></cell><cell>channel 12, 3</cell><cell></cell><cell></cell><cell>channel 12, 3</cell><cell></cell></row><row><cell>gate conv (t?f,ch)</cell><cell></cell><cell></cell><cell>3 ? 3, 2</cell><cell></cell><cell></cell><cell>3 ? 3, 2</cell><cell></cell><cell></cell><cell>3 ? 3, 2</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code is available at https : / / github . com / sony / airesearch-code/tree/master/d3net</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno>abs/1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dual path networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Music source separation in the waveform domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>D?fossez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.13254</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Stacked deconvolutional network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changyong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Image Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recurrent dilated densenets for a time-series segmentation task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Priewald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><surname>Pernkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Machine Learning and Applications (ICMLA)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="75" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shang-Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin-Yu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
		<title level="m">Res2Net: A new multi-scale backbone architecture. Trans. Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Impact of aliasing on deep cnn-based end-to-end acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Poellabauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhua</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-scale dense networks for resource efficient image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danlu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adaptive affinity fields for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyh-Jing</forename><surname>Tsung-Wei Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fractalnet: Ultra-deep neural networks without residuals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Psconv: Squeezing feature pyramid into one compact poly-scale convolutional layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anbang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pyramid attention network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanchao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxue</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Autodeeplab: Hierarchical neural architecture search for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dilated convolution with dilated gru for music source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jen-</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conferences on Artificial Intelligence Organization (IJCAI)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The 2018 signal separation evaluation campaign</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-R</forename><surname>St?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc LVA/ICA</title>
		<meeting>LVA/ICA</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Voice separation with an unknown number of multiple speakers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliya</forename><surname>Nachmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fast neural architecture search of compact semantic segmentation models via auxiliary cells</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Nekrasov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MICCAI</title>
		<meeting>MICCAI</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Meta-learning extractors for music source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ganeshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Naradowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sdc -stacked dilated convolution: A unified descriptor network for dense matching tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rene</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wasenmuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Unger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didier</forename><surname>Stricker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Large receptive field networks for high-scale image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Seif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Androutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Large receptive field networks for high-scale image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Seif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Androutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="876" to="87609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Single image super-resolution with dilated convolution based multi-scale information learning inception module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wuzhen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debin</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Open-unmix -a reference implementation for music source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian-Robert</forename><surname>St?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonie</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Open Source Software</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Inceptionv4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">MMDenseLSTM: An efficient combination of convolutional and recurrent neural networks for audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoya</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nabarun</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IWAENC</title>
		<meeting>IWAENC</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">AENet: Learning deep audio features for video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoya</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="513" to="524" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep Convolutional Neural Networks and Data Augmentation for Acoustic Event Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoya</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beat</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multi-scale Multiband DenseNets for Audio Source Separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoya</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WAS-PAA</title>
		<meeting>WAS-PAA</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="261" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Improving Music Source Separation Based On Deep Networks Through Data Augmentation And Network Blending</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Porcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Giron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Enenkl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoya</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="261" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<title level="m">Wavenet: A generative model for raw audio</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Understanding convolution for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panqu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehua</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrison</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WACV</title>
		<meeting>WACV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A dilated inception network for visual saliency prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuping</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weisi</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Evan Shelhamer, and Trevor Darrell. Deep layer aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Objectcontextual representations for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Ambrish Tyagi, and Amit Agrawal. Context encoding for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristin</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongruo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<title level="m">Resnest: Splitattention networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Psanet: Point-wise spatial attention network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="267" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Squeeze-and-attention networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rene</forename><surname>Zhong Qiu Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Bidart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibrahim</forename><forename type="middle">Ben</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Daya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Object detectors emerge in deep scene cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
