<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gated Fusion Network for Single Image Dehazing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Ren</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Information Security (SKLOIS)</orgName>
								<orgName type="institution">IIE</orgName>
								<address>
									<region>CAS</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">City University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Information Security (SKLOIS)</orgName>
								<orgName type="institution">IIE</orgName>
								<address>
									<region>CAS</region>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">School of Cyber Security</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Merced</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Gated Fusion Network for Single Image Dehazing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose an efficient algorithm to directly restore a clear image from a hazy input. The proposed algorithm hinges on an end-to-end trainable neural network that consists of an encoder and a decoder. The encoder is exploited to capture the context of the derived input images, while the decoder is employed to estimate the contribution of each input to the final dehazed result using the learned representations attributed to the encoder. The constructed network adopts a novel fusion-based strategy which derives three inputs from an original hazy image by applying White Balance (WB), Contrast Enhancing (CE), and Gamma Correction (GC). We compute pixel-wise confidence maps based on the appearance differences between these different inputs to blend the information of the derived inputs and preserve the regions with pleasant visibility. The final dehazed image is yielded by gating the important features of the derived inputs. To train the network, we introduce a multi-scale approach such that the halo artifacts can be avoided. Extensive experimental results on both synthetic and real-world images demonstrate that the proposed algorithm performs favorably against the state-of-the-art algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The single image dehazing problem <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b44">45]</ref> aims to estimate the unknown clean image given a hazy or foggy image. This is a classical image processing problem, which has received active research efforts in the vision communities since various high-level scene understanding tasks <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b39">40]</ref> require the image dehazing to recover the clear scene. Early approaches focus on developing handcrafted features based on the statistics of clear images, such * Part of this work was done while Wenqi Ren was with Tencent AI Lab as a Visiting Scholar. ? Corresponding author. as dark channel prior <ref type="bibr" target="#b8">[9]</ref> and local max contrast <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b32">33]</ref>. To avoid hand-crafted priors, recent work <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b40">41]</ref> automatically learns haze relevant features by convolutional neural networks (CNNs). In the dehazing literature, the hazing process is usually modeled as,</p><formula xml:id="formula_0">I(x) = J(x)t(x) + A 1 ? t(x) ,<label>(1)</label></formula><p>where I(x) and J(x) are the observed hazy image and the haze-free scene radiance, A is the global atmospheric light, and t(x) is the scene transmission describing the portion of light that is not scattered and reaches the camera sensors. In practice, transmission and atmospheric light are unknown. Thus, most dehazing methods try to estimate the transmission t(x) and the atmospheric light A, given a hazy image. Estimating transmission from a hazy image is a severely ill-posed problem. Some approaches try to use visual cues to capture deterministic and statistical properties of hazy images <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b29">30]</ref>. However, these transmission approximations are inaccurate, especially in the cases of the scenes where the colors of objects are inherently similar to those of atmospheric lights. Note that such an erroneous transmission estimation directly affects the quality of the recovered image, resulting in undesired haze artifacts. Instead of using hand-crafted visual cues, CNN-based methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b27">28]</ref> are proposed to estimate the transmissions. However, these methods still follow the conventional dehazing methods in estimating atmospheric lights to recover clear images. Thus, if the transmissions are not estimated well, they will interfere the following atmospheric light estimation and thereby lead to low-quality results.</p><p>To address the above issues, we propose a novel endto-end trainable neural network that does not explicitly estimate the transmission and atmospheric light. Thus, the artifacts arising from transmission estimation errors can be avoided in the final restored results. The proposed neural network is built on a fusion strategy which aims to seamlessly blend several input images by preserving only the specific features of the composite output image.</p><p>There are two major factors in hazy images that need to be dealt with. The first one is the color cast introduced by the atmospheric light. The second one is the lack of visibility due to attenuation. Therefore, we tackle these two problems by deriving three inputs from the original image with the aim of recovering the visibility of the scene in at least one of them. The first input ensures a natural rendition <ref type="figure" target="#fig_0">(Figure 1(b)</ref>) of the output by eliminating chromatic casts caused by the atmospheric light. The second contrast enhanced input yields a better global visibility, but mainly in the thick hazy regions (e.g., the rear wall in <ref type="figure" target="#fig_0">Figure 1(c)</ref>). However, the contrast enhanced images are too dark in the light hazy regions. Hence, to recover the light hazy regions, we find that the gamma corrected images restore information of the light hazy regions well (e.g., the front lawn in <ref type="figure" target="#fig_0">Figure 1(d)</ref>). Consequently, the three derived inputs are gated by three confidence maps <ref type="figure" target="#fig_0">(Figure 1</ref>(f)-(g)), which aim to preserve the regions with good visibility.</p><p>The contributions of this work are three-fold. First, we propose a deep end-to-end trainable neutral network that restores clear images without assuming any restrictions on scene transmission and atmospheric light. Second, we demonstrate the utility and effectiveness of a gated fusion network for single image dehazing by leveraging the derived inputs from an original hazy image. Finally, we train the proposed model with a multi-scale approach to eliminate the halo artifacts that hurt image dehazing. We show that the proposed dehazing model performs favorably against the state-of-the-arts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>There mainly exist three kinds of methods for image dehazing: multi-image based methods, hand-crafted priors based methods, and data-driven methods.</p><p>Multi-image aggregation. Early methods often require multiple images to deal with the dehazing problem <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b35">36]</ref>. Kopf et al. <ref type="bibr" target="#b12">[13]</ref> used an approximated 3D model of the scene for dehazing. Different polarized filters were used in <ref type="bibr" target="#b35">[36]</ref> to capture multiple images of the same scene, and then degrees of polarization were used for haze removal. Narasimhan and Nayar <ref type="bibr" target="#b22">[23]</ref> also used the differences between multiple images for estimating the haze properties.</p><p>All these methods make the same assumption of using multiple images in the same scene. However, there only exists one image for a specific scene in most cases.</p><p>Hand-crafted priors based methods. Different image priors have been explored for single image dehazing in previous methods <ref type="bibr" target="#b15">[16]</ref>. Tan et al. <ref type="bibr" target="#b32">[33]</ref> enhanced the visibility of hazy images by maximizing the contrast. The dehazed results of this method often present color distortions since this method is not physically valid. He et al. <ref type="bibr" target="#b8">[9]</ref> presented a dark channel prior (DCP) for outdoor images, which asserts that the local minimum of the dark channel of a haze-free image is close to zero. The DCP has been shown effective for image dehazing, and a number of methods improve <ref type="bibr" target="#b8">[9]</ref> in terms of efficiency <ref type="bibr" target="#b34">[35]</ref> or quality <ref type="bibr" target="#b23">[24]</ref>. Fattal <ref type="bibr" target="#b6">[7]</ref> discovered that pixels of image patches typically exhibit a one-dimensional distribution, and used it to recover the scene transmission. However, this approach cannot guarantee a correct classification of patches. Recently, Berman et al. <ref type="bibr" target="#b2">[3]</ref> observed that colors of a haze-free image can be well approximated by a few hundred distinct colors, and then proposed a dehazing algorithm based on this prior.</p><p>Another line of research tries to make use of a fusion principle to restore hazy images in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref>. However, these methods need complex blending based on luminance, chromatic and saliency maps. In contrast, we introduce a gated fusion based single image dehazing technique that blends only the derived three input images.</p><p>All of the above approaches strongly rely on the accuracy of the assumed image priors, so may perform poorly when the assumed priors are insufficient to describe real-world images. As a result, these approaches tend to introduce undesirable artifacts such as color distortions.</p><p>Data-driven methods. Tang et al. <ref type="bibr" target="#b33">[34]</ref> combined four types of haze-relevant features with Random Forest to estimate the transmission. Zhu et al. <ref type="bibr" target="#b45">[46]</ref> created a linear model for modeling the scene depth of the hazy image under a color attenuation prior, and learned the parameters of the model in a supervised manner. However, these methods are still developed based on hand-crafted features.</p><p>Recently, CNNs have also been used for image recovering problems <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44]</ref>. Cai et al. <ref type="bibr" target="#b3">[4]</ref> proposed a DehazeNet and a BReLU layer to estimate the transmissions from hazy inputs. In <ref type="bibr" target="#b27">[28]</ref>, a coarse-scale network was first used to learn the mapping between hazy inputs and their transmissions, and then a fine-scale network was exploited to refine the transmission. One problem of these CNNs based methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref> is that all these methods require an accurate transmission and atmospheric light <ref type="figure">Figure 2</ref>. The coarsest level network of GFN. The network contains layers of symmetric encoder and decoder. To retrieve more contextual information, we use Dilation Convolution (DC) to enlarge the receptive field in the convolutional layers in the encoder block. Skip shortcuts are connected from the convolutional feature maps to the deconvolutional feature maps. Three enhanced versions are derived from the input hazy image. Then, these three inputs are weighted by the three confidence maps learned by our network, respectively. estimation step for restoring the clear image. Although the recent AOD-Net <ref type="bibr" target="#b13">[14]</ref> bypasses the estimation step, this method still needs to compute a newly introduced variable K(x) which integrates both transmission t(x) and atmospheric light A. Therefore, AOD-Net still falls into a physical model in <ref type="bibr" target="#b0">(1)</ref>.</p><p>Different from these CNNs based approaches, our proposed network is built on the principle of image fusion, and is learned to produce the sharp image directly without estimating transmission and atmospheric light. The main idea of image fusion is to combine several images into a single one, retaining only the most significant features. This idea has been successfully used in a number of applications such as image editing <ref type="bibr" target="#b24">[25]</ref> and video super-resolution <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Gated Fusion Network</head><p>This section presents the details of our gated fusion network that employs an original hazy image and three derived images as inputs. We refer to this network as Gated Fusion Network, or GFN, as shown in <ref type="figure">Figure 2</ref>. The central idea is to learn the confidence maps to combine several input images into a single one by keeping only the most significant features of them. Obviously, the choice of inputs and weights is application-dependent. By learning the confidence map for each input, we demonstrate that our fusion based method is able to dehaze images effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Derived Inputs</head><p>We derive several inputs based on the following observations. The first one is that the colors in hazy images often change due to the influence of the atmospheric light. The second one is the lack of visibility in distant regions due to scattering and attenuation phenomena. Based on these observations, we generate three inputs that recover the color and visibility of the entire image from the original hazy image. We first estimate the White Balanced (WB) image I wb of the hazy input I to recover the latent color of the scene. Then we extract visible information including the Contrast White balanced input. Our first input is a white balanced image which aims to eliminate chromatic casts caused by the atmospheric color. In the past decades, a number of white balancing approaches <ref type="bibr" target="#b10">[11]</ref> have been proposed. In this paper, we use the gray world assumption <ref type="bibr" target="#b25">[26]</ref> based technique. Despite its simplicity, this low-level approach has shown to yield comparable results to those of more complex white balance methods <ref type="bibr" target="#b16">[17]</ref>. The gray world assumption is that given an image with a sufficient quantity of color variations, the average value of the Red, Green and Blue components of the image should average out to a common gray value. This assumption is in generally valid in any given real-world scene since the variations in colors are random and independent. It would be safe to say that given a large number of samples, the average should tend to converge to the mean value, which is gray. White balancing algorithms can make use of this gray world assumption by forcing images to have a uniform average gray value for the R, G, and B channels. For example, if an image is shot under a hazy weather condition, the captured image will have an atmospheric light A cast over the entire image. The effect of this atmospheric light cast disturbs the gray world assumption of the original image. By imposing the assumption on the captured image, we would be able to remove the atmospheric light cast and re-acquire the colors of our original scene. <ref type="figure" target="#fig_1">Figure 3</ref>(b) demonstrates such an effect. Although white balancing could discard the color shifting caused by the atmospheric light, the results still present low contrast. To enhance the contrast, we introduce the following two derived inputs. Contrast enhanced input. Inspired by the previous dehazing approaches <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b4">[5]</ref>, our second input is a contrast enhanced image of the original hazy input. Ancuti and Ancuti <ref type="bibr" target="#b0">[1]</ref> derived a contrast enhanced image by subtracting the average luminance value? of the entire image I from the hazy input, and then using a factor ? to linearly increase the luminance in the recovered hazy regions as follows:</p><formula xml:id="formula_1">I ce = ? I ?? ,<label>(2)</label></formula><p>where ? = 2(0.5 +?). Although? is a good indicator of image brightness, there is a problem in this input, especially in denser haze regions. The main reason is that the negative values of (I ??) may dominate the contrast enhanced input as? increases. As shown in <ref type="figure" target="#fig_1">Figure 3</ref>(c), the dark image regions tend to be black after contrast enhancing. Gamma corrected input. To overcome the dark limitation in I ce , we create another type of contrast enhanced image using gamma correction:</p><formula xml:id="formula_2">I gc = ?I ? .<label>(3)</label></formula><p>Gamma correction is a nonlinear operation which is used to encode (? &lt; 1) and decode (? &gt; 1) luminance or tristimulus values in image content, In this paper, we use ? = 1 and a decoding gamma correction ? = 2.5. We find that using these parameters achieves satisfactory results, as shown in <ref type="figure" target="#fig_1">Figure 3(d)</ref>. The derived inputs by decoding gamma correction effectively remove the severe dark aspects of I ce and enhance the visibility of the original image I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network Architecture</head><p>We use an encoder-decoder network, which has been shown to produce good results for a number of generative tasks such as image denoising <ref type="bibr" target="#b19">[20]</ref>, image harmonization <ref type="bibr" target="#b36">[37]</ref>, time-lapse video generation <ref type="bibr" target="#b37">[38]</ref>. In particular, we choose a variation of the residual encoder-decoder network model for image dehazing. We use skip connections between encoder and decoder halves of the network, where features from the encoder side are concatenated to be fed to the decoder. This significantly accelerates the convergence <ref type="bibr" target="#b19">[20]</ref> and helps generate a much clear dehazed image. We perform an early fusion by concatenating the original hazy image and three derived inputs in the input layer. The network is of a multi-scale style in order to prevent halo artifacts, which will be discussed in more details in Section 3.3. We show a diagram of GFN in <ref type="figure">Figure 2</ref>. Note that we only show the coarsest level network of GFN in <ref type="figure">Figure 2</ref>. To leverage more context without losing local details, we use dilation network to enlarge the receptive field in the convolutional layers. Rectification layers are added after each convolutional or deconvolutional layer. The convolutional layers act as a feature extractor, which preserve the primary information of scene colors in the input layer, meanwhile eliminating the unimportant colors from the inputs. The deconvolutional layers are then combined to recover the weight maps of three derived inputs. In other words, the outputs of the deconvolutional layers are the confidence maps of the derived input images I wb , I ce and I gc .</p><p>We use 3 convolutional blocks and 3 deconvolutional blocks with stride 1 in each scale. Each layer is of the same type: 32 filters of the size 3 ? 3 ? 32 except the first and last layers. The first layer operates on the input image with kernel size 5 ? 5, and the last layer is used for confidence map reconstruction. In this work, we demonstrate that explicitly modeling confidence maps has several advantages. These are discussed later in Section 5.2. Once the confidence maps for the derived inputs are predicted, they are multiplied by the three derived inputs to give the final dehazed image in each scale:</p><formula xml:id="formula_3">J = C wb ? I wb + C ce ? I ce + C gc ? I gc ,<label>(4)</label></formula><p>where ? denotes element-wise multiplication, and C wb , C ce , and C gc are the confidence maps for gating I wb , I ce , and I gc , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">The multi-Scale Refinement</head><p>The network described in the previous subsection is subject to halo artifacts, particularly for strong transitions within the confidence maps <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref>. Hence, we perform es-timation by varying the image resolution in a coarse-to-fine manner to prevent halo artifacts. The multi-scale approach is motivated by the fact that the human visual system is sensitive to local changes (e.g., edges) over a wide range of scales. As a merit, the multi-scale approach provides a convenient way to incorporate local image details over varying resolutions. <ref type="figure" target="#fig_2">Figure 4</ref> shows the proposed multi-scale fusion network, in which the coarsest level network is shown in <ref type="figure">Figure 2</ref>. Finer level networks basically have the same structure as the coarsest network. However, the first convolutional layer takes the sharp image from a previous stage as well as its own hazy image and derived inputs, in a concatenated form. Each input size is twice the size of its coarser scale network. There is an up-sampling layer before the next stage. At the finest scale, the original high-resolution image is restored.</p><p>The multi-scale approach desires that each scale output is a clear image of the corresponding scale. Thus, we train our network so that all intermediate dehazed images should form a pyramid of the sharp image. The MSE criterion is applied to every level of the pyramid. In specific, given a collection of N training pairs I i and J i , where I i is a hazy image and J i is the clean version as the ground truth, the loss function at the k-th scale is defined as follows:</p><formula xml:id="formula_4">L cont (?, k) = 1 N N i=1 F(I i,k , ?, k) ? J i,k 2 ,<label>(5)</label></formula><p>where ? keeps the weights of the convolutional and deconvolutional kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Adversarial Loss</head><p>Recently, generative adversarial networks (GANs) are reported to generate sharp realistic images <ref type="bibr" target="#b21">[22]</ref>. Therefore, we follow the architecture introduced in <ref type="bibr" target="#b21">[22]</ref>, and build a discriminator to take the output of the finest scale or the ground-truth sharp image as input. The adversarial loss is defined as follows:</p><formula xml:id="formula_5">L adv = E J pclear(J) log D(J) + E I phazy(I) log 1 ? D(F(I)) ,<label>(6)</label></formula><p>where F is our multi-scale network in <ref type="figure" target="#fig_2">Figure 4</ref>, and D is the discriminator. Finally, by combining the multi-scale content loss and adversarial loss, our final loss function is</p><formula xml:id="formula_6">L total = L cont + 0.001L adv .<label>(7)</label></formula><p>Through optimizing the network parameters, we train the model in the combination of two losses, multi-scale content loss (5) and adversarial loss (6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>We quantitatively evaluate the proposed algorithm on both synthetic dataset and real-world hazy photographs, with comparisons to the state-of-the-art methods in terms of accuracy and visual effect. The implementation code can be found at our project website.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>In our network, patch size is set as 128 ? 128. We use ADAM <ref type="bibr" target="#b11">[12]</ref> optimizer with a batch size 10 for training. The initial learning rate is 0.0001 and we decrease the learning rate by 0.75 every 10,000 iterations. For all the results reported in the paper, we train the network for 240,000 iterations, which takes about 35 hours on an Nvidia K80 GPU. Default values of ? 1 and ? 2 are used, which are 0.9 and 0.999, respectively, and we set weight decay to 0.00001. Since our approach dehazes images in a single forward pass, it is computationally very efficient. Using a NVidia K80 GPU, we can process a 640 ? 480 image within 0.3s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training Data</head><p>Generating realistic training data is a major challenge for tasks where ground truth data cannot be easily collected. For training our neural network, we adopt the NYU2 dataset <ref type="bibr" target="#b30">[31]</ref> and the synthetic method in <ref type="bibr" target="#b27">[28]</ref> to synthesize the training data. We use 1400 clean images and the corresponding labeled depth maps from the NYU Depth dataset <ref type="bibr" target="#b30">[31]</ref> to construct the training set. Given a clear image J, a random atmospheric light A ? (0.8, 1.0) and the ground truth depth d, we use t(x) = e ??d(x) to synthesize transmission first, then generate hazy image using the physical model <ref type="bibr" target="#b0">(1)</ref>. For scattering coefficient ?, we randomly select it from 0.5 to 1.5 as suggested in <ref type="bibr" target="#b27">[28]</ref>. We use 7 different ? for each clean image, so that we can synthesize different haze concentration images for each input image. In addition, 1% Gaussian noise is added to each hazy input to increase the robustness of the trained network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Quantitative Evaluation on Synthetic Dataset</head><p>For quantitative evaluation, we use the remaining 49 clean images in the label data except the 1400 training images from the NYU2 dataset <ref type="bibr" target="#b30">[31]</ref> to synthetic hazy images with known depth map d as like in <ref type="bibr" target="#b27">[28]</ref>. We evaluate these methods by two criteria: Structure Similarity (SSIM) and Peak Signal to Noise Ratio (PSNR). In this section, we compare the proposed algorithm with the following seven methods on the synthesized datasets. Priors based methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b2">3]</ref>. We use three prior based methods for comparisons. The first one is the DCP proposed by He et al. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. This is a commonly used baseline approach in most dehazing papers. The second is Boundary Constrained Context Regularization (BCCR) proposed by Meng et al. <ref type="bibr" target="#b20">[21]</ref> and the third is the Non-local Image Dehazing (NLD) algorithm in <ref type="bibr" target="#b2">[3]</ref>. Learning based methods <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b13">14]</ref>. We also use four learning based methods for comparisons. The first  <ref type="figure">Figure 5</ref>. Dehazed results on the synthetic dataset. Dehazed results generated by the priors based methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b2">3]</ref> have some color distortions in some regions. The learning based methods <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b13">14]</ref> tend to underestimate haze concentration so that the dehazed results have some remaining hazes. In contrast, the dehazed results by our method are close to the ground-truth images. one learns a linear model based on Color Attenuation Prior (CAP). The second and third are CNNs based methods of DehazeNet <ref type="bibr" target="#b3">[4]</ref> and MSCNN <ref type="bibr" target="#b27">[28]</ref>. These methods implement image dehazing by learning the map between hazy inputs and their transmission based on convolutional neural networks. The last AOD-Net <ref type="bibr" target="#b13">[14]</ref> is also a CNNs based method, but integrates the transmission and atmospheric light into a new variable. <ref type="figure">Figure 5</ref> shows some dehazed images by different methods. Since we directly restore the final dehazed image without transmission estimation in our algorithm, we only compare the final dehazed results with other methods. The priors based image dehazing methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b2">3]</ref> overestimate the haze thickness, so the dehazed results tend to be darker than the ground truth images and contain color distortions in some regions, e.g., the desks in the second row and the wall in the last row in <ref type="figure">Figure 5</ref>(b)-(d). We note that the dehazed results by CAP <ref type="bibr" target="#b45">[46]</ref>, DehazeNet <ref type="bibr" target="#b3">[4]</ref>, MSCNN <ref type="bibr" target="#b27">[28]</ref> and AOD-Net <ref type="bibr" target="#b13">[14]</ref> methods are similar as shown in <ref type="figure">Figure 5(e)-(h)</ref>. Although the dehazed results by CAP, De-hazeNet, MSCNN and AOD-Net are closer to ground truth than the results by <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b2">3]</ref>, there are still some remaining haze as shown in <ref type="figure">Figure 5</ref> In contrast, the dehazed results generated by our approach in <ref type="figure">Figure 5</ref>(i) are close to the ground truth haze-free images in <ref type="figure">Figure 5(j)</ref>. Overall, the dehazed results by the proposed algorithm have higher visual quality and fewer color distortions. The qualitative results are also reflected by the quantitative PSNR and SSIM metrics in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>In addition, to further test the dehazing effect on different haze concentration, we use three scattering coefficient ? = 0.8, 1.0 and 1.2 to synthesize three haze concentration on the 49 testing images, respectively. As shown in <ref type="table" target="#tab_0">Table 1</ref>, our method without adversarial loss performs favorably against the state-of-the-art image dehazing methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b13">14]</ref> on all of these haze concentrations. However, if we use adversarial loss, the network can still recover better dehazed results than without adding adversarial loss in terms of SSIM in some cases. Although the SSIM values by <ref type="bibr" target="#b13">[14]</ref> are close to ours in some cases, the PSNR generated by our method are higher than <ref type="bibr" target="#b13">[14]</ref> by up to 2dB, especially for heavy haze concentration images. RESIDE dataset. Recently, a dehazing benchmark is proposed in <ref type="bibr" target="#b14">[15]</ref>, which is an extended version of our data in <ref type="table" target="#tab_0">Table 1</ref>. We further evaluate our method on the RESIDE dataset in <ref type="table" target="#tab_1">Table 2</ref>. As shown, our method performs favor-(a) Hazy inputs (b) DCP <ref type="bibr" target="#b9">[10]</ref> (c) BCCR <ref type="bibr" target="#b20">[21]</ref> (d) NLD <ref type="bibr" target="#b2">[3]</ref> (e) CAP <ref type="bibr" target="#b45">[46]</ref> (f) MSCNN <ref type="bibr" target="#b27">[28]</ref> (g) DehazeNet <ref type="bibr" target="#b3">[4]</ref> (h) AOD-Net <ref type="bibr" target="#b13">[14]</ref> (i) GFN <ref type="figure">Figure 6</ref>. Qualitative comparison of different methods on real-world images. Best viewed on high-resolution display. ably against other competitors <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b27">28]</ref> in this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation on Real Images</head><p>To further evaluate the proposed method, we use the real image dataset in Fattal <ref type="bibr" target="#b6">[7]</ref> and compare with different stateof-the-art methods. <ref type="figure">Figure 6</ref> shows the qualitative comparison of results with the seven state-of-the-art dehazing algorithms <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b13">14]</ref> on challenging real-world images. <ref type="figure">Figure 6(a)</ref> shows the hazy images to be dehazed. <ref type="figure">Figure 6</ref>(b)-(h) shows the results of DCP <ref type="bibr" target="#b8">[9]</ref>, BCCR <ref type="bibr" target="#b20">[21]</ref>, NLD <ref type="bibr" target="#b2">[3]</ref>, CAP <ref type="bibr" target="#b45">[46]</ref>, MSCNN <ref type="bibr" target="#b27">[28]</ref>, DehazeNet <ref type="bibr" target="#b3">[4]</ref> and AOD-Net <ref type="bibr" target="#b13">[14]</ref>, respectively. The results generated by the proposed algorithm are given in <ref type="figure">Figure 6</ref>(i). As shown in <ref type="figure">Figure 6</ref>(b)-(d), most of the haze is removed by DCP, BCCR and NLD methods, and the details of the scenes and objects are well restored. However, the results significantly suffer from over-enhancement (for instance, the sky region of the first and second images are much darker than it should be as shown in <ref type="figure">Figure 6</ref>(b)-(d), and there are some color distortions in the second and last images in <ref type="figure">Figure 6</ref>(c) and (d)). This is because these algorithms are based on handcrafted priors which have an inherent problem of overesti-mating the transmission as discussed in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b45">46]</ref>. The results of CAP <ref type="bibr" target="#b45">[46]</ref> do not have the over-estimation problem and maintain the original colors of the objects as shown in <ref type="figure">Figure</ref> 6(e). But have some remaining haze in the dehazed results. For example, the third image. The dehazed results by MSCNN <ref type="bibr" target="#b27">[28]</ref> and DehazeNet <ref type="bibr" target="#b3">[4]</ref> have a similar problem as <ref type="bibr" target="#b45">[46]</ref> tends to have some remaining haze. Especially the last image in <ref type="figure">Figure 6</ref>(f) and the first image in <ref type="figure">Figure 6</ref>(g). The method of AOD-Net <ref type="bibr" target="#b13">[14]</ref> generates relatively clear results, but the images in first three rows are still dark than ours, while the results in last two rows still have some remaining haze as shown in <ref type="figure">Figure 6</ref>(h). In contrast, the dehazed results by our method are clear and the details of the scenes are enhanced moderately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Analysis and Discussions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Effectiveness of Multi-Scale Network</head><p>In this section we analyze how the multi-scale network helps refine dehazed results. The recovered images from coarser-scale network provide additional information in the finer-scale net, which can greatly improve the final dehazed results. We show the dehazed results generated by only using the finest-scale and the proposed multi-scale networks in <ref type="figure">Figure 7</ref>. <ref type="figure">Figure 7</ref> shows that dehazed results and corresponding confidence maps. The first row is the dehazed results by only using the finest scale network and the second row is the results by the proposed multi-scale approach. As shown in the first row in <ref type="figure">Figure 7</ref>(a) and (c), there are obvious halo around the head of the person in the confidence maps, so the final dehazed result in the first row <ref type="figure">Figure 7(d)</ref> has the halo artifacts. In contrast, the dehazed results generated by the proposed multi-scale approach has a more clean edge as shown in the second row in <ref type="figure">Figure 7</ref>(d).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Effectiveness of Gating Strategy</head><p>Image fusion is a method to blend several images into a single one by retaining only the most useful features. To blend effectively the information of the derived inputs, we filter their important information by computing corresponding confidence maps. Consequently, in our gated fusion network, the derived inputs are gated by three pixel-wise confidence maps that aim to preserve the regions with good visibility. Our fusion network has two advantages: the first one is that it can reduce patch-based artifacts (e.g. dark channel prior <ref type="bibr" target="#b8">[9]</ref>) by single pixel operations, and the other one is that it can eliminate the influence caused by transmission and atmospheric light estimation.</p><p>To show the effectiveness of fusion network, we also train an end-to-end network without fusion process. This network has the same architecture as DFN except the input is hazy image and output is dehazed result without confidence maps learning. In addition, we also conduct a experiment based on equivalent fusion strategy, i.e., all the three derived inputs are weighted equally using 1/3. <ref type="figure">Figure 8</ref> shows visual comparisons of on two real-world examples with different settings. In these examples, the approach without gating generates very dark images in <ref type="figure">Figure 8(b)</ref>, and the method without fusion strategy generates results with color distortion and dark regions as shown in (a) Hazy input (b) DCP <ref type="bibr" target="#b8">[9]</ref> (b) DehazeNet <ref type="bibr" target="#b3">[4]</ref> (d) GFN <ref type="figure">Figure 9</ref>. A failure case for a thick foggy image. <ref type="figure">Figure 8(c)</ref>. In contrast, our results recover most scene details and maintain the original colors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Limitations</head><p>The proposed DFN performs well in general natural images. However, as the previous methods <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b3">4]</ref>, a limitation of our method is that the DFN cannot handle corrupted images with very large fog as shown in <ref type="figure">Figure 9</ref>. As heavy haze seriously interferes the atmospheric light (which is not a constant), the hazy model does not hold for such examples. <ref type="figure">Figure 9(d)</ref> shows an example where the proposed method does not generate a clear image. Future work will consider this problem with haze-free reference retrieval based on an effective deep neural network model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we addressed the single image dehazing problem via a multi-scale gated fusion network (GFN), a fusion based encoder-decoder architecture, by learning confidence maps for derived inputs. Compared with previous methods which impose restrictions on scene transmission and atmospheric light, our proposed GFN is easy to implement and reproduce since the proposed approach does not rely on the estimations of transmission and atmospheric light. In the approach, we first applied white balance method to recover the scene color, and then generated two contrast enhanced images for better visibility. Third, we carried out the GFN to estimate the confidence map for each derived input. Finally, we used the confidence maps and derived inputs to render the final dehazed result. The experimental results on synthetic and real-world images demonstrate the effectiveness of the proposed approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>(a) Hazy input (b) WB of (a) (c) CE of (a) (d) GC of (a) (e) Our result (f) Weight of (b) (g) Weight of (c) (h) Weight of (d) Image dehazing result. We exploit a gated fusion network for single image deblurring. (a) Hazy input. (b)-(d) are the derived inputs. (f)-(h) are learned confidence maps for (b), (c) and (d), respectively. (e) Our result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>We derive three enhanced versions from an input hazy image. These three derived inputs contain different important visual cues of the input hazy image. Enhanced (CE) image I ce and the Gamma Corrected (GC) image I gc to yield a better global visibility.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Multi-scale GFN structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(e)-(h).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .Figure 8 .</head><label>78</label><figDesc>(a) Maps of I wb (b) Maps of Ice (c) Maps of Igc (d) GFN Effectiveness of the proposed multi-scale approach. The first and second rows are the results by single and multi-scale networks, respectively. The zoomed-in regions are shown in the lefttop corner in each image. (a) Hazy inputs (b) Without gating (c) Without fusion (d) GFN Effectiveness of the gated fusion network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Average PSNR and SSIM values of dehazed results on the synthetic dataset.</figDesc><table><row><cell>PSNR/SSIM</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Average PSNR/SSIM of dehazed results on the SOTS dataset from RESIDE.</figDesc><table><row><cell>NLD [3]</cell><cell>MSCNN [28]</cell><cell>DehazeNet [4]</cell><cell>AOD-Net [14]</cell><cell>GFN</cell></row><row><cell>17.27/0.75</cell><cell>17.57/0.81</cell><cell>21.14/0.85</cell><cell>19.06/0.85</cell><cell>22.30/0.88</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments.</head><p>This work is supported in part by National Key Research and Development Plan </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Single image dehazing by multiscale fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">O</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ancuti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An edge-preserving filtering framework for visibility restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="384" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Non-local image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dehazenet: An end-to-end system for single image haze removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5187" to="5198" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Referenceless prediction of perceptual fog density and perceptual image defogging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Single image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dehazing using color-lines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards fog-free in-vehicle vision systems through contrast restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hauti?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Tarel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Aubert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Single image haze removal using dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Single image haze removal using dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Camera spectral sensitivity and white balance estimation from sky images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ikeuchi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">IJCV</biblScope>
			<biblScope unit="page" from="187" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep photo: Model-based photograph enhancement and viewing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Neubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Deussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Uyttendaele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<editor>SIG-GRAPH Asia</editor>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Aod-net: Allin-one dehazing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.04143</idno>
		<title level="m">Reside: A benchmark for single image dehazing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Nighttime haze removal with glow and multiple light colors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06235</idno>
		<title level="m">Haze visibility enhancement: A survey and quantitative benchmarking</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Robust video super-resolution with learned temporal dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cross-domain human parsing via adversarial feature and label adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Image restoration using very deep convolutional encoder-decoder networks with symmetric skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient image dehazing with boundary constraint and contextual regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep multi-scale convolutional neural network for dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Contrast restoration of weather degraded images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="713" to="724" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nishino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kratz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lombardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bayesian defogging. IJCV</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="263" to="278" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gangnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Poisson image editing. TOG</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="313" to="318" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Color transfer between images. Computer graphics and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Reinhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Adhikhmin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gooch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shirley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="34" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep video dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PCM</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Single image dehazing via multi-scale convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Semantic foggy scene understanding with synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07819</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Instant dehazing of images using polarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Schechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from RGBD images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Real-time video decolorization using bilateral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Visibility in bad weather from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Investigating haze-relevant features in a learning framework for image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fast visibility restoration from a single color or gray level image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Tarel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hautiere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Polarization: Beneficial for visibility enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Treibitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Schechner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00069</idno>
		<title level="m">Deep image harmonization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning to generate time-lapse videos using multi-stage dynamic generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to super-resolve blurry face and text images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Temporal dynamic graph lstm for action-driven video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Densely connected pyramid dehazing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Density-aware single image deraining using a multi-stream dense network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Image de-raining using a conditional generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.05957</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning fully convolutional networks for iterative non-blind deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">A retina inspired model for enhancing visibility of hazy images. Frontiers in computational neuroscience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A fast single image haze removal algorithm using color attenuation prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
