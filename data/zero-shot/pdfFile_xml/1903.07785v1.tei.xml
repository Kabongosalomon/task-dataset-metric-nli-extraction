<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cloze-driven Pretraining of Self-attention Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Menlo Park, Seattle</settlement>
									<region>CA, WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Menlo Park, Seattle</settlement>
									<region>CA, WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Menlo Park, Seattle</settlement>
									<region>CA, WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Menlo Park, Seattle</settlement>
									<region>CA, WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Menlo Park, Seattle</settlement>
									<region>CA, WA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cloze-driven Pretraining of Self-attention Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a new approach for pretraining a bi-directional transformer model that provides significant performance gains across a variety of language understanding problems. Our model solves a cloze-style word reconstruction task, where each word is ablated and must be predicted given the rest of the text. Experiments demonstrate large performance gains on GLUE and new state of the art results on NER as well as constituency parsing benchmarks, consistent with the concurrently introduced BERT model. We also present a detailed analysis of a number of factors that contribute to effective pretraining, including data domain and size, model capacity, and variations on the cloze objective.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Language model pretraining has recently been shown to provide significant performance gains for a range of challenging language understanding problems <ref type="bibr" target="#b6">(Dai and Le, 2015;</ref><ref type="bibr" target="#b23">Peters et al., 2018;</ref><ref type="bibr" target="#b26">Radford et al., 2018)</ref>. However, existing work has either used unidirectional (left-to-right) language models (LMs) <ref type="bibr" target="#b26">(Radford et al., 2018)</ref> or bi-directional (both left-to-right and right-to-left) LMs (BiLMs) where each direction is trained with an independent loss function <ref type="bibr" target="#b23">(Peters et al., 2018)</ref>. In this paper, we show that even larger performance gains are possible by jointly pretraining both directions of a large language-model-inspired self-attention cloze model.</p><p>Our bi-directional transformer architecture predicts every token in the training data ( <ref type="figure">Figure 1</ref>). We achieve this by introducing a cloze-style training objective where the model must predict the center word given left-to-right and right-to-left context representations. Our model separately computes both forward and backward states with * Equal contribution.  <ref type="figure">Figure 1</ref>: Illustration of the model. Block i is a standard transformer decoder block. Green blocks operate left to right by masking future time-steps and blue blocks operate right to left. At the top, states are combined with a standard multi-head self-attention module whose output is fed to a classifier that predicts the center token. a masked self-attention architecture, that closely resembles a language model. At the top of the network, the forward and backward states are combined to jointly predict the center word. This approach allows us to consider both contexts when predicting words and to incur loss for every word in the training set, if the model does not assign it high likelihood.</p><p>Experiments on the GLUE <ref type="bibr" target="#b32">(Wang et al., 2018</ref>) benchmark show strong gains over the state of the art for each task, including a 9.1 point gain on RTE over <ref type="bibr" target="#b26">Radford et al. (2018)</ref>. These improvements are consistent with, if slightly behind, those achieved by the concurrently developed BERT pretraining approach <ref type="bibr" target="#b7">(Devlin et al., 2018)</ref>, which we will discuss in more detail in the next section. We also show that it is possible to stack taskspecific architectures for NER and constituency parsing on top of our pretrained representations, and achieve new state-of-the-art performance lev-els for both tasks. We also present extensive experimental analysis to better understand these results, showing that (1) cross sentence pretraining is crucial for many tasks; (2) pre-training continues to improve performance with up to 18B tokens and would likely continue to improve with more data; and finally (3) our novel cloze-driven training regime is more effective than predicting left and right tokens separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>There has been much recent work on learning sentence-specific representations for language understanding tasks. <ref type="bibr" target="#b19">McCann et al. (2017)</ref> learn contextualized word representations from a sequence to sequence translation task and uses the representations from the encoder network to improve a variety of language understanding tasks. Subsequent work focused on language modeling pretraining which has been shown to be more effective and which does not require bilingual data <ref type="bibr" target="#b35">(Zhang and Bowman, 2018)</ref>.</p><p>Our work was inspired by ELMo <ref type="bibr" target="#b23">(Peters et al., 2018)</ref> and the generative pretraining (GPT) approach of <ref type="bibr" target="#b26">Radford et al. (2018)</ref>. ELMo introduces language models to pretrain word representations for downstream tasks including a novel mechanism to learn a combination of different layers in the language model that is most beneficial to the current task. GPT relies on a left to right language model and an added projection layer for each downstream task without a task-specific model. Our approach mostly follows GPT, though we show that our model also works well with an ELMo module on NER and constituency parsing.</p><p>The concurrently introduced BERT model ( <ref type="bibr" target="#b7">Devlin et al., 2018)</ref> is a transformer encoder model that captures left and right context. There is significant overlap between their work and ours but there are also significant differences: our model is a bi-directional transformer language model that predicts every single token in a sequence. BERT is also a transformer encoder that has access to the entire input which makes it bi-directional but this choice requires a special training regime. In particular, they multi-task between predicting a subset of masked input tokens, similar to a denoising autoencoder, and a next sentence prediction task. In comparison, we optimize a single loss function that requires the model to predict each token of an input sentence given all surrounding tokens. We use all tokens as training targets and therefore extract learning signal from every single token in the sentence and not just a subset.</p><p>BERT tailors pretraining to capture dependencies between sentences via a next sentence prediction task as well as by constructing training examples of sentence-pairs with input markers that distinguish between tokens of the two sentences. Our model is trained similarly to a classical language model since we do not adapt the training examples to resemble the end task data and we do not solve a denoising task during training.</p><p>Finally, BERT as well as <ref type="bibr" target="#b26">Radford et al. (2018)</ref> consider only a single data source to pretrain their models, either BooksCorpus <ref type="bibr" target="#b26">(Radford et al., 2018)</ref>, or BooksCorpus and additional Wikipedia data <ref type="bibr" target="#b7">(Devlin et al., 2018)</ref>, whereas our study ablates the effect of various amounts of training data as well as different data sources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Two tower model</head><p>Our cloze model represents a probability distribution p(t i |t 1 , . . . , t i?1 , t i+1 , . . . , t n ) for a sentence with n tokens t 1 , . . . , t n . There are two selfattentional towers each consisting of N stacked blocks: the forward tower operates left-to-right and the backward tower operates in the opposite direction. To predict a token, we combine the representations of the two towers, as described in more detail below, taking care that neither representation contains information about the current target token.</p><p>The forward tower computes the representation F l i for token i at layer l based on the forward representations of the previous layer F l?1 ?i via selfattention; the backward tower computes representation B l i based on information from the opposite direction B l?1 ?i . When examples of uneven length are batched, one of the towers may not have any context at the beginning. We deal with this issue by adding an extra zero state over which the selfattention mechanism can attend.</p><p>We pretrain on individual examples as they occur in the training corpora ( ?5.1). For News Crawl this is individual sentences while on Wikipedia, Bookcorpus, and Common Crawl examples are paragraph length. Sentences are prepended and appended with sample boundary markers &lt; s &gt;.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Block structure</head><p>The structure of the blocks follows most of the architectural choices described in <ref type="bibr" target="#b31">Vaswani et al. (2017)</ref>. Each block consists of two sub-blocks: the first is a multi-head self-attention module with H = 16 heads for which we mask out any subsequent time-steps, depending on if we are dealing with the forward or backward tower. The second sub-block is a feed-forward module (FFN) of the form <ref type="bibr" target="#b31">Vaswani et al. (2017)</ref> we apply layer normalization before the self-attention and FFN blocks instead of after, as we find it leads to more effective training. Sub-blocks are surrounded by a residual connection <ref type="bibr" target="#b13">(He et al., 2015)</ref>. Position is encoded via fixed sinusoidal position embeddings and we use a character CNN encoding of the input tokens for word-based models <ref type="bibr" target="#b15">(Kim et al., 2016)</ref>. Input embeddings are shared between the two towers.</p><formula xml:id="formula_0">ReLU (W 1 X + b 1 )W 2 + b 2 where W 1 ? R e?f , W 1 ? R f ?e . Different to</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Combination of representations</head><p>The forward and backward representations computed by the two towers are combined to predict the ablated word. To combine them we use a self-attention module which is followed by an FFN block ( ?3.1). The output of the FFN block is projected into V classes representing the types in the vocabulary. When the model predicts token i, the input to the attention module are forward states F L 1 . . . F L i?1 and backward states B L i+1 . . . B : n where n is the length of the sequence and L is the number of layers. We implement this by masking B L ?i and F L ?i . The attention query for token i is a combination of F L i?1 and B L i+1 . For the base model we sum the two representations and for the larger models they are concatenated. Keys and values are based on the forward and backward states fed to the attention module. In summary, this module has access to information about the entire input surrounding the current target token. During training, we predict every token in this way. The output of this module is fed to an output classifier which predicts the center token. We use an adaptive softmax for the output classifier <ref type="bibr" target="#b11">(Grave et al., 2017)</ref> for the word based models and regular softmax for the BPE based models.</p><p>While all states that contain information about the current target word are masked in the final selfattention block during training, we found it beneficial to disable this masking when fine tuning the Figure 2: Illustration of fine-tuning for a singlesentence task where the output of the first and last token is fed to a task-specific classifier (W). Masking for the final combination layer (comb) is removed which results in representations based on all forward and backward states (cf. <ref type="figure">Figure 1</ref>).</p><p>pretrained model for downstream tasks. This is especially true for tasks that label each token, such as NER, as this allows the model to access the full context including the token itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Fine-tuning</head><p>We use the following approach to fine-tune the pretrained two tower model to specific downstream tasks <ref type="figure">(Figure 2</ref>).</p><p>Classification and regression tasks. For single sentence classification tasks, we consider the language model outputs for the boundary tokens &lt; s &gt; which we add before the start and end of each sentence. The outputs are of dimension d = 1024 and we concatenate them to project to the number of classes C in the downstream task with W 1 ? R C?2d <ref type="bibr" target="#b26">(Radford et al., 2018)</ref>; we add a bias term b ? R C and initialize all weights as well as the bias to zero. The output of the projection is softmax-normalized and the model is optimized with cross-entropy for classification tasks. Regression tasks such as the Semantic Textual Similarity benchmark (STS-B; Cer et al., 2017) use C = 1 and are trained with mean squared error.</p><p>For tasks involving sentence-pairs, we concatenate them and add a new separator token &lt; sep &gt; between them. We add the output of this token to the final projection W 2 ? R C?3d .</p><p>Structured prediction tasks. For named entity recognition and parsing we use task-specific architectures which we fine-tune together with the language model but with different learning rate. The architectures are detailed in the respective results sections. The input to the architectures are the output representations of the pretrained language model.</p><p>No Masking. For fine-tuning, we found it beneficial to remove masking of the current token in the final layer that pools the output of the two towers. It is important to have access to information about the token to be classified for token level classification tasks such as NER but we also found this to perform better for sentence classification tasks. In practice, we completely disable masking in the combination layer so that it operates over all forward and backward states. However, disabling masking below the combination layer does not perform well.</p><p>Optimization. During fine-tuning we use larger learning rates for the new parameters, that is W 1 , W 2 , b or the task-specific architecture, compared to the pretrained model. For GLUE tasks, we do so by simply scaling the output of the language model before the W 1 and W 2 projections by a factor of 16. For structured prediction tasks, we explicitly use different learning rates for the pretrained model and the task-specific parameters. We fine tune with the Adam optimizer <ref type="bibr" target="#b16">(Kingma and Ba, 2015)</ref>. For GLUE tasks, we disable dropout in the language model and add 0.1 dropout between language model output and the final output projection; for structured prediction tasks, we use 0.3 at all levels (within the pretrained model, within the task-specific architecture, and on the weights connecting them). In all settings, we use a batch size of 16 examples. We use a cosine schedule to linearly warm up the learning rate from 1e-07 to the target value over the first 10% of training steps, and then anneal the learning rate to 1e-06, following the cosine curve for the remaining steps. For GLUE tasks, we tuned the learning rate for each task and chose the best value over three settings: 1e-04, 5e-05 and 3e-05. For structured prediction tasks, we tuned on the pairs of learning rate, see the results section for details. For GLUE tasks, we train three seeds for each learning rate value for three epochs and choose the model after each epoch that performs best on the validation set. For structured prediction tasks, we train for up to 25 epochs and stop if the validation loss does not improve over the previous epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets for pretraining</head><p>We train the two tower model on several datasets.</p><p>Common Crawl. We consider various subsets of Common Crawl which is web data. We follow the same pre-processing as <ref type="bibr" target="#b10">Grave et al. (2018)</ref> which is based on the May 2017 Common Crawl dump. This setup add 20 copies of English Wikipedia resulting in about 14% of the final dataset to be Wikipedia. We subsample up to 18B tokens. All experiments use Common Crawl subsampled to 9B tokens, except ?6.4.</p><p>News Crawl. We use up to 4.5B words of English news web data distributed as part of WMT 2018 <ref type="bibr" target="#b3">(Bojar et al., 2018)</ref>.</p><p>BooksCorpus + Wikipedia. This is similar to the training data used by BERT which comprises the BooksCorpus <ref type="bibr" target="#b36">(Zhu et al., 2015)</ref> of about 800M words plus English Wikipedia data of 2.5B words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Pretraining hyper-parameters</head><p>We adapt the transformer implementation available in the fairseq toolkit to our two tower architecture <ref type="bibr" target="#b20">(Ott et al., 2019)</ref>. For hyper-parameter and optimization choices we mostly follow Baevski and Auli (2018). Our experiments consider three model sizes shown in <ref type="table" target="#tab_0">Table 1</ref>: There are two CNN input models in a base and large configuration as well as a Byte-Pair-Encoding based model (BPE; <ref type="bibr" target="#b28">Sennrich et al., 2016)</ref>. The CNN models have unconstrained input vocabulary, and an output vocabulary limited to 1M most common types for the large model, and 700K most common types for the base model. CNN models use an adaptive softmax in the output: the head band contains the 60K most frequent types with dimensionality 1024, followed by a 160K band with dimensionality 256.  <ref type="bibr" target="#b30">(Sutskever et al., 2013)</ref> with a momentum of 0.99 and we renormalize gradients if their norm exceeds 0.1 <ref type="bibr" target="#b22">(Pascanu et al., 2013)</ref>. The learning rate is linearly warmed up from 10 ?7 to 1 for 16K steps and then annealed using a cosine learning rate schedule with a single phase to 0.0001 <ref type="bibr" target="#b18">(Loshchilov and Hutter, 2016)</ref>.</p><p>We run experiments on DGX-1 machines with 8 NVIDIA V100 GPUs and machines are interconnected by Infiniband. We also use the NCCL2 library and the torch.distributed package for inter-GPU communication. We train models with 16bit floating point precision, following <ref type="bibr" target="#b21">Ott et al. (2018)</ref>. The BPE model trains much faster than the character CNN models <ref type="table" target="#tab_0">(Table 1)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">GLUE</head><p>First, we conduct experiments on the general language understanding evaluation benchmark (GLUE; <ref type="bibr" target="#b32">Wang et al., 2018)</ref> and present a short overview of the tasks. More information can be found in <ref type="bibr" target="#b32">Wang et al. (2018)</ref>. There are two singlesentence classification tasks: First, the Corpus of Linguistic Acceptability (CoLA; <ref type="bibr" target="#b33">Warstadt et al., 2018)</ref> is a binary task to judge sentence grammaticality; evaluation is in terms of the Matthews correlation coefficient (mcc). Second, the Stanford Sentiment Treebank (SST-2; <ref type="bibr" target="#b29">Socher et al., 2013)</ref> requires to judge if movie reviews have positive or negative sentiment; evaluation is in terms of accuracy (acc).</p><p>There are three tasks assessing sentence similarity: The Microsoft Research Paragraph <ref type="bibr">Corpus (MRPC;</ref><ref type="bibr">Dolan and Brockett, 2015)</ref> and the Quora Question Pairs benchmark (QQP); we evaluate in terms of F1. The Semantic Textual Similarity Benchmark <ref type="bibr">(STS-B;</ref><ref type="bibr">Cer et al., 2017)</ref> requires predicting a similarity score between 1 and 5 for a sentence pair; we report the Spearman correlation coefficient (scc).</p><p>Finally, there are four natural laguage inference tasks: the Multi-Genre Natural Language Inference (MNLI; <ref type="bibr" target="#b34">Williams et al., 2018)</ref>, the Stanford Question Answering Dataset (QNLI; <ref type="bibr" target="#b27">Rajpurkar et al., 2016)</ref>, the Recognizing Textual Entailment (RTE; <ref type="bibr" target="#b12">, Bar Haim et al., 2006</ref><ref type="bibr">, Ciampiccolo et al., 2007</ref><ref type="bibr" target="#b2">Bentivogli et al., 2009</ref>. We exclude the Winograd NLI task from our results similar to <ref type="bibr" target="#b26">Radford et al. (2018)</ref>; <ref type="bibr" target="#b7">Devlin et al. (2018)</ref> and report accuracy. For MNLI we report both matched (m) and mismatched (mm) accuracy on test.</p><p>We also report an average over the GLUE metrics. This figure is not comparable to the average on the official GLUE leaderboard since we exclude Winograd and do not report MRPC accuracy STS-B Pearson correlation as well as QQP accuracy. <ref type="table" target="#tab_2">Table 2</ref> shows results for three configurations of our approach (cf. <ref type="table" target="#tab_0">Table 1</ref>). The BPE model has more parameters than the CNN model but does not perform better in aggregate, however, it is faster to train. All our models outperform the uni-directional transformer (OpenAI GPT) of <ref type="bibr" target="#b26">Radford et al. (2018)</ref>, however, our model is about  50% larger than their model. We also show results for the concurrently introduced STILTs <ref type="bibr" target="#b24">(Phang et al., 2018)</ref> and BERT <ref type="bibr" target="#b7">(Devlin et al., 2018)</ref>. Our CNN base model performs as well as STILTs in aggregate, however, on some tasks involving sentence-pairs, STILTs performs much better (MRPC, RTE); there is a similar trend for BERT. STILTs adds another fine-tuning step on another downstream task which is similar to the final task. The technique is equally applicable to our approach. Training examples for our model are Common Crawl paragraphs of arbitrary length. We expect that tailoring training examples for language model pretraining to the end tasks to significantly improve performance. For example, BERT trains on exactly two sentences while as we train on entire paragraphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Structured Prediction</head><p>We also evaluated performance on two structured predictions tasks, NER and constituency parsing. For both problems, we stacked task-specific architectures from recent work on top of our pretrained two tower models. We evaluate two ways of stacking: (1) ELMo-style, where the pretrained models are not fine-tuned but are linearly combined at different depths, and (2) with fine-tuning, where we set different learning rates for the task-specific layers but otherwise update all of the parameters during the task-specific training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Named Entity Recognition</head><p>We evaluated span-level F1 performance on the CoNLL 2003 Named Entity Recognition (NER) task, where spans of text must be segmented and   labeled as Person, Organization, Location, or Miscellaneous. We adopted the NER architecture in Peters et al. <ref type="formula">(2018)</ref>, a biLSTM-CRF, with two minor modifications: (1) instead of two layers of biL-STM, we only used one, and (2) a linear projection layer was added between the token embedding and biLSTM layer. We did grid search on the pairs of learning rate, and found that projection-biLSTM-CRF with 1E-03 and pretrained language model with 1E-05 gave us the best result. <ref type="table" target="#tab_4">Table 3</ref> shows the results, with comparison to previous published ELMo BASE results <ref type="bibr" target="#b23">(Peters et al., 2018)</ref>   <ref type="table">Table 5</ref>: Different loss functions on the development sets of GLUE (cf. <ref type="table" target="#tab_2">Table 2</ref>). Results are based on the CNN base model <ref type="table" target="#tab_0">(Table 1)</ref> the art, but fine tuning gives the biggest gain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Constituency Parsing</head><p>We also report parseval F1 for Penn Treebank constituency parsing. We adopted the current state-ofthe-art architecture <ref type="bibr" target="#b17">(Kitaev and Klein, 2018)</ref>. We again used grid search for learning rates and number of layers in parsing encoder, and used 8E-04 for language model finetuning, 8E-03 for the parsing model parameters, and two layers for encoder. <ref type="table" target="#tab_5">Table 4</ref> shows the results. Here, fine tuning is required to achieve gains over the previous state of the art, which used ELMo embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Objective functions for pretraining</head><p>The two-tower model is trained to predict the current token given representations of the entire left and right context (cloze). Next we compare this choice to two alternatives: <ref type="bibr">First, Peters et al. (2018)</ref> train two language models operating leftto-right and right-to-left to predict the next word for each respective direction. We change the twotower model to predict the next word using the individual towers only and remove the combination module on top of the two towers (bilm); however, we continue to jointly train the two towers.</p><p>Second, we combine the cloze loss with the bilm loss to obtain a triplet loss which trains the model to predict the current word given both left and right context, as well as just right or left context. The latter is much harder than the cloze loss since less context is available and therefore gradients for the bilm loss are much larger: the cloze model achieves perplexity of about 4 while as for the bilm it is 27-30, depending on the direction. This results in the bilm loss dominating the triplet loss and we found that scaling the bilm term by a factor of 0.15 results in better performance. <ref type="table">Table 5</ref> shows that the cloze loss performs significantly better than the bilm loss and that combining the two loss types does not improve over the cloze loss by itself. We conjecture that in- dividual left and right context prediction tasks are too different from center word prediction and that their learning signals are not complementary enough.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Domain and amount of training data</head><p>Next we investigate how much pretraining benefits from larger training corpora and how the domain of the data influences end-task performance. <ref type="figure" target="#fig_2">Figure 3</ref> shows that more training data can significantly increase accuracy. We train all models with the exact same hyper-parameter settings on Common Crawl data using the CNN base architecture for 600K updates. We train on up to 18B Common Crawl tokens and the results suggest that more training data is likely to further increase performance. We also experiment with BooksCorpus <ref type="bibr" target="#b36">(Zhu et al., 2015)</ref> as well as English Wikipedia, similar to <ref type="bibr" target="#b7">Devlin et al. (2018)</ref>. Examples in BooksCorpus are a mix of individual sentences and paragraphs; examples are on average 36 tokens. Wikipedia examples are longer paragraphs of 66 words on average. To reduce the effect of training on examples of different lengths, we adopted the following strategy: we concatenate all training examples into a single string and then crop blocks of 512 consecutive tokens from this string. We train on a batch of these blocks (BWiki -blck). It turns out that this strategy did not work better compared to our existing strategy of simply using the data as is (BWikisent). BooksCorpus and Wikipedia performs very well on QNLI and MNLI but less well on other tasks.</p><p>In summary, more data for pretraining improves performance, keeping everything else equal. Also pretraining on corpora that retains paragraph structure performs better than individual sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We presented a pretraining architecture based on a bi-directional transformer model that predicts every token in the training data. The model is trained with a cloze-style objective and predicts the center word given all left and right context.</p><p>Results on the GLUE benchmark show large gains over <ref type="bibr" target="#b26">Radford et al. (2018)</ref> for each task, while experiments with model stacking set new state of the art performance levels for parsing and named entity recognition. We also did extensive experimental analysis to better understand these results, showing that (1) cross sentence pretraining is crucial for many tasks; (2) pre-training continues to improve performance up to 18B tokens and would likely continue to improve with more data; and finally (3) our novel cloze-driven training regime is more effective than predicting left and right tokens separately.</p><p>In future work, we will investigate variations of our architecture. In particular, we had initial success sharing the parameters of the two towers which allows training much deeper models without increasing the parameter count.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Average GLUE score with different amounts of Common Crawl data for pretraining.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The remaining types have dimensionality 64; there are 480K types for the small model and 780K for the large model. The BPE model uses a vocabulary of 55K types and we share input Hyper-parameters for our models. Parameter count excludes the (adaptive) softmax layer. Train time as measured on 128 Volta GPUs for the CNN models and 64 Volta GPUs for the BPE model.</figDesc><table><row><cell>Model</cell><cell cols="3">Parameters Updates Blocks FFN Dim</cell><cell>Attn Heads (final layer)</cell><cell>Query formation (final layer)</cell><cell>Train time (days)</cell></row><row><cell>CNN Base</cell><cell>177M</cell><cell>600K</cell><cell>6 4096</cell><cell>12</cell><cell>Sum</cell><cell>6</cell></row><row><cell>CNN Large</cell><cell>330M</cell><cell>1M</cell><cell>12 4096</cell><cell>32</cell><cell>Concat</cell><cell>10</cell></row><row><cell>BPE Large</cell><cell>370M</cell><cell>1M</cell><cell>12 4096</cell><cell>32</cell><cell>Concat</cell><cell>4.5</cell></row><row><cell cols="4">and output embeddings in a flat softmax with di-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">mension 1024 (Inan et al., 2016; Press and Wolf,</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">2017). The BPE vocabulary was constructed by</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">applying 30K merge operations over the training</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">data, then applying the BPE code to the training</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">data and retaining all types occurring at least three</cell><cell></cell><cell></cell><cell></cell></row><row><cell>times.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Every setup uses model dimensionaltiy d =</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">1024 with H = 16 attention heads for all but the</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">final attention layer. Model based on character in-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">puts use character embedding size 128 and we ap-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">ply six filters of size 1x128, 2x256, 3x384, 4x512,</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">5x512, 6x512 followed by a single highway layer.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">The models are trained with model and attention</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">dropout rate of 0.1 and ReLU dropout rate of 0.05.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Different to Vaswani et al. (2017) we use Nes-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">terov's accelerated gradient method</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Test results as per the GLUE evaluation server. The average column does not include the WNLI test set.</figDesc><table><row><cell>mcc = Matthews correlation, acc = Accuracy, scc = Spearman correlation. Concurrent work is shown below our</cell></row><row><cell>results.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>CoNLL-2003  Named Entity Recognition results. Test result was evaluated on parameter set with the best dev F1.</figDesc><table><row><cell>Model</cell><cell cols="2">dev F1 test F1</cell></row><row><cell>ELMo BASE</cell><cell>95.2</cell><cell>95.1</cell></row><row><cell>CNN Large + ELMo</cell><cell>95.1</cell><cell>95.2</cell></row><row><cell>CNN Large + fine-tune</cell><cell>95.5</cell><cell>95.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Penn Treebank Constituency Parsing results.</figDesc><table><row><cell>Test result was evaluated on parameter set with the best</cell></row><row><cell>dev F1.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6</head><label>6</label><figDesc>shows a breakdown into individual GLUE tasks. For pretraining on Common Crawl, CoLA and RTE benefit most from additional training data. The same table also shows results for News Crawl which contains newswire data. This data generally performs less well than Common Crawl, even on MRPC which is newswire. A News Crawl examples are individual sentences of 23 words on average which compares to several sentences or 50 words on average for Common Crawl. Mutli-sentence training examples are more effective for end-tasks based on sentence pairs, e.g., there is a 14 point accuracy gap on RTE between News Crawl and Common Crawl with 4.5B tokens. More News Crawl data is most beneficial for CoLA and STS-B.</figDesc><table><row><cell>train data (M tok)</cell><cell>CoLA (mcc)</cell><cell>SST-2 (acc)</cell><cell>MRPC (F1)</cell><cell>STS-B (scc)</cell><cell>QQP (F1)</cell><cell>MNLI-m (acc)</cell><cell>QNLI (acc)</cell><cell>RTE (acc)</cell><cell>Avg</cell></row><row><cell>ccrawl</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>likely reason is that</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Effect of different domains and amount of data for pretraining on the on the development sets of GLUE (cf. Table 2)</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note>Results are based on the CNN base model (Table 1</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Auli</surname></persName>
		</author>
		<idno>abs/1809.10853</idno>
		<title level="m">Adaptive input representations for neural language modeling. arXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The fifth pascal recognizing textual entailment challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of TAC</title>
		<meeting>of TAC</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Findings of the 2018 conference on machine translation (WMT18)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ond?ej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Fishel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WMT</title>
		<meeting>of WMT</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semeval-2017 task 1: Semantic textual similarity -multilingual and cross-lingual focused evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><forename type="middle">T</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I?igo</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SemEval</title>
		<meeting>of SemEval</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The pascal recognizing textual entailment challenge. Machine learning challenges, evaluating predictive uncertainty, visual object classification, and recognizing textual entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Ido Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Magnini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno>abs/1511.01432</idno>
		<title level="m">Semi-supervised sequence learning. arXiv</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>abs/1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatically constructing a corpus of sentential paraphrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IWP</title>
		<meeting>of IWP</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The pascal recognizing textual entailment challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ACL-PASCAL workshop on textual entailment and paraphrasing</title>
		<meeting>of the ACL-PASCAL workshop on textual entailment and paraphrasing</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>Ido Dagan, and Bill Dolan</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning word vectors for 157 languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prakhar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of LREC</title>
		<meeting>of LREC</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient softmax approximation for gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Ciss?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The pascal recognising textual entailment challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Bar Haim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Szpektor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Tying word vectors and word classifiers: A loss framework for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khashayar</forename><surname>Hakan Inan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<idno>abs/1611.01462</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Character-aware neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2741" to="2749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Constituency parsing with a self-attentive encoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno>abs/1608.03983</idno>
		<title level="m">SGDR: stochastic gradient descent with restarts. arXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learned in translation: Contextualized word vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL System Demonstrations</title>
		<meeting>of NAACL System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Scaling neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WMT</title>
		<meeting>of WMT</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Fevry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno>abs/1811.01088</idno>
		<title level="m">Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks. arXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Using the output embedding to improve language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EACL</title>
		<meeting>of EACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Tim Salimans, and Ilya Sutskever</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Squad: 100, 000+ questions for machine comprehension of text. arXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno>abs/1606.05250</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention Is All You Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno>abs/1804.07461</idno>
		<title level="m">GLUE: A multi-task benchmark and analysis platform for natural language understanding. arXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Corpus of linguistic acceptability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Warstadt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Bowman</surname></persName>
		</author>
		<ptr target="https://nyu-mll.github.io/CoLA" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Language modeling teaches you more syntax than translation does: Lessons learned through auxiliary task analysis. arXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowman</surname></persName>
		</author>
		<idno>abs/1809.10040</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<idno>abs/1506.06724</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
