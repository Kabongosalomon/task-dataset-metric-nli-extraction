<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PE-former: Pose Estimation Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paschalis</forename><surname>Panteleris</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science</orgName>
								<orgName type="institution">FORTH</orgName>
								<address>
									<settlement>Heraklion, Crete</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonis</forename><surname>Argyros</surname></persName>
							<email>argyros@ics.forth.gr</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science</orgName>
								<orgName type="institution">FORTH</orgName>
								<address>
									<settlement>Heraklion, Crete</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">University of Crete</orgName>
								<address>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PE-former: Pose Estimation Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>https://www.ics.forth.gr/hccv/</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Vision Transformers ? Human Pose estimation</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Vision transformer architectures have been demonstrated to work very effectively for image classification tasks. Efforts to solve more challenging vision tasks with transformers rely on convolutional backbones for feature extraction. In this paper we investigate the use of a pure transformer architecture (i.e., one with no CNN backbone) for the problem of 2D body pose estimation. We evaluate two ViT architectures on the COCO dataset. We demonstrate that using an encoder-decoder transformer architecture yields state of the art results on this estimation problem.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, transformers have been gaining ground versus traditional convolutional neural networks in a number of computer vision tasks. The work of Dosovitskiy et al <ref type="bibr" target="#b2">[3]</ref> demonstrated the use of the pure transformer model for image classification. This was followed by a number of recent papers demonstrating improved performance <ref type="bibr" target="#b11">[12]</ref>, reduced computational requirements <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b17">18]</ref> or both. All of these pure transformer models are applied only to the task of image classification. For more challenging vision tasks such as 2D human pose estimation, recent methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b7">8]</ref> use a combination of a convolutional backbone followed by a transformer front-end to achieve similar performance to that of convolutional architectures.</p><p>In this work, we investigate the use of a pure transformer-based architecture for the task of 2D human pose estimation. More specifically, we focus on single instance, direct coordinate regression of the human body keypoints. We evaluate an encoder-decoder architecture derived from the work of Carion et al <ref type="bibr" target="#b0">[1]</ref>. In our model we eliminate the need for a convolutional based backbone (a ResNet in the case of <ref type="bibr" target="#b0">[1]</ref>) and we replace the encoder part of the transformer with a visionbased <ref type="bibr" target="#b2">[3]</ref> transformer encoder. By foregoing with the need of a convolutional backbone, our model is realized by a much simpler architecture.</p><p>For the encoder part, we investigate the use of two vision transformer architectures: arXiv:2112.04981v1 [cs.CV] 9 Dec 2021 -Deit <ref type="bibr" target="#b11">[12]</ref>: ViT architecture trained with distillation, which exhibits baseline performance for vision transformers. -Xcit <ref type="bibr" target="#b3">[4]</ref>: Cross-covariance transformers. Xcit reduces the computational requirements by transposing the way ViT operates, i.e., instead of attending on tokens the transformer attends on the token-channels.</p><p>When comparing to other regression based methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> which use a combination of transformers and convolutional backbones, our Deit-based model performs on par with similar sized (in million parameters) models. Moreover, our best model using an Xcit-based encoder achieves state of the art results on the COCO dataset.</p><p>We compare our transformer-only architecture with two "baselines". One that uses a ResNet50 as the encoder section and another that uses a ViT (en-coder+decoder) feature extractor. The architecture and details of these baselines are discussed in Section 3. We demonstrate experimentally that against these baselines, when everything else is kept the same, the transformer encoder improves performance.</p><p>Finally, we further improve the performance of our models using unsupervised pre-training, as proposed by Caron et al <ref type="bibr" target="#b1">[2]</ref>. The training results of models that use supervised and unsupervised pre-training are presented in Section 4.</p><p>In summary, the contributions of our work 3 are the following:</p><p>-We propose a novel architecture for 2D human pose estimation that is using vision transformers without the need for a CNN backbone for feature extraction. -We demonstrate that our proposed architecture outperforms the SOTA on public datasets by as much as 4.4% in AP (compared to <ref type="bibr" target="#b6">[7]</ref>). -We evaluate the performance of different vision transformer encoders for the task of 2D human pose estimation. -We demonstrate that the unsupervised pretraining of our transformer-based models, improves their performance by 0.5% (for the Xcit model).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Vision Transformers</head><p>The success of transformers in Natural Language Processing (NLP) has motivated a lot of researchers to adopt transformers for the solution of vision tasks. While attention-based mechanisms acting on image features such as the ones produced from a CNN, have been around for many years, only recently, Dosovitskiy et al <ref type="bibr" target="#b2">[3]</ref> demonstrated successfully with ViT the use of transformers for image feature extraction, replacing the use of CNNs for the task of image classification. While the ViT approach is very promising, it still suffers from issues that arise from the quadratic complexity of the attention mechanism and translates to heavy computational and memory requirements. Additionally, the vanilla ViT models and, especially, their larger variants, are very hard to train and require to be trained on huge annotated datasets. Following ViT, many methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19</ref>] appeared trying to solve or circumvent these issues while also maintaining SOTA performance <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12]</ref>. In this work, we incorporate two very promising architectures, DEIT <ref type="bibr" target="#b11">[12]</ref> and Xcit <ref type="bibr" target="#b3">[4]</ref> in our models and evaluate their performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Transformers for pose estimation</head><p>Attention-based mechanisms have been applied to tackle demanding vision tasks such as 2D human pose estimation. However, these rely on CNN backbones for feature extraction. The addition of an attention mechanism enables methods such as the one proposed by Li et al <ref type="bibr" target="#b6">[7]</ref> to achieve state of the art results, improving on the best CNN based methods such as the HRNet by Sun et al <ref type="bibr" target="#b10">[11]</ref>. However, Li et al Li et al <ref type="bibr" target="#b6">[7]</ref> base their model and build on an HRNet CNN backbone. Similarly, methods such as TFPose by Mao et al <ref type="bibr" target="#b7">[8]</ref>, POET by Stoffl et al <ref type="bibr" target="#b9">[10]</ref> and Transpose by Yang et al <ref type="bibr" target="#b16">[17]</ref>, build on a robust CNN backbone and apply the attention mechanism on the extracted image features. In this work, similarly to <ref type="bibr" target="#b6">[7]</ref>, we use the transformer-based decoder module and the bipartite matching technique of Carion et al <ref type="bibr" target="#b0">[1]</ref>. However, instead of relying on a CNN backbone or encoder for image features extraction, we introduce an architecture that directly uses the output features from a vision transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>As shown in <ref type="figure" target="#fig_0">Figure 1</ref> (bottom row), our architecture consists of two major components: A Visual Transformer encoder, and a Transformer decoder. The input image is initially converted into tokens following the ViT paradigm. A position embedding <ref type="bibr" target="#b12">[13]</ref> is used to help retain the patch-location information. The tokens and the position embedding are used as input to transformer encoder. The transformed tokens are used as the memory <ref type="bibr" target="#b0">[1]</ref> input of the transformer decoder. The inputs of the decoder are M learned queries. For each query the network produces a joint prediction. The output tokens from the transformer decoder are passed through two heads. The heads are feed forward neural networks (FFNs) following the architecture of DETR. The first is a classification head used to predict the joint type (i.e., class) of each query. The second is a regression head that predicts the normalized coordinates in the range [0, 1] of the joint in the input image. Predictions that do not correspond to joints are mapped to a "non object" class.</p><p>In Section 3.1 we discuss the details of our encoder module and we present the architecture of the various encoders that we examined. In Section 3.2 we present the details of the decoder module which is derived from the DETR decoder. Finally, in Section 3.3, we discuss the training techniques and hyperparameters used for the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Transformer Encoder</head><p>Dosovitskiy et al <ref type="bibr" target="#b2">[3]</ref> proposed the use of image patches which, in the case of ViT, have a size of 16 ? 16. This approach of splitting the image into patches is adopted by all the vision transformer methods that appeared since ViT. In general, an input image of dimensions W ? H pixels, is split into n ? m patches, each of which is a square block of p ? p pixels. These patches are flattened into vectors (tokens) and are subsequently passed through the transformer <ref type="bibr" target="#b12">[13]</ref> layers. The self-attention mechanism operates on an input matrix X ? R N ?d , where N is the number of tokens. Each token has d dimensions. The input X is linearly projected to queries, keys and values (Q, K, V ). Keys and values are used to compute an attention map</p><formula xml:id="formula_0">A(K, Q) = Sof tmax(QK T / ? d).</formula><p>The results of the self-attention mechanism is the weighted sum of the values V with the attention map:</p><formula xml:id="formula_1">Attention(Q, K, V ) = A(K, Q)V.</formula><p>The computational complexity of this original self-attention mechanism scales quadratically with N , due to pairwise interactions between all N elements. The use of image patches in ViT instead of pixels makes the use of self-attention tractable, but the quadratic complexity still remains an issue. In addition to the tokens created from the image patches, a vision transformer also has an extra (learned) token. This token, usually called the CLS token, is used to generate the final prediction when the transformer is trained for a classification task. The transformed tokens at the output of the encoder have the same number of channels as the input. In our model, the output tokens of the encoder transformer including the CLS token are used as the "memory" input of the decoder transformer.</p><p>DeiT Encoder: Touvron et al <ref type="bibr" target="#b11">[12]</ref> proposed a training methodology for the ViT architecture that enables efficient training of the vision transformer without the use of huge datasets. The proposed methodology enabled DeiT models to surpass the accuracy of the ViT baseline, while training just on the ImageNet dataset. They further improved on that by proposing a distillation strategy that yielded accuracy similar to that of the best convolutional models. Xcit Encoder: El-Nouby et al <ref type="bibr" target="#b3">[4]</ref> recently proposed the Xcit transformer. In this approach they transpose the encoder architecture. Attention happens between the channels of the tokens rather than between the tokens themselves. The proposed attention mechanism is complemented by blocks that enable "local patch interations", enabling the exchange of information between neighboring tokens on the image plane.</p><p>The Xcit transformer scales linearly with the size of patches since the memory and computation expensive operation of attention is applied to the channels which are fixed in size. This characteristic of Xcit enables training with larger input image sizes and more patches. In our experiments we use the Xcit-Small-12 variant of the model which corresponds roughly to the DeiT-S variant of the DeiT architecture. Similar to the DeiT (Section 3.1) encoder we use pre-trained weights for this encoder. The weights are obtained with 1. supervised training on the ImageNet and 2. with the unsupervised methodology of Dino.</p><p>Resnet Encoder: In order to verify the positive impact of our proposed attentionbased encoder we implemented a third type of encoder, this time using Resnet50 <ref type="bibr" target="#b5">[6]</ref> as the architecture. The 50-layer variant was selected because it has a similar number of parameters as the examined transformer-based encoders.</p><p>The Resnet encoder replaces the "Visual Transformer Encoder" block shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Following the DETR implementation we use a 1 ? 1 convolution to project the output features of the Resnet to the number of channels expected by the decoder. Subsequently we unroll each feature channel to a vector. These vectors are passed as the memory tokens to the decoder. The Resnet50 encoder is used to experimentally validate the positive effect of using vision transformers (DEIT or Xcit) versus a conventional convolutional encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VAB (ViT as Backbone):</head><p>Related work that tackles pose estimation with transformers typically uses a convolutional backbone to extract image features. This model architecture is shown on the top row of <ref type="figure" target="#fig_0">Figure 1</ref>. The features created by the CNN backbone are flattened and used as input tokens to a transformer front-end. In this model the transformer consists of an encoder and a decoder block.</p><p>For our experiments we replace the CNN backbone for a visual transformer. The vision transformer outputs feature tokens which are subsequently processed by the (unchanged) transformer encoder-decoder block. We call this modification VAB for "ViT as Backbone". The VAB approach is closer in spirit to contemporary methods for pose estimation replacing only the CNN backbone with ViT. However it is larger in number of parameters and requires more computational resources than its CNN-based counterparts. In our experiments we use the DeiT-S as the visual transformer backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Transformer Decoder</head><p>We adapt the decoder block of DETR and use it in our model. A number of learned queries are used to predict joint locations. Predictions that do not correspond to a joint are mapped to the "non-object" class. Following the work of Li et al <ref type="bibr" target="#b6">[7]</ref> we use M = 100 queries for all our models. Adding more queries does not improve the result, while having queries equal to the number of expected joints (i.e., 17 for the COCO dataset) gives slightly worse results.</p><p>Bipartite graph matching is used at training time to map the regressed joints to the ground truth annotations. In contrast to the DETR decoder, our decoder is configured to regress to 2D keypoints rather than object bounding boxes. This translates to regressing 2 scalar values (x, y) in the range of [0, 1] for each joint.</p><p>In all our experiments we used a decoder with 6 layers which achieves a good performance balance. Adding more layers to the decoder gives slightly improved results at a cost of more parameters and higher computational and memory requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training</head><p>We use AdamW to train our models with a weight decay of 1e ? 4. For all our experiments we use pretrained encoders while the decoder weights are randomly initialized. The pretrained weights used are noted in each experiment. The learning rate for the encoder is set to 1e?5 and for the decoder to 1e?4. The learning rate drops by a factor of 10 after the first 50 epochs and the models are trained for a total of 80 epochs. For all our models we use a batch size of 42.</p><p>For data augmentation we follow the approach of Xiao et al <ref type="bibr" target="#b14">[15]</ref>. According to this we apply a random scaling in the range [0.7, 1.3], a random rotation in the range ([?40 ? , 40 ? ]) and a random flip to the image crop. We found experimentally that the jitter, blur and solarization used by <ref type="bibr" target="#b0">[1]</ref> for the object detection task were not helpful in our 2D human pose estimation problem, so these operations were not used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We compare against two other regression based methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> for single instance 2D human pose estimation. Both of these methods use a convolutional backbone and an attention mechanism derived from the DETR decoder to regress to the 2D keypoints of a human in the input image crop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation methodology</head><p>We train and evaluate our models on the COCO val dataset. We use the detection results from a person detector with AP 50.2 on the COCO Val2017 set. Following standard practice <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b6">7]</ref> we use flip test and average the results from the original and flipped images. Unless otherwise noted, the methods we compare against use the same evaluation techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with SOTA</head><p>As shown in <ref type="table">Table 1</ref>, our architecture performs on par or surpasses methods that use CNN backbones for the same task. Using the DEIT encoder requires more memory and CPU resources although the number of parameters is relatively low. Due to the increased requirements, we limit our comparison tests of DEIT to 192 ? 256 resolution for patch sizes of 8 ? 8 pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Input size #Parameters AP AR  <ref type="table">Table 1</ref>. A comparison of transformer based methods for 2D body pose estimation with direct regression. Both TFPose <ref type="bibr" target="#b7">[8]</ref> and PRTR <ref type="bibr" target="#b6">[7]</ref> models use a Resnet50 for backbone. The dataset is COCO2017-Val. Flip test is used on all methods. The decoder depth for all models is 6. Note: the code for TFPose <ref type="bibr" target="#b7">[8]</ref> is not available and the model is not provided by the authors, so the number of parameters is unknown. In our work the "-Dino" suffix denotes the use of unsupervised pretraining, Xcit refers to Xcit-small-12, and Deit refers to Deit-small.</p><p>Our DEIT variant scores just 0.4% lower than TFPose 4 for input 192 ? 256. However, it outperforms PRTR @ 288 ? 384, despite the higher input resolution and the greater number of parameters (5.1M ) of the later.</p><p>Due to its reduced computational and memory requirements, our Xcit variant can be trained to higher resolutions and, thus, larger number of input tokens. We train networks at 192 ? 256 and 288 ? 384 input resolutions for the 16 ? 16 and 8?8 patch sizes. Our Xcit based models outperform both the TFPose and PRTR networks at their corresponding resolutions, while having the same number of parameters as PRTR.</p><p>Overall, patch size of 8 ? 8 pixels and resolutions of 288 ? 384 yield the best performance. In fact, our Xcit-based network (Xcit-dino-p8) at 288 ? 384 outperforms the PRTR trained at the same resolution by 4.4% in AP and 3.4% in AR and even outperforms PRTR at resolution 384 ? 512 by 1.6% in AP. However, experiments of the Xcit variant at even higher resolutions (384 ? 512) did not show any significant improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Transformer vs CNN vs VAB Encoders</head><p>We evaluate the performance of a transformer based encoder (Deit, Xcit) versus a CNN (Resnet50) based encoder and a VAB model. The networks are trained on COCO and evaluated on the COCO2017-val. For all the encoders, the imagenet pretrained weights are used. The results are shown in As expected, VAB is the largest network with the highest memory and computational requirements. However, it performs on par with the Resnet50 variant. The Resnet variant despite having almost the same number of parameters as Xcit, requires less memory and computational resources during training and inference. Still, it under-performs compared to Xcit by 2.8% in AP and 2.4% in AR. The Deit variant performs a bit worse (1.2% in AP) than Resnet but it is also smaller by 2.6M parameters.</p><p>Xcit exhibits the best overall performance with a lead of 2.8% in AP and 2.4% in AR over the Resnet50 variant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Unsupervised pre-training</head><p>All the models presented in Section 3 use pre-trained weights for the encoder part, while the decoder's weights are randomly initialized. We use the weights as they are provided by the authors of each architecture Deit 5 , Xcit 6 ). Additionally, we evaluate the use of weights 7 created using unsupervised learning <ref type="bibr" target="#b1">[2]</ref>. More specifically, we evaluate using the Deit, Xcit and Resnet50 encoders. <ref type="table">Table 3</ref> presents the obtained results. For the purposes of our evaluation we train two Deit (deit-small) and two Xcit (xcit-small-12) variants. For all variants, input resolution of 192 ? 256 and patch size is set to 16 ? 16. All hyperparameters are kept the same. We initialize the encoders of the Dino variants using weights acquired with unsupervised learning on Imagenet. In contrast, the normal variants start with encoders initialized with weights acquired with supervised learning on Imagenet. For both Deit and Xcit we observe significant improvement on the performance. However, the amount of improvement drops as the overall performance of the network gets higher (see the entries of <ref type="table">Table 1</ref> for OURS-Xcit-p16 and OURS-Xcit-dino-p16 ).</p><p>As the authors of <ref type="bibr" target="#b1">[2]</ref> hypothesise, we believe that this improvement stems from having the dino-trained model learn a different embedding than the supervised variant. The model is focusing only on the salient parts of the image (i.e., the person) and is not miss-guided by annotation biases and errors that affect the supervised approach. This hypotheses is further supported by the recent work of He et al <ref type="bibr" target="#b4">[5]</ref> on unsupervised learning using masked autoencoders.</p><p>Interestingly, our CNN based encoder (Resnet50) does not benefit from the unsupervised pretraining. The resnet50-PE-former-dino pretrained with the DINO weights, yields worse results than the supervised variant (resnet50-PE-former). This result hints to a fundamentally different way the transformer-based methods learn about salient image parts: When training a CNN, interactions are local on each layer (i.e., depend on the size of the convolutional kernel). On the other hand, vision transformers such as Xcit and Deit, allow for long-range interactions. Long range interactions enable the exchange of information between areas in the image that are far apart. For DINO the implicit task during training is to identify salient parts in an image. As a consequence long range interactions help the network reason about larger objects (i.e, a person).</p><p>For models with the transformer based encoders (deit, xcit), it is possible that further fine tuning such as pre-training on the same resolution as the final network or pre-training on a person dataset (i.e COCO-person) instead of imagenet, could further boost the final gains. However these tasks are beyond the scope of this work and are left for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We presented a novel encoder-decoder architecture using only transformers that achieves SOTA results for the task for single instance 2D human pose estimation. We evaluated two very promising ViT variants as our encoders: Xcit and Deit. We verified the positive impact of our transformer based encoder-decoder by comparing with modified versions of our model with a CNN-encoder and a VAB variant. Out model using the Xcit based encoder, performs best both in accuracy and resource requirements, outperforming contemporary methods by as mach as 4.4% in AP on the COCO-val dataset. Our Deit based encoder variant is on par with methods using CNN based backbones for patch sizes of 8 ? 8 pixels. Furthermore, we demonstrate that using unsupervised pretraining can improve performance, especially for larger patch sizes (i.e., <ref type="bibr">16 ? 16)</ref>.</p><p>Attention-based models look promising for a range of vision tasks. It is conceivable that, with further improvements on the architectures or the training methodologies, these models will dethrone the older CNN-based architectures both in accuracy and resource requirements.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The DETR model adapted for 2D human pose estimation (top) and our proposed model using Visual transformers (bottom). In our model, the cropped image of a person (bottom left), is split into N tokens of p ? p pixels (typically p is equal to 8 or 16 pixels). The tokens are processed by the transformer encoder (can be Deit or Xcit based). The output feature tokens (N + 1 for the class token) are used as the memory tokens in the DETR based transformer decoder. The decoder input is M joint queries. M = 100 in our experiments. The Decoder outputs M prediction tokens which are processed by a classification and a regression head (FFNs). The output is 2D joint locations (range [0, 1]) and class predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Our DeiT encoder uses the DeiT-S model architecture. For our experiments, the encoders are initialized using either the weights provided by the DeiT authors (DeiT-S, trained with distillation at 224 ? 224) of the weights provided by the Dino authors. For our experiments we use two versions of the DeiT-S model, both trained with input sizes of 224 ? 224 pixels: 1. the DeiT-S variant trained with distillation and 2. the DeiT-S trained using the unsupervised methodology of Dino.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .Table 2 .Table 3 .</head><label>223</label><figDesc>The networks are trained with an input resolution of 192 ? 256 and patch size of 16 ? 16 pixels. Apart from replacing the encoders, the rest of the model (decoder) and training hyperparameters are kept the same. Encoder Comparison. Deit vs Xcit vs resnet50 vs VAB. For all experiments the patch size is set to 16 ? 16 pixels. Input resolution is 192 ? 256. All Networks are trained and evaluated on the COCO val dataset. Deit performs worse while also having the smallest number of parameters. Xcit is the best performing overall, however it also has 10% more parameters than Deit. Comparison of Unsupervised Dino weight init. Patch size is set to 16 and input size is 192 ? 256. Dino networks are identical but initialized with the weights of networks trained with the dino unsupervised methodology. Networks are trained and evaluated on the COCO val dataset. Interestingly, the Resnet50 variant does not show the same improvements as the attention-based encoders with unsupervised pretraining.</figDesc><table><row><cell>Method</cell><cell cols="3">#Parameters AP AR</cell></row><row><cell>Resnet50-PE-former</cell><cell></cell><cell cols="2">39.0M 63.4 72.2</cell></row><row><cell>VAB</cell><cell></cell><cell cols="2">47.4M 63.2 72.6</cell></row><row><cell>Deit</cell><cell></cell><cell cols="2">36.4M 62.2 71.6</cell></row><row><cell>Xcit</cell><cell></cell><cell cols="2">40.6M 66.2 74.6</cell></row><row><cell>Method</cell><cell></cell><cell cols="2"># Parameters AP AR</cell></row><row><cell>Deit</cell><cell></cell><cell>36.4M</cell><cell>62.2 71.6</cell></row><row><cell>Dino Deit</cell><cell></cell><cell>36.4M</cell><cell>66.7 75.0</cell></row><row><cell>Xcit</cell><cell></cell><cell>40.6M</cell><cell>66.2 74.6</cell></row><row><cell>Dino Xcit</cell><cell></cell><cell>40.6M</cell><cell>68.0 76.1</cell></row><row><cell>Resnet50-PE-former</cell><cell></cell><cell>39.0M</cell><cell>63.4 72.2</cell></row><row><cell cols="2">Dino Resnet50-PE-former</cell><cell>39.0M</cell><cell>61.0 70.1</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Code is available on https://github.com/padeler/PE-former</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We contacted the authors of TFPose for additional information to use in our comparison, such as number of parameters and AR scores but got no response</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/facebookresearch/deit 6 https://github.com/facebookresearch/xcit</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">https://github.com/facebookresearch/dino</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14294</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09681</idno>
		<title level="m">Xcit: Cross-covariance image transformers</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.06377</idno>
		<title level="m">Masked autoencoders are scalable vision learners</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pose recognition with cascade transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1944" to="1953" />
		</imprint>
	</monogr>
	<note>2021) 1, 2, 4, 6</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15320</idno>
		<title level="m">Tfpose: Direct human pose estimation with transformers</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Mobilevit: Light-weight, general-purpose, and mobilefriendly vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.02178</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">End-to-end trainable multi-instance pose estimation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Stoffl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mathis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.12115</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note>PMLR (2021) 1, 2, 3</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<title level="m">Linformer: Self-attention with linear complexity</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Nystr\&quot; omformer: A nystr\&quot; om-based algorithm for approximating self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.03902</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Transpose: Keypoint localization via transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">E</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<title level="m">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.12723</idno>
		<title level="m">Aggregating nested transformers</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
