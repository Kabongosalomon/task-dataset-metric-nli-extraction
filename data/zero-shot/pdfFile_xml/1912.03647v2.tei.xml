<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Compressing 3DCNNs Based on Tensor Train Decomposition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingheng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Information Engineering</orgName>
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
								<address>
									<postCode>710049</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangshe</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Information Engineering</orgName>
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
								<address>
									<postCode>710049</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqi</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Precision Instrumentation</orgName>
								<orgName type="department" key="dep2">Center for Brain Inspired Computing Research and Beijing Innovation Center for Future Chip</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Deng</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>Santa Barbara</addrLine>
									<postCode>CA93106</postCode>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wu</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Institute for Research Initiatives</orgName>
								<orgName type="institution">Nara Institute of Science and Technology</orgName>
								<address>
									<addrLine>Takayama-cho</addrLine>
									<settlement>Ikoma, Nara</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Compressing 3DCNNs Based on Tensor Train Decomposition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>A R T I C L E I N F O</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Tensor train decomposition 3DCNN Neural network compression Tensorizing</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A B S T R A C T Three dimensional convolutional neural networks (3DCNNs) have been applied in many tasks, e.g., video and 3D point cloud recognition. However, due to the higher dimension of convolutional kernels, the space complexity of 3DCNNs is generally larger than that of traditional two dimensional convolutional neural networks (2DCNNs). To miniaturize 3DCNNs for the deployment in confining environments such as embedded devices, neural network compression is a promising approach. In this work, we adopt the tensor train (TT) decomposition, a straightforward and simple in situ training compression method, to shrink the 3DCNN models. Through proposing tensorizing 3D convolutional kernels in TT format, we investigate how to select appropriate TT ranks for achieving higher compression ratio. We have also discussed the redundancy of 3D convolutional kernels for compression, core significance and future directions of this work, as well as the theoretical computation complexity versus practical executing time of convolution in TT. In the light of multiple contrast experiments based on VIVA challenge, UCF11, and UCF101 datasets, we conclude that TT decomposition can compress 3DCNNs by around one hundred times without significant accuracy loss, which will enable its applications in extensive real world scenarios.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the past few years, deep neural networks (DNNs) <ref type="bibr">(Le-Cun et al., 2015)</ref> have achieved great success in machine learning, especially the convolutional neural networks (CNNs) with some representative instances such as AlexNet <ref type="bibr" target="#b26">(Krizhevsky et al., 2012)</ref>, VGG-Net <ref type="bibr" target="#b45">(Simonyan and Zisserman, 2015)</ref>, Goo-gleNet <ref type="bibr" target="#b48">(Szegedy et al., 2015)</ref>, ResNet , Dense-Net <ref type="bibr" target="#b18">(Huang et al., 2017)</ref>, etc. Nowadays, three dimensional convolutional neural networks (3DCNNs) <ref type="bibr" target="#b21">(Ji et al., 2013;</ref><ref type="bibr" target="#b52">Tran et al., 2015)</ref> have been applied in many tasks of recognition of spatio-temporal data from videos <ref type="bibr" target="#b65">Zhu et al., 2017;</ref><ref type="bibr" target="#b32">Molchanov et al., 2015a</ref><ref type="bibr">Molchanov et al., ,b, 2016</ref><ref type="bibr" target="#b2">Camgoz et al., 2016;</ref><ref type="bibr" target="#b54">Varol et al., 2018;</ref><ref type="bibr" target="#b16">Hara et al., 2018)</ref>, pure 3D data from depth cameras <ref type="bibr" target="#b12">(Ge et al., 2017)</ref>, and stacking utterances from speech data <ref type="bibr" target="#b51">(Torfi et al., 2018)</ref>. However, these high-dimensional 3DCNN architectures, e.g., long-term temporal convolutions with large sized 3D convolutional kernels <ref type="bibr" target="#b54">(Varol et al., 2018)</ref>, make the situation of inflated sizes of DNNs <ref type="bibr" target="#b5">(Cheng et al., 2018)</ref> more serious. Even worse, to the best of our knowledge, there are few practices to compress 3DCNNs to satisfy miniaturization requirements in confining environments such as embedded devices.</p><p>Fortunately, there are researches on the compression of other neural networks <ref type="bibr" target="#b5">(Cheng et al., 2018)</ref>, which provide opportunity to compress 3DCNNs, e.g., compact architecture, weight sharing or quantization, sparsifying or pruning, knowledge distillation, and matrix/tensor decomposition or low-rank factorization. Among these methods, compact architecture can just obtain limited compression ratio and elaborate design is necessary, quantization maps the weights from floating numbers to integers to accelerate computation so that its compression ratio is not very high, pruning usually needs to pre-train corresponding uncompressed models and the data structure of pruned weight appears to be intricate thus extra marked data may be indispensable, and distillation is generally inefficient in training since two networks should be dealt with. By contrast, decomposition method may afford us the so-called in situ training <ref type="bibr" target="#b0">(Alibart et al., 2013)</ref> which can directly get a trained model from scratch with sufficient compression performance because of its inherent theory of linear algebra. Therefore, in this work, we focus on compressing 3DCNNs by applying decomposition methods.</p><p>In the aspect of decomposition methods, singular value decomposition (SVD) is the most widely employed matrix decomposition method for the compression of DNNs. For instance, <ref type="bibr" target="#b62">Zhang et al. (2015</ref><ref type="bibr" target="#b61">Zhang et al. ( , 2016</ref> split one convolutional kernel into two sub kernels, and <ref type="bibr" target="#b43">Shim et al. (2017)</ref> compress the last softmax layer for neural networks with large vocabulary. However, it may be not enough to completely eliminate the inherent redundancy in DNNs <ref type="bibr" target="#b9">(Denil et al., 2013)</ref> from the point view of tensor. Hence, a higher compression ratio could be approached by reshaping the weight matrices to tensors, termed as tensorizing <ref type="bibr" target="#b36">(Novikov et al., 2015)</ref>. Nevertheless, traditional tensor decomposition methods, such as CP <ref type="bibr" target="#b3">(Caroll and Chang, 1970)</ref> and Tucker <ref type="bibr" target="#b53">(Tucker, 1966)</ref>, are inevitable to fall in the curse of dimensionality because their kernel tensors still give an exponential contribution to the space complexity <ref type="bibr" target="#b8">(Cichocki et al., 2015)</ref>.</p><p>Tensor network decomposition methods <ref type="bibr" target="#b7">(Cichocki, 2018)</ref>, including hierarchical Tucker <ref type="bibr" target="#b15">(Hackbusch and K?hn, 2009;</ref><ref type="bibr" target="#b13">Grasedyck, 2010)</ref>, tensor train (TT) <ref type="bibr" target="#b39">(Oseledets, 2011)</ref>, and tensor chain <ref type="bibr" target="#b22">(Khoromskij, 2011;</ref><ref type="bibr" target="#b63">Zhao et al., 2018)</ref>, can completely avoid the curse of dimensionality by representing a tensor as linked tiny factor tensors with restricted orders. Thereinto, TT decomposition is the most concise format so that many compression applications are based on it. <ref type="bibr" target="#b36">Novikov et al. (2015)</ref> first utilize TT decomposition to compress the weight matrices in fully connected (FC) layers. Since then, <ref type="bibr" target="#b19">Huang et al. (2018)</ref>, <ref type="bibr" target="#b47">Su et al. (2018)</ref> and <ref type="bibr" target="#b20">Huang and Yu (2019)</ref> extend this idea to the applications based on CNNs with TT decomposed FC layers. Only <ref type="bibr" target="#b11">Garipov et al. (2016)</ref> apply the TT format to convolutional layers by first viewing the kernel as a 4th-order tensor, then reshaping the tensor to a matrix, and finally matching the matrix to the thorder tensorizing TT approach <ref type="bibr" target="#b36">(Novikov et al., 2015)</ref>. In the domain of recurrent neural networks (RNNs), <ref type="bibr" target="#b49">Tjandra et al. (2017</ref><ref type="bibr" target="#b50">Tjandra et al. ( , 2018</ref> utilize TT format to compress all matrices within different kinds of gated structures, and further, <ref type="bibr" target="#b57">Yang et al. (2017)</ref> test the performance of TT-RNNs and achieve extremely high compression ratio with miraculous accuracy improvement rather than loss based on larger models and datasets.</p><p>From the above recent practices on TT-based compression, we observe that: 1) all but <ref type="bibr" target="#b11">Garipov et al. (2016)</ref>'s method are based on tensorizing TT approach for weight matrices including FC layers and RNN gated units; 2) all but <ref type="bibr" target="#b57">Yang et al. (2017)</ref>'s work have more or less accuracy losses; 3) although it is important to make TT format to be low-rank <ref type="bibr" target="#b28">(Lee and Cichocki, 2014;</ref><ref type="bibr" target="#b1">Bengua et al., 2017)</ref>, how to select suitable TT ranks with given tensor shape especially for training DNNs has not been addressed yet. Based on these observations and facing the specific 3D convolutional kernels in 3DC-NNs, in this work, we will first study a tensorizing method to compress a 5th-order 3D convolutional kernel tensor into the TT format as a th-order tensor according to fundamental methods reported by <ref type="bibr" target="#b36">Novikov et al. (2015)</ref> and <ref type="bibr" target="#b11">Garipov et al. (2016)</ref>. Secondly, we will provide a general rule to decide the values of TT ranks for a specific tensor with given shape. Thirdly, inspired by <ref type="bibr" target="#b57">Yang et al. (2017)</ref>, multiple experiments on VIVA challenge (Ohn-Bar and Trivedi, 2014), UCF11 <ref type="bibr" target="#b31">(Liu et al., 2009)</ref> and UCF101 <ref type="bibr" target="#b46">(Soomro et al., 2012)</ref> datasets will be conducted to give empirical proof that accuracy loss can be avoided in TT compressed 3DCNNs with around one hundred times compression ratio if their original uncompressed ones have comparatively higher level of redundancy, i.e., larger scale of networks. Last but not least, some other characteristics of TT will also be discussed to draw forth the core significance and future direction of TT CNNs, e.g., regularization derived from fewer parameters of TT, and theoretical computation complexity versus practical executing time of convolution in TT.</p><p>We list the main contributions of this work as follows.</p><p>? To the best of our knowledge, we are the first to utilize the TT format to compress convolutional kernels in 3DCNNs. This method can provide an direct in situ training approach without pre-training or elaborate de-sign to compress large-scale 3DCNNs for application scenarios with limited storage space.</p><p>? We establish a general principle to select TT ranks for a size-fixed tensor based on two bases. One is the theoretical analysis to explain the source of TT ranks which come from hierarchical Tucker decomposition, and the other is the experimental verification.</p><p>? We empirically demonstrate that the accuracy loss in compression can be avoided in 3DCNNs with high redundancy by combining the inherent regularity of TT decomposition, and further a very high compression ratio (about one hundred times) can be obtained.</p><p>The rest of the paper is organized as follows. Section 2 first introduces fundamental knowledge of the TT format including tensorizing for matrices, then proposes the tensorizing for 3D convolutional kernels and discusses the selection of TT ranks. Section 3 presents the elaborate contrast experiments based on VIVA challenge, UCF11 and UCF101 datasets to verify that the accuracy loss can be avoided when compressing a redundant 3DCNN model based on TT decomposition. Section 4 further discusses some experimental phenomena and possible internal mechanisms of TT 3DC-NNs. Section 5 concludes this work and mentions the future direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Tensor Train Decomposition for 3DCNNs</head><p>In this section, we first introduce basic knowledge of TT format. Then we propose the tensorizing method for compressing 3D convolutional kernels. Finally we investigate the principle regarding how to select TT ranks. For convenience, we will use the bold lower case letter as the vector symbol (e.g. ), the bold upper case letter as the matrix symbol (e.g. ), the calligraphic bold upper case letter as the tensor notation (e.g. ?), and the corresponding ordinary letters (e.g. , , and ?) to denote their respective elements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Tensor Train Format</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">Basic TT Format</head><p>According to <ref type="bibr" target="#b39">Oseledets (2011)</ref>, the basic TT format of a th-order tensor ? ? ? 1 ? 2 ??? can be represented in the measure of entry as</p><formula xml:id="formula_0">?( 1 , 2 , ? , ) = 1 [ 1 ] 2 [ 2 ] ? [ ]<label>(1)</label></formula><p>where ? {1, 2, ? , } ( ? {1, 2, ? , }) is the th index of the entry in tensor ?, serial products on the right side of the equation are core matrices to calculate the entry, each matrix [ ] has the shape of ?1 ? and 0 = = 1. There are totally + 1 values of which are collectively called TT ranks. Additionally, all</p><p>[ ] corresponding to the same mode can be stacked into a 3rd-order core tensor ? ? ? ?1 ? ? . Therefore, the TT format of ? can also be represented as <ref type="bibr" target="#b29">(Lee and Cichocki, 2016)</ref>  </p><formula xml:id="formula_1">? = ? 1 ? 1 ? 2 ? 1 ? ? 1 ? (2) ? ? ? G 1 ? ? 1 1 ? 1 G 2 ? ? 1 ? 2 2 ? 2 ? 1 ? 1 = ? 1 G ?1 ? ? ?2 ? ?1 ?1 ? ?1 G ? ? ?1 ? ? 1 ? ? (a) Tensorizing for weight matrix in TT format K ? ? ? ? ? G 0 ? ? 2 ? 1 G 1 ? ? 1 ? 1 1 ? 2 G ?1 ? ? ?1 ? ?1 ?1 ? G ? ? ? = ? 1 ? 1 ? 1 ? 1 ? ? (b) Tensorizing for convolutional kernel in TT format K 3 ? ? ??? ? ? G 0 ? ? ? 1 G 1 ? ? 1 ? 1 1 ? 2 G ?1 ? ? ?1 ? ?1 ?1 ? G ? ? ? = ? 1 ? 1 ? 1 ? 1 ? ? (c) Tensorizing for 3D convolutional kernel in TT format</formula><formula xml:id="formula_2">? =1 , = ? =1 , = ? =1 , = ? =1 and ? ? ? = ? .</formula><p>where ? 1 is called mode-( , 1) contracted product, which means just one pair equal modes in any th-order tensor ? and th-order tensor ? will be contracted to produce a new ( + ? 2)th-order tensor ? ? 1 ?. Suppose that the maximum value of all modes is , and the maximal rank is . It is easy to work out that the space complexity of tensor ? can be reduced from ?( ) to ?( 2 ). Obviously, the compression ratio grows exponentially as the value of order increases linearly. This means that the more complex a data structure is, the higher compression ratio we can obtain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">Tensorizing and TT for Matrices</head><p>It is meaningless to directly use Equation (2) to decompose a matrix as a 2nd-order tensor, because such naive approach will make TT decomposition degenerating as normal low-rank matrix decomposition. From analysis of space complexity above, significant compression ratio can be obtained if the original matrix is reshaped to set the value of order higher. Such idea is the so-called tensorizing <ref type="bibr" target="#b36">(Novikov et al., 2015)</ref>, which makes it possible to utilize TT for matrices not only in the field of DNNs.</p><p>In detail, consider a large matrix ? ? ? and each value of its modes can be factorized into integers like = ? =1 and = ? =1 ( ? {1, 2, ? , }). Then a thorder tensor ? ? ? 1 1 ? 2 2 ??? can be constructed by two bijections which can map the original matrix mode or to tensor modes or separately. The corresponding relationship between the original matrix and the reshaped tensor can be represented as</p><formula xml:id="formula_3">( , ) = ?(( 1 ( ), 1 ( )), ( 2 ( ), 2 ( )), ? , ( ( ), ( ))) where ? {1, 2, ? , }, ? {1, 2, ? , }, and ( ) = ( 1 ( ), 2 ( ), ? , ( )) ( ) = ( 1 ( ), 2 ( ), ? , ( ))</formula><p>are the two bijections.</p><p>After tensorizing, one can use Equation (1) to rewrite the tensor ? into its TT format as ?(( 1 ( ), 1 ( )), ? , ( ( ), ( ))) = 1 [( 1 ( ), 1 ( ))] ? [( ( ), ( ))].</p><p>(3)</p><p>Relevant space complexity can be reduced from ?( ) to ?( 2 ) where and are the maximal and ( ? {1, 2, ? , }), respectively. The visualized structure of TT for matrix ? ? ? is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>(a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">TT for 3D Convolutional Kernels</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">From 2D to 3D</head><p>A normal 2D convolutional kernel could always be regarded as a 4th-order tensor ? ? ? ? ? ? , which can be reshaped to a ( +1)th-order tensor ? ? ? 2 ? 1 1 ? 2 2 ??? by referring to tensorizing for matrices, where means the edge length of convolutional filter, = ? =1 and = ? =1 denote the input and the output channels respectively <ref type="bibr" target="#b11">(Garipov et al., 2016)</ref>. For TT format, such reshaping approach is more efficient than naively utilizing Equation (1), because the value of or is usually much larger than , and a tensor with more balanced shape can usually get less errors <ref type="bibr" target="#b36">(Novikov et al., 2015)</ref>. Based on Equation (3), <ref type="bibr" target="#b11">Garipov et al. (2016)</ref> give the TT format for convolu- Define ? ? ? ? ? +1 in disk; 20: end for 21: Define ? ? ? ? in disk; 22: ? 3 ? ? 0 ? 1 ? 1 ? 1 ? 2 ? 1 ? ? 1 ? ; 23: Reshape as <ref type="bibr" target="#b11">(Garipov et al., 2016)</ref>,? 3 ? ? 3 ; 24: Reshape as Equation <ref type="formula" target="#formula_6">(5)</ref>, ? 3 ?? 3 ; 25: ? ? ? * ? 3 ; 26: return ?.</p><p>tional kernels</p><formula xml:id="formula_4">?(( ? , ? ), ( ? 1 , ? 1 ) ? , ( ? , ? )) = 0 [( ? , ? )] 1 [( ? 1 , ? 1 )] ? [( ? , ? )]<label>(4)</label></formula><p>where ? or ? ? {1, 2, ? , }, ? ? {1, 2, ? , }, and ? ? {1, 2, ? , } ( ? {1, 2, ? , }). The visualized structure of TT for convolutional kernel ? ? ? ? ? ? is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>(b). Inspired by <ref type="bibr" target="#b11">Garipov et al. (2016)</ref>, we intend to propose a similar method to reconstruct a 3D convolutional kernel to a ( + 1)th-order tensor with relatively balanced shape, and then utilize the TT format on this tensor. However, a 3D convolutional kernel ? 3 ? ? ??? ? ? is a 5th-order tensor which has a convolutional filter with 3 sizes ( ? ? ? ) rather than regular hexahedron in most cases. Thus, in order to utilize Equation (4), we should make a mapping to transfer the entry from ? 3 to a new 4th-order tensor? 3 ? ? ? ? ? with the constraint ? ? ? = ? .</p><p>First, let us ignore and , stretch ? 3 to a 3rd-order tensor? 3 ? ? ? ? with the constraint ? = . Then, suppose the value of each index begins at 0, and we have</p><formula xml:id="formula_5">? 3 ( ? , ? ? , ? , ? , ? ) =? 3 ( ? , ? , ? ) where ? , ? ? , ? , ? , ? , ? are indices of the modes ,?, , , , , respectively, with ? ? + ? ? + ? = ? . Second, fold? 3 to a 4th-order tensor? 3 ? ? ? ? ? with the constraint = , we hav? ? 3 ( ? , ? , ? ) =? 3 ( ? , ? , ? , ? )</formula><p>where ? , ? are indices of the modes , , with ? = ? + ? . Combining the above two steps, the mapping from ? 3 to? 3 should include 3 bijections: ( ) = ( 1 ( ), 2 ( )), ( ) = ( 1 ( ), 2 ( )), and ( ) = ( 1 ( ), 2 ( )), which let</p><formula xml:id="formula_6">? 3 ( , , , ? , ? ) = ? 3 (( 1 ( ), 1 ( ), 1 ( )), ( 2 ( ), 2 ( ), 2 ( )), ? , ? )<label>(5)</label></formula><p>and ? + + = ( 2 ( ), 2 ( ), 2 ( )) +( 1 ( ), 1 ( ), 1 ( )).</p><p>Finally, as the same as Equation <ref type="formula" target="#formula_4">(4)</ref>, by further reshap-ing? 3 to ? 3 ? ? ? 1 1 ? 2 2 ??? , we can get the TT format for 3D convolutional kernels like</p><formula xml:id="formula_7">? 3 (( ? , ? ), ( ? 1 , ? 1 ) ? , ( ? , ? )) = 0 [( ? , ? )] 1 [( ? 1 , ? 1 )] ? [( ? , ? )].<label>(6)</label></formula><p>The visualized structure of TT for 3D convolutional kernel ? 3 ? ? ??? ? ? is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>(c). Moreover, for easily programming and directly using convolutional operation * , the proposed Algorithm 1 shows how to design the structure of a 3D convolutional kernel and compute the convolutional output with input data. Note that we calculate the values of and as close as possible to ensure the data ranges of ? and ? are sufficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Compression Ratio and TT Rank</head><p>The corresponding compression ratio of Equation <ref type="formula" target="#formula_7">(6)</ref> can be calculated as</p><formula xml:id="formula_8">= ? =1 1 + ? ?1 =1 +1 + where ( ? 1, ? , ) denotes the TT ranks = [ 1 , ? , ].</formula><p>Note that 0 = +1 = 1 because ? 3 is a ( + 1)th-order tensor. It is easy to see that the compression ratio depends on the TT ranks significantly. Let ( ) and ( ) be the maximum and minimum in , we obtain that</p><formula xml:id="formula_9">( ) = ? =1 ( + ? ?1 =1 + ) ( ( ) ) ? ? ( ( ) ).<label>(7)</label></formula><p>This means that TT ranks influences the range of compression ratio under a certain tensorizing format. As a rule of thumb, higher rank may signify lower error while lower rank may cause more information loss. Detailedly, if we select high rank, the practical compression ratio will be very close to the lower bound ( ( ) ) which can certainly hinder fulfilling the meaning of compression. On the contrary, if we choose small rank to pursue extremely high compression ratio which follows the upper bound ( ( ) ) closely, considerable accuracy loss may occur. That is, the trade-off between compression ratio and accuracy is critically based on . Thus, how to decide the values of TT ranks has significant correlation to low loss compressing 3DCNNs, and becomes the remaining core topic of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">The Selection of TT ranks 2.3.1. Theoretical Foundation</head><p>As mentioned in Section 1, there is still a lack of verified principles for the selection of TT ranks to represent a tensor with given shape, although <ref type="bibr" target="#b36">Novikov et al. (2015)</ref> have summarized some phenomena when TT ranks growing. In fact, it is widely accepted that the TT format is a special form of the hierarchical Tucker format <ref type="bibr" target="#b13">(Grasedyck, 2010;</ref><ref type="bibr" target="#b29">Lee and Cichocki, 2016;</ref><ref type="bibr" target="#b14">Grasedyck and Hackbusch, 2011;</ref><ref type="bibr" target="#b23">Khrulkov et al., 2018)</ref>, so it is possible to find the theoretical foundation of TT ranks from researching the details of hierarchical Tucker.</p><p>In order to explain the hierarchical Tucker format, we first introduce the concept of -modes matricization <ref type="bibr" target="#b13">(Grasedyck, 2010)</ref>. Consider a th-order tensor ? ? ? 1 ? 2 ??? with the set of indices of modes = {1, 2, ? , } which can be split into two subsets = { 1 , ? , } and = { 1 , ? , ? } ( = ? ). If the set can also be split into = ? , we can conclude <ref type="bibr" target="#b24">(Kressner and Tobler, 2011)</ref> </p><formula xml:id="formula_10">span( ( ) ) ? span( ( ) ? ( ) )</formula><p>where ( ) ? ? 1 2 ? ? 1 2 ? ? and ? means the Kronecker product. Similarly, modes of ( ) are serial products of elements in and its complementary set ? , separately, and so does ( ) . Such form like ( ) is called -modes matricization of tensor ?. Furthermore, given , , and as bases of the column spaces of ( ) , ( ) , and ( ) , respectively, we have</p><formula xml:id="formula_11">= ( ? )<label>(8)</label></formula><p>where ? ? 1 2 ? ? , and ? ? ? is called transfer matrix. Note that , , and are the respective ranks of ( ) , ( ) , and ( ) .</p><p>It can be easily observed that only two subsets will be produced by utilizing Equation <ref type="formula" target="#formula_11">(8)</ref> each time. Thus, a specific hierarchical Tucker format is corresponding to a binary tree which continuously divides the original set of full indices of modes until all the leaf nodes appear to be a singleton set of one index of mode. Such binary tree is called dimension tree, and there is a special form called degenerate = 1,2, ?</p><formula xml:id="formula_12">1 = 1 ? 1 = 2,3, ? ? 2 = 3,4, ? 2 = 2 ? ? ? ? ?1 = ? 1 ? ?1 = Figure 2: The degenerate dimension tree of tensor ? ? ? 1 ? 2 ??? .</formula><p>dimension tree which splits out only one index of mode each time as shown in <ref type="figure">Figure 2</ref>. According to Lemma 5.2 in <ref type="bibr" target="#b13">Grasedyck (2010)</ref> which proves that the hierarchical Tucker corresponding to the degenerate dimension tree yields the TT format. For the th-order tensor in <ref type="figure">Figure 2</ref>, each entry of the th ( ? {2, 3, ? , ? 1}) TT core tensor can be represented like</p><formula xml:id="formula_13">? ( , , ) = ? ? =1 ?? ?1 ( , , ) ( , ) where = { },? = { + 1, ? , }, ? ? ? ? , ?? ?1 ? ? ? ? ? ?1 , ? ? ? ? ? ?1 , and , , , are indices of the modes , , ?1 , ? , respectively. Note that the im- plicit ? ? ? +1 +2 ? ? is defined based on Equation (8) in ? ?1 = ( ? ? ) ? ?1</formula><p>where the transfer matrix ? ?1 ? ? ? ? ?1 is the initial form of tensor ?? ?1 . Trace back to Equation <ref type="formula">(2)</ref>, any TT rank of a th-order tensor ? is actually the which comes from the matrix rank of (? ) ? ? +1 +2 ? ? 1 2 ? with the base of column space ? . For example, a 4th-order tensor ? ? ? 4?5?6?7 whose serial -modes matricizations based on the degenerate dimension tree are (5?6?7)?4 , (6?7)?(4?5) , and 7?(4?5?6) , then the TT ranks should be 1 = 4, 2 = 20, and 3 = 7 if we suppose that these three matrices are all full rank. However, if higher value of rank is decided, the compression ratio will be closer to the lower bound ( ( ) ) which makes compression meaningless according to Inequality (7). Besides, higher values of TT ranks always cause ripples for numerical computation. In practice of DNNs, because of the considerable redundancy in weight matrices <ref type="bibr" target="#b9">(Denil et al., 2013)</ref>, we can just consider the truncated TT format by selecting a comparatively small rank to replace all original ranks, e.g., set 2 = 1 = 4 and 3 = 1 = 4 for the above tensor ? , even the final compression ratio should be very close to the upper bound ( ( ) ). <ref type="table">Table 1</ref> Design of CNNs for verifying the principle to select TT ranks. Thereinto, each content in "Conv" and "Shape" denotes a convolutional kernel ? ? ? ? ? ? as ( ? ) ? ( ? ), while its corresponding content in "Conv" and "TT" denotes the matching TT format from Equation <ref type="formula" target="#formula_4">(4)</ref> </p><formula xml:id="formula_14">as ( ? ) ? ( 1 ? 1 ) ? ? ? ( ? ). - CNN-1 CNN-2 Layers Shape TT Shape TT Input 32 ? 32 ? 3 - 32 ? 32 ? 3 - Conv 1.1 (3 ? 3) ? (3 ? 64) - (3 ? 3) ? (3 ? 64) - Conv 1.2 (3 ? 3) ? (64 ? 64) (3 ? 3) ? (4 ? 4) ? (4 ? 4) ? (4 ? 4) (3 ? 3) ? (64 ? 64) (3 ? 3) ? (4 ? 4) ? (4 ? 4) ? (4 ? 4) Max Pooling 1 2 ? 2 - 2 ? 2 - Conv 2.1 (3 ? 3) ? (64 ? 128) (3 ? 3) ? (4 ? 4) ? (4 ? 8) ? (4 ? 4) (1 ? 1) ? (64 ? 256) - Conv 2.2 (3 ? 3) ? (128 ? 128) (3 ? 3) ? (4 ? 8) ? (8 ? 4) ? (4 ? 4) (3 ? 3) ? (256 ? 256) (3 ? 3) ? (4 ? 4) ? (4 ? 4) ? (4 ? 4) ? (4 ? 4) Max Pooling 2 2 ? 2 - 2 ? 2 - Conv 3.1 (3 ? 3) ? (128 ? 128) (3 ? 3) ? (4 ? 8) ? (8 ? 4) ? (4 ? 4) (3 ? 3) ? (256 ? 256) (3 ? 3) ? (4 ? 4) ? (4 ? 4) ? (4 ? 4) ? (4 ? 4) Conv 3.2 (3 ? 3) ? (128 ? 128) (3 ? 3) ? (4 ? 8) ? (8 ? 4) ? (4 ? 4) (3 ? 3) ? (256 ? 256) (3 ? 3) ? (4 ? 4) ? (4 ? 4) ? (4 ? 4) ? (4 ? 4) Average Pooling 4 ? 4 - 4 ? 4 - Linear 128 - 256 - Output 10 - 10 -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2.">Pragmatic Verification</head><p>In order to verify the effectiveness of truncated TT format for training DNNs, we make two brief tests based on CIFAR-10 dataset <ref type="bibr" target="#b25">(Krizhevsky, 2009)</ref> to examine whether it is enough to just select a relatively small value of matrix rank from any <ref type="bibr">(? )</ref> . Each test is executed through a CNN, and its TT CNN form compresses all but 1 ? 1 convolutional kernels based on Equation (4). The detailed network architectures are shown in <ref type="table">Table 1</ref>. Furthermore, we force all the TT ranks to be the same value with range from 2 to 100 for easily observing the relationship between the accuracy and the value of rank.</p><p>According to the definitions of TT format in <ref type="table">Table 1</ref>, the theoretical values of truncated TT ranks should be 9, 16 or 32 in CNN-1, and 9 or 16 in CNN-2, respectively. Because = 16 occupies the most proportion of all feasible values of original in both CNN-1 and CNN-2, 16 could be the most suitable rank for truncated TT format with the premise that all values of TT ranks are the same. Moreover, the test results illustrated in <ref type="figure" target="#fig_1">Figure 3</ref> show that the accuracy improvements have slowed down when passing the point at rank of 16 in both two TT CNNs.</p><p>In a nutshell, to design a truncated TT format for one layer in DNNs, we deem that for an arbitrary tensorizing thorder weight or convolutional kernel tensor ? ? ? 1 ? 2 ??? , one can select an appropriate rank 1 or ?1 which is the full rank of (? 1 ) ? ? 2 3 ? ? 1 or (? ?1 ) ? ? ? 1 2 ? ?1 , respectively. Approximately, in practice it is usually better to fine tune the th TT rank to be the full rank of ( ) ? ? ? 1 ? ?1 +1 ? . Although we have mentioned that making compression ratio draw near to the upper bound ( ( ) )  <ref type="table">Table 1</ref>. Thereinto, dash lines denote the performance of uncompressed networks.</p><p>may cause accuracy loss according to Inequality <ref type="formula" target="#formula_9">(7)</ref>, selecting truncated TT ranks is still sufficient for DNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>According to the inherent redundancy of DNNs <ref type="bibr" target="#b9">(Denil et al., 2013)</ref>, we suppose that 3DCNNs are more redundant for easier compression, i.e., the accuracy loss of a TT com- Step a</p><p>Step a</p><p>Step b</p><p>Step c</p><p>Step  pressed 3DCNN may be small or even taken away. Therefore, in this section, we first design five different 3DCNN models and their TT formats by continuously enlarging the network scale based on VIVA challenge dataset to observer how the accuracy loss is wiped out when the network redundancy increases. Second, under the limitation of our hardware, a carefully designed two stream 3DCNN and its TT format are trained on UCF11 and UCF101 datasets for further verification. All of our experiments are executed on TensorFlow and t3f library .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Continuously Enlarged 3DCNNs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Dataset and Preprocessing</head><p>We choose VIVA challenge dataset to observe whether the accuracy loss will reduce when scale of 3DCNN is growing, since it is a tiny but challenging hand gesture dataset <ref type="bibr" target="#b37">(Ohn-Bar and Trivedi, 2014;</ref><ref type="bibr" target="#b32">Molchanov et al., 2015a)</ref>, which is comprised of 885 video sequences including 19 dynamic hand gestures, and each video sequence contains two consistent channels that are intensity and depth.</p><p>Directly feeding the video into networks is doubtlessly naive, thus we mainly follow the existing data preprocessing strategy <ref type="bibr" target="#b32">(Molchanov et al., 2015a)</ref>, which increases the amount of samples, and the detailed steps are illustrated in <ref type="figure" target="#fig_3">Figure 4</ref>. Note that the nearest neighbor interpolation (NNI) <ref type="bibr" target="#b32">(Molchanov et al., 2015a</ref>) is used to normalize each video sequence to 32 frames, and we shrink every frame image to 62 ? 28 as width ? height by bicubic interpolation. Finally, every sample is a stacked frame sequence which has three channels, and we call each single channel as a volume, e.g., intensity volume, depth volume, or Sobel gradient volume.</p><p>Besides, some data augmentation methods are also considered based on different characteristics of three channels, which can be observed by average gray histograms of different channels as shown in <ref type="figure" target="#fig_4">Figure 5</ref>. Hence, except common affine transformation, we apply random contrast adjustment in the range of ?25% for intensity frame images, increase the brightness randomly from 0% ? 50% for depth frame images, and drop out random 50% pixels (set their values to 0) for Sobel gradient frame images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Design of Networks</head><p>In order to study the relationship between the compression performance and the network scale, as shown in <ref type="figure" target="#fig_6">Figure 6</ref>, we design five different 3DCNN models with larger and larger scale on VIVA challenge dataset. The detailed architectures are largely considered by referring to those in <ref type="bibr" target="#b32">Molchanov et al. (2015a)</ref>, and we gradually enlarge the number of parameters in convolutional parts and whole networks from 3DCNN-VIVA-1 to 3DCNN-VIVA-5 as gathered in <ref type="table">Table 2</ref>. We do not use batch normalization (BN) here to avoid its possible potential impact that may influence the continuous comparisons between original and compressed networks throughout these five 3DCNNs. We select the comparatively small -modes matricization ranks for the truncated TT decomposition. For instance, the tensorizing for-   mat of Conv2 layer in 3DCNN-VIVA-3 from <ref type="figure" target="#fig_6">Figure 6</ref> is (15 ? 5) ? (4 ? 8) ? (4 ? 8) ? (4 ? 4) which yields a 4th-order tensor with the shape of 75 ? 32 ? 32 ? 16, then we let 16 to be the value of all TT ranks. Furthermore, it is important to make the keeping probability (dropout parameter) of TT compressed network higher than the uncompressed ones because the TT FC layers have much less neurons than the original FC layers. Finally, all the activation functions omitted in <ref type="figure" target="#fig_6">Figure 6</ref> are ReLU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.">Learning and Results</head><p>We consider the k-fold cross validation that split 3100 samples (after preprocessing) into 2500 samples as training set and the rest 600 samples as validation set, and train totally 100 epochs for all networks. On account of the varying scale of different networks and multiple attempts, the optimal hyper-parameters for all networks are not very same, but we keep the paired original and TT compressed networks un-der the same condition. The initial learning rate is set to 0.01 for 3DCNN-VIVA-1, 3DCNN-VIVA-2 and 3DCNN-VIVA-3, 0.005 for 3DCNN-VIVA-4 and 3DCNN-VIVA-5, respectively. The learning rate decreases by a factor of 0.1 after every 30 epochs and the momentum is set to 0.9. Due to the limitation of our GPU resources, the mini-batch size is carefully designed for each network, and concretely, the batch size is 100 for 3DCNN-VIVA-1 and 3DCNN-VIVA-2, 50 for 3DCNN-VIVA-3, 25 for 3DCNN-VIVA-4 and 3DCNN-VIVA-5. Besides, we use stochastic gradient descent (SGD) to optimize our networks and the loss function is cross entropy with the softmax layer.</p><p>The results of these experiments about networks above are shown in <ref type="table">Table 2</ref>, in which their storage requirements ("*.data" file in TensorFlow) and the amounts of parameters (proportions of convolutional parts are highlighted) are also listed. One can easily observe that the accuracy loss, which is also termed as degeneration in the table, decreases as the scale of each network increases in turn until it is vanished. <ref type="table">Table 2</ref> Experimental results on VIVA challenge dataset. Note that the unit of degeneration "pp" means the percentage point and every decimal of degeneration is the difference value between the accuracy of the original network and its TT counterpart. "Conv" and "Whole" mean the number of parameters of convolutional part and whole network respectively.  Compression ratio, which is calculated based on the amount of whole parameters, is promoted from 11.5? to 180.4? that implies better compression performance can be easier obtained for redundant networks. Further detailed discussions are given in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Two Stream 3DCNN for Verification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Dataset and Preprocessing</head><p>The foregoing experiments on VIVA challenge dataset imply that the redundant enough 3DCNNs may be easier to be compressed even losslessly. Here we choose two widely used benchmark datasets, i.e., UCF11 and UCF101, and design a redundant enough (limited by our hardware) two stream 3DCNN to experiment based on RGB and optical flow frames. The final frame size to be fed is 80 ? 60 as weight ? height, and the optical flow data is calculated by Farneback algorithm <ref type="bibr" target="#b10">(Farneb?ck, 2003)</ref>.</p><p>Whether UCF11 or UCF101 has more complex and flat distribution of video length than VIVA as shown in <ref type="figure" target="#fig_7">Figure 7</ref>, thus NNI cannot be considered right along. Here we choose the random clipping <ref type="bibr" target="#b54">(Varol et al., 2018;</ref><ref type="bibr" target="#b44">Simonyan and Zisserman, 2014)</ref>, which samples a consecutive frame sequence with a fixed length (50 in our experiments), for sampling training datasets. Particularly, since UCF101 is very easier to fall into over-fitting during training <ref type="bibr" target="#b16">(Hara et al., 2018)</ref>, following <ref type="bibr" target="#b54">Varol et al. (2018)</ref> we down sample the original frame from 320 ? 240 to 123 ? 92, and further randomly extract the frame with the size of 80 ? 60. Other data augmentation approaches we considered includes random affine transformation for both RGB and optical flow data, contrast and saturation adjustment for RGB frames only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Design of Networks</head><p>The architecture of two stream 3DCNN is shown in <ref type="figure" target="#fig_9">Figure 8</ref>, which mostly follows the 8 layers network in <ref type="bibr" target="#b54">Varol et al. (2018)</ref> but has some adjustment of channels and filter size. Here BN is used ahead of activation function which is ReLU, since UCF datasets are more complex and we intend to check whether BN will influence the comparison between original and TT compressed networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Learning and Results</head><p>There are three official splits in UCF101 dataset, thus we construct training and validation sets following every split and learn several times on them. For UCF11, both of the existing k-fold <ref type="bibr" target="#b57">(Yang et al., 2017)</ref> and leave-one-group (LOG) <ref type="bibr" target="#b41">(Peng et al., 2014)</ref> cross validation are considered in our experiments. The initial learning rate is 0.003, the total number of training epochs is up to 100, and the learning rate decreases exponentially by 0.1 factor after every 30 epochs. We also use SGD optimizer and the momentum coefficient is set to 0.9. The batch size is 20 for both original and TT networks.</p><p>The results are shown in <ref type="table" target="#tab_4">Table 3</ref>, among which the storage consumption of original 3DCNN is huge (nearly 1 GB), thus the compression ratio is considerable (around one hundred times). Fortunately, these results on UCF datasets verify that our TT compressed 3DCNN can still keep performance (even a little degeneration occurs on UCF101), which is hard to be implemented in 2DCNNs <ref type="bibr" target="#b11">(Garipov et al., 2016)</ref>. By observing the variation of parameters, both convolutional and FC parts are compressed heavily, and the proportion of  convolutional parameters is promoted. The tiny difference of space complexity between UCF11 and UCF101 is caused by the last FC layer whose dimension of output is decided by the number of classes. Besides, it should be emphasized that we only record the average accuracy on UCF11 (LOG) because the distribution of validation accuracy for different groups has significant variation, which can be obviously noticed in <ref type="figure">Figure 9</ref>.  <ref type="figure">Figure 9</ref>: Experimental results on UCF11 dataset (LOG).</p><p>The horizontal axis represents which group is decided to be the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Comparison with Typical Works</head><p>For making our experiments convinced and basic for the following discussions, here we make a comparison between our results and recent typical practices on VIVA challenge, UCF11 and UCF101 datasets in <ref type="table" target="#tab_5">Table 4</ref>. It can be confirmed that the results of our experiments can be comparable to corresponding state-of-the-art practices. More importantly, since there are not any significant accuracy losses for our TT networks, we can say that the TT compressed 3DCNNs have ability to compete with the state-of-the-arts at least on VIVA challenge, UCF11 and UCF101 datasets.</p><p>Notice that since training 3DCNNs from scratch on UCF-101 is difficult <ref type="bibr" target="#b16">(Hara et al., 2018)</ref> and we concern more about in situ training of TT compressed 3DCNNs in this work, we only list intermediate results which are trained from scratch by corresponding typical works. Similarly, results produced by some pre-trained approaches on UCF11 are not listed either, e.g., pre-trained CNNs + LSTM in <ref type="bibr" target="#b40">Pan et al. (2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussions</head><p>Any potential of compression is derived from the inherent redundancy of DNNs <ref type="bibr" target="#b9">(Denil et al., 2013)</ref>, and we are aware of that 3DCNNs have higher level of redundancy than traditional normal 2DCNNs in the light of our experiments and such phenomenon can permit us to develop a low less compression method based on TT decomposition. Therefore, in this section, we will emphasize more on the redundancy of 3DCNNs. Besides, some other aspects, e.g., regularization, latent degradation, computation complexity, and core significance of TT, shall also be analyzed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Redundancy of 3DCNNs 4.1.1. Large Convolutional Kernel Size</head><p>A stack of two 3?3 convolutional kernels with fewer parameters has the equivalent receptive field as a single 5?5 convolutional kernel which is realized in VGG-Net <ref type="bibr" target="#b45">(Simonyan and Zisserman, 2015)</ref>, and the principle of using 3?3 kernel is widely adopted nowadays <ref type="bibr" target="#b18">Huang et al., 2017)</ref>. Furthermore, <ref type="bibr" target="#b52">Tran et al. (2015)</ref> also claim that 3?3?3 kernel in 3DCNNs is the best. However, 3?3?3 convolutional kernel is unfriendly for TT decomposition, since the stacked tiny convolutional kernels is inherently a design of compact architecture which can reduce the redundancy. That may be the reason why compressing classical CNNs with 3?3 kernels is hard to avoid accuracy loss <ref type="bibr" target="#b11">(Garipov et al., 2016)</ref>. Some examples are described below for further explanation.</p><p>From network 3DCNN-VIVA-1 to network 3DCNN-VIVA-3, the kernel sizes and channels increase gradually without deepening the network, and the accuracy degeneration decreases accordingly. In contrast, we try to redesign the kernels in 3DCNN-VIVA-2 as 3?3, i.e., transform the Conv1 from 5 ? 5 ? 5 to a stack of two 3 ? 3 ? 3 layers, transform the Conv2 from 3 ? 5 ? 5 to a stack of 3 ? 3 ? 3 and 1 ? 3 ? 3 layers, and transform the Conv3 from 3 ? 3 ? 5 to a stack of 3 ? 3 ? 3 and 1 ? 1 ? 3 layers. As we do so, the degeneration increases even the performance of both the uncompressed and TT networks increase concurrently. Similarly, for our two stream 3DCNN in <ref type="figure" target="#fig_0">Figure 8 on UCF11 (k-fold)</ref>, if all the kernel sizes are replaced by 3 ? 3 ? 3, degeneration will occur (the accuracy of TT network is around 92.19%) and the accuracy of original network has no evident variation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Appropriate Number of Channels</head><p>In CNNs, more channels represent more possible feature combinations, which can positively improve the performance of networks. The convolution in TT format in Equation (4) or Equation <ref type="formula" target="#formula_7">(6)</ref> allows us to design wider CNNs under the restriction of storage capacity. The performance of 3DCNN-VIVA-3 verifies this by comparing with 3DCNN-VIVA-2, in which the former network has higher proportion of convolutional parameters shown in <ref type="table">Table 2</ref>.</p><p>Furthermore, more channels contribute to more balanced TT shapes that may bring higher compression ratio and better keeping information, particularly for FC layers <ref type="bibr" target="#b36">(Novikov et al., 2015)</ref>. Comparing 3DCNN-VIV-A-4 with 3DCNN-VIVA-5 in <ref type="table">Table 2</ref>, the latter with higher compression ratio still has comparable performance with the former, by just enlarging the final output channels from 256 to 384. It is obvious in <ref type="figure" target="#fig_6">Figure 6</ref> that the shapes of (8 ? 8 ? 8 ? 8 ? 6) and (4 ? 4 ? 4 ? 4 ? 4) in 3DCNN-VIVA-5 are better than the corresponding shapes of (8 ? 16 ? 16 ? 8) and (4 ? 4 ? 4 ? 8) in 3DCNN-VIVA-4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3.">Layer Coupling</head><p>It is necessary to point out that there exists coupling between convolutional and FC layers that affects the performance of TT CNNs. <ref type="bibr" target="#b36">Novikov et al. (2015)</ref>; <ref type="bibr" target="#b11">Garipov et al. (2016)</ref> show that compressing the FC part may get sufficient compression ratio with a little accuracy loss, while compressing both FC and convolutional parts is still hard to avoid degeneration. Thus, it seems that compressing the convolutional part is unnecessary. However, we find the coupling between convolutional layers and FC layers gives the meaning to compress convolutional kernels.</p><p>In detail, for the network 3DCNN-VIVA-1, we severally compress the FC part, the convolutional part, and both of these two parts. The results in <ref type="table" target="#tab_6">Table 5</ref> indicate that just compressing either part can cause considerable accuracy loss, while compressing both parts has not produced more sub- stantial degeneration. Furthermore, compressing the whole network can improve the compression ratio obviously (from 9.2? to 11.5?) comparing with compressing FC part only. In a word, compressing the whole networks especially for 3DCNNs will not be worse than compressing either layer part (convolutional or FC). On the other hand, when the scale of 3DCNN grows, amount of parameters in convolutional part appears to be more considerable, e.g., 3DCNN-VIVA-5 and two stream 3DCNN have 4.79 ? 10 6 and 4.95 ? 10 6 convolutional parameters according to <ref type="table">Table 2</ref> and 3 respectively, so compressing convolutional kernels is necessary. Therefore, by taking into account both the layer coupling and the considerable amount of convolutional parameters in large scaled 3DCNNs, we deem that discussing the redundancy of 3DC-NNs should focus on the entire network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4.">Scale of Entire Networks</head><p>Regarding the network scale, we find that the larger network may be easier to be compressed. Comparing the performance of network 3DCNN-VIVA-4 and network 3DCNN-VIVA-5 in <ref type="table">Table 2</ref>, the latter can obtain better accuracy with even lower storage cost. We believe that the larger 3DCNNs contain stronger redundancy.</p><p>Moreover, in general, a large and sparse DNN can get better performance than the dense one with same network scale, which phenomenon is also concluded by <ref type="bibr" target="#b66">Zhu and Gupta (2018)</ref>. This rule can be perceived in <ref type="figure" target="#fig_0">Figure 10</ref> which illustrates the variations of parameters and accuracy degeneration from 3DCNN-VIVA-1 to 3DCNN-VIVA-5 according to <ref type="table">Table 2</ref>. It can be sensed that, not only the amount of whole parameters, but also that of convolutional parameters, can be extremely reduced when entire scale of 3DCNN increases. Therefore, if compression is necessary, we suggest to design large and sparse 3DCNNs rather than tiny and dense ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.5.">Data Distribution</head><p>In macro sense, network redundancy has strong correlation to the data distribution. More challenging dataset may need more redundant network for low loss compression. For example, according to <ref type="figure" target="#fig_0">Figure 11</ref>, the performances of original and TT networks on UCF11 are still matched, but a lit- tle degeneration occurs on UCF101. That is to say, the redundancy of this network is sufficient for UCF11, but not enough for UCF101. Hence, arbitrarily estimating whether a network is redundant or not seems inadvisable. However, researchers can enlarge the size of convolutional kernels and the number of channels brick by brick until their requirements are satisfied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Other Characteristics of TT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Regularization</head><p>It is observed that TT format can bring a certain level of regularization to DNNs. This is the reason why we make the keeping probability of dropout higher in TT networks. Besides, abandoning dropout completely is also inadvisable because even utilizing dropout with 0.9 keeping probability can still avoid over-fitting significantly. For example, we vary the dropout ratio of 3DCNN-VIVA-3 in TT format to observe the regularization effect of TT decomposition in FC layers. The keeping probability of dropout in the uncompressed network is set to 0.5 that illustrates in <ref type="figure" target="#fig_6">Figure 6</ref>, but the values of keeping factor in the TT network are varied from 0.5 to 1.0. The test result is shown in <ref type="figure" target="#fig_0">Figure 12</ref>. We can find that the network presents under-fitting if the keeping probability is less than 0.7, and over-fitting occurs when the keeping probability exceeds 0.7. It is obvious that keeping probability of 0.7 should be the best configuration so that we decide to set this value which can be seen in <ref type="figure" target="#fig_6">Figure 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Latent Degradation</head><p>To be fair, TT decomposition is certain to cause more or less degradation of expressive ability, even the final score may have no degeneration. Thus, accuracy loss will come sooner or later for a DNN with certain scale only if complexity of dataset keeps growing. For instance, according to <ref type="figure" target="#fig_0">Figure 11</ref>, losses of TT networks are always higher than original networks, and the signs of accuracy loss have appeared from UCF11 to UCF101.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">Computation Complexity</head><p>For a 3D convolutional kernel ? 3 ? ? ??? ? ? with input ? ? ? ? ? ? , one can easily obtain its computation complexity as ?( ( ? + 1)), and the corresponding computation complexity of TT in Algorithm 1 is ?( ( ? +1)+ ? ( + ? ?1 =1 2 ?( ) )), where is the maximal TT rank and ?( ? ( + ? ?1 =1 2 ?( ) )) represents the extra amount of calculations which is necessary to recover the TT convolutional kernel to normal format.</p><p>Obviously, 3D convolution in TT format has lower computational efficiency than normal format, which is not pointed out clearly by <ref type="bibr" target="#b11">Garipov et al. (2016)</ref>. The reason is that vector matrix multiplication in TT FC layer is completely different from TT convolution process. In fact, vector matrix multiplication is inherently a specific case of mode-( , 1) contracted product <ref type="bibr" target="#b30">(Lee and Cichocki, 2018)</ref>, thus for a weight matrix in TT format like Equation (3), its computational process can be written as ? = ? ? 1 ? 1 ? 1 ? 2 ? 1 ? ? 1 ? where ? ? ? 1 ? 2 ??? and ? ? ? 1 ? 2 ??? . The above equation can be calculated from left to right in sequence, so ?( 2 max { , } max { , }) should be the computation complexity which avoids recovering weight tensor to original matrix <ref type="bibr" target="#b36">(Novikov et al., 2015)</ref>. However, for the computational process in Algorithm 1, it can just be represented like</p><formula xml:id="formula_15">? = ? * (? 0 ? 1 ? 1 ? 1 ? ? 1 ? )</formula><p>where * is the convolution operator and there is no associative law between * and ? 1 . Even if we enforce ? to convolute with ? 0 firstly, the subsequent calculations will have no convolutions any more, which certainly can harm feature extraction heavily.</p><p>Anyway, the situation is not so bad because of parallel ability of modern hardware. As shown in <ref type="figure" target="#fig_0">Figure 13</ref>, we test the executing time of original and TT 3DCNNs on UCF11 by using the tool "timeline" in TensorFlow. It can be observed that, in TT network, recovering operations can be parallelly computed in multiple threads, thus the entire executing time of TT 3DCNN has not exceeded that of original network very much. Additionally, in the real practice, recovering process from TT parameters in disk to normal shapes in memory is one-off, i.e., the following calculations will not be influenced by TT at least during forward running.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4.">Core Significance</head><p>The core significance of using TT decomposition to compress DNNs is that one can easily and directly construct a large network which just consumes tiny storage in the socalled in situ training approach <ref type="bibr" target="#b0">(Alibart et al., 2013)</ref> without any delicate design or slow pre-training. That is, TT decomposition affords us a simple and convenient approach to compress DNNs with high compression ratio. Although previous researches <ref type="bibr" target="#b36">(Novikov et al., 2015;</ref><ref type="bibr" target="#b19">Huang et al., 2018;</ref><ref type="bibr" target="#b47">Su et al., 2018;</ref><ref type="bibr" target="#b11">Garipov et al., 2016;</ref><ref type="bibr" target="#b49">Tjandra et al., 2017</ref><ref type="bibr" target="#b50">Tjandra et al., , 2018</ref> show that the accuracy loss is hard to avoid, our study demonstrates that 3DCNNs with sufficient redundancy can realize low loss compression based on the TT decomposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>This paper introduces a compression method for convolutional kernels in 3DCNNs based on TT decomposition. How to select suitable truncated TT ranks is analyzed and demonstrated in both theory and practice. Our experiments on VIVA challenge, UCF11 and UCF101 datasets verify that 3DCNNs with sufficient redundancy can be compressed in TT format without significant accuracy loss. Moreover, fully utilizing the redundant design for 3DCNNs can result in better performance including higher compression ratio and lower degeneration. Although some latent problems are still to be dealt with, we believe that TT decomposition is a promising approach to compress large scaled 3DCNNs and even other types of DNNs. We would like to point out that currently there are some other compression methods including neural architecture search (NAS) <ref type="bibr" target="#b67">(Zoph et al., 2018)</ref> to automatically optimize the neural network architectures, and the methods focus on directly designing more compact network  <ref type="figure" target="#fig_0">Figure 13</ref>: Forward executing time of the two stream 3DCNN on UCF11. These figures are made by the tool "timeline" in TensorFlow and run on CPU with 20 batch size. For clarity, "enisum" function in t3f is replaced by normal transposition and matrix multiplication, and BN is also disabled. models , and data quantization methods <ref type="bibr" target="#b56">(Wu et al., 2018)</ref> or network sparsification <ref type="bibr" target="#b66">(Zhu and Gupta, 2018)</ref> methods to respectively reduce the number of data bits or connections/neurons. The proposed method in this work is perpendicular to these schemes. For future works, the joint-way compression across these techniques also emerges to pursue extreme compression. We shall also aim at extending the proposed method to more comprehensive data sets and exploring its advantages in extensive real world applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>TT decomposition structures of weight matrix, convolutional kernel, and 3D convolutional kernel, where =</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Variation of accuracy with increasing the values of TT ranks. Network architectures are defined in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Data preprocessing for VIVA challenge dataset. Note that not all samples can be processed in Step c because a minority of dynamic gestures do not have characteristic of time symmetry. All the frame images shown here are from the video with the serial number "02_01_03" in original dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Average gray histograms of frame images from volumes of different channels. The corresponding video clip has the serial number "01_01_01" in original dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Larger and larger network architectures for VIVA challenge dataset. The numbers in red bold formats denote the modes of channels or in ? 3 ? ? ??? ? ? which represents the 3D convolutional kernel. Note that the number of neurons in every "Linear1" layer is equal to the number of output elements of corresponding last convolutional layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Statistical histograms of number of frames of VIVA and UCF11 datasets. The horizontal axis denotes the number of frames and the vertical axis denotes the amount of videos.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Network architecture for UCF11 and UCF101 datasets. The abbrev "NC" equals 11 or 101 for UCF11 or UCF101 respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :Figure 11 :</head><label>1011</label><figDesc>Variations of parameter amount and accuracy degeneration of the networks on VIVA challenge dataset. Note that the number of whole parameters of uncompressed networks is too large to draw in this figure. Learning curves of two stream 3DCNN on UCF11 and UCF101 datasets. "OF" in the legend means optical flow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 :</head><label>12</label><figDesc>Testing the regularization of TT decomposition on 3DCNN-VIVA-3. The dash lines denote the top accuracy of the uncompressed network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc>Experimental results on UCF11 and UCF101 datasets.</figDesc><table><row><cell></cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell>Original</cell><cell>TT</cell></row><row><cell></cell><cell></cell><cell cols="3">UCF11 (k-fold)</cell><cell>93.78 ? 1.09</cell><cell>93.56 ? 1.16</cell></row><row><cell></cell><cell>Accuracy (%)</cell><cell cols="3">UCF11 (LOG)</cell><cell>83.72</cell><cell>83.81</cell></row><row><cell></cell><cell></cell><cell cols="3">UCF101</cell><cell>78.38 ? 0.37</cell><cell>77.86 ? 0.51</cell></row><row><cell></cell><cell cols="2">Storage (MB)</cell><cell cols="2">UCF11</cell><cell>1009.57</cell><cell>9.9</cell></row><row><cell></cell><cell></cell><cell cols="3">UCF101</cell><cell>1011.68</cell><cell>12</cell></row><row><cell></cell><cell>Parameters</cell><cell cols="2">(10 6 )</cell><cell>UCF11</cell><cell>4.95/88.17</cell><cell>0.36/0.82</cell></row><row><cell></cell><cell>Conv/Whole</cell><cell></cell><cell></cell><cell>UCF101</cell><cell>4.95/88.36</cell><cell>0.36/1.01</cell></row><row><cell></cell><cell cols="2">Compression Ratio</cell><cell></cell><cell>UCF11</cell><cell>107.5?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>UCF101</cell><cell>87.5?</cell></row><row><cell></cell><cell>100</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Original</cell></row><row><cell></cell><cell>90</cell><cell></cell><cell></cell><cell>TT</cell></row><row><cell>Accuracy(%)</cell><cell>70 80</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>60</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>50</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Group</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc>Comparing our experimental results with typical works on VIVA challenge, UCF11 and UCF101 datasets.</figDesc><table><row><cell>VIVA challenge</cell><cell></cell></row><row><cell>Method</cell><cell>Accuracy (%)</cell></row><row><cell>Dense Trajectories (Wang et al., 2013)</cell><cell>54</cell></row><row><cell>HON4D (Oreifej and Liu, 2013)</cell><cell>58.7</cell></row><row><cell>HOG + HOG 2 (Ohn-Bar and Trivedi, 2014)</cell><cell>64.5</cell></row><row><cell>Multi-resolution 3DCNNs (Molchanov et al., 2015a)</cell><cell>77.5</cell></row><row><cell>Ours Original/TT</cell><cell>81.47/81.83</cell></row><row><cell>UCF11</cell><cell></cell></row><row><cell>Method</cell><cell>Accuracy (%)</cell></row><row><cell>Fisher Vector (LOG) (Peng et al., 2014)</cell><cell>93.77</cell></row><row><cell>Local Motion (LOG) Cho et al. (2014)</cell><cell>88.0</cell></row><row><cell>Visual Attention (k-fold) (Sharma et al., 2016)</cell><cell>85.0</cell></row><row><cell>TT-GRU (k-fold) (Yang et al., 2017)</cell><cell>81.3</cell></row><row><cell>BT-LSTM (k-fold) (Ye et al., 2018)</cell><cell>85.3</cell></row><row><cell>TR-LSTM (k-fold) (Pan et al., 2019)</cell><cell>86.9</cell></row><row><cell>Ours Original/TT (LOG)</cell><cell>83.72/83.81</cell></row><row><cell>Ours Original/TT (k-fold)</cell><cell>93.78/93.56</cell></row><row><cell>UCF101 (Training from scratch)</cell><cell></cell></row><row><cell>Method</cell><cell>Accuracy (%)</cell></row><row><cell>C3D (Tran et al., 2015)</cell><cell>44</cell></row><row><cell>3DConvNet (Carreira and Zisserman, 2017)</cell><cell>79.9</cell></row><row><cell>3D ResNet-18 (Hara et al., 2018)</cell><cell>42.4</cell></row><row><cell>MiCT (Zhou et al., 2018)</cell><cell>58.7</cell></row><row><cell>LTC (Varol et al., 2018)</cell><cell>80.5</cell></row><row><cell>Ours Original/TT</cell><cell>78.38/77.86</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc>Compressing different parts of 3DCNN-VIVA-1. The "Base" column denotes the uncompressed network.</figDesc><table><row><cell cols="2">Compressing Part</cell><cell>Base</cell><cell>FC</cell><cell>Convolution</cell><cell>Both</cell></row><row><cell cols="2">Accuracy (%)</cell><cell>78.61</cell><cell>72.97</cell><cell>72.58</cell><cell>71.75</cell></row><row><cell></cell><cell></cell><cell>?2.05</cell><cell>?1.78</cell><cell>?0.68</cell><cell>?1.46</cell></row><row><cell cols="2">Degeneration (pp)</cell><cell>-</cell><cell>5.64</cell><cell>6.03</cell><cell>6.86</cell></row><row><cell>Parameters</cell><cell cols="3">(10 6 ) 0.07/2.3 0.07/0.25</cell><cell>0.015/2.25</cell><cell>0.015/0.2</cell></row><row><cell>Conv/Whole</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Compression Ratio</cell><cell>-</cell><cell>9.2?</cell><cell>1.02?</cell><cell>11.5?</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was partially supported by National Key R&amp; D Program of China (2018AAA0102600, 2018YFE0200200), and Beijing Academy of Artificial Intelligence (BAAI), Tsinghua University Initiative Scientific Research Program, and a grant from the Institute for Guo Qiang, Tsinghua University.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pattern classification by memristive crossbar circuits using ex situ and in situ training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Alibart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zamanidoost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Strukov</surname></persName>
		</author>
		<idno type="DOI">10.1038/ncomms3072</idno>
		<ptr target="https://doi.org/10.1038/ncomms3072" />
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="131" to="140" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Efficient tensor completion for color image and video recovery: Low-rank tensor train</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bengua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">N</forename><surname>Phien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2017.2672439</idno>
		<ptr target="https://doi.org/10.1109/TIP.2017.2672439" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2466" to="2479" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Using convolutional 3D neural networks for user-independent continuous gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Camgoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hadfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICPR.2016.7899606</idno>
		<ptr target="https://doi.org/10.1109/ICPR.2016.7899606" />
	</analytic>
	<monogr>
		<title level="m">23rd International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="49" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Analysis of individual differences in multidimensional scaling via n-way generalization of Eckart-Young decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Caroll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF02310791</idno>
		<ptr target="https://doi.org/10.1007/BF02310791" />
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="283" to="319" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.502</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.502" />
		<title level="m">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
	<note>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Model compression and acceleration for deep neural networks: The principles, progress, and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/MSP.2017.2765695</idno>
		<ptr target="https://doi.org/10.1109/MSP.2017.2765695" />
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="126" to="136" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Robust action recognition using local motion and group sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2013.12.004</idno>
		<ptr target="https://doi.org/10.1016/j.patcog.2013.12.004" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="1813" to="1825" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tensor networks for dimensionality reduction, big data and deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cichocki</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-67946-4_1</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-67946-4_1" />
	</analytic>
	<monogr>
		<title level="m">Advances in Data Analysis with Computational Intelligence Methods</title>
		<imprint>
			<publisher>Springer International Publishing AG</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">738</biblScope>
			<biblScope unit="page" from="3" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Tensor decompositions for signal processing applications: From two-way to multiway component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cichocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mandic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>De Lathauwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Caiafa</surname></persName>
		</author>
		<idno type="DOI">10.1109/MSP.2013.2297439</idno>
		<ptr target="https://doi.org/10.1109/MSP.2013.2297439" />
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="145" to="163" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Predicting parameters in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shakibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Freitas</surname></persName>
		</author>
		<idno type="DOI">https:/dl.acm.org/doi/10.5555/2999792.2999852</idno>
		<ptr target="https://dl.acm.org/doi/10.5555/2999792.2999852" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>the 26th International Conference on Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2148" to="2156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Two-frame motion estimation based on polynomial expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Farneb?ck</surname></persName>
		</author>
		<idno type="DOI">10.1007/3-540-45103-X_50</idno>
		<ptr target="https://doi.org/10.1007/3-540-45103-X_50" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Scandinavian conference on Image analysis (SCIA)</title>
		<meeting>the 13th Scandinavian conference on Image analysis (SCIA)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Ultimate tensorization: compressing convolutional and FC layers alike</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Novikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vetrov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03214v1</idno>
		<ptr target="https://arxiv.org/abs/1611.03214v1" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3D convolutional neural networks for efficient and robust hand pose estimation from single depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.602</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.602" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5679" to="5688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hierarchical singular value decomposition of tensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Grasedyck</surname></persName>
		</author>
		<idno type="DOI">10.1137/090764189</idno>
		<ptr target="https://doi.org/10.1137/090764189" />
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An introduction to Hierarchical (?-) Rank and TT-Rank of tensors with examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Grasedyck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hackbusch</surname></persName>
		</author>
		<idno type="DOI">10.2478/cmam-2011-0016</idno>
		<ptr target="https://doi.org/10.2478/cmam-2011-0016" />
	</analytic>
	<monogr>
		<title level="j">Computational Methods in Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="291" to="304" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A new scheme for the tensor representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hackbusch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>K?hn</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00041-009-9094-9</idno>
		<ptr target="https://doi.org/10.1007/s00041-009-9094-9" />
	</analytic>
	<monogr>
		<title level="j">Journal of Fourier Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="706" to="722" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Can spatiotemporal 3D CNNs retrace the history of 2D CNNs and ImageNet?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00685</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2018.00685" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6546" to="6555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.90" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.243</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.243" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A highly parallel and energy efficient three-dimensional multilayer CMOS-RRAM accelerator for tensorized neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNANO.2017.2732698</idno>
		<ptr target="https://doi.org/10.1109/TNANO.2017.2732698" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Nanotechnology</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="645" to="656" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">LTNN: A layerwise tensorized compression of multilayer neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1109/tnnls.2018.2869974</idno>
		<ptr target="https://doi.org/10.1109/tnnls" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1497" to="1511" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">3D convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2012.59</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2012.59" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">?( log )-quantics approximation of ? tensors in high-dimensional numerical modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">N</forename><surname>Khoromskij</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00365-011-9131-1</idno>
		<ptr target="https://doi.org/10.1007/s00365-011-9131-1" />
	</analytic>
	<monogr>
		<title level="j">Constructive Approximation</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="257" to="280" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Expressive power of recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Khrulkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Novikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Oseledets</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1711.00811v2" />
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Preconditioned low-rank methods for highdimensional elliptic PDE eigenvalue problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kressner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tobler</surname></persName>
		</author>
		<idno type="DOI">10.2478/cmam-2011-0020</idno>
		<ptr target="https://doi.org/10.2478/cmam-2011-0020" />
	</analytic>
	<monogr>
		<title level="j">Computational Methods in Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="363" to="381" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning Multiple Layers of Features from Tiny Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<ptr target="https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf" />
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Computer Science Department, University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">https:/dl.acm.org/doi/10.5555/2999134.2999257</idno>
		<ptr target="https://dl.acm.org/doi/10.5555/2999134.2999257" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>the 25th International Conference on Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature14539</idno>
		<ptr target="https://doi.org/10.1038/nature14539" />
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Big data matrix singular value decomposition based on low-rank tensor train decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cichocki</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-12436-0_14</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-12436-0_14" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Networks ??? International Symposium on Neural Networks (ISNN)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="121" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Regularized computation of approximate pseudoinverse of large matrices using low-rank tensor train decompositions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cichocki</surname></persName>
		</author>
		<idno type="DOI">10.1137/15M1028479</idno>
		<ptr target="https://doi.org/10.1137/15M1028479" />
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="598" to="623" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Fundamental tensor operations for large-scale data analysis using tensor network formats. Multidimensional Systems and Signal Processing 29, 921???960</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cichocki</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11045-017-0481-0</idno>
		<ptr target="https://doi.org/10.1007/s11045-017-0481-0" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Recognizing realistic actions from videos ???in the wild???</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2009.5206744</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2009.5206744" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1996" to="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Hand gesture recognition with 3D convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPRW.2015.7301342</idno>
		<ptr target="https://doi.org/10.1109/CVPRW.2015.7301342" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-sensor system for driver&apos;s hand-gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pulli</surname></persName>
		</author>
		<idno type="DOI">10.1109/FG.2015.7163132</idno>
		<ptr target="https://doi.org/10.1109/FG.2015.7163132" />
	</analytic>
	<monogr>
		<title level="m">11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Online detection and classification of dynamic hand gestures with recurrent 3D convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.456</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.456" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4207" to="4215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Novikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Khrulkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Oseledets</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01928</idno>
		<ptr target="https://arxiv.org/abs/1801.01928v1" />
		<title level="m">Tensor train decomposition on tensorflow (t3f)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Tensorizing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Novikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Osokin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Vetrov</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5787-tensorizing-neural-networks" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>the 28th International Conference on Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="442" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hand gesture recognition in real time for automotive interfaces: A multimodal vision-based approach and evaluations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ohn-Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
		<idno type="DOI">10.1109/TITS.2014.2337331</idno>
		<ptr target="https://doi.org/10.1109/TITS.2014.2337331" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="2368" to="2377" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">HON4D: Histogram of oriented 4D normals for activity recognition from depth sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Oreifej</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2013.98</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2013.98" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="716" to="723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Tensor-train decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Oseledets</surname></persName>
		</author>
		<idno type="DOI">10.1137/090752286</idno>
		<ptr target="https://doi.org/10.1137/090752286" />
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2295" to="2317" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Compressing recurrent neural networks with tensor ring for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1811.07503v1" />
	</analytic>
	<monogr>
		<title level="m">Thirty-Third AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Action recognition with stacked fisher vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10602-1_38</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-10602-1_38" />
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="581" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Action recognition using visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1511.04119v3" />
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">SVD-Softmax: Fast softmax approximation on large vocabulary neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sung</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7130-svd-softmax-fast-softmax-approximation-on-large-vocabulary-neural-networks" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5463" to="5473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5353-two-stream-convolutional-networks-for-action-recognition-in-videos" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1409.1556v6" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">UCF101: A Dataset of 101 Human Action Classes From Videos in The Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>CRCV-TR-12-01</idno>
		<ptr target="https://www.crcv.ucf.edu/papers/UCF101_CRCV-TR-12-01.pdf" />
		<imprint>
			<date type="published" when="2012" />
			<pubPlace>Orlando, Florida, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Center for Research in Computer Vision, University of Central Florida. Central Florida Blvd</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Tensorized spectrum preserving compression for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10352v2</idno>
		<ptr target="https://arxiv.org/abs/1805.10352v3" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298594</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2015.7298594" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Compressing recurrent neural network with tensor train</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tjandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sakti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nakamura</surname></persName>
		</author>
		<idno type="DOI">10.1109/IJCNN.2017.7966420</idno>
		<ptr target="https://doi.org/10.1109/IJCNN.2017.7966420" />
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4451" to="4458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Tensor decomposition for compressing recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tjandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sakti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nakamura</surname></persName>
		</author>
		<idno type="DOI">10.1109/IJCNN.2018.8489213</idno>
		<ptr target="https://doi.org/10.1109/IJCNN.2018.8489213" />
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Text-independent speaker verification using 3D convolutional neural networks, in</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torfi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dawson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Nasrabadi</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICME.2018.8486441</idno>
		<ptr target="https://doi.org/10.1109/ICME.2018.8486441" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3D convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.510</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2015.510" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Some mathematical notes on three-mode factor analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Tucker</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF02289464</idno>
		<ptr target="https://doi.org/10.1007/BF02289464" />
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="279" to="311" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Long-term temporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2017.2712608</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2017.2712608" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1510" to="1517" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Dense trajectories and motion boundary descriptors for action recognition. International Journal of Computer Vision 103</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kl?ser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-012-0594-8</idno>
		<ptr target="https://doi.org/10.1007/s11263-012-0594-8" />
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="60" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Training and inference with integers in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1802.04680v1" />
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Tensor-train recurrent neural networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krompass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v70/yang17e.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning (ICML)</title>
		<meeting>the 34th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3891" to="3900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learning compact recurrent neural networks with block-term tensor decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00977</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2018.00977" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9378" to="9387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features using 3DCNN and convolutional LSTM for gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCVW.2017.369</idno>
		<ptr target="https://doi.org/10.1109/ICCVW.2017.369" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshop (IC-CVW)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3120" to="3128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00716</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2018.00716" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Accelerating very deep convolutional networks for classification and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2015.2502579</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2015.2502579" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1943" to="1955" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Efficient and accurate approximations of nonlinear convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298809</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2015.7298809" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1984" to="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Learning efficient tensor representations with ring structure networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cichocki</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1705.08286v3" />
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">MiCT: Mixed 3D/2D convolutional tube for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00054</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2018.00054" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="449" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Multimodal gesture recognition using 3-D convolution and convolutional LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2017.2684186</idno>
		<ptr target="https://doi.org/10.1109/ACCESS.2017.2684186" />
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="4517" to="4524" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">To prune, or not to prune: Exploring the efficacy of pruning for model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1710.01878v2" />
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00907</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2018.00907" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
