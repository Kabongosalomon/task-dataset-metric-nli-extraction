<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Online Multi-Object Tracking with Unsupervised Re-Identification Learning and Occlusion Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiankun</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Cloud AI</orgName>
								<address>
									<settlement>Redmond</settlement>
									<country>U.S</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Chu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Cloud AI</orgName>
								<address>
									<settlement>Redmond</settlement>
									<country>U.S</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">International Digital Economy Academy</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Online Multi-Object Tracking with Unsupervised Re-Identification Learning and Occlusion Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Muti-object tracking</term>
					<term>occlusion</term>
					<term>unsupervised learning</term>
					<term>re-identification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Occlusion between different objects is a typical challenge in Multi-Object Tracking (MOT), which often leads to inferior tracking results due to the missing detected objects. The common practice in multi-object tracking is re-identifying the missed objects after their reappearance. Though tracking performance can be boosted by the re-identification, the annotation of identity is required to train the model. In addition, such practice of re-identification still can not track those highly occluded objects when they are missed by the detector. In this paper, we focus on online multi-object tracking and design two novel modules, the unsupervised reidentification learning module and the occlusion estimation module, to handle these problems. Specifically, the proposed unsupervised re-identification learning module does not require any (pseudo) identity information nor suffer from the scalability issue. The proposed occlusion estimation module tries to predict the locations where occlusions happen, which are used to estimate the positions of missed objects by the detector. Our study shows that, when applied to stateof-the-art MOT methods, the proposed unsupervised re-identification learning is comparable to supervised re-identification learning, and the tracking performance is further improved by the proposed occlusion estimation module.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Multi-Object Tracking (MOT) is a fundamental computer vision task with a wide range of applications, including autonomous driving, robot navigation and video analysis. Benefiting from the advance of object detection <ref type="bibr" target="#b19">[16,</ref><ref type="bibr" target="#b38">35,</ref><ref type="bibr" target="#b28">25,</ref><ref type="bibr" target="#b62">59]</ref>, the tracking-by-detection paradigm has become popular for MOT in the past decade. Though great performance has been achieved recently <ref type="bibr" target="#b58">[55,</ref><ref type="bibr" target="#b61">58,</ref><ref type="bibr" target="#b43">40,</ref><ref type="bibr" target="#b59">56,</ref><ref type="bibr" target="#b42">39]</ref>, occlusion between objects still remains challenging for MOT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b)</head><p>Objects with different appearance Objects share similar appearance data association &amp; lost objects refinding (a) tracking results in previous frame tracking results in current frame detection results in current frame occlusion centers in current frame missed objects by detector similar appearance different appearances <ref type="figure">Figure 1</ref>: (a) The left and right are two adjacent frames from a video while the middle is an image from another scene. An apparent observation is that objects with the same identity in adjacent frames share the similar appearance and objects in different scenes (or within the same image) have different identities and appearances. (b): The left and right are tracking results in previous and current frames, and the middle are the detected objects and occlusion centers in current frame. Some lost objects that are missed by detector can be tracked with the help of predicted occlusion centers. Please refer to Section 3.2.2 for more details about lost objects refinding.</p><p>In MOT scenarios, an object may be missed by the detector due to heavy occlusion, and then reappear after a short while. In order to identify such reappeared objects, re-identification (Re-ID) is often used to associate these reappeared objects with existing tracklets. Most existing MOT works <ref type="bibr" target="#b51">[48,</ref><ref type="bibr" target="#b32">29,</ref><ref type="bibr" target="#b5">2,</ref><ref type="bibr" target="#b34">31]</ref> adopt an independent Re-ID model to learn discriminative representations for objects, which introduces extra high computational cost since each object needs to be cropped out and fed into the pre-trained Re-ID model. To achieve real-time tracking, some works try to share the Re-ID feature computation with the backbone in anchor-based detector <ref type="bibr" target="#b33">[30,</ref><ref type="bibr" target="#b50">47]</ref> or point-based detector <ref type="bibr" target="#b58">[55]</ref> by introducing an extra Re-ID branch that is parallel to the detection branch. Thanks to the sharing of feature maps between different branches, such methods can enable tracking multiple objects in a real-time way.</p><p>However, these methods <ref type="bibr" target="#b63">[60,</ref><ref type="bibr" target="#b51">48,</ref><ref type="bibr" target="#b50">47,</ref><ref type="bibr" target="#b58">55]</ref> still suffer from the scalability issue in the Re-ID representation learning. For example, <ref type="bibr" target="#b63">[60,</ref><ref type="bibr" target="#b50">47,</ref><ref type="bibr" target="#b58">55]</ref> combine several existing tracking and human detection (or Re-ID) datasets together and then learn the Re-ID representation by classifying each identity appeared in the combined dataset as one class (pseudo identity label). Such classification methods may work well for small datasets, but will encounter the learning difficulty when the identity number is huge, because the dimension of the sibling classification layer (fully connected layer) is linearly proportional to the identity number. More importantly, such supervised Re-ID module learning requires the annotation of identities, which is highly expensive and unscalable.</p><p>To address this problem, we first propose a new Re-ID module learning mechanism. It adopts an unsupervised matching based loss between two frames (images) rather than the supervised classification loss used in <ref type="bibr" target="#b63">[60,</ref><ref type="bibr" target="#b50">47,</ref><ref type="bibr" target="#b58">55]</ref>. This is based on the observation that objects with the same identity in adjacent frames share the similar appearance and objects in different scenes (or within the same image) have different identities and appearances, which is shown in <ref type="figure">Fig. 1  (a)</ref>. Compared to the aforementioned methods, this newly proposed unsupervised Re-ID learning mechanism has two merits: 1) it does not need any (pseudo) identity annotation; 2) The matching based loss is irrelevant to the number of identities, thus can be directly trained on massive videobased data that with large number of identities. In addition, the image-based data can also be used for training if we treat two augmentations of one image as the adjacent frames. 2</p><p>Though the Re-ID module can re-identify the reappeared objects after their short-term disappearance, how to proactively track the objects with highly occlusion is still challenging. This is because the severely occluded objects are easily missed by the detector, as shown in <ref type="figure">Fig. 1  (b)</ref>. For example, in anchor-based detectors <ref type="bibr" target="#b38">[35,</ref><ref type="bibr" target="#b23">20]</ref>, the Non-Maximum Suppression (NMS) module will remove highly overlapped boxes. In point-based detectors <ref type="bibr" target="#b28">[25,</ref><ref type="bibr" target="#b62">59]</ref>, as the object centers are invisible for occluded objects, it is also difficult to learn reliable center point-based features. Recently, how to address the missing detection issue caused by occlusion has attracted lots of attention. Some initial attempts <ref type="bibr" target="#b63">[60,</ref><ref type="bibr" target="#b9">6,</ref><ref type="bibr" target="#b13">10,</ref><ref type="bibr" target="#b64">61]</ref> emerge, including detecting visible parts of an object <ref type="bibr" target="#b63">[60,</ref><ref type="bibr" target="#b9">6]</ref>, using one proposal for multi-prediction <ref type="bibr" target="#b13">[10]</ref>, and using paired anchors for one detection <ref type="bibr" target="#b64">[61]</ref>.</p><p>Different from existing methods, we propose a novel occlusion estimation module to predict whether two objects are occluded. Specifically, an occlusion map which shows all possible occlusion locations in the current frame is predicted. By further combing the status of existing tracklets, we finally design a lost object refinding mechanism to find the occluded objects back.</p><p>To evaluate the effectiveness of the above two modules, we conduct extensive experiments by integrating them with different existing state-of-the-art MOT methods. For example, by replacing the supervised classification based Re-ID module in FairMOT <ref type="bibr" target="#b58">[55]</ref>, the unsupervised Re-ID learning module can still achieve comparable results on the MOT Challenge datasets <ref type="bibr" target="#b36">[33,</ref><ref type="bibr" target="#b14">11]</ref> but neither needs any identity annotation nor suffers from any scalability issue. By integrating the occlusion estimation module, both FairMOT <ref type="bibr" target="#b58">[55]</ref> and CenterTrack <ref type="bibr" target="#b61">[58]</ref> can handle the occlusion better and achieve the performance gain.</p><p>To summarize, our contributions are three-fold as below:</p><p>? We propose a novel unsupervised Re-ID learning module without using any identity information. It can be trained on video-/image-based data, and also has better scalability to datasets that with massive identities.</p><p>? We propose a new occlusion estimation module, which can effectively recognize and track occluded objects when they are missed by the detector by estimating the occlusion location.</p><p>? Both the unsupervised Re-ID learning and occlusion module can be applied to existing MOT methods in a natural way. Experimental results demonstrate the effectiveness of the proposed method.</p><p>The rest of this paper is organized as follows. In Section 2, we review some related works in terms of MOT, person re-identification, and occlusion handling. Then in Section 3, we elaborate the details of the two newly proposed modules, and the designed lost object refinding mechanism. To demonstrate the effectiveness, extensive experiment and ablation analysis are conducted in Section 4. Finally, we conclude our work in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we first provide a brief overview about the popular tracking-by-detection paradigm for MOT, and then introduce the re-identification for data association in MOT as well as existing occlusion handling mechanisms in object detection and tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Tracking-by-detection</head><p>Most existing MOT frameworks follow a tracking-by-detection paradigm thanks to the advances of object detectors <ref type="bibr" target="#b38">[35,</ref><ref type="bibr" target="#b23">20,</ref><ref type="bibr" target="#b28">25,</ref><ref type="bibr" target="#b62">59]</ref>. Specifically, an object detector is used to detect objects in each frame, then a subsequent tracker is utilized to associate the objects across different frames. In terms of temporal information usage, existing MOT methods can be categorized into online <ref type="bibr" target="#b51">[48,</ref><ref type="bibr" target="#b5">2,</ref><ref type="bibr" target="#b32">29,</ref><ref type="bibr" target="#b61">58,</ref><ref type="bibr" target="#b58">55]</ref> and offline methods <ref type="bibr" target="#b8">[5,</ref><ref type="bibr" target="#b24">21]</ref>. Online methods process video sequences frame-by-frame and track objects by only using information up to the current frame. By contrast, offline methods process video sequences in a batch and can even utilize the whole video information. From the network structure perspective, they can be further categorized into separate modeling <ref type="bibr" target="#b51">[48,</ref><ref type="bibr" target="#b5">2,</ref><ref type="bibr" target="#b32">29,</ref><ref type="bibr" target="#b8">5,</ref><ref type="bibr" target="#b24">21]</ref> and joint modeling methods <ref type="bibr" target="#b61">[58,</ref><ref type="bibr" target="#b58">55,</ref><ref type="bibr" target="#b50">47]</ref>. In separate modeling methods, the tracker is independently trained and assumes the detection results are available in advance. In joint modeling methods, the tracker is jointly trained with the detector by sharing the same feature extractor backbone. Therefore, they are often more efficient than the separate modeling methods. Both the newly proposed Re-ID module and occlusion estimation module can be naturally integrated into the online tracking-by-detection MOT system and jointly learned with the detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Re-identification for Association</head><p>Learning discriminative representations for objects is crucial to identity association in tracking. The representation can be used to re-identify lost objects that reappear after disappearing for a while. Early methods <ref type="bibr" target="#b51">[48,</ref><ref type="bibr" target="#b5">2,</ref><ref type="bibr" target="#b32">29,</ref><ref type="bibr" target="#b4">1]</ref> crop the image patch of a detected object, resize and feed it into a separate Re-ID model. It is inevitably time-consuming since the feature representation of different objects has to be computed independently. To reduce the computation, some works attempt to share the Re-ID feature computation with the backbone in anchor-based detector <ref type="bibr" target="#b33">[30,</ref><ref type="bibr" target="#b50">47,</ref><ref type="bibr" target="#b47">44,</ref><ref type="bibr" target="#b37">34]</ref> or point-based detector <ref type="bibr" target="#b58">[55]</ref> by introducing an extra Re-ID branch that is parallel to detection branch. The common practice in MOT to train the Re-ID module is to classify each identity into one class <ref type="bibr" target="#b63">[60,</ref><ref type="bibr" target="#b25">22,</ref><ref type="bibr" target="#b58">55,</ref><ref type="bibr" target="#b50">47]</ref>. There are two fundamental weaknesses of such methods: 1) the Re-ID module is less scalable especially when the amount of identities is huge, because the classifier takes up a lot of memory. For example, FairMOT <ref type="bibr" target="#b58">[55]</ref> performs about 339K classification task to train the Re-ID module. 2) the training of Re-ID module needs to be supervised by identity information. For example, several datasets dedicated for Re-ID are adopted in <ref type="bibr" target="#b58">[55,</ref><ref type="bibr" target="#b50">47]</ref>. However, the acquisition of well annotated data costs a lot.</p><p>Despite the advance in supervised Re-ID learning <ref type="bibr" target="#b21">[18,</ref><ref type="bibr" target="#b30">27,</ref><ref type="bibr" target="#b44">41,</ref><ref type="bibr" target="#b31">28]</ref>, some works for unsupervised Re-ID learning have been proposed <ref type="bibr" target="#b20">[17,</ref><ref type="bibr" target="#b49">46,</ref><ref type="bibr" target="#b25">22,</ref><ref type="bibr" target="#b58">55,</ref><ref type="bibr" target="#b52">49,</ref><ref type="bibr" target="#b18">15]</ref>. These works can be divided into two categories: pseudo identity based <ref type="bibr" target="#b25">[22,</ref><ref type="bibr" target="#b58">55,</ref><ref type="bibr" target="#b52">49,</ref><ref type="bibr" target="#b18">15]</ref> and identity free methods <ref type="bibr" target="#b49">[46]</ref>. The proposed method is also an identity free method. For the former category, pseudo identities can be obtained by clustering <ref type="bibr" target="#b52">[49,</ref><ref type="bibr" target="#b18">15]</ref> or tracking <ref type="bibr" target="#b25">[22]</ref>. However, the errors may accumulate and it is challenging to estimate the number of pseudo identities while clustering, and a trajectory of an object breaks into several short trajectories easily while tracking. For the latter category, the correspondence between adjacent frames is used <ref type="bibr" target="#b49">[46]</ref>. However, the birth and death of objects are not handled and the relation between objects within one frame is also not exploited. Inspired by these works, we propose to learn Re-ID representation in an unsupervised and matching based loss without using any (pseudo) identity information. It is built upon the observation that objects with the same identity in adjacent frames share the similar appearance and objects in different scenes (or within the same image) have different identities and appearances. Different from the works in <ref type="bibr" target="#b49">[46]</ref>, our method 1) introduces a placeholder to handle the birth and death of objects 4</p><p>and 2) makes usage of the information that objects in different scenes (or within the same image) have different identities. Besides, since our matching based loss is irrelevant to the number of identities, it does not suffer from the scalability issue and can be directly trained on massive data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Occlusions between Objects</head><p>Handling highly occluded objects is a challenging task for both detection <ref type="bibr" target="#b13">[10,</ref><ref type="bibr" target="#b64">61,</ref><ref type="bibr" target="#b9">6]</ref> and tracking <ref type="bibr" target="#b11">[8,</ref><ref type="bibr" target="#b63">60,</ref><ref type="bibr" target="#b10">7,</ref><ref type="bibr" target="#b12">9,</ref><ref type="bibr" target="#b55">52,</ref><ref type="bibr" target="#b32">29]</ref>. Some anchor-based methods are proposed to handle occlusion <ref type="bibr" target="#b13">[10,</ref><ref type="bibr" target="#b64">61,</ref><ref type="bibr" target="#b9">6]</ref> in detection, including predicting multiple instances for one proposal <ref type="bibr" target="#b13">[10]</ref> or detecting one pedestrian with a pair of anchors <ref type="bibr" target="#b64">[61]</ref> (one is for head and another is for full body). However, both works require a carefully designed NMS algorithm for post processing. In <ref type="bibr" target="#b9">[6]</ref>, a maskguided module is proposed to force the detector to pay attention to the more visible head part and thus detect the whole body of a pedestrian. Different from these methods, we propose a new occlusion estimation mechanism upon key-point based detection, which detects the locations where occlusions happen and finds the missed objects caused by detector by combining the tracking status of existing tracklets.</p><p>Instead of handling occlusions in the detection stage, there also exist some methods <ref type="bibr" target="#b11">[8,</ref><ref type="bibr" target="#b63">60,</ref><ref type="bibr" target="#b10">7,</ref><ref type="bibr" target="#b12">9,</ref><ref type="bibr" target="#b55">52,</ref><ref type="bibr" target="#b32">29]</ref> attempting to handle occlusions in the tracking stage for MOT task. The works in <ref type="bibr" target="#b11">[8,</ref><ref type="bibr" target="#b12">9,</ref><ref type="bibr" target="#b63">60,</ref><ref type="bibr" target="#b10">7]</ref> utilize the single object tracking (SOT) method for MOT. In details, a SOT tracker is created and maintained for each object. Once an object is heavily occluded and missed by the detector, the position of it could be estimated by the corresponding SOT tracker. Such practice indeed is an extra detection stage with dedicated detectors (i.e., SOT trackers). The topology between different objects is also exploited to handle occlusions for MOT <ref type="bibr" target="#b55">[52,</ref><ref type="bibr" target="#b32">29]</ref>. The hypothesis is that the topology between different objects in adjacent frames is invariant, which is positive to the association of objects, especially when some objects are partially occluded. In addition, the position of a lost object is estimated using the positions of its tracked neighbors <ref type="bibr" target="#b32">[29]</ref> based on the topology among them in previous frame.</p><p>Different from these works, our method detects the locations of occlusion in a frame, and utilizes them to refind the missed objects while tracking online. More detail, if an object is missed by the detector, then it is likely to be heavily occluded by other objects. The detected occlusion locations could be used as the prior information to find it back.</p><p>Occlusion is also one critical issue in SOT task <ref type="bibr" target="#b46">[43,</ref><ref type="bibr" target="#b16">13]</ref>. Trying to find the visible region of the single object is the main focus in SOT. However, our method detects the overlapped region between different objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>As mentioned above, our paper proposes two key modules for existing multiple object tracking systems. One is the unsupervised Re-ID module learning mechanism, which is competitive to existing supervised counterparts and has better scalability. Another is the occlusion estimation module, which predicts occlusion map to find the occluded objects back. In this section, we first elaborate the details of these two modules, and then show how to naturally integrate them with existing MOT systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Unsupervised Re-ID Learning</head><p>The proposed unsupervised Re-ID learning can be trained on both video-based and imagebased data. Note that it is only used in the training stage. Once the training procedure is finished, 5  Middle is the desired assignment results. For a better viewing, the identities of objects are encoded by color. However, the identity information is unused in our method. Two types of supervision signals are exploited. 1) Strong supervision signals: objects within the same frame should not be matched with each other. 2) Weak supervision signals: objects in one frame are likely to be matched with objects in another frame.</p><p>the trained models can be directly used to extract discriminative features for different objects, which is the same as existing supervised counterparts. For better understanding, we start with the learning from video-based data, then illustrate learning from image-based data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Learning From Video-Based Data</head><p>Let I t ? R W?H?3 be the t-th frame from one video and b t i = (x t i l , y t i t , x t i r , y t i b ) be the groundtruth bounding box of object i in frame I t . W, H are the width and height of the frame, and (x t i l , y t i t ), (x t i r , y t i b ) are the coordinates of the top-left and bottom-right corners respectively. For Re-ID representation learning, we denote the appearance feature for object i in frame I t as f t i ? R D , where D is the dimension of the appearance feature vector. Our unsupervised Re-ID representation learning mechanism is general in how f t i is calculated as long as it is differentiable. Possible solutions include cropping the image patch based on the given bounding box and feeding the cropped image patch into an extra Re-ID network like <ref type="bibr" target="#b49">[46,</ref><ref type="bibr" target="#b5">2,</ref><ref type="bibr" target="#b32">29]</ref>, extracting the ROI based appearance features by sharing the same backbone as the detector like <ref type="bibr" target="#b50">[47,</ref><ref type="bibr" target="#b47">44]</ref>, and extracting the center point based appearance features like <ref type="bibr" target="#b58">[55]</ref>. The usage of annotated bounding boxes is equivalent to traditional Re-ID task in which the well cropped image patches are provided. Even though, no identity information is used in the proposed method. In the following, the superscript t of appearance feature is omitted for simplicity.</p><p>Given two adjacent frames I t?1 and I t , let i ? {0, ..., N t?1 ?1, ..., N t?1 + N t ?1} be the index of all objects in both two frames, where N t is the number of objects in frame I t . The first N t?1 objects are from frame I t?1 and the rest are from frame I t . We have two observations: 1) objects in the same frame have different identities; 2) an object is likely to appear in both adjacent frames. Accordingly, as shown in <ref type="figure">Fig. 2</ref>, if we want to assign an object to another object, two types of supervision signals can be exploited: 1) objects within the same frame should not be matched with each other, which is regarded as strong supervision signal; 2) objects in one frame are likely to be matched with objects in another adjacent frame, which is weak supervision signal. In order to learn the Re-ID representation with such supervision signals, we first define a similarity matrix 6</p><formula xml:id="formula_0">S ? R (N t?1 +N t )?(N t?1 +N t )</formula><p>that measures the similarity between each pair of objects, where:</p><formula xml:id="formula_1">S i, j = ? ? ? ? ? ? ? f i ? f j || f i || 2 || f j || 2 i f i j, ?? otherwise.<label>(1)</label></formula><p>Obviously, S i, j = S j,i . The values in the diagonal of S are set to negative infinity to avoid assigning an object to itself (Eq. <ref type="formula" target="#formula_3">(2)</ref>). In general, if objects i and j share the same identity,</p><formula xml:id="formula_2">S i, j &gt; 0, otherwise, S i, j &lt; 0. The assignment matrix M ? R (N t?1 +N t )?(N t?1 +N t )</formula><p>can be obtained by applying row-wise softmax function to S as:</p><formula xml:id="formula_3">M i, j = e S i, j T j e S i, j T ,<label>(2)</label></formula><p>where T is the temperature of the softmax function. Consider the fact that the number of objects in adjacent frames (i.e., the size of S ) could be various, we follow the works in CycAs <ref type="bibr" target="#b49">[46]</ref> and set T = 2log(C + 1), where C = N t?1 + N t is the number of columns in S . With this adaptive temperature, the maximum values in each row are almost equally highlighted/maximized even the size of S varies. Since objects in the same frame have different identities, we can supervise the values in the top-left and right-bottom part of M by a intra-frame loss:</p><formula xml:id="formula_4">L intra id = 0?i, j&lt;N t?1 M i, j + N t?1 ?i, j&lt;N t?1 +N t M i, j .<label>(3)</label></formula><p>This corresponds to the aforementioned strong supervision signal. To leverage the weak supervision signal, we first consider the ideal case where all the objects appear in both frames for better understanding. In this case, all the objects in the frame I t?1 should be matched to the objects in the frame I t in a one-to-one manner. Then for each row in M, we encourage each object to be matched to another object with a high confidence by using the below inter-frame margin loss:</p><formula xml:id="formula_5">L inter id = i max{ max j , j j * M i, j + m ? M i, j * , 0}, where j * = arg max j M i, j .<label>(4)</label></formula><p>This shares a similar spirit as the popular triple loss, i.e., the maximum matching probability M i, j * is larger than the sub-maximum value by a pre-defined margin m (0.5 by default). Besides the above margin loss, we further add another cycle constraint loss L cycle id for M, which means the forward and backward assignment should be consistent with each other. In details, if an object i in frame I t?1 is matched with object j in frame I t , then the object j in frame I t must be matched with object i in frame I t?1 :</p><formula xml:id="formula_6">L cycle id = N t?1 ?i&lt;N t?1 +N t ,0? j&lt;N t?1 |M i, j ? M j,i |.<label>(5)</label></formula><p>Since two adjacent frames in video-based data often share some objects with the same identities, we call such two adjacent frames a positive sample for the Re-ID module training. The total loss for unsupervised Re-ID learning on such positive samples is:</p><formula xml:id="formula_7">L pos id = 1 N t?1 + N t (L intra id + L inter id + L cycle id ). (6) 7</formula><p>Unlike the above ideal case, an object in frame I t?1 may disappear in frame I t (death of objects) and an object may appear in frame I t for the first time but invisible in frame I t?1 (birth of objects) in a general case. However, for each row in assignment matrix M, the inter-frame margin loss L inter id will force the maximum value to be larger than the other values by a margin m, which is unsuitable when the corresponding object is disappeared or newly appeared since it does not share the same identity with any one of the other objects. To handle this issue, a new similarity</p><formula xml:id="formula_8">matrix S ? R (N t?1 +N t )?(N t?1 +N t +1)</formula><p>is obtained by padding a placeholder column to S . All values in the padded placeholder column are the same, which is denoted as p. The detailed discussion on p is presented in Experiments Section 4.3.1. With the existence of placeholder column, the similarity scores between disappeared/newly appeared objects and other objects are encouraged to be learned smaller than p.</p><formula xml:id="formula_9">Let M ? R (N t?1 +N t )?(N t?1 +N t +1)</formula><p>be the assignment matrix by applying row-wise softmax function to S 2 . Then we replace M with M in Eq. (3), Eq. (4) and Eq. <ref type="formula" target="#formula_6">(5)</ref> for this general case. In our implementation, the loss for this general case is adopted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Learning From Image-Based Data</head><p>To train the proposed Re-ID module on image-based data, a straightforward way is to get two augmentations of one image and treat these two augmentations as adjacent frames like the video-based data. However, we find only using the above positive sample based loss does not perform very well, since objects in the two augmentations have very similar appearance, thus not strong enough in learning discriminative Re-ID features. Considering the fact that objects in two different static images usually have different identities, we further introduce a negative sample based loss L neg id by treating two different static images from different scenes as a negative sample pair:</p><formula xml:id="formula_10">L neg id = 0?i, j&lt;N t?1 +N t M i, j .<label>(7)</label></formula><p>Similarly, in this formulation, we introduce the extra placeholder p and encourage the cosine distance between the objects in the negative pair to be less than p, which also means that all objects should be assigned to the placeholder. Note that the design of L neg id shares the same spirit with the intra-frame loss L intra id , while the inter-frame margin loss L inter id and cycle constraint loss L cycle id are not used for negative sample pairs. Therefore, the overall unsupervised Re-ID learning loss for the image based data is:</p><formula xml:id="formula_11">L id = N pos N pos + N neg L pos id + N neg N pos + N neg L neg id ,<label>(8)</label></formula><p>where N pos and N neg are the number of positive and negative samples in a batch. In our default setting, N neg N pos is set to 0.25. Although the Re-ID module can help re-identify reappeared objects after their short-term disappearance, it is inherently unable to track the occluded objects if they are not detected by the detector. To mitigate the issue caused by the missed detection, we propose an occlusion estimation module to predict whether any occlusion occurs and find lost objects back by combining the predicted occlusions and the tracking status of existing tracklets. 8 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Occlusion Estimation Module 3.2.1. Occlusion Detection</head><p>Inspired by the work of key-point estimation <ref type="bibr" target="#b28">[25,</ref><ref type="bibr" target="#b62">59]</ref>, the locations of occlusion are treated as key-points and detected by key-point estimation. Different from the above Re-ID module, the learning of the occlusion estimation module is designed in a supervised way. We automatically generate occlusion annotation based on the bounding boxes of objects, which are available in existing tracking datasets like MOT16 and MOT17 <ref type="bibr" target="#b36">[33]</ref>.</p><p>First, we need to define when an occlusion occurs. Given the bounding box coordinates of two objects i and j within one frame, their overlapped region is defined as</p><formula xml:id="formula_12">o i j = O(b i , b j ) = (x i j l , y i j t , x i j r , y i j b ).</formula><p>Considering two typical occlusion examples as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>, we define an indicator function H(?) that indicates whether an occlusion is valid or not. Only when the overlapped region occupies a large portion of object i or j, the occlusion o i j is valid. Specifically:</p><formula xml:id="formula_13">H(o i j ) = ? ? ? ? ? ? ? 1 i f A(o i j ) min(A(b i ),A(b j )) &gt; ?, 0 else,<label>(9)</label></formula><p>where A(?) is the function computing the area of a box, and ? is a hyper-parameter which is set as 0.7 by default. In order to refind an occluded object back (Section 3.2.2), we define the center point of o i j as the occlusion location of two overlapped objects. The groundtruth occlusion map Y is rendered by a 2D Gaussian kernel function based on all the valid occlusions defined in Eq. (9) as: ) is the center point of occlusion o i j . The standard deviation ? o i j of the Gaussian kernel is set to be relative to the size of o i j following the definition in <ref type="bibr" target="#b28">[25]</ref>. In our implementation, we introduce an extra CNN head to obtain the predicted occlusion center heatmap? ? R W R ? H R . It is parallel to the detection head and shares the same backbone network. R is the downsampling factor of the backbone network. Intuitively, the value? x,y ? [0, 1] denotes the probability of an occlusion center that locates in (x, y) and is supervised by: where L(?, ?) is a variant of focal loss function used in <ref type="bibr" target="#b28">[25]</ref> with two hyper-parameters ?, ? (default values are 2 and 4 respectively):</p><formula xml:id="formula_14">Y x,y = max i j G(o i j , (x, y)), subject to H(o i j ) = 1,<label>(10)</label></formula><formula xml:id="formula_15">L cen occ = x,y L(Y x,y ,? x,y ),<label>(11)</label></formula><formula xml:id="formula_16">! ! "#$ " ! ! " " ! ! " ! % ! " # $ !&amp; ! " ! ! " (a) (b) (c) (d)</formula><formula xml:id="formula_17">L(y,?) = ? ? ? ? ? ? ? ? (1 ??) ? log(?) i f y = 1, ? (1 ? y) ? (?) ? log(1 ??) else.<label>(12)</label></formula><p>Considering that R is often larger than 1, we take the inspiration from <ref type="bibr" target="#b62">[59]</ref> and add another CNN head to produce an offset heatmap? ? R W R ? H R ?2 , which can help compensate the quantization error in generating the occlusion center heatmap Y. The simple L1 loss is used to regress the center offset:</p><formula xml:id="formula_18">L o f f occ = i j |? p i j R ? ( p i j R ? p i j R )|.<label>(13)</label></formula><p>Need to note that the offset supervision is only given at the center locations. The overall occlusion estimation loss is:</p><formula xml:id="formula_19">L occ = 1 i, j H(o i j ) (L cen occ + L o f f occ ).<label>(14)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Lost Object Refinding</head><p>While tracking online, the occlusion estimation module is used to detect the possible occlusion locations, i.e., the center points of overlapped regions between different objects in a frame. For severely occluded objects, they are easily missed by the detector (thus lost by the tracker). In such case, the corresponding occlusion locations can be used as the prior information to refind them. Specifically, given the set of existing tracklets in frame t ? 1 and the set of newly detected objects in frame t, we match the newly detected objects with existing tracklets. If some tracklets cannot match with any newly detected objects, we treat them as potential lost tracklets/objects and try to find them back. The detailed tracking logic is elaborated in Algorithm 1. Through the refinding of lost objects, the number of false negative objects could be reduced, leading to a higher tracking performance.</p><p>Once there exist some potential lost objects, we propose to find the lost objects back by using the predicted occlusion locations and the motion information of the corresponding tracklets, which can be estimated by Kalman filter. In details, suppose we want to refind the lost object i in I t , its bounding box in I t?1 is denoted as</p><formula xml:id="formula_20">b t?1 i = (x t?1 i l , y t?1 i t , x t?1 i r , y t?1 i b ).</formula><p>We first predict its location at I t via Kalman filter and denote the location asb</p><formula xml:id="formula_21">t i = (x t i l ,? t i t ,x t i r ,? t i b ).</formula><p>Then we search all the detected objects that possibly occlude i by considering the estimated occlusion centers close tob t i . The detailed search process is illustrated in <ref type="figure">Fig. 4</ref>. Specifically, for each box b t j that possibly overlapped withb t i , we first calculate the overlapped region as?</p><formula xml:id="formula_22">i j = O(b t i , b t j )</formula><p>. Then we get a score between? i j and one of the predicted occlusion centersp t ik = (x t ik ,? t ik ) that locates withinb t i using the aforementioned Gaussian kernel function. Finally, we choose the best matched pair by ( j , k ) = argmax j,k G(? t i j ,p t ik ). If G(? t i j ,p t ik ) &gt; ? o (? o = 0.7 by default), then object i is likely to be occluded by object j , leading to missing detection. Suppose that the box b t j and occlusion centerp t ik are all correctly estimated and the size of object i in adjacent frames keeps unchanged, the estimated box b t i for object i can be calculated as:</p><formula xml:id="formula_23">? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? x t i l = F (x t i l ,x t i r , x t j l , x t j r ,x t ik ), y t i t = F (? t i t ,? t i b , y t j t , y t j b ,? t ik ), x t i r = x t i l +x t i r ?x t i l , y t i b = y t i t +? t i b ?? t i t ,<label>(15)</label></formula><p>where F (a 1 ,</p><formula xml:id="formula_24">a 2 , b 1 , b 2 , z) = ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 2z ? b 1 ? (a 2 ? a 1 ) i f a 1 ? b 1 and a 2 ? b 2 , z ? (a 2 ? a 1 )/2 i f a 1 &gt; b 1 and a 2 ? b 2 , 2z ? b 2 i f a 1 &gt; b 1 and a 2 &gt; b 2 , a 1 else.<label>(16)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Integration into Existing Methods</head><p>The above two modules can be naturally integrated into existing state-of-the-art MOT systems, such as <ref type="bibr" target="#b50">[47,</ref><ref type="bibr" target="#b58">55,</ref><ref type="bibr" target="#b61">58,</ref><ref type="bibr" target="#b32">29]</ref>. In details, as long as the original MOT system has or is able to add the differentiable Re-ID feature learning part, the proposed Re-ID learning mechanism can be applied into it and allows large scale unsupervised Re-ID learning. For the occlusion estimation 11</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Tracking logic between Two Consecutive Frames</head><p>Input :</p><formula xml:id="formula_25">T t?1 = {(b t?1 i , id i , w i , p t?1 i )} N t?1 i=1 : cached trackltes in frame t ? 1 with box b t?1 i , identity id i , number of consecutive frames w i being lost, center point p t?1 i . D t = {(b t j , p t j )} N t j=1 : detected objects in frame t with box b t j , center point p t j . P t = {p t k } N t occ k=1</formula><p>: predicted occlusion center points in frame t Output: T t : cached tracklets in frame t. B t : tracking results in frame t. 1 Step1: Initialize as empty set 2 T t ? ?, B t ? ? 3 Step2: Assign objects to tracklets, and get the assigned index pairs, lost tracklet and unassigned object indices</p><formula xml:id="formula_26">4 {(i a , j a )} A a=1 , {i l } L l=1 , { j u } U u=1 ? Assign(T t?1 , D t ) 5</formula><p>Step3: Update tracklets with assigned objects</p><formula xml:id="formula_27">6 for (i a , j a ) ? {(i a , j a )} A a=1 do 7 w ja ? 0, id ja ? id ia 8 T t ? T t ? {(b t ja , id ja , w ja , p t ja )} 9 B t ? B t ? {b t ja , id ja } 10 Step4: Initialize new tracklets with unassigned objects 11 for j u ? { j u } U u=1 do 12 w ju ? 0, id ju ? NewID() 13 T t ? T t ? {(b t ju , id ju , w ju , p t ju )} 14 B t ? B t ? {b t ju , id ju } 15</formula><p>Step5: Handle lost tracklets <ref type="bibr" target="#b19">16</ref> for i l ? {i l } L l=1 do <ref type="bibr" target="#b20">17</ref> if w i l &lt; ? w then /*? w is the time window threshold*/ 18 /* select the occlusion point and box in current frame to recover the box for lost tracklet */</p><formula xml:id="formula_28">19 j , k = argmax j,k G(O(b t i l , b t j ),p t k ) 20 if G(O(b t i l , b t j ),p t k ) &gt; ? o then 21 b t i l ? get occluded object box based on Eq. (15) 22 T t ? T t ? {(b t i l , id i l , w i l , p t i l )} 23 B t ? B t ? {b t i l , id i l } 24 else 25 w i l ? w i l + 1 26 T t ? T t ? {(b t i l , id i l , w i l , p t i l )} 27 Return T t , B t .</formula><p>module, it is compatible with MOT systems that are equipped with the modern CNN detector. We can simply implement it by adding the occlusion estimation module as a parallel head to the original detection head and sharing the same CNN backbone. In <ref type="figure" target="#fig_4">Fig. 5</ref>, we take the popular tracking framework FairMOT <ref type="bibr" target="#b58">[55]</ref> as an example and show the integrated framework. The original FairMOT has one point based detection module, and one supervised Re-ID module that is learnt by classifying each identity as one independent class, which needs costly Re-ID annotation and suffers from the aforementioned dimension explosion problem for huge identity number. We integrate the two proposed modules by changing its Re-ID learning mechanism and adding the occlusion estimation module as described above. In the following experiments, besides FairMOT, we also try to integrate our modules into CenterTrack <ref type="bibr" target="#b61">[58]</ref>. Since CenterTrack does not have the Re-ID feature learning module, we only incorporate the occlusion estimation module into it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>The state-of-the-art methods FairMOT <ref type="bibr" target="#b58">[55]</ref> and CenterTrack <ref type="bibr" target="#b61">[58]</ref> are both implemented based on the key-point based detector CenterNet <ref type="bibr" target="#b62">[59]</ref>. We integrate the proposed modules into them to demonstrate the effectiveness. The occlusion loss L occ is added to the detection loss of FairMOT and CenterTrack with the weight of 0.5. The estimation branch for occlusion centers and offsets in the occlusion estimation module both consists of one 3 ? 3 convolutional layer whose output is a 256-channel feature map and one 1 ? 1 convolutional layer that produces the task-specific heatmap. Between these two layers, a ReLU activation function is adopted. For the occlusion center branch, the output heatmap? is activated by the sigmoid function, while for the occlusion offset heatmap?, no activation function is adopted. When replacing the supervised Re-ID learning in FairMOT <ref type="bibr" target="#b58">[55]</ref> with our unsupervised Re-ID learning, we directly substitute the original Re-ID loss in FairMOT with the loss L id in Eq. (8) while keeping other unchanged. The dimension D of Re-ID feature is set to 256.</p><p>By default, the Adam optimizer <ref type="bibr" target="#b27">[24]</ref> with the initial learning rate 1e ? 4 is used. The models in FairMOT and CenterTrack are trained for 30 and 70 epochs respectively. For the positive samples of unsupervised Re-ID learning from video-based data, the adjacent frames are randomly sampled from consecutive 20 frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Datasets</head><p>The proposed method is evaluated on the standard MOTChallenge datasets, including MOT16, MOT17 <ref type="bibr" target="#b36">[33]</ref> and MOT20 <ref type="bibr" target="#b14">[11]</ref>. There are 7 training and other 7 testing videos in MOT16. MOT17 contains the same videos as MOT16 but with different annotations. MOT20 contains 4 training videos and 4 testing videos. The videos in MOT20 are captured in crowd scenes, which are quite different from those in MOT16 and MOT17. External dataset CrowdHuman <ref type="bibr" target="#b41">[38]</ref> is adopted for pre-training. Note that pre-training on external dataset is a common practice in previous works <ref type="bibr" target="#b39">[36,</ref><ref type="bibr" target="#b58">55,</ref><ref type="bibr" target="#b50">47,</ref><ref type="bibr" target="#b48">45,</ref><ref type="bibr" target="#b59">56]</ref>. Besides the bounding box annotation for detection, identity information is also provided in MOT16, MOT17 and MOT20. However, the identity information is not used in our training process.</p><p>We adopt the standard metrics of MOTChallenge for evaluation, including: Multi-Object Tracking Accuracy (MOTA) <ref type="bibr" target="#b6">[3]</ref>, Multi-object Tracking Precision (MOTP) <ref type="bibr" target="#b6">[3]</ref>, ID F1 Score (IDF1), Mostly Tracked objects (MT), Mostly Lost objects (ML), Number of False Positives 13  <ref type="bibr" target="#b29">[26]</ref> and number of Fragments (Frag). Some other metrics, including R1 and mAP, are also introduced for the evaluation of different Re-ID methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>Without losing generality, we do ablation study on the MOT17 dataset for simplicity, following the work in FairMOT <ref type="bibr" target="#b58">[55]</ref> and CenterTrack <ref type="bibr" target="#b61">[58]</ref>. Since no validation data is provided in the MOTChallenge, the common practice is to split each video in the training set into two half videos, the first part is for training and the second part is for validation <ref type="bibr" target="#b58">[55,</ref><ref type="bibr" target="#b61">58,</ref><ref type="bibr" target="#b40">37,</ref><ref type="bibr" target="#b48">45,</ref><ref type="bibr" target="#b53">50]</ref>. No external dataset is used if not specified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Unsupervised Re-ID Learning</head><p>In this sub-section, we conduct the ablation study for the unsupervised Re-ID learning module by integrating it into the MOT system FairMOT <ref type="bibr" target="#b58">[55]</ref>.</p><p>Comparison with other Re-ID learning methods: We first compare different learning methods for the Re-ID feature module, including the proposed method (UTrack), the unsupervised method CycAs <ref type="bibr" target="#b49">[46]</ref> and the supervised method in FairMOT <ref type="bibr" target="#b58">[55]</ref>. CycAs utilizes the cycle assignment consistence to learn the Re-ID module, which is the latest identity free method for Re-ID learning. FairMOT treats each identity as a class and the Re-ID module is trained in a supervised classification manner. The detailed comparison results are shown in Tab. 1. In order to demonstrate the effectiveness of the Re-ID module, the tracking results without Re-ID are also presented in the first row and denoted as FairMOT w/o . Note that, except the training method of the Re-ID module, all the other parts remain the same.</p><p>Comparing the first row with the remaining three rows, we can find that MOTA, IDF1 and IDS are all greatly improved with the Re-ID module. For example, our UTrack improves MOTA and IDF1 by 1.8% and 10.8% respectively when the tracker is equipped with the Re-ID module trained by the proposed unsupervised method. Compared to the supervised Re-ID used in FairMOT 3 , our UTrack can achieve almost the same MOTA even without using any Re-ID supervision. Though FairMOT possesses a slightly lower IDS, our method UTrack performs better in IDF1 by 1.6%, demonstrating the effectiveness of the proposed unsupervised Re-ID learning method. More importantly, our method does not suffer from the dimension explosion issue and is more friendly to the real large-scale MOT systems. Comparing our method UTrack with CysAs <ref type="bibr" target="#b49">[46]</ref>, both of whom are unsupervised methods, we find that both trackers achieve the same IDS, but our method performs better in terms of MOTA and IDF1. We attribute this to the introduction of the placeholder in the similarity matrix and the strong supervision signal L intra id within the same frame <ref type="figure" target="#fig_1">(Eq. (3)</ref>).</p><p>(a) FairMOT (b) FairMOT+CycAs <ref type="bibr" target="#b49">[46]</ref> (c) FairMOT+UTrack (Ours) <ref type="figure">Figure 6</ref>: Visualized Re-ID features for identities in MOT17 validation set using t-SNE <ref type="bibr" target="#b35">[32]</ref>. From left to right are the features learnt by (a): the supervised method in FairMOT <ref type="bibr" target="#b58">[55]</ref>. (b): the unsupervised method CysAs <ref type="bibr" target="#b49">[46]</ref>.  In <ref type="figure">Fig. 6</ref>, we visualize the learned Re-ID features for different methods by t-SNE <ref type="bibr" target="#b35">[32]</ref>. As we can see, compared with the supervised method originally used in FairMOT <ref type="bibr" target="#b58">[55]</ref> and the unsupervised method CycAs <ref type="bibr" target="#b49">[46]</ref>, the features for the same identities are better grouped by the proposed unsupervised method. We further present the assignment between two frames in <ref type="figure">Fig. 7</ref>. As we can see, the object pair with the same identity achieves a much higher similarity score than the counterpart with different identities.</p><p>Analysis of Re-ID loss: We then do some ablation analysis on the Re-ID loss L pos id in Eq. (6). It consists of three components. 1) L intra id : the loss used to avoid assigning an object to another one that locates in the same frame. 2) L inter id : the loss that makes sure an object can be successfully matched with another object that locates in different frames or the placeholder. 3) L cycle id : the loss that constraints that the forward and backward assignment should be consistent with each other. Results are shown in Tab. 2. It can be seen that, the performance gain from different Re-ID losses in terms of MOTA is not that significant since MOTA is highly affected by the detection performance, i.e., FN and FP. But both IDS and IDF1 are improved by introducing more constraints on the Re-ID module, which demonstrates the effectiveness of each loss in   Eq. <ref type="bibr" target="#b9">(6)</ref>. Discussion on the temperature T : In our default settings, T is adaptive to the number of objects. To show its superiority, we train several models with different fixed temperatures, the tracking results are shown in Tab. 3. As we can see, provided with larger temperatures (T ? 4), the trackers achieve better IDF1 score but degraded MOTA score compared with those trackers equipped with smaller temperatures (T ? 3). With the help of adaptive temperature, the tracker obtains a better balance between IDF1 and MOTA scores.</p><p>Discussion on the placeholder: Finally, we have a discussion on the value of placeholder p. As mentioned before, the placeholder p is introduced to handle the birth and death of objects, i.e., the newly appeared objects in I t and the objects appearing in I t?1 but disappearing in I t should be assigned to the placeholder. Let S i, j be the cosine similarity between the Re-ID feature of objects i and j. For sensible matching, S i, j should be greater than p if object i and j have the same identity, otherwise S i, j &lt; p.</p><p>Taking into intuitive consideration that the cosine similarity between the Re-ID feature of two objects should be positive if they have the same identity, otherwise negative, it is straightforward to set p = 0. However, at the early training stage, we observe that the variance of the values in S is small (about 0.015) and the cosine similarity between any pair of objects is around 0.75. So it is hard for the model to handle the birth and death of objects well at the beginning if p = 0. Therefore, we set p as the dynamic mean of the values in S except the diagonal values by default. Interestingly, we observe that the mean of the values in S is about 0 after convergence when trained with this strategy.</p><p>The results of three different placeholder settings are shown in Tab. 4: without placeholder p, p = 0, and p as the dynamic mean. As we can see that the tracker achieves the best results in terms of MOTA, IDF1 and FN when the placeholder is set to the dynamic mean of similarity values. Compared to the setting without the placeholder, using placeholder can achieve much lower IDS, which demonstrates the effectiveness of the placeholder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Occlusion Estimation Module</head><p>To demonstrate the effectiveness of the proposed occlusion estimation module, we apply it to both FairMOT <ref type="bibr" target="#b58">[55]</ref> and CenterTrack <ref type="bibr" target="#b61">[58]</ref>. Another work of lost object refinding, GSM <ref type="bibr" target="#b32">[29]</ref>, is also evaluated. As shown in Tab. 5, with the help of the occlusion estimation module (OccE), many lost objects can be found back, leading to lower FN. Though the FP is slightly increased, <ref type="table">Table 5</ref>: Tracking results on the MOT17 validation set with the occlusion estimation module (OccE) integrated in Cen-terTrack <ref type="bibr" target="#b61">[58]</ref> and FairMOT <ref type="bibr" target="#b58">[55]</ref> to show its effectiveness respectively.  the main tracking metric MOTA is still improved. In addition, more objects can be mostly tracked (MT), and fewer objects are mostly lost (ML). Besides, IDS are greatly reduced. Compared with OccE, GSM introduces more FP, resulting a slightly lower MOTA. Though lower IDS is achieved by GSM, the construction and matching of graphs are time consuming. In <ref type="figure" target="#fig_6">Fig. 8</ref>, some typical cases where occlusion happens are presented. For each case, the left to the right columns are the tracking results in the previous frame, the detection results and the predicted occlusions, as well as the tracking results in the current frame respectively. As we can see, some occluded objects are missed by the detector and are challenging for existing MOT systems to track successfully. Without such refinding mechanism, they can only handle the detected objects and keep undetected boxes untracked. By integrating the proposed occlusion estimation module and the accompanying object refinding algorithm, we can refind the missed objects back and link them with existing tracklets.</p><p>Discussion on the threshold ?: The hyper-parameter ? in Eq. (9) controls the valid number of occlusions in a frame while training. We evaluate the impact of ? on tracking performance by   applying it to FairMOT <ref type="bibr" target="#b58">[55]</ref>. Results are shown in Tab. 6. Specifically, the tracker achieves more FN but less FP with a larger ?. This is because fewer occlusions could be detected if the model is trained with a larger ?, thus fewer lost objects could be found back. However, the overall tracking performance (MOTA) is not sensitive to the value of ? and we set it to 0.7 by default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3.">Pre-Training on Image-Based Data</head><p>The proposed method can benefit from pre-training on image-based data. To demonstrate it, we use the CrowdHuman dataset <ref type="bibr" target="#b41">[38]</ref> for pre-training. Need to note that, the original FairMOT <ref type="bibr" target="#b58">[55]</ref> also pre-trains their model on CrowdHuman and its Re-ID module is trained with pseudo identity labels, i.e., a unique identity is assigned to each annotated box, in a classification manner. There are about 339K boxes in CrowdHuman, so the pseudo identity number is massive, causing the number of parameters in the classifier to be even larger than the total number of parameters in the other modules (54.0M vs. 19.4M). By contrast, there are no extra parameters introduced in the proposed unsupervised Re-ID learning method. While training, two augmentations of an image are treated as a positive sample pair, and two different static images are sampled as a negative sample pair.</p><p>We first compare the re-identification capability of the proposed unsupervised Re-ID learning method UTrack, pseudo identity based method FairMOT <ref type="bibr" target="#b58">[55]</ref>, and the latest identity free method CysAs <ref type="bibr" target="#b49">[46]</ref> in Tab. 7. While evaluation, each tracklet in MOT17 train split is divided into two half parts. The first part is used as query and the rest part is used as gallery. As we can see, both CysAs and the proposed UTrack achieve much better results than FairMOT. Compared with CysAs, UTrack obtains 1.6% higher R1 and 1.2% higher mAP. We argue this to the introduction of placeholder and the strong supervision signal.</p><p>We then evaluate the impact of N neg N pos on re-identification capability while training on imagebased data in Tab. 8. The model achieves the best R1 score when N neg N pos = 3 7 , but achieves the best mAP score when N neg N pos = 2 8 . Interestingly, we find that the Re-ID module cannot converge if N neg N pos ? 1. We set N neg N pos to <ref type="bibr">2 8</ref> for the balance between R1 and mAP scores. In Tab. 9, we show the tracking results of trackers on MOT17 dataset by directly applying the CrowdHuman pre-trained model without fine-tuning. Compared with the supervised Re-ID learning in FairMOT and unsupervised Re-ID learning CysAs <ref type="bibr" target="#b49">[46]</ref>, our UTrack performs much better in terms of MOTA, IDF1, MT, FN and IDS, demonstrating the superiority of the proposed  unsupervised Re-ID learning method. We further show the results in Tab. 10 by fine-tuning the CrowdHuman pre-trained models (marked by ) on the MOT17 dataset. For reference, we also provide the results without pretraining. From the results, we can observe that pre-training on the image-based data can generally boost the overall tracking performance. Compared to the supervised Re-ID learning method used in FairMOT, our unsupervised tracker UTrack can achieve very comparable tracking performance without using any ID supervision. By contrast, pre-training on image-based data using CysAs cannot improve the IDF1 performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results on MOTChallenge</head><p>Though the main focus of this paper is not to achieve the state-of-the-art performance, we still show the tracking results on the standard MOTchallenge benchmark by integrating the proposed modules into FairMOT <ref type="bibr" target="#b58">[55]</ref> and CenterTrack <ref type="bibr" target="#b61">[58]</ref> respectively. For notation simplicity, we denote the variant of FairMOT that integrates our unsupervised Re-ID learning and occlusion estimation module as "OUTrack fm ", and the variant of CenterTrack that integrates our occlusion estimation module as "OTrack ct ". We conduct the evaluation on both public and private detection. For the public detection evaluation, we follow the works in <ref type="bibr" target="#b5">[2,</ref><ref type="bibr" target="#b32">29,</ref><ref type="bibr" target="#b61">58,</ref><ref type="bibr" target="#b22">19,</ref><ref type="bibr" target="#b26">23,</ref><ref type="bibr" target="#b40">37,</ref><ref type="bibr" target="#b48">45,</ref><ref type="bibr" target="#b53">50]</ref> to refine the public detections and keep the bounding boxes that are close to the tracked objects. Note that only the provided training sequences are used to train the model for public detection evaluation. For private detection, we follow CenterTrack and FairMOT to pre-train our tracker with the CrowdHuman dataset. However, the original FairMOT also involves a mixture dataset that consists of five extra datasets 4 . For fair comparison, we train FairMOT on CrowdHuman and MOTChallenge without these five extra datasets, which is denoted as FairMOT ? .  Results are shown in Tab. 11. Overall, our FairMOT based tracker OUTrack fm achieves stateof-the-art performance on all datasets and our CenterTrack based tracker OTrack ct outperfoms CenterTrack by a large margin. Compared to offline methods, such as Lif T <ref type="bibr" target="#b24">[21]</ref> and MPNTrack <ref type="bibr" target="#b8">[5]</ref>, both OUTrack fm and OTrack ct have a higher Frag, the reason is that the short broken tracklets can be linked to a long trajectory by a post process, which is not allowed in online trackers.</p><p>Compared with FairMOT, OUTrack fm achieves very comparable performance on MOT17 with much less pretraining data and better performance with the same pretraining data in terms of MOTA. As for IDF1, FairMOT performs better than OUTrack fm by 2.1%. The main reason is that FairMOT supervises Re-ID module with identity information, while the Re-ID module in OUTrack fm is trained in a totally unsupervised manner. It is interesting that OUTrack fm performs much better than FairMOT on MOT20 with private detection. The main reasons may be in two folds: 1) FairMOT fine-tunes the models on MOT20 after pre-training on the mixture dataset, while the mixed datasets used in FairMOT are different from MOT20 that is captured in crowd scenes. However, we fine-tune the model on MOT20 after pre-training on CrowdHuman and the images in CrowdHuman are all collected from crowd scenes. 2) A higher detection confidence (0.5) is used in OUTrack fm , since FairMOT (0.3) has a much higher FP. When training FairMOT on the same dataset as OUTrack fm and increasing the detection confidence to 0.5, the main metric MOTA is greatly improved. But OUTrack fm still performs better.</p><p>Compared to CenterTrack, OTrack ct performs much better on MOT17 for both the private and public detection settings. For example, our OTrack ct surpasses CenterTrack on MOT17 in terms of MOTA by 2.4% and 1.2% with public and private detection, respectively. Through finding the lost objects based on the predicted occlusions, a higher Recall is also achieved. Though more FP are involved when finding the missed objects, the FN is greatly reduced.</p><p>In <ref type="figure" target="#fig_7">Fig. 9</ref>, some tracking results and the predicted occlusion heatmaps of OUTrack fm are presented. For the first case, we can see that some objects (63 and 35) can be re-identified when they reappeared after a short-term disappearance, demonstrating the effectiveness of our unsupervised Re-ID learning method. For both cases, occlusions between different objects can be effectively detected by occlusion estimation module and those highly occluded objects still can be tracked, 21</p><p>indicating the effectiveness of our occlusion estimation module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we present a new occlusion-aware multi-object tracking framework. It involves two key modules: unsupervised Re-ID learning and occlusion estimation module. The unsupervised Re-ID learning adopts an unsupervised matching based loss between adjacent frames, whose motivation is that objects with the same identity in adjacent frames share similar appearance and objects in two images that from different scenes (or within the same image) have different identities and appearances. Compared to the supervised classification based Re-ID learning, it does not suffer from the dimension explosion issue for a large identity number and is more friendly to real large-scale applications. The occlusion estimation module can alleviate the tracking lost issue caused by missing detection. It can find the occluded objects back by estimating the occlusion map that shows all possible occlusion locations. The two proposed modules can be applied to existing MOT systems in a natural way and demonstrate their effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgement</head><p>This work is supported by the National Natural Science Foundation of China (No. U20B2047, No. 62002336).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 Figure 2 :</head><label>12</label><figDesc>The proposed un-supervised Re-ID learning method. Left and right are the two adjacent frames and the objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Typical occlusion cases. Translucent blue areas denote the positions where occlusions happen and red circles are the occlusion centers. Left: A small box covered by a larger box. Right: Two boxes overlapped with each other.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>)x i j l +x i jr 2 , y i jt +y i j b 2</head><label>22</label><figDesc>where G(o i j , (x, y)) = exp(? ((x,y)? is the Gaussian kernel function, and p i j = (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :i from b t? 1 i</head><label>41</label><figDesc>Illustration of lost object refinding. (a): Tracking results in the previous frame. (b): Predicted boxb t via motion, the overlapped boxes withb t i , and the predicted occlusion center points that locate withinb t i . (c): The chosen occlusion centerp t ik and boxb t j used for recovering b t i . (d): Recovered box b t i for the lost object i.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Illustration of applying of our unsupervised Re-ID module and occlusion module to FairMOT<ref type="bibr" target="#b58">[55]</ref>. While integrating, the occlusion module is added to be parallel with detection module which remains unchanged. Re-ID features from two frames are needed to train the Re-ID module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1 Figure 7 :</head><label>17</label><figDesc>Illustration of assignment between two frames. Left and right are the detection results, in which the color of boxes and the numbers attached to the boxes indicate the identities. Middle is the assignment matrix M .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Some cases where lost objects are re-found by the proposed occlusion estimation module. For each case, from left to right are tracking results in the previous frame, detection results, predicted occlusions and tracking results in the current frame respectively. Specifically, objects 36, 3, 11 and 9 are found back for cases (a), (b), (c) and (d), respectively. Note that the images here are cropped from original images for better viewing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>The tracking results and predicted occlusion heatmaps of OUTrack fm .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Tracking results on the MOT17 validation set with respect to different Re-ID learning methods. FairMOT 3 and FairMOT w/o mean the results with or without the original supervised Re-ID method used in FairMOT. ? means the larger the better and ? means the smaller the better. Best results are shown in bold and highlighted with underline.</figDesc><table><row><cell>Trackers</cell><cell>MOTA? IDF1 ? MT? ML? FP? FN ? IDS?</cell></row><row><cell>FairMOT w/o</cell><cell>65.8% 61.0% 133 62 2620 14735 1098</cell></row><row><cell>FairMOT 3 [55]</cell><cell>67.5% 70.2% 134 55 2814 14263 492</cell></row><row><cell cols="2">FairMOT+CysAs [46] 67.0% 70.8% 134 60 2631 14681 503</cell></row><row><cell>FairMOT+UTrack</cell><cell>67.6% 71.8% 137 63 2621 14388 503</cell></row><row><cell cols="2">(FP), Number of False Negatives (FN), Number of Identity Switches (IDS)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Analysis of Re-ID loss on validation set.</figDesc><table><row><cell>L inter id</cell><cell>L intra id</cell><cell>L id cycle</cell><cell>MOTA? IDF1 ? MT? ML? FP? FN ? IDS?</cell></row><row><cell></cell><cell></cell><cell></cell><cell>67.3% 69.9% 134 58 2422 14669 592</cell></row><row><cell></cell><cell></cell><cell></cell><cell>67.5% 70.6% 133 63 2492 14566 531</cell></row><row><cell></cell><cell></cell><cell></cell><cell>67.6% 71.8% 137 63 2621 14388 503</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>The impact of temperature T on tracking performance. Evaluated on MOT17 validation set. C is the number of columns in similarity matrix.</figDesc><table><row><cell>Tracker</cell><cell>T</cell><cell>MOTA? IDF1 ? MT? ML? FP? FN ? IDS?</cell></row><row><cell></cell><cell>1.0</cell><cell>67.5% 63.2% 145 57 2778 13978 788</cell></row><row><cell></cell><cell>2.0</cell><cell>67.9% 63.5% 147 58 2717 13840 789</cell></row><row><cell>FairMOT+UTrack</cell><cell>3.0 4.0</cell><cell>67.6% 65.0% 147 57 2894 13791 838 65.5% 69.1% 128 62 3186 14874 565</cell></row><row><cell></cell><cell>5.0</cell><cell>65.4% 70.1% 129 59 3130 15059 502</cell></row><row><cell></cell><cell cols="2">2log(C + 1) 67.6% 71.8% 137 63 2621 14388 503</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Tracking results on the MOT17 validation set with respect to different settings for placeholder.</figDesc><table><row><cell cols="6">placeholder MOTA? IDF1 ? MT? ML? FP?</cell><cell cols="2">FN ? IDS?</cell></row><row><cell>w/o</cell><cell>67.4%</cell><cell>71.1%</cell><cell>142</cell><cell>62</cell><cell cols="2">2457 14588</cell><cell>557</cell></row><row><cell>zero</cell><cell>66.9%</cell><cell>70.7%</cell><cell>138</cell><cell>60</cell><cell cols="2">2685 14735</cell><cell>494</cell></row><row><cell>mean</cell><cell>67.6%</cell><cell>71.8%</cell><cell>137</cell><cell>63</cell><cell cols="2">2621 14388</cell><cell>503</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>The impact of ? on tracking performance. Evaluated on MOT17 validation set.</figDesc><table><row><cell>trackers</cell><cell>? MOTA? IDF1 ? MT? ML? FP? FN ? IDS?</cell></row><row><cell></cell><cell>0.3 68.3% 70.4% 145 57 3228 13501 412</cell></row><row><cell>FairMOT+UTrack+OccE</cell><cell>0.5 68.3% 72.0% 143 55 3114 13609 426 0.7 68.5% 72.0% 142 57 2840 13797 396</cell></row><row><cell></cell><cell>0.9 68.4% 71.8% 141 58 2758 13928 397</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Comparison of re-identification capability of different methods on the MOT17 train split by directly applying the pre-trained models on CrowdHuman<ref type="bibr" target="#b41">[38]</ref> without fine-tuning.</figDesc><table><row><cell>Trackers</cell><cell>R1?</cell><cell>mAP ?</cell></row><row><cell>FairMOT[55]</cell><cell>42.9%</cell><cell>25.4%</cell></row><row><cell>FairMOT+CysAs [46]</cell><cell>54.8%</cell><cell>32.9%</cell></row><row><cell>FairMOT+UTrack</cell><cell>56.4%</cell><cell>34.1%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>The impact of the ratio between the number of negative and positive samples on re-identification capability. These models are trained on CrowdHuman<ref type="bibr" target="#b41">[38]</ref> and evaluated on MOT17 train split without fine-tuning.</figDesc><table><row><cell>Tracker</cell><cell>N neg /N pos</cell><cell>R1?</cell><cell>mAP ?</cell></row><row><cell></cell><cell>1/9</cell><cell>54.6%</cell><cell>32.8%</cell></row><row><cell></cell><cell>2/8</cell><cell>56.4%</cell><cell>34.1%</cell></row><row><cell>FairMOT+UTrack</cell><cell>3/7 4/6</cell><cell>58.1% 57.9%</cell><cell>31.8% 31.3%</cell></row><row><cell></cell><cell>5/5</cell><cell cols="2">not converged</cell></row><row><cell></cell><cell>6/4</cell><cell cols="2">not converged</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Tracking results on the MOT17 validation set by directly applying the pre-trained trackers on CrowdHuman<ref type="bibr" target="#b41">[38]</ref> without fine-tuning.</figDesc><table><row><cell>Trackers</cell><cell>MOTA? IDF1 ? MT? ML? FP? FN ? IDS?</cell></row><row><cell>FairMOT [55]</cell><cell>64.0% 64.6% 138 63 2130 16806 501</cell></row><row><cell cols="2">FairMOT+CysAs [46] 63.9% 64.9% 137 62 2781 16105 594</cell></row><row><cell>FairMOT+UTrack</cell><cell>64.8% 69.2% 143 64 2390 16203 418</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Tracking results on the MOT17 validation set by fine-tuning the CrowdHuman pre-trained trackers on the MOT17 dataset. Trackers marked with/without correspond pre-training on CrowdHuman or not.</figDesc><table><row><cell>trackers</cell><cell>MOTA? IDF1 ? MT? ML? FP? FN ? IDS?</cell></row><row><cell></cell><cell>impact on Re-ID module</cell></row><row><cell>FairMOT [55]</cell><cell>67.5% 70.2% 134 55 2814 14263 492</cell></row><row><cell>FairMOT [55]</cell><cell>70.7% 74.7% 172 48 3255 12171 431</cell></row><row><cell>FairMOT+CysAs [46]</cell><cell>67.0% 70.8% 134 60 2631 14681 503</cell></row><row><cell>FairMOT+CysAs [46]</cell><cell>69.4% 70.8% 158 46 3412 12515 592</cell></row><row><cell>FairMOT+UTrack</cell><cell>67.6% 71.8% 137 63 2621 14388 503</cell></row><row><cell>FairMOT+UTrack</cell><cell>70.8% 73.8% 165 45 3222 12052 524</cell></row><row><cell cols="2">impact on occlusion estimation module</cell></row><row><cell cols="2">FairMOT+UTrack+OccE 68.5% 72.0% 142 57 2840 13797 396</cell></row><row><cell cols="2">FairMOT+UTrack+OccE 72.0% 73.1% 168 46 3565 11119 417</cell></row><row><cell>CenterTrack[58]</cell><cell>60.7% 62.7% 112 76 2179 18447 564</cell></row><row><cell>CenterTrack [58]</cell><cell>66.1% 64.2% 140 72 2442 15286 588</cell></row><row><cell>CenterTrack+OccE</cell><cell>62.1% 64.6% 127 68 3372 16583 440</cell></row><row><cell>CenterTrack+OccE</cell><cell>67.4% 67.6% 158 63 4086 13107 414</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>Benchmark results on MOTChallenge. Trackers marked with ? track objects in an offline manner. FairMOT ? is pre-trained on CrowdHuman without five extra datasets<ref type="bibr" target="#b7">4</ref> . Best results are shown in bold and highlighted with underline.</figDesc><table><row><cell>benchmark</cell><cell>methods</cell><cell>MOTA? IDF1? MT? ML? FP? FN? Recall? IDS? Frag? Hz?</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">In this case, C = N t?1 + N t + 1 for the calculation of temperature T .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">This tracker is trained by ourselves using its official code since the model is not available, which achieves the same MOTA, higher IDF1 and lower IDS.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">These datasets are ETH<ref type="bibr" target="#b17">[14]</ref>, CityPerson<ref type="bibr" target="#b56">[53]</ref>, CalTech<ref type="bibr" target="#b15">[12]</ref>, CUHK-SYSU<ref type="bibr" target="#b54">[51]</ref> and PRW<ref type="bibr" target="#b60">[57]</ref>. Besides the box annotations, the identity information is also provided in the last three datasets.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">OTrack ct (ours)</title>
		<imprint>
			<biblScope unit="page">65</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Centertrack</surname></persName>
		</author>
		<idno>58] 61.5% 59.6% 26.4% 31.9% 14076 200672 64.4% 2583 4965 17.5 TMOH [40] 62.1% 62.8% 26.9% 31.4% 10951 201195 64.3% 1897 4622 0.7</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Siammtot</surname></persName>
		</author>
		<idno>39] 65.9% 63.3% 34.6% 23.9%</idno>
		<imprint>
			<biblScope unit="volume">18098</biblScope>
			<biblScope unit="page" from="170955" to="170972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">OUTrack fm (ours)</title>
		<imprint>
			<biblScope unit="page">68</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A dual cnn-rnn for multiple people tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Babaee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">368</biblScope>
			<biblScope unit="page" from="69" to="83" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tracking without bells and whistles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Meinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="941" to="951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Evaluating multiple object tracking performance: the clear mot metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bernardin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal on Image and Video Processing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Simple online and realtime tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Upcroft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3464" to="3468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning a neural solver for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bras?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6247" to="6257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pedhunter: Occlusion robust pedestrian detector in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10639" to="10646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Online multi-object tracking with instance-aware tracker and dynamic model refreshment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="161" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Online multi-object tracking using cnn-based single object tracker with spatial-temporal attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4836" to="4845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dasot: A unified framework integrating data association and single object tracking for online multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10672" to="10679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Detection in crowded scenes: One proposal, multiple predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12214" to="12223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Mot20: A benchmark for multi object tracking in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dendorfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.09003</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pedestrian detection: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="304" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Clnet: A compact latent network for fast adjusting siamese trackers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020-08-23" />
			<biblScope unit="page" from="378" to="395" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XX 16</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A mobile vision system for robust multi-person tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification: Clustering and fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Multimedia Computing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Communications, and Applications (TOMM)</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised pre-training for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improving person re-identification with iterative impression aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learnable graph matching: Incorporating graph partitioning with deep feature learning for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5299" to="5309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Lifted disjoint paths with application in multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hornakova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Swoboda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Simple unsupervised multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karthik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gandhi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.02609</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Discriminative appearance modeling with multi-track pooling for real-time multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fuxin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alotaibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9553" to="9562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Europeon Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to associate: Hybridboosted multi-target tracker for crowded scene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2953" to="2960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Triplet online instance matching loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">433</biblScope>
			<biblScope unit="page" from="10" to="18" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Prgcn: Probability prediction with graph convolutional network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">423</biblScope>
			<biblScope unit="page" from="57" to="70" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Gsm: Graph similarity model for multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conferences on Artificial Intelligence Organization</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="530" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Real-time online multi-object tracking in compressed domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="76489" to="76499" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi-object tracking with hard-soft attention network and group-based cost minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">447</biblScope>
			<biblScope unit="page" from="80" to="91" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00831</idno>
		<title level="m">Mot16: A benchmark for multi-object tracking</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning multi-object tracking and segmentation from automatic annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hofinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6846" to="6855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Tracking the untrackable: Learning to track multiple cues with long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="300" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Probabilistic tracklet scoring and inpainting for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aliakbarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14329" to="14339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Crowdhuman: A benchmark for detecting human in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00123</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Siammot: Siamese multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Modolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tighe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12372" to="12382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Improving multiple pedestrian tracking by track management and occlusion handling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stadler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Beyerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10958" to="10967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Visible-infrared cross-modality person re-identification based on whole-individual training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">440</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Multiple people tracking by lifted multicut and person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3539" to="3548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A novel domain activation mapping-guided network (da-gnt) for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">449</biblScope>
			<biblScope unit="page" from="443" to="454" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Mots: Multi-object tracking and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Osep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B G</forename><surname>Sekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7942" to="7951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Multiple object tracking with correlation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3876" to="3886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Cycas: Self-supervised cycle association for learning re-identifiable descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="72" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Towards real-time multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Europeon Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Simple online and realtime tracking with a deep association metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wojke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paulus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3645" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Tracklet self-supervised learning for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12362" to="12369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Track to detect and segment: An online multiobject tracker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12352" to="12361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Joint detection and identification feature learning for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3415" to="3424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Spatial-temporal relation networks for multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3988" to="3998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Citypersons: A diverse dataset for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3213" to="3221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Multiplex labeling graph for near-online tracking in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet of Things Journal</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="7892" to="7902" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Fairmot: On the fairness of detection and reidentification in multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.01888</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Improving multiple object tracking with single object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2453" to="2462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Person re-identification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1367" to="1376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Tracking objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="474" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<title level="m">Objects as points</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Online multi-object tracking with dual matching attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Europeon Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="366" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Crowded human detection via an anchor-pair network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1391" to="1399" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
