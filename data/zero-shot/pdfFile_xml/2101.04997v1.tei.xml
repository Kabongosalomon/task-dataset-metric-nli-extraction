<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Joint Learning of Hyperbolic Label Embeddings for Hierarchical Multi-label Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumya</forename><surname>Chatterjee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology</orgName>
								<address>
									<settlement>Bombay</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Maheshwari</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology</orgName>
								<address>
									<settlement>Bombay</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Ramakrishnan</surname></persName>
							<email>ganesh@cse.iitb.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology</orgName>
								<address>
									<settlement>Bombay</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saketha</forename><surname>Nath</surname></persName>
							<email>saketha@iith.ac.in</email>
							<affiliation key="aff1">
								<orgName type="institution">Indian Institute of Technology Hyderabad</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jagaralpudi</forename></persName>
						</author>
						<title level="a" type="main">Joint Learning of Hyperbolic Label Embeddings for Hierarchical Multi-label Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider the problem of multi-label classification, where the labels lie in a hierarchy. However, unlike most existing works in hierarchical multi-label classification, we do not assume that the label-hierarchy is known. Encouraged by the recent success of hyperbolic embeddings in capturing hierarchical relations, we propose to jointly learn the classifier parameters as well as the label embeddings. Such a joint learning is expected to provide a twofold advantage: i) the classifier generalises better as it leverages the prior knowledge of existence of a hierarchy over the labels, and ii) in addition to the label co-occurrence information, the label-embedding may benefit from the manifold structure of the input datapoints, leading to embeddings that are more faithful to the label hierarchy. We propose a novel formulation for the joint learning and empirically evaluate its efficacy. The results show that the joint learning improves over the baseline that employs label co-occurrence based pre-trained hyperbolic embeddings. Moreover, the proposed classifiers achieve state-of-the-art generalization on standard benchmarks. We also present evaluation of the hyperbolic embeddings obtained by joint learning and show that they represent the hierarchy more accurately than the other alternatives. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The problem of multi-label text classification is well known and extensively studied in literature <ref type="bibr" target="#b17">(McCallum, 1999;</ref><ref type="bibr" target="#b30">Yang et al., 2009;</ref><ref type="bibr" target="#b15">Liu et al., 2017)</ref>. The fundamental assumption is that a document is associated with multiple labels from a fixed vocabulary of labels. Often, these labels are organised in a hierarchical structure. We undertake the task of labelling documents with classes that are hierarchically organised; this problem is popularly known as hierarchical multi-label text classification (HMC). HMC methods have found several applications in online advertising systems <ref type="bibr" target="#b0">(Agrawal et al., 2013)</ref>, bio-informatics <ref type="bibr" target="#b21">(Peng et al., 2016;</ref><ref type="bibr" target="#b27">Triguero and Vens, 2016</ref>), text classification <ref type="bibr" target="#b23">(Rousu et al., 2006;</ref><ref type="bibr" target="#b16">Mao et al., 2019)</ref>.</p><p>The main challenge in HMC is in modelling classification of the document into a large, imbalanced and structured output space. In HMC, the label taxonomy is a partially ordered set (L, ?) where L is a finite set of all class labels. Relation ? refers to is-a relationship between labels, which is asymmetric, anti-reflexive and transitive <ref type="bibr" target="#b25">(Silla and Freitas, 2011)</ref>.</p><p>Hierarchical structures can provide important insights for learning and classification tasks. However, explicit knowledge of hierarchy is not available in several domains, for instance, extreme classification datasets <ref type="bibr">(Bhatia et al., 2016)</ref>. In this paper, we consider the problem of structured prediction from unstructured text, in which label hierarchy is not known apriori. We infer hierarchies from classification judgements on the outputs that are readily available. We focus on discovering relationships between the labels in a hyperbolic space, which has natural capacity to encode hierarchical structures.</p><p>In our approach, HIDDEN (HyperbolIc label embeDDings for hiErarchical multi-label classi-ficatioN), the labels are represented in a hyperbolic space to help respect their latent hierarchical organisation. We use this intuition to learn label embeddings for HMC without explicit supervision on the label hierarchy.</p><p>Apart from employing hyperbolic embeddings, another key aspect of our methodology is that the parameters of the classifier as well as of the label embedding are learnt jointly. We next explain the advantage in doing so. In the absence of any partial information regarding the hierarchy, label embeddings are typically learnt using the weak supervision available in label co-occurrences <ref type="bibr">Kiela, 2017, 2018)</ref>. This weak form of supervision can be complemented if the label embedding learning is also aware of the manifold structure of the input (documents). For e.g., similar documents may have similar labels, etc. Such a strengthening is possible only if learning happens in a joint fashion. Moreover, the generalization of the classifier also improves because of the improved embeddings (and vice-versa). Our contributions can be summarised as follows: 1. We present an approach HIDDEN, that models the implicit hierarchical organisation of labels for improved classification. It leverages properties of hyperbolic geometry to help learn embeddings for the hierarchically organised labels. 2. We present a novel formulation for jointly learning the parameters of the classifier as well as the label embedding, which can be trained solely using the supervision from the training data, and without using any explicit information regarding the label hierarchy. 3. We evaluate HIDDEN on real-world as well as synthetic datasets and show: (a) significant improvement over classical multi-label classification methods as well as baselines that employ hyperbolic label embeddings learnt in isolation solely based on label co-occurrence information (b) HIDDEN sometimes generalizes even better than state-of-the-art hierarchical multi-label classifiers that have complete access to the true label hierarchy (c) label embeddings learnt using the joint optimisation approach correlate better with the ground truth than other alternatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Several conventional classification methods are capable of handling classification in multi-label settings. However, relatively fewer of these are designed to incorporate the possibly hierarchical organisation of the class labels. These include both traditional methods <ref type="bibr" target="#b5">(Gopal and Yang, 2013;</ref><ref type="bibr" target="#b13">Lewis et al., 2004)</ref> as well as deep learning methods <ref type="bibr" target="#b8">(Johnson and Zhang, 2015;</ref><ref type="bibr" target="#b20">Peng et al., 2018)</ref> across varied domains such as news articles, web content, etc. Traditional or flat classification approaches typ-ically perform prediction assuming that all the classes are independent of each other, ignoring the class hierarchy. Whereas 'local' classification approaches <ref type="bibr" target="#b11">(Koller and Sahami, 1997;</ref><ref type="bibr" target="#b3">Cesa-Bianchi et al., 2006)</ref> train a set of classifiers at each level of the hierarchy. However, it has also been argued <ref type="bibr" target="#b2">(Cerri et al., 2011)</ref> that it is impractical to train separate classifiers at each level. On the other hand, 'global' approaches <ref type="bibr" target="#b26">(Silla Jr and Freitas, 2009;</ref><ref type="bibr" target="#b28">Wang et al., 2001</ref>) train a single classifier that factors in the complete class hierarchy. Unlike the local approach, 'global' approaches do not suffer from the error propagation problem, although they are prone to under-fit by not considering local information in the hierarchy.</p><p>Some recent papers have proposed a mix of local and global approaches for HMC. <ref type="bibr" target="#b29">Wehrmann et al. (2018)</ref> propose an objective that leverages both local and global information while introducing global hierarchical violation penalty. <ref type="bibr" target="#b16">Mao et al. (2019)</ref> employ a reinforcement learning framework to learn a label assignment policy. They model HMC as a markov decision process, wherein, the agent takes an action of label assignment on the tree hierarchy and receives scalar rewards as feedback for the actions. <ref type="bibr" target="#b4">Chen et al. (2019)</ref> embed both document and label hierarchy in the same hyperbolic space and use interactions between these embeddings for HMC. Our approach differs from these in two important ways: (i) we embed only labels into the hyperbolic space and (ii) label hierarchy is not known apriori -all we assume is that there is some hidden hierarchy.</p><p>Recently, the use of hyperbolic geometry has been found to be promising in machine learning and network sciences to model data with latent hierarchies. <ref type="bibr" target="#b12">Krioukov et al. (2010)</ref> showed that properties of complex networks, namely heterogeneous degree distribution and strong clustering, naturally manifest in hyperbolic geometry. They showed that if a network has some heterogeneous degree distribution and metric structure, the network can be mapped effectively to the hyperbolic space (since euclidean distance has limitations in approximating the distance between nodes in a tree). <ref type="bibr" target="#b6">Gromov (1987)</ref> have shown that any finite tree structure can be embedded into a finite hyperbolic space while preserving the distance between nodes. <ref type="bibr" target="#b18">Nickel and Kiela (2017)</ref> learnt hierarchical representations of symbolic data by embedding them into an n-dimensional Poincar? ball by lever-aging the distance property of hyperbolic spaces. Instead of relying on the true hierarchy to learn embeddings, <ref type="bibr" target="#b19">Nickel and Kiela (2018)</ref> inferred hierarchies from real-valued similarity scores using the Lorentz model of hyperbolic geometry. We used a similar formulation in our model HIDDEN to build our HMC model by leveraging the co-occurrence count of labels for each document, but additionally (and more importantly), in a joint manner, learn the parameters of the classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Hyperbolic Geometry &amp; the Poincar? Model</head><p>In this section, we give an overview of hyperbolic geometry and the Poincar? model for embedding in hyperbolic spaces <ref type="bibr" target="#b18">(Nickel and Kiela, 2017)</ref>. A hyperbolic space is a non-Euclidean Riemannian manifold of constant negative curvature. Though there are several fundamental differences between the Euclidean and the hyperbolic geometry, the most interesting characteristic of hyperbolic spaces is their ability to naturally represent hierarchical relations <ref type="bibr" target="#b12">(Krioukov et al., 2010)</ref>. In the Poincar? ball model, which is one of the standard models for hyperbolic geometry, the Euclidean distances between equidistant points, according to the inherent manifold metric d, fall exponentially as one moves from origin towards the surface of the ball. This interesting property is the key for enabling learning of continuous embeddings of hierarchies. For example, one can imagine root node of hierarchy at origin and leaf nodes near the ball's surface. Then, this model can easily accommodate exponentially growing number of equidistant siblings at deeper levels of the hierarchy. Whereas, such an accommodation is not possible using Euclidean geometry. Below we provide some details of this model.</p><formula xml:id="formula_0">Let B n = {x ? R n | x &lt; 1} be the open n-dimensional unit ball, where . is the Euclidean 2 norm. The Poincar? ball model is a Riemannian Manifold (B n , g x ),</formula><p>the open unit ball equipped with the Riemannian metric tensor</p><formula xml:id="formula_1">g x = 2 1? x 2 2 g E , where x ? B d and g E is</formula><p>the Euclidean metric tensor. The geodesic distance between two points u, v ? B d is given as</p><formula xml:id="formula_2">d(u, v) = arcosh 1 + 2 u ? v 2 (1 ? u 2 )(1 ? v 2 )</formula><p>(1) Given any x ? R n , one can show that</p><formula xml:id="formula_3">x 1+ ? 1+ x 2 2</formula><p>always lies in the Poincare ball (refer Appendix for detailed explanation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Problem Formulation and Approach</head><p>In this section, we present present details of our model, training, as well as inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Problem Formulation</head><p>Here we consider an interesting special case of multi-label classification. The training data is of the form:</p><formula xml:id="formula_4">D = {(D 1 , y 1 ) , (D 2 , y 2 ) , . . . , (D m , y m )}, where D i ? R n is the input representation of the i th document, y i ? {0, 1} L represents the set of active/annotated labels for it (y l i = 1 ?? D i is labelled with l)</formula><p>, and L is the total number of labels. Importantly, the labels are assumed to be nodes of an unknown, yet fixed, hierarchy. Using this prior knowledge and the training data, the goal is to learn a classifier that generalises well for labelling new documents.</p><p>Classical text classification methods ignore the informative prior knowledge that the set of labels form a hierarchy. Most of the hierarchical multiclass classification models assume that the hierarchy over the labels is completely known, which might not be a pragmatic assumption, since constructing hierarchies is an expensive process, especially when the number of labels is large <ref type="bibr">(Bhatia et al., 2016)</ref>. In contrast, here we assume no explicit information regarding the hierarchy other than it's existence, and the implicit information encoded in the training data. Also, in our set-up, we do not restrict the labels to be the leaves nodes in the hierarchy. As motivated earlier, here we propose to learn a classifier that jointly learns the classifier parameters as well as the label embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Our Model: HIDDEN</head><p>Our proposed model HIDDEN has two key components: one for representing the documents that may lead to well-generalizing classifiers and the other for embedding the labels in a hyperbolic space. Recall that hyperbolic spaces have shown to be wellsuited for data satisfying hierarchical relations.</p><p>Document Model F w accepts as input a document, D, and outputs a n-dimensional representation of it, F w (D) ? R n . Here, w is the set of parameters to be learnt. In this work, we use TextCNN <ref type="bibr" target="#b9">(Kim, 2014)</ref> as the document model. But our approach remains valid irrespective of the chosen document model.</p><p>Label Embedding Model G ? accepts as input a label l and outputs a finite dimensional representation, G ? (l). Here, ? is the set of parameters to be learnt. In this work, following <ref type="bibr" target="#b19">Nickel and Kiela (2018)</ref>, we employ the simple look-up based model defined by G ? (l) ? ? * y l = ? l , where ? ? R n?L and ? l is the l th column of ?. These Euclidean embeddings ? l are then projected onto the Poincare manifold using the transformation</p><formula xml:id="formula_5">?(x) = x 1+ ? 1+ x 2 2</formula><p>. In summary, the hyperbolic embedding of label, l, is given by ?(? l ).</p><p>We next assume that there exists some optimal set of parameters w * , ? * such that the labels annotated/active for a document, D, are exactly those whose label representations are highly aligned with that of D's representation. Here, alignment between the representations is intended to model the natural intuition of appropriateness between label and document. Following the principle of largemargin separation, in this paper we employ the alignment model defined below:</p><formula xml:id="formula_6">y l D (w, ?) ? ? F w (D) ? l<label>(2)</label></formula><p>where? l D (w, ?) denotes the alignment between the document, D, and the l th label as per the model with parameters (w, ?), and ? is the Sigmoid activation function.</p><p>Inference: Given the learnt parameters (?,?), the labels with? l D (?,?) &gt; 0.5 are predicted to be the active ones for D. We next detail the proposed joint objective for learning the parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Joint Objective</head><p>The proposed objective consists of two terms: the first is an empirical multi-label loss term over the training data, and the second is a loss for ensuring that the hyperbolic label embeddings respect the pairwise label co-occurrence or any other such (pairwise) partial information regarding the underlying label hierarchy.</p><p>First Term is simply a binary cross entropy loss to promote high alignment scores for each annotated label and vice-versa:</p><formula xml:id="formula_7">L 1 (w, ?) = m i=1 L l=1 y l i log ? l i (w, ?) + (1 ? y l i ) log 1 ?? l i (w, ?)<label>(3)</label></formula><p>where? l i is a short-hand for? l D i . Second Term induces lesser geodesic distance in the hyperbolic space between the label embeddings that have higher co-occurrences than those between label pairs that have less cooccurrence <ref type="bibr" target="#b19">(Nickel and Kiela, 2018)</ref>:</p><formula xml:id="formula_8">L 2 (?) = l,l ?L, l =l log ? ? ? e ?d(?(? l ),?(? l )) z?N (l,l ) e ?d(?(? l ),?(? l )) ? ? ?</formula><p>(4) where d is the metric in the hyperbolic space given by Eq.1, ?(? l ) is hyperbolic embedding of l th label, and N (l, l ) is the set of all labels that less frequently co-occur with l than l co-occurs with l.</p><p>The overall objective function is a weighted sum of the two components described above</p><formula xml:id="formula_9">L (w, ?) = L 1 (w, ?) + ?L 2 (?)<label>(5)</label></formula><p>We refer to the model corresponding to the parameters (w jnt , ? jnt ) that minimizes this joint objective in Eq.5 as HIDDEN jnt :</p><formula xml:id="formula_10">(w jnt , ? jnt ) ? arg min w,? L(w, ?)<label>(6)</label></formula><p>Both components of our loss interact with each other to minimize the distance between document and label embeddings in the hyperbolic space. The advantage of the joint learning is well illustrated when HIDDEN jnt is compared with the following baseline, henceforth referred to as HIDDEN cas : (1) L 2 is minimized to obtain label embedding? ? cas ? arg min ? L 2 (?).</p><p>(2) These are then used in L 1 to obtain document parameters:? cas ? arg min w L 1 (w,? cas ).</p><p>We also empirically compare with the following multi-class classification baseline, henceforth referred to as HIDDEN flt :</p><p>(1) ? flat is fixed to the identity matrix.</p><p>(2) These are then used in L 1 to obtain document parameters:? flat ? arg min w L 1 (w,? flat ). To evaluate the benefit of using hyperbolic spaces for embedding labels, we also compare with a variant of HIDDEN jnt called HIDDEN euc for which L 2 is modified to be</p><formula xml:id="formula_11">L 2Euc (?) = l,l ?L, l =l log ? ? ? e ? ? l ?? l 2 z?N (l,l ) e ? ? l ?? l 2 ? ? ?<label>(7)</label></formula><p>to obtain the parameters (w euc , ? euc ) as (w euc , ? euc ) ? arg min</p><formula xml:id="formula_12">w,? L 1 (w, ?) + ?L 2Euc (?)</formula><p>Note that none of the variants of HIDDEN assume any explicit information regarding the underlying hierarchy. However, the former three exploit the prior knowledge that there exists a label hierarchy; whereas the latter, which is the classical multi-label classification network, completely ignores this useful information. Moreover, since the proposed model HIDDEN jnt performs joint learning, it is expected that HIDDEN jnt not only achieves better generalization, but also leads to better label embeddings, when compared to HIDDEN cas . The simulation results in section 5 confirm the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Training Details</head><p>In all our experiments, the initial word embedding layer of TextCNN in the document model is initialized using 300 dimensional GloVe embeddings <ref type="bibr" target="#b22">(Pennington et al., 2014)</ref>. Following Nickel and Kiela <ref type="formula" target="#formula_6">(2017)</ref>, we randomly initialize ? from the uniform distribution U(?0.001, 0.001). Both the document and label representations are are of length n = 300. We randomly choose 10% of training set as the validation set and report test set results on the best validation epoch. During training, dropout is applied to the outputs of document model as well the label model with probabilities 0.1 and 0.6 respectively. We found ? = 0.1 to yield the best validation performance. The number of training epochs are set to 30 for all experiments. Both models are optimized using stochastic gradient descent using Adam optimizer (Kingma and Ba, 2014) with learning rate as 0.001 for TextCNN.</p><p>We run all our experiments on Nvidia RTX 2080 Ti GPUs 12 GB RAM over Intel Xeon Gold 5120 CPU having 56 cores and 256 GB RAM. It takes around 1, 2 and 5 hours to train the model on RCV1, NYT and Yelp datasets respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we compare our approach against the baseline models and other state-of-the-art HMC approaches. First we describe the evaluation metrics and illustrating results in a synthetic setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation Measures</head><p>Classifier Evaluation Measures: We use standard measures for evaluating any HMC, viz., Macro-F1 and Micro-F1. Let T P , T N , F N , F P denote the true positive, true negative, false negative and false positive labels respectively. Precision is T P T P +F P and recall is T P T P +F N . F1-score is the harmonic mean of precision and recall. Macro-F1 assigns equal weightage to each class and is computed as the averaged F1 score over all classes. Micro-F1 is the F1-score computed over all instances. Label Embedding Evaluation Measures: For a given application, let us say H * is provided to us as the ground truth hierarchy of labels/nodes, which was assumed to be unknown in our problem formulation in Section 4.1. Recall that none of the variants of HIDDEN has access to H * . How consistent with respect to H * are the label embeddings learnt by these models? We attempt to assess this consistency by adopting standard measures such as Spearman's rank correlation coefficient <ref type="bibr" target="#b31">(Zar, 2005)</ref> and Normalized Discounted cumulative gain (NDCG) <ref type="bibr" target="#b7">(J?rvelin and Kek?l?inen, 2002)</ref>.</p><p>Recall from Section 4.3, that the hyperbolic embedding for label l is ? (? l ) and likewise for l , it is ? (? l ). The model parameters ? might be learnt using any variant of HIDDEN. Given a query label, l, the geodesic distance d(? (? l ) , ? (? l )) is used to rank all other labels l = l; smaller the distance, larger the rank. Any two labels l = l that are at the same geodesic distance from l, will be assigned the same rank r = r . Next, we define a graded relevance score for labels with respect to the ground truth hierarchy, H * . For any given query label l ? H * , we also assign a graded relevance rel ? N to every other label l = l based on the distance (number of hops hops(l, l )) of l from l in the hierarchy H D ; smaller the distance, larger the graded relevance (we considered rel ? 1 hops(l,l ) , for example).</p><p>Discounted Cumulative Gain (DCG) <ref type="bibr" target="#b7">(J?rvelin and Kek?l?inen, 2002)</ref> is a standard measure of the quality of ranking of an approach with respect to the graded relevance provided in the ground truth. DCG@k measures items (eg: labels l ) k hops away from the query l. The gain is accumulated from the top of the ranked list upto some pre-specified position k in the list, with the gain of each result discounted at lower ranks:</p><formula xml:id="formula_13">DCG k = k i=1 rel i log 2 (i+1) .</formula><p>Here, rel i is the graded relevance at position i. This result is itself averaged over all query labels l ? H * .</p><p>Spearman's rank correlation coefficient denoted by r is a non-parametric metric to measure statistical dependence between ranking of two variables. For each query label (l), we first measure rank cor- . Here, cov is the covariance and ?, the standard deviation. The final score, r is the averaged score across all labels l in the set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Validation via Synthetic Experiments</head><p>To observe the behaviour of the proposed approach HIDDEN with respect to the evaluation measures in a controlled environment, we present one such synthetic setup. The goal in this section is to illustrate the advantage of joint learning of parameters over isolated learning. Consider 2D data generated from 16 neatly separated Gaussians laid out on a grid as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. Each of the 16 gaussians corresponds to a single label l 1 , l 2 ...l 16 . We consider a second layer of 4 labels l 17 ...l 20 , obtained by grouping the gaussians into 4; each quadrant in the larger square would correspond to a single label. Finally, we have a third layer consisting of a single label l 21 -viz., the entire large square. This simple hierarchy is hidden from our variants of HIDDEN as well as from the flat model. The synthesized data is split randomly into train-test in the ratio 60:40. For each of the jointly optimised model HIDDEN jnt , the cascaded model HIDDEN cas as well as the flat model, we observe (i) the performance of the classification models F w (D) measured in terms of Micro-F1 and Macro-F1 as well as (ii) the consistency of the label embedding models G ? (l) with respect to the hidden 3-level hierarchy over the 21 labels. We record observations in two settings:</p><p>SETTING 1 in which for each training instance, one of the annotated labels are dropped, uniformly at random: In <ref type="table" target="#tab_0">Table 1</ref> we note the performances of the different approaches with increasing rate at which labels are dropped. The jointly optimised model HIDDEN jnt accounts for the classification task through loss component L 1 as also the somewhat redundant label co-occurrence through the loss component L 2 .</p><p>As expected, we observe that the performance of HIDDEN jnt is more robust to this form of label noise than the HIDDEN cas and HIDDEN flt models. This is because HIDDEN flt entirely relies on the training data and ignores the prior knowledge of existence of a label hierarchy. HIDDEN cas is also less robust as it over-relies on the label co-occurrence by minimising L 2 (which, in isolation will be sensitive to label noise), before venturing into the classification task by minimising L 1 .</p><p>SETTING 2 in which the size of the training set is decreased without corrupting labels: We observe the performances of the different approaches with decreasing size of the training set and note that the performance of the jointly optimised model HIDDEN jnt falls back on the label correlation signals through the loss component L 2 and is therefore more robust to decreasing size of the data set than the flat classifier. We observed similar results for other synthetic settings. Owing to space constraints, the plots and other ranking results are provided in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Real-world Text Datasets</head><p>We used three datasets, namely, RCV1, Yelp and NYT in our experiments:</p><p>(1) RCV1 <ref type="figure">(Lewis et al., 2004)</ref>   <ref type="bibr" target="#b16">Mao et al. (2019)</ref>, we use the set of reviews for a business to predict the categories to which the business belongs. Some statistics pertaining to these datasets are presented in <ref type="table" target="#tab_2">Table 2</ref>. We compare them against the baseline TextCNNflat model reported in <ref type="bibr" target="#b16">Mao et al. (2019)</ref>. The results are presented in <ref type="table" target="#tab_3">Table 3</ref> for ? = 0.1. Overall, our baseline (HIDDEN flt ) performs better than the previous baseline with the exception of Macro-F1 on NYT. We observe improvement of the joint model HIDDEN jnt over the flat (HIDDEN flt ) and cascaded (HIDDEN cas ) models on RCV1 and NYT (for each of which, the labels form a tree) in <ref type="table" target="#tab_3">Table 3</ref>. However, on the Yelp dataset, the cascaded model (HIDDEN cas ) performs somewhat worse  Macro-F1) than our baseline model (HIDDEN flt ), hinting at the possibility that label co-occurrence information might not be helpful toward the classification task. This could be partly also because the labels in Yelp are structured in the form of a DAG. Constant curvature property of hyperbolic spaces makes them unsuitable for learning DAG structures . However, the Macro-F1 performance of HIDDEN jnt is far better than that of the cascaded model HIDDEN cas . This illustrates that our joint model is able to better recover from less reliable (or less useful) label co-occurrence information, just as was illustrated in the <ref type="table" target="#tab_0">Table 1</ref> for the synthetic setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Comparison of Hyperbolic space and Euclidean space</head><p>To assess the utility of the hyperbolic space for embedding hierarchical labels, we compare HIDDEN jnt and HIDDEN euc . <ref type="table" target="#tab_4">Table 4</ref> presents this comparison on the three datasets. HIDDEN euc per- forms worse than HIDDEN jnt which uses the hyperbolic space for embedding labels (except Micro-F1 for Yelp due to reasons stated before). This is expected since embedding trees is much more effective in the hyperbolic space compared to Euclidean space since in the hyperbolic space, volume grows exponentially with distance from the origin while in Euclidean space, this growth is polynomial. The number of nodes in a tree also increases exponentially with distance from the root, making Hyperbolic spaces useful for embedding hierarchies. We compare performance of our joint approach HIDDEN jnt against a state-of-the-art hierarchical multi-label classifier, HiLAP <ref type="bibr" target="#b16">(Mao et al., 2019)</ref>. However, unlike our proposed models (variants of HIDDEN), HiLAP has access to the true hierarchy both training and inference. Thus, HiLAP serves as some form of skyline for the HIDDEN suite of approaches proposed in this paper. HiLAP learns label assignment policy using the reinforcement learning framework.</p><p>In <ref type="table" target="#tab_5">Table 5</ref>, we compare the performance of HIDDEN jnt against HiLAP model as reported in <ref type="bibr" target="#b16">Mao et al. (2019)</ref>. Interestingly, on RCV1, we obtain better Micro-F1 score (+0.7) for the joint model HIDDEN jnt over the HiLAP method. On NYT, our Micro-F1 score is far better (+7.1) than HiLAP; our Macro-F1 score is also marginally better (+0.4) than their Macro-F1 scores. These results are interesting because HIDDEN jnt seems to obtain better generalisation through joint learning of the document classifier and label embeddings in a hyperbolic space, even without access to the true hierarchy. However, on Yelp, HiLAP seems to benefit over HIDDEN jnt by explicitly using the true hierarchy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Evaluating performance of embeddings</head><p>We compare embeddings learned using different approaches with the ground truth hierarchy to evaluate the effectiveness of the embeddings. <ref type="figure">Figure  2</ref> shows the plot of NDCG scores for different values of k on the RCV1 and NYTimes dataset across HIDDEN cas and HIDDEN jnt (for two different values of ?). In <ref type="table" target="#tab_6">Table 6</ref>, we compare the Spearman rank correlation. The superior performance of HIDDEN jnt strongly suggests that the embeddings learnt using the joint model are more representative of the true hierarchical organisation of the labels than those obtained using the flat and cascaded variants. This also goes to show that even the first term in our objective has positive contribution towards the learning of hyperbolic embeddings and indeed joint learning is beneficial. <ref type="figure">Figure 2</ref>: Plot of NDCG versus k for assessing the quality of the learnt label embeddings with respect to the actual hierarchy on RCV1 and NYT datasets. The better performance of HIDDEN jnt indicates the label embeddings ? jnt are most representative of the true hierarchy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose a novel approach to hierarchical multilabel classification based on joint learning of document classifier and label embeddings in hyperbolic space. The proposed framework HIDDEN allows us to discover label hierarchical relationship by leveraging properties of hyperbolic geometry. Even though label-hierarchy is assumed to be unavailable, our method achieves comparable results with state-of-the-art hierarchy aware methods. We performed extensive experiments on three datasets and demonstrate effectiveness of the learned embeddings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Gaussian used for the synthetic experiment relation between the predicted rank r p of a label l and its own rank r h as per ground truth hierarchy h. The correlation coefficient between r lp and r l h is computed as r l = cov(rp,r h ) ?r p ?r h</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of methods on the Synthetic SETTING 1 with respect to increasing probability with which a label is randomly dropped. Synthetic data used here has 12000 training and 8000 test samples.</figDesc><table><row><cell>Prob</cell><cell cols="6">0.00 Micro F1 Macro F1 Micro F1 Macro F1 Micro F1 Macro F1 0.20 0.40</cell></row><row><cell>HIDDENflt</cell><cell>96.8</cell><cell>89.1</cell><cell>93.2</cell><cell>87.8</cell><cell>90.4</cell><cell>87.7</cell></row><row><cell>HIDDENcas</cell><cell>98.0</cell><cell>93.4</cell><cell>94.4</cell><cell>88.9</cell><cell>91.9</cell><cell>91.0</cell></row><row><cell>HIDDENjnt</cell><cell>98.1</cell><cell>94.0</cell><cell>94.8</cell><cell>91.6</cell><cell>92.3</cell><cell>91.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Statistics of the datasets used in the experiments. |L| denotes the number of labels, Avg(|L|) is the average number of labels per instance and Max(|L|) is the maximum number of labels for an instance.</figDesc><table><row><cell cols="6">Dataset Hierarchy |L| Avg(|L|) Max(|L|) Train</cell><cell>Val</cell><cell>Test</cell></row><row><cell>RCV1</cell><cell>Tree</cell><cell>104</cell><cell>3.24</cell><cell>17</cell><cell cols="3">20833 2314 781265</cell></row><row><cell>NYT</cell><cell>Tree</cell><cell>120</cell><cell>6.58</cell><cell>24</cell><cell cols="2">86461 9606</cell><cell>9903</cell></row><row><cell>Yelp</cell><cell>DAG</cell><cell>539</cell><cell>4.07</cell><cell>32</cell><cell cols="3">98460 10939 46884</cell></row><row><cell cols="8">5.4 Comparison of models that do not use the</cell></row><row><cell></cell><cell cols="3">true hierarchy</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">We compare performance of the different models</cell></row><row><cell cols="8">that do not use the true hierarchy. These include</cell></row><row><cell cols="8">our flat baseline HIDDEN flt , the cascaded model</cell></row><row><cell cols="8">HIDDEN cas as well as our joint model HIDDEN jnt .</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison on all three datasets with TextCNN as the base classification model. Recall that HIDDEN flt is our own multi-class classification baseline. We observe that the numbers reported by<ref type="bibr" target="#b16">Mao et al. (2019)</ref> (indicated by * ) for the TextCNN based flat baseline model are consistently outperformed by the proposed HIDDEN models.</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell cols="2">Micro-F1 Macro-F1</cell></row><row><cell></cell><cell cols="2">TextCNN-Flat  *  76.6</cell><cell>43.0</cell></row><row><cell></cell><cell>HIDDEN flt</cell><cell>77.9</cell><cell>44.5</cell></row><row><cell>RCV1</cell><cell>HIDDEN cas</cell><cell>78.0</cell><cell>45.5</cell></row><row><cell></cell><cell>HIDDEN jnt</cell><cell>79.3</cell><cell>47.3</cell></row><row><cell></cell><cell cols="2">TextCNN-Flat  *  69.5</cell><cell>39.5</cell></row><row><cell></cell><cell>HIDDEN flt</cell><cell>76.4</cell><cell>37.1</cell></row><row><cell cols="2">NYTimes HIDDEN cas</cell><cell>74.6</cell><cell>33.2</cell></row><row><cell></cell><cell>HIDDEN jnt</cell><cell>77.0</cell><cell>43.6</cell></row><row><cell></cell><cell cols="2">TextCNN-Flat  *  62.8</cell><cell>27.3</cell></row><row><cell></cell><cell>HIDDEN flt</cell><cell>62.5</cell><cell>37.9</cell></row><row><cell>Yelp</cell><cell>HIDDEN cas</cell><cell>60.5</cell><cell>33.9</cell></row><row><cell></cell><cell>HIDDEN jnt</cell><cell>60.8</cell><cell>35.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Performance comparison for HIDDEN jnt with HIDDEN euc . HIDDEN jnt consistently has better Macro-F1 better than HIDDEN euc and generally better Micro-F1 too. This illustrates the utility of Hyperbolic spaces for embeddding label hierarchies</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell cols="2">Micro-F1 Macro-F1</cell></row><row><cell></cell><cell cols="2">HIDDEN euc 78.4</cell><cell>47.6</cell></row><row><cell>RCV1</cell><cell cols="2">HIDDEN jnt 79.3</cell><cell>47.3</cell></row><row><cell></cell><cell cols="2">HIDDEN euc 76.4</cell><cell>40.4</cell></row><row><cell cols="3">NYTimes HIDDEN jnt 77.0</cell><cell>43.6</cell></row><row><cell></cell><cell cols="2">HIDDEN euc 61.1</cell><cell>34.2</cell></row><row><cell>Yelp</cell><cell cols="2">HIDDEN jnt 60.8</cell><cell>35.6</cell></row><row><cell cols="4">5.6 Comparison with model that explicitly</cell></row><row><cell cols="3">uses the true hierarchy</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Performance comparison of HIDDEN jnt with HiLAP with respect to Macro-F1 and Micro-F1. It it interesting to note here that though HIDDEN jnt does not know the true hierarchy, it performs better than HiLAP (which uses the true hierarchy) in some cases. (HiLAP numbers are those reported by<ref type="bibr" target="#b16">Mao et al. (2019))</ref> </figDesc><table><row><cell>Dataset</cell><cell cols="2">HIDDEN jnt</cell><cell cols="2">HiLAP</cell></row><row><cell></cell><cell cols="2">Micro Macro</cell><cell cols="2">Micro Macro</cell></row><row><cell>RCV1</cell><cell>79.3</cell><cell>47.3</cell><cell>78.6</cell><cell>50.5</cell></row><row><cell cols="2">NYTimes 77.0</cell><cell>43.6</cell><cell>69.9</cell><cell>43.2</cell></row><row><cell>Yelp</cell><cell>60.8</cell><cell>35.6</cell><cell>65.5</cell><cell>37.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Spearman rank correlation test for the generated embeddings for all the datasets. Each method is compared against the ground truth hierarchy. Highest values for HIDDEN jnt indicates the label embeddings ? jnt are most representative of the true hierarchy.</figDesc><table><row><cell></cell><cell cols="3">HIDDEN flt HIDDEN jnt HIDDEN cas</cell></row><row><cell>RCV1</cell><cell>21.2</cell><cell>53.9</cell><cell>44.1</cell></row><row><cell>NYTimes</cell><cell>11.4</cell><cell>39.5</cell><cell>36.1</cell></row><row><cell>Yelp</cell><cell>16.3</cell><cell>31.9</cell><cell>28.8</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">* Equal contribution 1 Code can be found at https://github.com/ soumyac1999/hyperbolic-label-emb-for-hmc</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.yelp.com/dataset/ challenge</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Explanation for ?(x)</head><p>The Lorentz model is defined as the Riemannian Manifold, L n = (H n , g l ), where H n = {x ? R n+1 : x, x L = ?1, x 0 &gt; 0}, and g l = diag([?1 1 . . . 1]). Here, x, y L , known as the Minkowski inner-product, is given by</p><p>The Poincar? model and the Lorentz model are equivalent in isometry. Therefore points in Lorentz manifold can be mapped into Poincar? ball as, p :</p><p>A point x in the Euclidean space R n can be projected onto the Lorentz manifold H n using the transformation ?(x) = 1 + x 2 , x . This transformation ensures that the Minkowski innerproduct, ?(x), ?(x) L , is equal to ?1, and that the first component, ?(x) 0 ? 1 + x 2 is positive, as required for membership in the Lorentz manifold. Now using the isometry between Poincar? and Lorentz models <ref type="bibr" target="#b19">(Nickel and Kiela, 2018)</ref>, we have ? : R n ? P n as ?(x) = p (?(x)) = x 1 + 1 + x 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Dataset Details</head><p>We describe the details of the datasets used in our experiments. For RCV1 dataset <ref type="bibr" target="#b13">(Lewis et al., 2004)</ref>, we use the original training/test split and use 10% of the training set as the validation set. We introduce an extra Root label in addition to the 103 labels present in the dataset. Each document in the dataset is labelled with this label.</p><p>The details for the other datasets used are same as in <ref type="bibr" target="#b16">Mao et al. (2019)</ref> and we refer the readers to the same.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-label learning with millions of labels: Recommending advertiser bid phrases for web pages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Archit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashoteja</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manik</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on World Wide Web</title>
		<meeting>the 22nd international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Anshul Mittal, Yashoteja Prabhu, and Manik Varma. 2016. The extreme classification repository: Multi-label datasets and code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kush</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunal</forename><surname>Dahiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Jain</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hierarchical multi-label classification for protein function prediction: A local approach based on neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Cerri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr? Cplf De</forename><surname>Barros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carvalho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th International Conference on Intelligent Systems Design and Applications</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="337" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hierarchical classification: combining bayes with svm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicol?</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Gentile</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Zaniboni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="177" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Hyperbolic interaction model for hierarchical multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boli</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixin</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liping</forename><surname>Jing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10802</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recursive regularization for large-scale classification with hierarchical and graphical dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Gopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="257" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hyperbolic groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhael</forename><surname>Gromov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Essays in group theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1987" />
			<biblScope unit="page" from="75" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cumulated gain-based evaluation of ir techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalervo</forename><surname>J?rvelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaana</forename><surname>Kek?l?inen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="422" to="446" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Effective use of word order for text categorization with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rie</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="103" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Hierarchically classifying documents using very few words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehran</forename><surname>Sahami</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<pubPlace>Stanford InfoLab</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hyperbolic geometry of complex networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitri</forename><surname>Krioukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fragkiskos</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksim</forename><surname>Kitsak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari?n</forename><surname>Bogun?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">36106</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rcv1: A new benchmark collection for text categorization research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><forename type="middle">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="361" to="397" />
			<date type="published" when="2004-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Smoothing the geometry of probabilistic box embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Boratko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep learning for extreme multilabel text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingzhou</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hierarchical text classification with reinforced label assignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="445" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-label text classification with a mixture model trained by em</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Kachites</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI 99 workshop on text learning</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Poincar? embeddings for learning hierarchical representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximillian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6338" to="6347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning continuous hierarchies in the lorentz model of hyperbolic geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximillian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3779" to="3788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Large-scale hierarchical text classification with recursively regularized deep graph-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaopeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengjiao</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference</title>
		<meeting>the 2018 World Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1063" to="1072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deepmesh: deep semantic representation for improving large-scale mesh indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghui</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Mamitsuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanfeng</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="70" to="79" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Kernel-based learning of hierarchical multilabel classification models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Rousu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandor</forename><surname>Szedmak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1601" to="1626" />
			<date type="published" when="2006-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The new york times annotated corpus. Linguistic Data Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Sandhaus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">26752</biblScope>
			<pubPlace>Philadelphia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A survey of hierarchical classification across different application domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">A</forename><surname>Silla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="31" to="72" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A globalmodel naive bayes approach to the hierarchical prediction of protein functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">A</forename><surname>Carlos N Silla</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ninth IEEE International Conference on Data Mining</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="992" to="997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Labelling strategies for hierarchical multi-label classification techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Triguero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Celine</forename><surname>Vens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="170" to="183" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hierarchical classification of real life documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senqiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2001 SIAM International Conference on Data Mining</title>
		<meeting>the 2001 SIAM International Conference on Data Mining</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hierarchical multi-label classification networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonatas</forename><surname>Wehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Cerri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Barros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5075" to="5084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Effective multi-label active learning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Tao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengjiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="917" to="926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jerrold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zar</surname></persName>
		</author>
		<title level="m">Spearman rank correlation. Encyclopedia of Biostatistics</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
