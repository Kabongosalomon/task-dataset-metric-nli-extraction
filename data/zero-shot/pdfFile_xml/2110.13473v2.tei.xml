<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DAI, DAS, BREMOND: CTRN 1 CTRN: Class Temporal Relational Network for Action Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Dai</surname></persName>
							<email>rui.dai@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="department">Inria</orgName>
								<orgName type="institution">Universit? C?te d&apos;Azur</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srijan</forename><surname>Das</surname></persName>
							<email>srijan.das@stonybrook.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Stony Brook University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Br?mond</surname></persName>
							<email>francois.bremond@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="department">Inria</orgName>
								<orgName type="institution">Universit? C?te d&apos;Azur</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DAI, DAS, BREMOND: CTRN 1 CTRN: Class Temporal Relational Network for Action Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Action detection is an essential and challenging task, especially for densely labelled datasets of untrimmed videos. There are many real-world challenges in those datasets, such as composite action, co-occurring action, and high temporal variation of instance duration. For handling these challenges, we propose to explore both the class and temporal relations of detected actions. In this work, we introduce an end-to-end network: Class-Temporal Relational Network (CTRN). It contains three key components: (1) The Representation Transform Module filters the class-specific features from the mixed representations to build a graph structured data. (2) The Class-Temporal Module models the class and temporal relations in a sequential manner. (3) G-classifier leverages the privileged knowledge of the snippet-wise co-occurring action pairs to further improve the co-occurring action detection. We evaluate CTRN on three challenging densely labelled datasets and achieve state-of-the-art performance, reflecting the effectiveness and robustness of our method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Action detection is a challenging computer vision problem which targets at finding precise temporal boundaries of actions occurring in an untrimmed video. Many studies on action detection focus on videos with sparse and well-separated instances of action <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25]</ref>. For instance, action detection algorithms on popular datasets like THUMOS <ref type="bibr" target="#b11">[12]</ref> and Activi-tyNet <ref type="bibr" target="#b1">[2]</ref> generally learn representations for single actions in a video. However, in daily life, human actions are continuous and can be very dense. Every minute is filled with potential actions to be detected and labelled. The methods designed for sparsely labelled datasets are hard to generalize to such real-world scenarios.</p><p>Towards this research direction, several methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref> have been proposed to model complex temporal relationships and to process datasets like Charades <ref type="bibr" target="#b18">[19]</ref>, TSU <ref type="bibr" target="#b6">[7]</ref> and Mul-tiTHUMOS <ref type="bibr" target="#b23">[24]</ref>. Those datasets encompassing real-world challenges share the following characteristics: Firstly, the actions are densely labelled and background instances are rare in these videos compared to sparsely labelled datasets. Secondly, the video has rich temporal <ref type="figure">Figure 1</ref>: Class-Temporal Relation. In a dense labelled video, there are dependencies between action classes (1) across different time steps in black arrows and (2) at same time step (i.e. co-occurring actions) in green arrows. structure and a set of actions occurring together often follows a well defined temporal pattern. For example, drinking from bottle always happens after taking a bottle and reading a book also related to opening a book in <ref type="figure">Fig 1.</ref> Finally, humans are great at multitasking, multiple actions can co-occur at the same time. For example, reading book while drinking water.</p><p>Existing methods have mostly focused on modelling the variation of visual cues across time locally <ref type="bibr" target="#b14">[15]</ref> or globally <ref type="bibr" target="#b16">[17]</ref> within a video. However, these methods take into account the temporal information without any further semantics. Real-world videos contain many complex actions with inherent relationships between action classes at the same time steps or across distant time steps (see <ref type="figure">Fig. 1</ref>). Modelling such class-temporal relationships can be extremely useful for locating actions in those videos.</p><p>To this end, we introduce Class-Temporal Relational Network (CTRN) to harness the relationships among the action classes in a video. To explore such relations, CTRN first filters the class-specific representation from the input features at each time step in a video. Then the transformed per-class representation is utilized for modelling the inter-class relations. (1) Across different time-steps, a graph-based layer is proposed to learn the dependencies between different action classes of the video. This learned relation map is shared among all the time-steps to refine the action features from the related actions (e.g. open the book and read book). Then a temporal layer is used to aggregate features from the same class over time to enable the graph-based layer to explore both short-term and long-term class dependencies.</p><p>(2) At the same time step, a graph-based classifier is proposed to leverage the privileged co-occurring action probabilities to improve co-occurring action detection.</p><p>To summarize, the main contributions of this work are: 1) A graph-based module to explore action class relations across different time-steps. 2) A graph-based classifier to tackle the co-occurring action challenge. 3) We evaluate our model on three challenging densely labelled datasets for the action detection task. Our method outperforms state-of-the-art results using fewer parameters and FLOPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Action detection has received a lot of interest in recent years <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17]</ref>. In this work, we focus on densely labelled action detection for handling videos with additional temporal relationships between different action classes <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b23">24]</ref>. Different from the sparsely labelled detection methods which output a sparse set of action snippets <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12]</ref>, densely labelled de-tection methods need to predict what action is occurring at every snippet in a video. There are two principle frameworks for densely labelled action detection: the anchor-based <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b21">22]</ref> and the Seq2Seq-based <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15]</ref> frameworks. The anchor-based frameworks are often slow and suffer from over-generated proposals and rigid boundaries. Seq2Seq-based <ref type="bibr" target="#b17">[18]</ref> frameworks apply temporal filters over snippet-wise features with a snippet-level classification, therefore, it interprets the sequence of images to a sequence of predictions. Compared to anchor-based methods, the Seq2Seq methods achieve better performance for action detection on datasets with densely distributed actions. This is mainly because of the combinatorial explosion of action proposals generated by the former methods. Thus, the recent methods are following the Seq2Seq framework: Lea et al. <ref type="bibr" target="#b14">[15]</ref> introduced temporal convolutional network (TCN) for the action detection task. This method increases the temporal reception field by using dilated convolutions to model long temporal patterns. Similarly, Piergiovanni et al. <ref type="bibr" target="#b17">[18]</ref> introduced Temporal Gaussian Mixture (TGM) layers. In contrast to standard convolution layer, TGM computes the filter weights based on Gaussian distributions, which enables TGM to learn longer temporal structures with a limited number of parameters. However, both methods focus only on the temporal modelling while overlooking the action class relations in the untrimmed videos. Although Super-event <ref type="bibr" target="#b16">[17]</ref> models the latent contextual representation among actions, the modelled Super-event represents only the latent correlation among the time steps and not among the action classes. Most related to our research direction, Tirupattur et al. <ref type="bibr" target="#b20">[21]</ref> introduced MLAD that can explore the class-temporal relations with a set of self-attention layers: an inter-class attention map for every time-step and an inter-time attention map for every action class. However, the large number of attention maps leads to huge computational costs for long untrimmed videos and hence, limits the model to learn the discriminative relations among the action classes. To tackle this, we propose CTRN, which is a graph-based model. Different from MLAD, CTRN explores the action class relation shared by all the time steps but in different temporal scales. This design enables CTRN to effectively handle both short-term and long-term action relations simultaneously. Moreover, we propose a G-Classifier that focuses on the co-occurring actions taking into account the inter-dependencies of the actions in the training distribution. The proposed method is introduced in details in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>In this section, we formulate the proposed end-to-end model Class-Temporal Relational Network (CTRN) for action detection. As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, our model is composed of four major components. The Visual Encoder encodes the video into a sequence of snippet-level spatio-temporal representation. This representation is fed to a Class-temporal Relational Network (CTRN) that predicts the action labels at each time instant. The sub-components in CTRN consist of the following: Firstly, a Representation Transform Module, which transforms the mixed visual representation into a class-wise representation. Secondly, a Class-Temporal Module explores the action class relations across different time-steps and at different temporal resolutions. Finally, a G-Classifier which classifies the class-temporal features into action categories. Unlike previous binary classifiers <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> that overlook the dependencies between the action classes, G-Classifier leverages the privilege class dependencies within the training data, thus improving the co-occurring action detection performance. In the following, we introduce these modules in details. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Visual Encoder</head><p>Similar to most action detection models <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>, our model processes the features on top of video snippet representations extracted from 2D/3D CNNs. In this work, we use spatio-temporal features extracted from RGB and Optical Flow (OF) I3D networks <ref type="bibr" target="#b2">[3]</ref> to encode appearance and motion information respectively. Then, a video is divided into T non-overlapping snippets, each snippet consisting of 16 frames. The inputs to the RGB and Flow deep networks are either the color images or the corresponding OF frames of a snippet. We stack the snippet-level features along the temporal axis to form a T ? D 1 dimensional video representation, denoted as X. The action instances in X are always longer than a snippet and their visual representation mixes information of all action classes. As a result, X is not discriminative enough, and needs both temporal and class modelling. To this end, we develop the class-temporal relationship from the input representation X within CTRN which is described in the following. Note that the model architecture remains the same for both RGB and OF streams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Representation Transform Module</head><p>The input X is first fed into the Representation Transform Module (RTM). The goal of this module is to transform the input to a class-specific representation and to lightweight the channel size to facilitate the following computation. In practice, RTM duplicates the input features C times into a new dimension representing the action classes followed by a channelmixer MLP with non-linear activation and dropout.MLP is the linear transformation layer <ref type="bibr" target="#b0">[1]</ref> to do the linear projection. The equation can be formulated as:</p><formula xml:id="formula_0">X i = ReLU(MLP(X)) (1) X = DropOut([X 1 , X 2 , ...X C ]))<label>(2)</label></formula><p>where X ? R T ?C?D 2 is the output representation of RTM. D 2 = D 1 ? in which ? is larger than 1 to shallow the channel size. In order to learn class-specific representation, we embed an auxiliary branch with a G-classifier that maps X to the action labels (see <ref type="figure" target="#fig_0">Fig. 2</ref>). This transformed feature representation is further exploited to explore the class and temporal relations in the subsequent modules of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Class-Temporal Modeling</head><p>The Class-Temporal Module (CTM) is the key component of CTRN that exploits the classtemporal relations of its input feature. Inspired by the recent success of Graph Convolutional Network (GCN) in relational reasoning <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25]</ref>, we build this module with GCN. The objective of this component is to update the feature representations by propagating the information across different classes and across different time steps. For modelling the action class relations, we introduce a Class-GCN (C-GCN) layer while the traditional Temporal Convolutional Network (TCN) layer <ref type="bibr" target="#b14">[15]</ref> is utilized to aggregate the temporal information.</p><p>The combination of C-GCN and TCN enables CTM to capture the class semantic information along different temporal hierarchies. Thanks to the learnable graph structure, C-GCN is adaptive with the temporal scale set by TCN.</p><p>In the following, we first introduce how we map the feature representation to the graph structure and then we introduce the CTM components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Representation-to-Graph Mapping</head><p>For GCN to process the action relations, the data is to be converted into a graphical structure. As we have transformed the representation into the class-specific format, thus each vertex of the graph represents an action class at a time step with an embedding vector belonging to R D 2 . In total, the graph consists of C ? T vertices whose topology is defined by an adjacency matrix (A C ). This matrix determines whether there are connections (i.e. relations) and its weights determine the intensity of the connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Class-GCN (C-GCN)</head><p>Class-GCN aims at performing the cross-class reasoning over the constructed graph representation. The relations between the many action instances are complex and are different across videos. Besides, multiple C-GCNs are stacked in CTM through which C-GCNs capture different levels of semantic information. Consequently, the graph adjacency A C learns from the data itself for it to be adaptive across different temporal scales.</p><p>In practice, A C ? R C?C is parameterized and is optimized together with other parameters in the training process. Moreover, to differentiate the class relations owing to different videos, the adjacency matrix A C learns the inter-dependencies among the classes using a self-attention mechanism. For this, the input feature X C in ? R D 2 ?T ?C is first embedded using bottleneck convolutional layer (i.e. 1 ? 1). After that, the output feature maps are rearranged into R D 2 T ?C and R C?D 2 T followed by a matrix multiplication. The value of the resultant matrix is then normalized by a softmax activation. Now, the superimposed adjacency matrix A C can be formulated as:</p><formula xml:id="formula_1">A C = A C + so f tmax(W 1 X C in W 2 X C in )<label>(3)</label></formula><p>where X C in is the input of the C-GCN, and W 1 and W 2 are the weights of the bottleneck convolutions. Each value in this matrix can be seen as a soft edge between two vertices. The learned graph is shared across different time-steps but unique for different layers and videos. This design choice can capture the inter-class dependencies in a video and makes C-GCN scalable across different temporal scales. Finally, we perform the graph convolutional operation with the formulation in <ref type="bibr" target="#b13">[14]</ref>:</p><formula xml:id="formula_2">X C out = A C X C in W 3<label>(4)</label></formula><p>where W 3 ? R D 2 ?D 2 is the learnable weight matrix. The operation with A C and with W 3 represents the message passing and vertex feature updating, respectively. Finally, X C out is rearranged to R D 2 ?T ?C .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">CTM Block</head><p>As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, there are L blocks in CTM, each block is composed of a C-GCN and a TCN layer along with batch normalization and non-linear activations. To stabilize the training, two residual connections are added in each block. As mentioned earlier, TCN <ref type="bibr" target="#b14">[15]</ref> aggregates the features across the temporal dimension while increasing the size of the temporal receptive field. In this work, we set a fixed kernel size K for all the TCNs. Thanks to the hierarchical structure of CTM, C-GCN can focus on short-term action-dependencies in lower blocks and long-term action dependencies in higher blocks. The refined feature representation from the last block is fed into G-Classifier for the snippet-level classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">G-Classifier (G-Clf)</head><p>Finally, we introduce a graph-based G-Classifier to perform the final snippet-level classification. In action detection, multiple actions could happen simultaneously; thus, prior knowledge of inter-dependencies among different action classes can benefit in making precise predictions. Inspired from <ref type="bibr" target="#b4">[5]</ref> in image recognition, we introduce a GCN-based classifier in our task, which has different working mechanisms. In the previous work <ref type="bibr" target="#b4">[5]</ref>, the input of GCN is the word embedding of the label and the output is the classifier weights between the feature representation and the prediction. In contrast to that, our input and output to the GCN are the video representation and the prediction scores respectively. As our input is directly the video representation, thus the action occurrence information within each snippet are captured, which benefits the information propagation between relevant classes through graph connections. Compared to the standard binary classifier <ref type="bibr" target="#b16">[17]</ref>, G-Classifier has an additional message passing step between the potential co-occurring action pairs, thus improving the co-occurring action detection performance. Different to C-GCN, G-Classifier models only the snippet-level features and focuses only on the actions that occur simultaneously.</p><p>In practice, firstly, we compute the co-occurrence probabilities of all the action pairs in the training snippets. M i j indicates the concurring times for action class C i and C j . Then, the conditional probability matrix P i j = P(C j |C i ) is given by: <ref type="bibr" target="#b4">5)</ref> where N i indicates the occurrence times of C i in training set, and P i j ? R C?C indicates the probability of class C j given that C i occurs at the same time. In fine-grained action datasets, some rare co-occurrences may add noise for detecting other common actions, and the number of co-occurrences from training and test set may not be completely consistent. In this work, we perform a thresholding operation to binarize the conditional probability matrix to filter the noisy edges and make the classifier more robust to inconsistent action classes. If P i j ? ? , A S i j is assigned 1, otherwise 0, where ? is the threshold. The computed co-occurrence matrix A S is a binary correlation matrix which in turn defines the adjacency matrix of the graph for G-Classifier. The feature of a node is computed by the weighted sum of its own features and the adjacent nodes' features. However, the binary correlation matrix may change the feature   scale <ref type="bibr" target="#b13">[14]</ref> and make the node feature over-smoothed <ref type="bibr" target="#b15">[16]</ref>. To alleviate this problem, we normalized the A S following the re-weighted scheme in <ref type="bibr" target="#b4">[5]</ref>. Different to the learnable adjacency matrices in C-GCN, A S is fixed during training. The formulation of this G-Classifier is given below:</p><formula xml:id="formula_3">P i j = M i j /N i<label>(</label></formula><formula xml:id="formula_4">S = ? (A S X L W S )<label>(6)</label></formula><p>where S is the prediction score, ? is the sigmoid activation. X L is the output feature from the last block of the Class-Temporal Module, and W S ? R 1?D 2 are the learnable weights of the G-Classifier. To learn the parameters, we optimize the multi-label binary cross-entropy loss with the prediction results from the RTM and CTM. The total objective is formulate as:</p><formula xml:id="formula_5">L total = L CT M + ?L RT M<label>(7)</label></formula><p>where ? is a weighting factor. Thus, by jointly optimizing both the entropy losses, the model learns the relevant action labels per segment along with learning the class-specific semantics across the Representation transform module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>Datasets. We evaluate our method on three densely labelled action detection datasets: Charades <ref type="bibr" target="#b19">[20]</ref>, TSU <ref type="bibr" target="#b6">[7]</ref> and MultiTHUMOS <ref type="bibr" target="#b23">[24]</ref>. These datasets contain videos of different types: (1) sports and daily living videos, (2) short and long videos. We follow the original settings of these datasets for action detection. All these datasets are evaluated by the perframe mAP.  <ref type="table">Table 2</ref>: Evaluation on the Charades dataset using the action-conditional metric <ref type="bibr" target="#b20">[21]</ref>. P AC -Action-Conditional Precision, R AC -Action-Conditional Recall, F1 AC -Action-Conditional F1-Score, mAP AC -Action-Conditional Mean Average Precision. ? indicates the temporal window size. ? = 0 corresponds to the actions occuring at the same time.</p><p>Implementation. To have a fair comparison with previous works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21]</ref>, our network is built on top of I3D, where D 1 is 1024 and D 2 is 64. Dropout probability is 0.3. For CTM, we choose a 5-block (L) structure. For C-GCN, the adjacency matrix is initialized by 1 and normalized by columns. In TCN, the kernel size K is 9 and padding rate is 4. For G-Classifier, ? is set to 0.05. While learning the parameters, the weighting factor ? is 1.2 and the random seed is fixed. We use Adam optimizer <ref type="bibr" target="#b12">[13]</ref> with an initial learning rate of 0.001, and we scale it by a factor of 0.3 with a patience of 10 epochs. The network is trained on a 4-GPU machine for 300 epochs. For two-stream network, a mean pooling is performed between the prediction logits of the RGB and Flow streams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with State-of-the-Art Methods</head><p>The proposed CTRN is compared with previous state-of-the-art methods on the Charades, TSU and MultiTHUMOS datasets in <ref type="table" target="#tab_0">Table 1</ref>. Our proposed method outperforms current state-of-the-art methods on all three datasets. For example, +6.9% (relatively +37.5%) w.r.t. MLAD <ref type="bibr" target="#b20">[21]</ref> on Charades while using only RGB. We then show the ability of CTRN capturing action co-occurrence, we evaluate with the action-conditional metric <ref type="bibr" target="#b20">[21]</ref> in <ref type="table">Table 2</ref>. Compared with state-of-the-art methods, our method achieves higher performance on all action-conditional metrics showing that CTRN effectively models action dependencies both within a time-step (i.e. co-occurring action, ? = 0) and throughout time (? &gt; 0). To confirm the advancement of our method, we present further comparisons with MLAD. Firstly, we compare the model efficiency and complexity in <ref type="figure" target="#fig_1">Fig. 3</ref>. MLAD is about 2 times larger in parameters and 3.5 times larger in FLOPs than CTRN while processing the same batch of videos. Hence, our method is more lightweight and computationally efficient than MLAD. Secondly, we visualize the detection results of an example video in <ref type="figure" target="#fig_2">Fig. 4</ref> for both MLAD and CTRN. Qualitatively, we find that CTRN can detect the actions more precisely than MLAD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation studies</head><p>Firstly, in <ref type="table" target="#tab_3">Table 3</ref>, we study the complementation of the components in the proposed network on the Charades dataset. We first discuss how RTM leads to a better feature representation of the input spatio-temporal feature map from I3D. RTM is an essential pre-step before class-temporal modelling. Thanks to RTM that filters the class-specific feature, the model can slightly improve the detection performance (+0.5%). We then explore how the different components in CTM affects the action detection performance. We find that both C-GCN    <ref type="table">Table 5</ref>: Ablation study on threshold.</p><p>and TCN improve the performance w.r.t. a model with only RTM (+23.6, and 32.9% relatively). The action detection performance is further improved by the combination of both C-GCN and TCN, thus reflecting the complementary nature of both the operations. Finally, we study the performance with/without G-Classifier. With the proposed classifier, RTM and RTM+CTM further improve the action detection performance by +2.3% and +0.6% respectively. Note that for the baseline without G-Classifier, similar to the previous work <ref type="bibr" target="#b16">[17]</ref>, we utilize a 1 ? 1 convolution as the classifier. These results show that the different components of CTRN contribute to the overall performance of our network. Secondly, we analyse if G-Classifier can better detect the co-occurring actions. We compared our method with the standard binary classifier <ref type="bibr" target="#b16">[17]</ref>, Chen et al. <ref type="bibr" target="#b4">[5]</ref> by computing the mAP with the snippets containing more than one action. All three classifiers are build upon the same backbone (CTRN). We find that the standard binary classifier, Chen et al. and G-Classifier achieve 24.1%, 25.3% and 27.6% respectively on Charades, reflecting that G-Classifier can better detect the co-occurring actions.</p><p>To further explore the relationship between inter-classes and temporal modules, we perform ablations with C-GCN and TCN having different numbers of blocks. In <ref type="table" target="#tab_4">Table 4</ref>, we find that while having more blocks: C-GCN improves marginally since it captures action relations in a single temporal scale; TCN improves the performance with more blocks due to increased temporal receptive field; C-GCN+TCN achieves the best performance than TCN since C-GCN can capture action relations in more temporal scales when combined with TCN (see the improvements for 5 blocks).</p><p>Finally, we explore how hard thresholding affects the performance of G-Classifier. In <ref type="table">Table 5</ref>, we find that G-Classifier performs better with the threshold since hard threshold filters the noisy edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Qualitative Analysis</head><p>In <ref type="figure">Fig. 5</ref>, we show the adjacency matrix of G-Classifier in Charades (157 classes), which provides the information of all the co-occurring action pairs with high probabilities. For example, holding a vacuum &amp; tiding something on the floor and fixing hair &amp; watching <ref type="figure">Figure 6</ref>: Visualization of the learned C-GCN adjacency matrix A C for different layers. Here, we visualize the 1 st , 3 rd and 5 th block's adjacency matrices. For simplicity, we provide only the relevant action classes in the example video.</p><p>in a mirror are the actions always occur at the same time. Prior access to such privilege knowledge is crucial for detecting the co-occurring actions in the densely labelled videos.</p><p>In CTM, TCN is used to aggregate the temporal information which enables C-GCN to explore action relations at different temporal scales. To validate the usage of these layers, in <ref type="figure">Fig. 6</ref>, we visualize the learned adjacency matrix of C-GCN from three different blocks. We find that in Block 1, C-GCN focuses on capturing the contextual information pertaining to locally related action classes. For example, eat sandwich &amp; hold sandwich and drink water &amp; hold cup are always occurring closely in the video. Then we find that, Block 3 has increased the temporal receptive field, thus, C-GCN can capture the long-term dependencies between distant action classes. For example, Pour water and Drink water. Finally, Block 5 possess the largest receptive field where each local snippet feature contains the whole video information. Therefore, C-GCN in this block models all the potential action relations in the video, resulting in many activated links in the adjacency matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we propose a novel class-temporal relation network for action detection. This network can handle both action class relations at the same time-step, and across different time-steps by using its three key components, namely Representation Transform Module, Class-Temporal Module, and G-Classifier. The network is evaluated on three challenging densely labelled datasets and achieves state-of-the-art performance on all of them. Furthermore, this network has less computation cost and parameters than the representative baseline method. For future perspectives, we will investigate combining C-GCN and TCN into a single layer.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Overall structure. The model composed of a Visual Encoder, a Representation Transform Module, a Class-Temporal Module (with C-GCN and TCN) and a G-Classifier (i.e. G-Clf). Note: Two G-Clfs are sharing the weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Computation efficiency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Model prediction visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison with the State-of-the-art on three densely labelled datasets. The results are given in per-frame mAP (%). RGB +OF indicates the late fusion performance.</figDesc><table><row><cell>Model</cell><cell>Modality</cell><cell>Charades</cell><cell>TSU</cell><cell>MultiTHUMOS</cell></row><row><cell>R-C3D [22]</cell><cell>RGB</cell><cell>12.7</cell><cell>8.7</cell><cell>-</cell></row><row><cell>I3D + TAN [10]</cell><cell>RGB+OF</cell><cell>17.6</cell><cell>-</cell><cell>33.3</cell></row><row><cell>I3D + Superevent [17]</cell><cell>RGB</cell><cell>18.6</cell><cell>17.2</cell><cell>36.4</cell></row><row><cell>I3D + TGM [18]</cell><cell>RGB</cell><cell>20.6</cell><cell>26.7</cell><cell>37.2</cell></row><row><cell>I3D + TGM [18]</cell><cell>RGB+OF</cell><cell>21.5</cell><cell>-</cell><cell>44.3</cell></row><row><cell>I3D + TGM + Superevent [18]</cell><cell>RGB+OF</cell><cell>22.3</cell><cell>-</cell><cell>46.4</cell></row><row><cell>I3D + MLAD [21]</cell><cell>RGB</cell><cell>18.4</cell><cell>-</cell><cell>42.2</cell></row><row><cell>I3D + MLAD [21]</cell><cell>RGB+OF</cell><cell>22.9</cell><cell>-</cell><cell>49.6</cell></row><row><cell>I3D + CTRN</cell><cell>RGB</cell><cell>25.3</cell><cell>33.5</cell><cell>44.0</cell></row><row><cell>I3D + CTRN</cell><cell>RGB+OF</cell><cell>27.8</cell><cell>-</cell><cell>51.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">: Ablation study on Charades</cell><cell>Figure 5: The adjacency matrix of the G-</cell></row><row><cell cols="3">dataset using only RGB.</cell><cell></cell><cell></cell><cell>Classifier A S .</cell></row><row><cell cols="2">CTM Component</cell><cell></cell><cell>#Block</cell><cell></cell></row><row><cell>C-GCN</cell><cell>TCN</cell><cell>1</cell><cell>3</cell><cell>5</cell></row><row><cell></cell><cell></cell><cell>17.9</cell><cell>19.3</cell><cell>20.0</cell></row><row><cell></cell><cell></cell><cell>18.7</cell><cell>20.1</cell><cell>21.9</cell></row><row><cell></cell><cell></cell><cell>19.8</cell><cell>23.3</cell><cell>25.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Analysis between C-GCN and TCN.</figDesc><table><row><cell></cell><cell cols="2">Backbone</cell></row><row><cell></cell><cell>RTM</cell><cell>CTRN</cell></row><row><cell>w/o Threshold</cell><cell>17.8</cell><cell>24.9</cell></row><row><cell>with Threshold</cell><cell>18.4</cell><cell>25.3</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work has been supported by the French government, through the 3IA Cote d'Azur Investments in the Future project managed by the National Research Agency (ANR) with the reference number ANR-19-P3IA-0002. The authors are also grateful to the OPAL infrastructure from Universit? C?te d'Azur for providing resources and support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">C1. 2 multilayer perceptrons. Handbook of</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Almeida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation C</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Afnet: Temporal locality-aware network with dual structure for accurate and fast action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexian</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-label image recognition with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Zhao-Min Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5177" to="5186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Gianpiero Francesca, and Fran?ois Bremond. Self-attention temporal convolutional network for long-term daily living activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Minciullo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Garattoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Toyota smarthome untrimmed: Real-world untrimmed videos for activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srijan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurav</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Minciullo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Garattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Bremond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianpiero</forename><surname>Francesca</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.14982</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning an augmented rgb representation with cross-modal knowledge distillation for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srijan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Bremond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="13053" to="13064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pdan: Pyramid dilated attention network for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srijan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Minciullo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Garattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianpiero</forename><surname>Francesca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Bremond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2021-01" />
			<biblScope unit="page" from="2970" to="2979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Tan: Temporal aggregation network for dense multi-label action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Yue-Hei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="151" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improving action segmentation via graph-based temporal reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoichi</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="14024" to="14034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu-Gang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sukthankar</surname></persName>
		</author>
		<ptr target="http://crcv.ucf.edu/THUMOS14/" />
		<title level="m">THUMOS challenge: Action recognition with a large number of classes</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6980" />
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6980</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks for action segmentation and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rene</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory D</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="156" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning latent super-events to detect multiple activities in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael S Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael S Ryoo</surname></persName>
		</author>
		<title level="m">Temporal gaussian mixture layer for videos. International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gunnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?l</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision(ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Asynchronous temporal fields for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Gunnar A Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="585" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Modeling multi-label action dependencies for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Tirupattur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">R-c3d: Region convolutional 3d network for temporal activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abir</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5783" to="5792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">G-tad: Sub-graph localization for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Rojas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10156" to="10165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Every moment counts: Dense detailed labeling of actions in complex videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">2-4</biblScope>
			<biblScope unit="page" from="375" to="389" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
