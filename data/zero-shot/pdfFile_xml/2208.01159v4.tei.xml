<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BATMAN: Bilateral Attention Transformer in Motion-Appearance Neighboring Space for Video Object Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yu</surname></persName>
							<email>yu.ye@microsoft.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Microsoft</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialin</forename><surname>Yuan</surname></persName>
							<email>yuanjial@oregonstate.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Oregon State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Mittal</surname></persName>
							<email>gaurav.mittal@microsoft.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Microsoft</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fuxin</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Oregon State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Chen</surname></persName>
							<email>mei.chen@microsoft.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Microsoft</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">BATMAN: Bilateral Attention Transformer in Motion-Appearance Neighboring Space for Video Object Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Bilateral attention</term>
					<term>Motion-appearance space</term>
					<term>Optical flow calibration</term>
					<term>Video object segmentation</term>
					<term>Vision transformer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video Object Segmentation (VOS) is fundamental to video understanding. Transformer-based methods show significant performance improvement on semi-supervised VOS. However, existing work faces challenges segmenting visually similar objects in close proximity of each other. In this paper, we propose a novel Bilateral Attention Transformer in Motion-Appearance Neighboring space (BATMAN) for semi-supervised VOS. It captures object motion in the video via a novel optical flow calibration module that fuses the segmentation mask with optical flow estimation to improve within-object optical flow smoothness and reduce noise at object boundaries. This calibrated optical flow is then employed in our novel bilateral attention, which computes the correspondence between the query and reference frames in the neighboring bilateral space considering both motion and appearance. Extensive experiments validate the effectiveness of BATMAN architecture by outperforming all existing state-of-the-art on all four popular VOS benchmarks: Youtube-VOS 2019 (85.0%), Youtube-VOS 2018 (85.3%), DAVIS 2017Val/Testdev (86.2%/82.2%), and DAVIS 2016 (92.5%).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Video Object Segmentation (VOS) is fundamental to video understanding with broad applications in content creation, content moderation, and autonomous driving. In this paper, we focus on the semi-supervised VOS task, where we segment target objects in each frame of the entire video sequence (query frames) given their segmentation masks in the first frame (reference frame) only. Moreover, the task is class-agnostic in that we do not have any class annotation for any object to be segmented in either training or testing phases. The key challenge in semi-supervised VOS is how to propagate the mask from the reference frame to all the query frames in the rest of the sequence without any class annotation.</p><p>Due to the absence of class-specific features, VOS models need to match features of the reference frame to that of the query frames both spatially and temporally to capture the class-agnostic correspondence and propagate the segmentation masks. Previous methods attempt to store features from preceding frames in memory networks and match the query frame through a non-local attention mechanism <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b6">7]</ref>, or compute a global-to-global attention through an encoder-decoder transformer <ref type="bibr" target="#b24">[25]</ref>, or propagate and calibrate features from the reference frame to the query frames using a propagation-correction scheme <ref type="bibr" target="#b46">[47]</ref>. These methods employ a global attention mechanism to establish correspondence between the full reference frame and the full query frame. This can lead to failure in distinguishing the target object(s) from the background particularly when there are multiple objects with a similar visual appearance. A spatial local attention is proposed in <ref type="bibr" target="#b49">[50]</ref> to mitigate this problem, where the attention is only computed between each query token and its surrounding key tokens within a spatial local window. However, it still suffers from incorrectly segmenting visually similar objects in close proximity of each other.</p><p>In addition to spatial correspondence, it is essential to match features temporally for optimal object segmentation across video frames. To this end, some VOS methods <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b7">8]</ref> leverage optical flow to capture object motion. <ref type="bibr" target="#b44">[45]</ref> warps the memory frame mask using optical flow before performing local matching between memory and query features based on the warped mask, while <ref type="bibr" target="#b7">[8]</ref> simultaneously trains the model for object segmentation and optical flow estimation by bidirectionally fusing feature maps from the two branches. However, these methods are not able to perform optimally as optical flow is usually noisy and warping features/masks to match objects across frames accumulates errors in both optical flow and segmentation mask along the video sequence.</p><p>To overcome the above challenges, we propose Bilateral Attention Transformer in Motion-Appearance Neighboring space (BATMAN). BATMAN introduces a novel bilateral attention module that computes the local attention map between the query frame and memory frames with both motion and appearance in consideration. Unlike the conventional spatial local attention mechanism ( <ref type="figure" target="#fig_0">Fig. 1(a)</ref>) that computes the attention within a predefined fixed local window, our bilateral attention adaptively computes the local attention based on the tokens' spatial distance, appearance similarity, and optical flow smoothness, as shown in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>. Observing that optical flow may be especially noisy for fastmoving object(s), BATMAN introduces a novel optical flow calibration module that leverages the mask information from the memory frame to smooth the optical flow within the same object while reducing noise at the object boundary.</p><p>We conduct extensive experiments on four popular VOS benchmarks: Youtube-VOS 2019 <ref type="bibr" target="#b45">[46]</ref>, Youtube-VOS 2018 <ref type="bibr" target="#b45">[46]</ref>, DAVIS 2017 <ref type="bibr" target="#b31">[32]</ref>, and DAVIS 2016 <ref type="bibr" target="#b29">[30]</ref> to validate the BATMAN architecture. We show that BATMAN achieves superior performance on all benchmarks and outperforms all previous state-of-the-art methods. We summarize the main contributions of our work below, ? A novel bilateral attention module that computes attention between query and memory features in the bilateral space of motion and appearance, which improves the correspondence matching by adaptively focusing on relevant object features while reducing the noise from the background. ? A novel optical flow calibration module that fuses the object segmentation mask and the initial optical flow estimation to smooth the within-object optical flow and reduce noise at the object boundary. ? Incorporating the optical flow calibration and bilateral attention mechanisms, we design a novel BATMAN architecture. BATMAN establishes new state-of-the-art performance on Youtube-VOS 2019 / 2018 and DAVIS 2017 / 2016 benchmarks. To the best of our knowledge, BATMAN is the first work to compute attention in the bilateral space of motion and appearance for VOS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Semi-supervised VOS. The task aims to segment the particular object instances throughout the entire video sequence given one or more annotated frames (the first frame in general). Early DNN works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b43">44]</ref> fine-tune the pre-trained networks on the first frame using multiple data augmentations on the given mask at test time to adapt to specific instances. Therefore, these methods are extremely slow during inference due to excessive fine-tuning. Later trackingbased works <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b4">5]</ref> adopt object tracking technologies to indicate the target location of objects for segmentation to improve inference time. However, these approaches are not robust to occlusion and drifting with error accumulated during the propagation. "Tracking-by-detection" paradigm is introduced into VOS in <ref type="bibr" target="#b15">[16]</ref> to take object segmentation as a subtask of tracking, in which the accuracy of tracking often limits the performance. To handle occlusion and drifting, matching-based methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b38">39]</ref> perform feature matching to find objects that are similar to the target objects in the reference frames. STM <ref type="bibr" target="#b26">[27]</ref> and its following works <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b44">45]</ref> leverage an external memory to store past frames' features and then distinguish objects with a similar appearance by pixel-level attention-based matching from the memory.</p><p>Vision Transformer. Initially proposed for machine translation, Transformers <ref type="bibr" target="#b37">[38]</ref> replace the recurrence and convolutions entirely with hierarchical attentionbased mechanisms and achieve outstanding performance. Later, transformer networks became dominant models used in natural language processing (NLP) tasks <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b51">52]</ref>. Recently, with the observance of its strength in parallel modeling global correlation or attention, transformer blocks were introduced to computer vision tasks, such as image recognition <ref type="bibr" target="#b9">[10]</ref>, saliency prediction <ref type="bibr" target="#b52">[53]</ref>, object detection <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b3">4]</ref>, and object segmentation <ref type="bibr" target="#b40">[41]</ref>, where vision transformers have achieved excellent performance compared to the CNN-based counterparts. Researchers then employed transformer architecture into the VOS task <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b49">50]</ref>. SST <ref type="bibr" target="#b10">[11]</ref> adopts the transformer's encoder to compute attention based on the spatialtemporal information among multiple history frames. In <ref type="bibr" target="#b22">[23]</ref>, a transductive branch is used to capture the spatial-temporal information, which is integrated with an online inductive branch within a unified framework. TransVOS <ref type="bibr" target="#b24">[25]</ref> introduces a transformer-based VOS framework with intuitive structure from the transformer networks in NLP. AOT <ref type="bibr" target="#b49">[50]</ref> proposes an Identification Embedding to construct multi-object matching and computes attention for multiple objects simultaneously. In this paper, we introduce a novel bilateral attention transformer framework, where it computes the attention with both the encoded appearance features and the motion features in consideration. Therefore, it is robust to occlusion, drift, and ambiguity between objects with a similar appearance.</p><p>Optical Flow. Applying optical flow to VOS can encourage motion consistency through the entire video sequence. Early approaches <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b7">8]</ref> consider VOS and optical flow estimation simultaneously with the assumption that the two tasks are complementary. Recently, RMNet <ref type="bibr" target="#b44">[45]</ref> introduces using optical flow generated with an offline model to warp object mask from the previous frame to the query frame and then performing regional matching. It avoids unnecessary matching in regions without target objects or mismatching of objects with a similar appearance. Instead of simply warping the object's mask to indicate the target area, our BATMAN computes the correlation of each pair of tokens considering their optical flow estimation, appearance similarity, and spatial distance simultaneously. Thus, it is more effective in removing irrelevant matching tokens </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2:</head><p>Overview of the BATMAN architecture. Frame-level features of the reference frames and the query frame are extracted through the memory and query encoders, respectively. A pre-trained FlowNet is used to generate an initial optical flow estimation between the previous frame and the query frame, which is then improved by the optical flow calibration module. A bilateral space encoder is used to encode the query features and the calibrated optical flow into a bilateral space encoding, which is used by the bilateral attention. Multiple layers of bilateral transformer blocks are stacked for matching the correspondence between the reference and query features. Lastly, a decoder is used to predict the query frame segmentation mask compared to <ref type="bibr" target="#b44">[45]</ref>. Meanwhile, our method is more robust to the accumulated error in warping from the optical flow estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we first introduce the proposed BATMAN architecture, and then discuss in depth its core modules: bilateral attention and optical flow calibration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Bilateral Attention Transformer in Motion-Appearance</head><p>Neighboring space (BATMAN) <ref type="figure">Fig. 2</ref> provides an overview of the proposed BATMAN architecture. We first extract frame-level features through the memory and query encoders (details in Sec. 4.1) to capture the target object features for establishing correspondence in the later transformer layers. Meanwhile, we compute the initial optical flow between the query frame and its previous frame through a frozen pre-trained FlowNet <ref type="bibr" target="#b35">[36]</ref>. Then, we feed the object mask from the previous frame, together with the initial optical flow estimation, into our optical flow calibration module to improve the optical flow (Sec. 3.3). We then encode the calibrated optical flow and the query frame features into tokens in the bilateral space of motion and appearance. Following this, we stack multiple bilateral transformer blocks to model the spatial-temporal relationships among the reference and query frames at pixel-level, based on the bilateral space encoding tokens (Sec. 3.2). After aggregating the spatial-temporal information, the decoder predicts an object mask for the query frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Bilateral transformer and bilateral attention</head><p>As shown in <ref type="figure">Fig. 2</ref>, in each bilateral transformer block, the query frame features first go through a self-attention <ref type="bibr" target="#b37">[38]</ref> to aggregate the information within the query frame followed by adding a sinusoidal position embedding <ref type="bibr" target="#b37">[38]</ref> encoding the tokens' relative positions. Then we apply cross-attention and bilateral attention (described below) to it with the reference frame features and add the results. Following the common practice in vision transformers <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b24">25]</ref>, we insert layer normalization <ref type="bibr" target="#b0">[1]</ref> before and after each attention module. Finally, we employ a two-layer feed-forward MLP block before feeding the output to the next layer.</p><p>Bilateral space encoding (E) is used to index each position (token) of the query frame features in the bilateral space. As shown in <ref type="figure">Fig. 2</ref>, we first encode the calibrated optical flow using a flow encoder (details in Sec. 4.1). Then we concatenate the optical flow encoding and the query image encoding (from query encoder) in channel dimension. Finally, we use a 1 ? 1 convolutional layer to project the concatenation to a 1-dimensional space (in channel) where each position (token) has a single scalar coordinate for the bilateral space of motion and appearance. Bilateral space encoding is employed in bilateral attention below.</p><p>Bilateral attention is used to aggregate spatial-temporal information between the query tokens and neighboring key tokens from the reference frames in the bilateral space of motion and appearance. Unlike global cross-attention where each query token computes attention with all key tokens from the reference frames, our bilateral attention adaptively selects the most relevant key tokens for each query token based on the bilateral space encoding. To formulate, we define query tokens Q ? R HW ?C , key tokens K ? R HW ?C , and value embedding tokens V ? R HW ?C , where Q is from the query frame and K and V are aggregated from multiple reference frames. H, W , and C represent the height, width, and channel dimensions of the tokens, respectively. Mathematically, we define bilateral attention as,</p><formula xml:id="formula_0">BiAttn(Q, K, V ) = sof tmax( QK T M ? C )V<label>(1)</label></formula><p>where M ? [0, 1] HW ?HW is the bilateral space binary mask that defines the attention scope for each query token. For each query token Q h,w at (h, w) position, we define the corresponding bilateral space binary mask M h,w as,</p><formula xml:id="formula_1">M h,w (i, j, E) = ? ? ? ? ? 1 if |i ? h| ? W d and |j ? w| ? W d and |argsort W d (E h,w ) ? argsort W d (E i,j )| ? W b 0 otherwise (2)</formula><p>where (i, j) is the position for each key token, E ? R HW ?1 is the bilateral space encoding of the queries discussed above, W d and W b are predefined local windows in spatial and bilateral domains, respectively. argsort W d (E i,j ) denotes sorting all bilateral space encoding E within the spatial local window W d and finding the corresponding index at position (i, j). To train the bilateral space encoding E by stochastic gradient descent directly, in practice, instead of computing QK T M as shown in Eq. 1, we compute QK T + E if M = 1, while computing QK T ? L if M = 0, where L ? R is a large positive number. This approximates to QK T M in Eq. 1 after using sof tmax. Eq. 2 shows that for each query token, it computes the attention with another key token only if they are close to each other spatially and share similar bilateral space encoding (similar motion and appearance). We further analyze the bilateral space binary mask with visualization in Sec. 4.3.</p><p>We implement the bilateral attention modules via a multi-headed formulation <ref type="bibr" target="#b37">[38]</ref> where we linearly project queries, keys, and values multiple times with different learnable projections, and we feedforward the multiple heads of bilateral attention in parallel followed by concatenation and a linear projection. Mathematically, we define the multi-head bilateral attention as,</p><formula xml:id="formula_2">M ultiHead(Q, K, V ) = Concat(head 1 , ..., head h )W O where head i = BiAttn(QW Q i , KW K i , V W V i )<label>(3)</label></formula><p>where projection matrices are W Q i ? R C?d hidden , W K i ? R C?d hidden , W V i ? R C?d hidden , and W O ? R C?C . In this work, we set the number of heads (h = C/d hidden ) to 8 <ref type="bibr" target="#b37">[38]</ref>, where d hidden is the hidden dimension of each head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Optical flow calibration</head><p>As mentioned in the introduction, optical flow estimation can be noisy for objects with large motion and in texture-less areas. We introduce an optical flow calibration module to improve flow estimation by leveraging the segmentation mask from the previous frame. As shown in <ref type="figure" target="#fig_1">Fig. 3</ref>, the module employs an architecture similar to U-Net <ref type="bibr" target="#b32">[33]</ref> with 11-layers total. To train this module to improve optical flow, we compute the Mean Square Error (MSE) between the initial optical flow and the output optical flow in training. Without the MSE loss, mask information can dominate the calibration module and thereby generate an embedding feature for the mask instead. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We validate BATMAN on popular benchmark datasets YouTube-VOS 2019/2018 and DAVIS 2017/2016. We first provide implementation details, followed by the experimental results. We then present the ablation study on our design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation details</head><p>We use ResNet50 <ref type="bibr" target="#b13">[14]</ref> as the feature extractor for memory/query/flow encoder. We follow the identification embedding in <ref type="bibr" target="#b49">[50]</ref> to encode multiple object masks in the memory encoding simultaneously. We use a RAFT <ref type="bibr" target="#b35">[36]</ref> model pre-trained on FlyingThings3D <ref type="bibr" target="#b23">[24]</ref> for optical flow generation. We use FPN <ref type="bibr" target="#b18">[19]</ref> with Group Normalization <ref type="bibr" target="#b42">[43]</ref> as the decoder. We employ 12 bilateral transformer blocks with W d and W b set to 7 <ref type="bibr" target="#b49">[50]</ref> and 84 (details in supplementary), respectively. We implement our model in PyTorch <ref type="bibr" target="#b27">[28]</ref> and train with a batch size of 16 distributed on 8 V100 GPUs. Following previous works <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b44">45]</ref>, we first pre-train our model on synthetic video sequences generated from static image datasets (COCO <ref type="bibr" target="#b19">[20]</ref>, ECSSD <ref type="bibr" target="#b34">[35]</ref>, MSRA10K <ref type="bibr" target="#b8">[9]</ref>, SBD <ref type="bibr" target="#b12">[13]</ref>, PASCALVOC2012 <ref type="bibr" target="#b11">[12]</ref>) by applying random augmentations. We then train the model on the VOS benchmarks. The loss function is a combination of bootstrapped cross-entropy loss, soft Jaccard loss <ref type="bibr" target="#b25">[26]</ref>, and mean squared error loss. The training is optimized using AdamW <ref type="bibr" target="#b20">[21]</ref> optimizer and Exponential Moving Average (EMA) <ref type="bibr" target="#b30">[31]</ref>. The learning rate for training is set to 2 ? 10 ?4 with a weight decay of 0.07. We train the model for 100,000 iterations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental results</head><p>We present validation results on the popular Youtube 2019/2018 and DAVIS 2017/2016 benchmarks compared to existing state-of-the-art methods.</p><p>Metrics. The region similarity (J ) and the boundary accuracy (F) are computed following the standard evaluation setting proposed in <ref type="bibr" target="#b29">[30]</ref>. On DAVIS, we report the two metrics and their mean value (J &amp;F). On YouTube-VOS, we report all the metrics on seen categories and unseen categories separately as generated by the evaluation server at CodaLab.</p><p>Youtube-VOS <ref type="bibr" target="#b45">[46]</ref> is a large-scale dataset for multi-object video segmentation with objects in multiple categories. In YouTube-VOS 2018, the training set contains 3, 471 videos with 5, 945 unique objects in 65 categories and the validation set has 474 videos containing of 894 unique objects in 65 seen categories and additional 26 unseen categories. YouTube-VOS 2019 expands the YouTube-VOS 2018 dataset with more videos and object annotations. Its training set contains the same 3, 471 videos but has 6, 459 objects. Its validation set has 507 videos containing of 1, 063 objects. With the existence of the unseen object categories, the YouTube-VOS is useful to evaluate the generalization capability of the VOS model on unseen object categories. We evaluate all the results on the official YouTube-VOS evaluation servers on CodaLab. <ref type="table" target="#tab_1">Table 1</ref> shows that BATMAN outperforms all state-of-the-art on Youtube-VOS 2019 and 2018 benchmarks. The higher region similarity (J ) and better boundary accuracy (F) validate that bilateral attention is able to learn the most informative features from the reference frames and match the query frames.</p><p>DAVIS is one of the most popular benchmarks for video object segmentation with high-quality masks for salient objects. As part of DAVIS, DAVIS 2016 <ref type="bibr" target="#b29">[30]</ref> is a single-object benchmark and DAVIS 2017 <ref type="bibr" target="#b31">[32]</ref> is a multi-object extension of DAVIS 2016. In DAVIS 2016, the training and validation sets contain 30 and 20 videos, respectively. In DAVIS 2017, the training set consists of 60 videos, and the validation set consists of 30 videos, and the test-dev set consists of 30 videos with only the first frame annotated. <ref type="table" target="#tab_2">Table 2</ref> compares BATMAN with existing state-of-the-art methods on DAVIS 2017 validation set, test-dev set, and DAVIS 2016 validation set. Note that KMN <ref type="bibr" target="#b33">[34]</ref> only reports the results of DAVIS 2017 test-dev split with images resized to 600p. We follow the standard practice of most previous works and keep the images in the original 480p resolution in evaluation. On both multi-object datasets (DAVIS2017 val/test) and single-object dataset (DAVIS 2016), BATMAN outperforms all existing state-of-the-art methods. Moreover, BATMAN achieves the largest absolute accuracy improvement (2.6%) on the hardest DAVIS 2017 testdev split, which validates the robustness of our model for VOS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation study</head><p>In this section, we analyze the effectiveness of the bilateral attention and compare it to the conventional spatial local attention, as well as the efficacy of the  calibrated optical flow. For qualitative analysis, we visualize the bilateral space binary mask generated by the bilateral attention, and the optical flow output from our calibration module.</p><p>Bilateral Attention. <ref type="table" target="#tab_3">Table 3</ref> compares the accuracy (J &amp;F) between our proposed bilateral attention and the conventional spatial local attention, and validates that the bilateral attention achieves superior performance on all benchmarks. We also visualize the segmentation masks from the two attention mechanisms in <ref type="figure" target="#fig_2">Fig. 4</ref> for both DAVIS 2017 and Youtube-VOS 2019. We can see that with spatial local attention, the model tends to fail to segment objects with similar appearances (e.g., the second camel is included in the mask of the first camel <ref type="figure" target="#fig_2">(Fig. 4a)</ref>; part of the red pig is segmented as the green pig <ref type="figure" target="#fig_2">(Fig. 4c)</ref>; the right hand of the man in green is mistakenly segmented as part of the man in red <ref type="figure" target="#fig_2">(Fig. 4e)</ref>; the tail of the zebra in the green mask is mistakenly segmented as that of the zebra in yellow <ref type="figure" target="#fig_2">(Fig. 4f)</ref>). Besides, when the appearance features (especially at the object boundary) are fuzzy (e.g., the shade of the goat <ref type="figure" target="#fig_2">(Fig. 4b)</ref>, The bilateral attention adaptively generates binary masks for on and off object query tokens. Better view in color version the reflection on the TV box <ref type="figure" target="#fig_2">(Fig. 4d)</ref>, and the reflection of the bird's legs in the water <ref type="figure" target="#fig_2">(Fig. 4g)</ref>), the model with spatial local attention finds it difficult to segment the object properly. In contrast, the bilateral attention and the resultant adaptive bilateral space binary masks enables our model to segment target objects correctly, especially when the target object exhibits salient motion (e.g., the Frisbee <ref type="figure" target="#fig_2">(Fig. 4h</ref>) and the skydiving men <ref type="figure" target="#fig_2">(Fig. 4i)</ref>). We provide additional visualizations for segmentation in the supplementary. <ref type="figure" target="#fig_3">Fig. 5</ref> shows some examples of the binary masks generated from the bilateral attention. The first row shows the optical flow of the query frames. One offobject (background) query token is highlighted (in red) in the second row for each scene. The corresponding bilateral space binary mask is highlighted in the third row. In comparison, we also show an on-object query token in the fourth row, and the corresponding binary mask is given in the last row. We can see that for an off-object query token, the bilateral attention module tends to focus on the background locations (e.g., the water around the swan neck <ref type="figure" target="#fig_3">(Fig. 5a</ref>) or the sky around the woman with dogs <ref type="figure" target="#fig_3">(Fig. 5f)</ref>). On the other hand, when the query token is on the object, it tends to select the neighboring on-object tokens (e.g., the leg of the dancing man <ref type="figure" target="#fig_3">(Fig. 5c</ref>) or the camel hump <ref type="figure" target="#fig_3">(Fig. 5d)</ref>) for the attention computation. This qualitatively validates that adaptive attention computation enables propagating segmentation masks from the reference frames to the query frame more accurately.   <ref type="table" target="#tab_4">Table 4</ref> compares the bilateral attention w/ and w/o calibrated optical flow. With the calibrated optical flow, BATMAN achieves higher accuracy on all benchmarks, validating that optical flow is improved with the help of the previous frame segmentation mask. As shown in <ref type="figure" target="#fig_5">Fig. 6</ref>, the calibrated optical flow is smoother, both within the same object and within the background. Meanwhile, the object boundary is sharper. Specifically, the blocky artifacts along the object boundary, which exists in the initial optical flow, are reduced effectively without affecting the object boundary sharpness.</p><p>Limitations. The bilateral space binary mask generation is influenced by the motion in the scene. Therefore, if the target objects do not exhibit salient motion,  <ref type="figure">Fig. 7</ref>: Failure cases of the bilateral binary mask generation. The bilateral attention may lose focus when a background object exhibits dominant motion and/or the target object does not exhibit salient motion or some background object(s) exhibit salient motion and/or share(s) a similar appearance to the target object(s), the bilateral mask can be noisy and the bilateral attention may lose focus. <ref type="figure">Fig. 7</ref> shows two failure cases: in the upper row, a man on a motorcycle moves quickly across the scene, which overwhelms the motion of the target woman. Hence, the bilateral attention fails to focus on the target object. Similarly, in the bottom row, the motion of the target woman is not salient (especially on the boundary) so the bilateral mask scatters. We plan to extend our method to better handle such scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>This paper proposes a novel architecture, BATMAN, for semi-supervised VOS by adaptively computing attention between the query frame and reference frames based on the bilateral encoding of motion and appearance. Compared to conventional spatial local attention, bilateral attention adaptively selects the most relevant tokens to compute the correlation attention which helps to match the object correspondence spatially and temporally with the help of calibrated optical flow. Extensive experiments validate that BATMAN outperforms all existing state-of-the-art on all popular Youtube-VOS and DAVIS benchmarks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Spatial local attention vs. bilateral attention. (a) Conventional spatial local attention. For any given token in the query frame (top), compute the attention with the neighboring tokens within a predefined fixed local window from the memory frame (bottom). (b) Our proposed bilateral attention. Given a token in the query frame (top), adaptively select the most relevant tokens (bottom), based on the distance in the bilateral space of appearance and motion (right), for cross attention computation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>The optical flow calibration module. A CNN in the U-Net architecture<ref type="bibr" target="#b32">[33]</ref> is used to fuse the segmentation mask into the optical flow</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Qualitative results. Compared to spatial local attention, bilateral attention segments objects better especially when background shares similar appearance with the target object</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Visualization of bilateral space binary masks from the bilateral attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Comparison of the initial optical flow (middle) and the calibrated optical flow (bottom). The calibrated optical flow is smoother within the same object, and sharper at object boundary Blocky artifact on the initial optical flow is decreased on the calibrated optical flow</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Visualization of optical flow on Davis 2017 val. set. The calibrated optical flow is smoother within the object and sharper at the boundary Optical flow calibration. The optical flow calibration module leverages the predicted previous frame mask to improve the optical flow estimation for the current frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Optical Flow Generation Query Encoder Memory Encoder Reference frames with masks Previous frame and query frame Query frame Bilateral Transformer Block Bilateral Space Encoder Optical Flow Calibration Decoder Previous frame mask Predicted mask</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Pos. emb. Memory enc.</cell><cell cols="2">Cross-attn.</cell><cell></cell></row><row><cell>Query enc.</cell><cell>LN</cell><cell>Self-attn.</cell><cell>LN Memory enc.</cell><cell cols="2">Bi-attn. Bi. space enc.</cell><cell>LN</cell><cell>MLP</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Bilateral</cell><cell cols="2">Bilateral</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Transformer</cell><cell cols="2">Transformer</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Block</cell><cell cols="2">Block</cell><cell></cell></row><row><cell cols="3">Initial optical</cell><cell cols="2">Calibrated</cell><cell></cell><cell></cell></row><row><cell></cell><cell>flow</cell><cell></cell><cell cols="2">optical flow</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Flow</cell><cell>Encoder Flow Query enc.</cell><cell></cell><cell>concat.</cell><cell>1x1 conv.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Results on Youtube-VOS 2019/2018 validation split. Subscript s and u denote scores in seen and unseen categories, respectively. BATMAN outperforms all state-of-the-art methods on both benchmarks</figDesc><table><row><cell>Method</cell><cell cols="4">Youtube-VOS 2019 J &amp;F Js Ju Fs</cell><cell cols="3">Youtube-VOS 2018 Fu J &amp;F Js Ju Fs</cell><cell>Fu</cell></row><row><cell>STM[27]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">79.4 79.7 72.8 84.2 80.9</cell></row><row><cell>AFB-URR[18]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">79.6 78.8 74.1 83.1 82.6</cell></row><row><cell>KMN[34]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">81.4 81.4 75.3 85.6 83.3</cell></row><row><cell>CFBI[49]</cell><cell cols="7">81.0 80.6 75.2 85.1 83.0 81.4 81.1 75.3 85.8 83.4</cell></row><row><cell>LWL[2]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">81.5 80.4 76.4 84.9 84.4</cell></row><row><cell>RMN[45]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">81.5 82.1 75.7 85.7 82.4</cell></row><row><cell>SST[11]</cell><cell cols="3">81.8 80.9 76.6</cell><cell>-</cell><cell>-</cell><cell>81.7 81.2 76.0</cell><cell>-</cell><cell>-</cell></row><row><cell>TransVOS[25]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">81.8 82.0 75.0 86.7 83.4</cell></row><row><cell>LCM[15]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">82.0 82.2 75.7 86.7 83.4</cell></row><row><cell>CFBI+[51]</cell><cell cols="7">82.6 81.7 77.1 86.2 85.2 82.8 81.8 77.1 86.6 85.6</cell></row><row><cell>STCN[7]</cell><cell cols="7">82.7 81.1 78.2 85.4 85.9 83.0 81.9 77.9 86.5 85.7</cell></row><row><cell cols="8">RPCMVOS[47] 83.9 82.6 79.1 86.9 87.1 84.0 83.1 78.5 87.7 86.7</cell></row><row><cell>AOT[50]</cell><cell cols="7">84.1 83.5 78.4 88.1 86.3 84.1 83.7 78.1 88.5 86.1</cell></row><row><cell>BATMAN</cell><cell cols="7">85.0 84.5 79.0 89.3 87.2 85.3 84.7 79.2 89.8 87.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparisons to the state-of-the-art methods on DAVIS benchmarks. (Y) indicates including Youtube-VOS dataset in training. BATMAN outperforms all state-of-the-art methods on all three DAVIS benchmarks</figDesc><table><row><cell>Method</cell><cell cols="2">DAVIS 2017 val J &amp;F J F</cell><cell cols="3">DAVIS 2017 test-dev J &amp;F J F</cell><cell cols="3">DAVIS 2016 val J &amp;F J F</cell></row><row><cell>AFB-URR[18]</cell><cell cols="2">74.6 73.0 76.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>LWL[2]</cell><cell cols="2">81.6 79.1 84.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>STM[27](Y)</cell><cell>-</cell><cell>79.2 84.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">88.7 89.9</cell></row><row><cell>CFBI[49](Y)</cell><cell cols="4">81.9 79.3 84.5 75.0 71.4</cell><cell>78.7</cell><cell cols="3">89.4 88.3 90.5</cell></row><row><cell>SST[11](Y)</cell><cell cols="2">82.5 79.9 85.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>KMN[34](Y)</cell><cell cols="4">82.8 80.0 85.6 77.2 74.1</cell><cell>80.3</cell><cell cols="3">90.5 89.5 91.5</cell></row><row><cell>CFBI+[51](Y)</cell><cell cols="4">82.9 80.1 85.7 75.6 71.6</cell><cell>79.6</cell><cell cols="3">89.9 88.7 91.1</cell></row><row><cell>RMN[45](Y)</cell><cell cols="4">83.5 81.0 86.0 75.0 71.9</cell><cell>78.1</cell><cell cols="3">88.8 88.9 88.7</cell></row><row><cell>LCM[15](Y)</cell><cell cols="4">83.5 80.5 86.5 78.1 74.4</cell><cell>81.8</cell><cell cols="3">90.7 89.9 91.4</cell></row><row><cell cols="5">RPCMVOS[47](Y) 83.7 81.3 86.0 79.2 75.8</cell><cell>82.6</cell><cell cols="3">90.6 87.1 94.0</cell></row><row><cell>TransVOS[25](Y)</cell><cell cols="4">83.9 81.4 86.4 76.9 73.0</cell><cell>80.9</cell><cell cols="3">90.5 89.8 91.2</cell></row><row><cell>AOT[50](Y)</cell><cell cols="4">84.9 82.3 87.5 79.6 75.9</cell><cell>83.3</cell><cell cols="3">91.1 90.1 92.1</cell></row><row><cell>STCN[7](Y)</cell><cell cols="4">85.4 82.2 88.6 76.1 72.7</cell><cell>79.6</cell><cell cols="3">91.6 90.8 92.5</cell></row><row><cell>BATMAN(Y)</cell><cell cols="5">86.2 83.2 89.3 82.2 78.4 86.1</cell><cell cols="3">92.5 90.7 94.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation on bilateral attention. The model with bilateral attention outperforms that with spatial local attention on all benchmarks</figDesc><table><row><cell>Attention</cell><cell>DAVIS</cell><cell>DAVIS 2017</cell><cell>DAVIS</cell><cell>Youtube-</cell><cell>Youtube-</cell></row><row><cell>type</cell><cell>2017 val</cell><cell>test-dev</cell><cell>2016 val</cell><cell>VOS 2019</cell><cell>VOS 2018</cell></row><row><cell>Spatial local</cell><cell>84.9</cell><cell>77.5</cell><cell>91.6</cell><cell>84.1</cell><cell>83.8</cell></row><row><cell>Bilateral</cell><cell>86.2</cell><cell>82.2</cell><cell>92.5</cell><cell>85.0</cell><cell>85.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparisons of bilateral attention w/ and w/o optical flow calibration. Calibrating the optical flow leads to higher accuracy on all benchmarks</figDesc><table><row><cell>Optical</cell><cell>DAVIS</cell><cell>DAVIS 2017</cell><cell>DAVIS</cell><cell>Youtube-</cell><cell>Youtube-</cell></row><row><cell>flow type</cell><cell>2017 val</cell><cell>test-dev</cell><cell>2016 val</cell><cell>VOS 2019</cell><cell>VOS 2018</cell></row><row><cell>w/o calibration</cell><cell>86.0</cell><cell>81.7</cell><cell>92.4</cell><cell>84.6</cell><cell>84.8</cell></row><row><cell>w/ calibration</cell><cell>86.2</cell><cell>82.2</cell><cell>92.5</cell><cell>85.0</cell><cell>85.3</cell></row><row><cell>Optical flow</cell><cell></cell><cell cols="2">On-object query token</cell><cell cols="2">Bilateral mask</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">At Oregon State University, Jialin Yuan and Li Fuxin are supported in part by NSF grant 1911232.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer normalization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning what to learn for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Lawin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="777" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="221" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">State-aware tracker for realtime video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9384" to="9393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Blazingly fast video object segmentation with pixel-wise metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1189" to="1198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rethinking space-time networks with improved memory coverage for efficient video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Segflow: Joint learning for video object segmentation and optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="686" to="695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Global contrast based salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="569" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sstvos: Sparse spatiotemporal transformers for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Aarabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5912" to="5921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 international conference on computer vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="991" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning position and target consistency for memory-based video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4144" to="4154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast video object segmentation with temporal aggregation network and dynamic template matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8879" to="8889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Lucid data dreaming for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1175" to="1197" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Video object segmentation with adaptive feature bank and uncertain-region refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3430" to="3441" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Video object segmentation with episodic graph memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="661" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Joint inductive and transductive learning for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9670" to="9679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>H?usser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.00588</idno>
		<title level="m">Transvos: Video object segmentation with transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Optimal decisions from probabilistic models: The intersection-overunion case</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="548" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Video object segmentation using space-time memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9225" to="9234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2663" to="2672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="724" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Acceleration of stochastic approximation by averaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Juditsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM journal on control and optimization</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="838" to="855" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Kernelized memory network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Seong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hyun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="629" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Hierarchical image saliency detection on extended cssd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="717" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Raft: Recurrent all-pairs field transforms for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="402" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Video segmentation via object flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3899" to="3908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Attention is all you need. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Feelvos: Fast end-to-end embedding learning for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9481" to="9490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fast online object tracking and segmentation: A unifying approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1328" to="1338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">End-to-end video instance segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8741" to="8750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations</title>
		<meeting>the 2020 conference on empirical methods in natural language processing: system demonstrations</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Monet: Deep motion exploitation for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1140" to="1148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Efficient regional memory network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1286" to="1295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.03327</idno>
		<title level="m">Youtube-vos: A large-scale video object segmentation benchmark</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Reliable propagation-correction modulation for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.02853</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dynamic video segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6556" to="6565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Collaborative video object segmentation by foreground-background integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="332" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Associating objects with transformers for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Collaborative video object segmentation by multiscale foreground-background integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="17283" to="17297" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning generative vision transformer with energy-based latent space for saliency prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
