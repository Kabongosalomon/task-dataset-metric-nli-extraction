<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE SUBMISSION 1 Auto-Rectify Network for Unsupervised Indoor Depth Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Wang</forename><surname>Bian</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huangying</forename><surname>Zhan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Jun</forename><surname>Chin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE SUBMISSION 1 Auto-Rectify Network for Unsupervised Indoor Depth Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Single-View Depth Estimation</term>
					<term>Unsupervised Learning</term>
					<term>Image Rectification !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Single-View depth estimation using the CNNs trained from unlabelled videos has shown significant promise. However, excellent results have mostly been obtained in street-scene driving scenarios, and such methods often fail in other settings, particularly indoor videos taken by handheld devices. In this work, we establish that the complex ego-motions exhibited in handheld settings are a critical obstacle for learning depth. Our fundamental analysis suggests that the rotation behaves as noise during training, as opposed to the translation (baseline) which provides supervision signals. To address the challenge, we propose a data pre-processing method that rectifies training images by removing their relative rotations for effective learning. The significantly improved performance validates our motivation. Towards end-to-end learning without requiring pre-processing, we propose an Auto-Rectify Network with novel loss functions, which can automatically learn to rectify images during training. Consequently, our results outperform the previous unsupervised SOTA method by a large margin on the challenging NYUv2 dataset. We also demonstrate the generalization of our trained model in ScanNet and Make3D, and the universality of our proposed learning method on 7-Scenes and KITTI datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>I NFERRING 3D geometry from 2D images is a long-standing problem in robotics and computer vision. Depending on the specific use case, it is usually solved by Structure-from-Motion <ref type="bibr" target="#b0">[1]</ref> or Visual SLAM <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b3">[4]</ref>. Underpinning these traditional pipelines is searching for correspondences <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref> across multiple images and triangulating them via epipolar geometry <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b8">[9]</ref> to obtain 3D points. Following the growth of deep learning-based approaches, Eigen et al. <ref type="bibr" target="#b9">[10]</ref> show that the depth map can be inferred from a single color image by a CNN, which is trained with the groundtruth depth supervisions captured by range sensors. Subsequently a series of supervised methods <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b16">[17]</ref> have been proposed and the accuracy of estimated depth is progressively improved.</p><p>Based on epipolar geometry <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b8">[9]</ref>, learning depth without requiring the ground-truth supervision has been explored. Garg et al. <ref type="bibr" target="#b17">[18]</ref> showed that the single-view depth CNN can be trained from stereo image pairs with known baseline via photometric loss. Zhou et al. <ref type="bibr" target="#b18">[19]</ref> further explored the unsupervised framework and proposed to train the depth CNN from unlabelled videos. They additionally introduced a Pose CNN to estimate the relative camera pose between consecutive frames, and the photometric loss remains as the main supervision signal. Following that, a number of of unsupervised methods have been proposed, which can be categorised into stereo-based <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b22">[23]</ref> and video-based <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b31">[32]</ref>, according to the type of training data. Our work follows the latter paradigm, since unlabelled videos are easier to obtain in real-world scenarios.</p><p>Unsupervised methods have shown promising results in driving scenes, e.g., KITTI <ref type="bibr" target="#b32">[33]</ref> and Cityscapes <ref type="bibr" target="#b33">[34]</ref>. However, as reported in <ref type="bibr" target="#b34">[35]</ref>, they usually fail in generic scenarios such as the indoor scenes in NYUv2 dataset <ref type="bibr" target="#b35">[36]</ref>. For example, GeoNet <ref type="bibr" target="#b25">[26]</ref>, one of the state-of-the-art methods in KITTI, is unable to obtain reasonable results in NYUv2. To this end, <ref type="bibr" target="#b34">[35]</ref> proposes to use optical flow as the supervision signal to train the depth CNN, and <ref type="bibr" target="#b36">[37]</ref> also uses flow with epipolar geometry to estimate egomotion to replace the Pose CNN. However, the reported depth accuracy <ref type="bibr" target="#b36">[37]</ref> is still limited, i.e., 0.189 in terms of AbsRel-see also qualitative results in <ref type="figure" target="#fig_3">Fig. 5</ref>.</p><p>Our work investigates the fundamental reasons behind poor results of unsupervised depth learning in indoor scenes. In addition to the usual challenges such as non-Lambertian surfaces and low-texture scenes, we identify the camera motion profile in the training videos as a critical factor that affects the training process. To develop this insight, we conduct an in-depth analysis of the effects of camera pose to current unsupervised depth learning framework. Our analysis shows that (i) fundamentally the camera rotation behaves as noise to training, while the translation contributes effective gradients; (ii) the rotation component dominates the motions in indoor videos captured using handheld cameras, while the opposite is true in autonomous driving scenarios.</p><p>Inspired by image rectification <ref type="bibr" target="#b37">[38]</ref> which has been used a standard pre-processing for the stereo matching task <ref type="bibr" target="#b38">[39]</ref>, we propose to weakly rectify video frames for the effective depth learning. To be specific, we compute the relative rotation between image pairs and warp them to their common image plane, which results in rectified pairs that have no relative rotational motions, and we use these pairs for unsupervised depth learning. This is achieved by leveraging the traditional feature matching <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref> and epipolar geometry <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b39">[40]</ref>, while no ground truth data is required. With our proposed data pre-processing, we demonstrate that existing state-of-the-art (SOTA) unsupervised methods <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b31">[32]</ref> can be trained well in the challenging indoor NYUv2 dataset <ref type="bibr" target="#b35">[36]</ref>. Consequently, our results outperform the unsupervised SOTA <ref type="bibr" target="#b36">[37]</ref> by a large margin.</p><p>Furthermore, towards end-to-end learning without requiring a manual pre-processing, we propose an Auto-Rectify Network (ARN), which draws philosophies from Spatial Transformer Network <ref type="bibr" target="#b40">[41]</ref> and elegantly incorporates the proposed weak rectification into the modern deep learning framework. The ARN, arXiv:2006.02708v2 [cs.CV] 14 Dec 2021 along with our proposed novel loss functions, can automatically learn to rectify images during the unsupervised training. The obtained results are comparable to training models on the preprocessed data. We make comprehensive ablation studies on the proposed methods, and validates the generalization of our trained model and learning method in several standard datasets, including ScanNet <ref type="bibr" target="#b41">[42]</ref>, 7-Scenes <ref type="bibr" target="#b42">[43]</ref>, and KITTI <ref type="bibr" target="#b32">[33]</ref>.</p><p>To summarize, our main contributions include:</p><p>? We theoretically analyze the relation between depth and two motion components in the warping. Along with the experimental analysis of the camera motion distribution in different scenarios, we answer the question why it is so challenging to train unsupervised depth CNNs from indoor videos captured by handheld cameras.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We propose a data pre-processing method to address the complex motion, which significantly boosts the learned depth accuracy and validates our motivation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We propose an Auto-Rectify Network with novel loss functions towards end-to-end learning, resulting in an improved unsupervised depth learning framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>This paper address the challenges of unsupervised learning of monocular depth from motion-complex videos, which are often captured by a hand-held camera. Based on our findings that the rotation between image pairs makes depth learning more challenging, we propose two image rectification methods for removing the rotation by either data pre-processing or end-to-end learning. The related work is discussed in the following paragraphs.</p><p>Unsupervised Video Depth Estimation. Zhou et al. <ref type="bibr" target="#b18">[19]</ref> propose the first video-based unsupervised learning method for monocular depth estimation. They rely on the color consistency between consecutive frames and the underlying 3D geometry constraint to train both depth and pose networks jointly. Following this seminal work, <ref type="bibr" target="#b25">[26]</ref>- <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref> propose different contributions to address the moving object issue in the dynamic scenes. <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref> propose additional geometric optimization to improve the pose estimation, and <ref type="bibr" target="#b45">[46]</ref> proposes a better depth network architecture. The significantly improved results are presented, however, these methods only demonstrate high-quality depth estimation results in driving videos, e.g., KITTI <ref type="bibr" target="#b32">[33]</ref> or Cityscapes <ref type="bibr" target="#b33">[34]</ref> datasets. As pointed by <ref type="bibr" target="#b34">[35]</ref>, many state-ofthe-art methods such as GeoNet <ref type="bibr" target="#b25">[26]</ref> fail to get reasonable results in NYUv2 indoor dataset <ref type="bibr" target="#b35">[36]</ref>. We experiment with Mon-odepth2 <ref type="bibr" target="#b28">[29]</ref>, which sometimes also meets this issue. Through careful investigation, we identify the complex camera pose in the hand-held camera setting as the main challenge, and we address the challenge in this paper. Before us, Zhou et al. <ref type="bibr" target="#b34">[35]</ref> and Zhao et al. <ref type="bibr" target="#b36">[37]</ref> have experimented in the indoor NYUv2 dataset. Specifically, <ref type="bibr" target="#b34">[35]</ref> estimates the optical flow between image pairs, and then uses the flow instead of photometric loss to supervise the depth and pose based warping. <ref type="bibr" target="#b34">[35]</ref> also removes purely rotated image pairs from training data, while the rotation in the remaining training image pairs are not addressed. We instead deal with the rotation in all image pairs automatically in this paper. <ref type="bibr" target="#b36">[37]</ref> replaces the pose network with an optical flow assisted geometric module for pose estimation. However, they did not realize the rotation issue in the hand-held camera setting, and the performance is limited. Besides, a recent work <ref type="bibr" target="#b46">[47]</ref> proposes to address the low-texture issue in the indoor scene by using patch-match and plane regularization. Their contributions are complementary to ours, and the results show that our method achievers a higher performance.</p><p>Image Rectification. Rectifying images for better depth estimation is not new in the computer vision community. For example, stereo rectification <ref type="bibr" target="#b37">[38]</ref> makes left and right images captured by a stereo camera to be row-to-row aligned, which is a widely used pre-processing for stereo matching <ref type="bibr" target="#b38">[39]</ref>. Besides, in the classic multi-view stereo system <ref type="bibr" target="#b47">[48]</ref>, images are also rectified before depth triangulation and optimization. We borrow this idea from classic geometry algorithms and integrate it into the deep learning based framework for monocular depth learning from motioncomplex videos. A similar idea is used in <ref type="bibr" target="#b48">[49]</ref> to learn the surface normal of tiled images. Moreover, our method is related to Spatial Transform Networks <ref type="bibr" target="#b49">[50]</ref> and Homography estimation <ref type="bibr" target="#b50">[51]</ref>. The former learns the image transform parameters and proposes a differentiable image warping, and the latter also learns the image rotation. The detailed differences are discussed in Sec. 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM ANALYSIS</head><p>We first overview the unsupervised framework for depth learning. Then, we revisit the depth and ego-motion based image warping and demonstrate the relationship between depth and decomposed motions. Finally, we compare the ego-motion statistics in different scenarios to verify the impact of ego-motion on depth learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Unsupervised depth learning from video</head><p>Following SfMLearner <ref type="bibr" target="#b18">[19]</ref>, plenty of unsupervised depth estimation methods have been proposed. The SC-Depth <ref type="bibr" target="#b31">[32]</ref>, which is the current SOTA framework, additionally constrains the geometry consistency over <ref type="bibr" target="#b18">[19]</ref>, leading to more accurate and scaleconsistent results. In this paper, we use SC-Depth as our baseline, and we overview its pipeline in <ref type="figure">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Forward.</head><p>A training image pair (I a , I b ) is first passed into a weight-shared depth CNN to obtain the depth maps (D a , D b ), respectively. Then, the pose CNN takes the concatenation of two images as input and predicts their 6D relative camera pose P ab . With the predicted depth D a and pose P ab , the warping flow between two images is generated according to Sec. 3.2.</p><p>Loss. First, the main supervision signal is the photometric loss L P . It calculates the color difference in each pixel between I a with its warped position on I b using a differentiable bilinear interpolation <ref type="bibr" target="#b40">[41]</ref>. Second, depth maps are regularized by the geometric inconsistency loss L GC , where it enforces the consistency of predicted depths between different frames. Besides, a weighting mask M is derived from L GC to handle dynamics and occlusions, which is applied on L P to obtain the weighted L M P . Third, depth maps are also regularized by a smoothness loss L S , which ensures that depth smoothness is guided by the edge of images. Overall, the objective function is:</p><formula xml:id="formula_0">L = ?L M P + ?L S + ?L GC ,<label>(1)</label></formula><p>where ?, ?, and ? are hyper-parameters to balance different losses.  <ref type="bibr" target="#b31">[32]</ref>. Firstly, in the forward pass, training images (Ia, I b ) are passed into the network to predict depth maps (Da, D b ) and relative camera pose P ab . With Da and P ab , we obtain the warping flow between two views according to Eqn. 2. Secondly, given the warping flow, the photometric loss L P and the geometry consistency loss L GC are computed. Also, the weighting mask M is derived from L GC and applied over L P to handle dynamics and occlusions. Moreover, an edge-aware smoothness loss L S is used to regularize the predicted depth map. Here we use this system as our baseline for problem analysis and illustrate our proposed components in <ref type="figure">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Depth and camera pose based image warping</head><p>The image warping builds the link between networks and losses during training, i.e., the warping flow is generated by network predictions (depth and camera motion) in forward pass, and the gradients are back-propagated from the losses via the warping flow to networks. Therefore, we investigate the warping to analyze the camera pose effects on the depth learning, which avoids involving image content factors, such as illumination changes and lowtexture scenes.</p><p>Full transformation. The camera pose is composed of rotation and translation components. A point (u 1 , v 1 ) in the first image is reprojected to (u 2 , v 2 ) in the second image, which satisfies the following relationship:</p><formula xml:id="formula_1">K ?1 ? ? ? ? ? d 2 ? ? ? ? ? u 2 v 2 1 ? ? ? ? ? ? ? ? ? ? = RK ?1 ? ? ? ? ? d 1 ? ? ? ? ? u 1 v 1 1 ? ? ? ? ? ? ? ? ? ? + t,<label>(2)</label></formula><p>where d i is the depth of this point in two images and K is the 3x3 camera intrinsic matrix. R is a 3x3 rotation matrix and t is a 3x1 translation vector. We decompose the full warping flow and discuss each component below.</p><p>Pure-rotation transformation. If two images are related by a pure-rotation transformation (i.e., t = 0), based on Eqn. 2, the warping satisfies:</p><formula xml:id="formula_2">d 2 ? ? ? ? ? u 2 v 2 1 ? ? ? ? ? = KRK ?1 ? ? ? ? ? d 1 ? ? ? ? ? u 1 v 1 1 ? ? ? ? ? ? ? ? ? ? ,<label>(3)</label></formula><p>where [KRK ?1 ] is as known as the homography matrix H [8], and we have</p><formula xml:id="formula_3">? ? ? ? ? u 2 v 2 1 ? ? ? ? ? = d 1 d 2 H ? ? ? ? ? u 1 v 1 1 ? ? ? ? ? = c ? ? ? ? ? h 11 h 12 h 13 h 21 h 22 h 23 h 31 h 32 h 33 ? ? ? ? ? ? ? ? ? ? u 1 v 1 1 ? ? ? ? ? ,<label>(4)</label></formula><p>where c = d1 d2 , standing for the depth relation between two views, is determined by the third row of the above equation, i.e., c = 1/(h 31 u 1 + h 32 v 1 + h 33 ). It indicates that we can obtain (u 2 , v 2 ) without d 1 . Specifically, solving the above equation, we have</p><formula xml:id="formula_4">u 2 = (h 11 u 1 + h 12 v 1 + h 13 )/(h 31 u 1 + h 32 v 1 + h 33 ) v 2 = (h 21 u 1 + h 22 v 1 + h 23 )/(h 31 u 1 + h 32 v 1 + h 33 ).</formula><p>(5) This demonstrates that the rotational flow in image warping is independent to the depth, and it is only determined by K and R. Consequently, the rotational motion in image pairs cannot contribute effective gradients to supervise the depth CNN during training, even when it is correctly estimated. More importantly, if the estimated rotation is inaccurate, noisy gradients will arise and harm the depth CNN in backpropagation. Therefore, we conclude that the rotational motion behaves as the noise to unsupervised depth learning.</p><p>Pure-translation transformation. A pure-translation transformation means that R is an identity matrix in Eqn. 2. Then we have</p><formula xml:id="formula_5">d2 ? ? ? ? u2 v2 1 ? ? ? ? = d1 ? ? ? ? u1 v1 1 ? ? ? ? + Kt = d1 ? ? ? ? u1 v1 1 ? ? ? ? + ? ? ? ? fx 0 cx 0 fy cy 0 0 1 ? ? ? ? ? ? ? ? t1 t2 t3 ? ? ? ? ,<label>(6)</label></formula><p>where (f x f y ) are camera focal lengths, and (c x , c y ) are principal point offsets. Solving the above equation, we have</p><formula xml:id="formula_6">? ? ? ? ? d 2 u 2 = d 1 u 1 + f x t 1 + c x t 3 d 2 v 2 = d 1 v 1 + f y t 2 + c y t 3 d 2 = d 1 + t 3 ? ? ? ? ? u 2 = d1u1+fxt1+cxt3 d1+t3 v 2 = d1v1+fyt2+cyt3 d1+t3</formula><p>.</p><p>(7) It shows that the translation vector t is coupled with the depth d 1 during the warping from (u 1 , v 1 ) to (u 2 , v 2 ). This builds the link between the depth CNN and the warping, so that gradients from the photometric loss can flow to the depth CNN via the warping. Therefore, we conclude that the translational motion provides effective supervision signals to depth learning. "Rectified" stands for the proposed pre-processing method described in Sec. 4. In each figure, the first row shows the averaged magnitude of camera poses, i.e., R for rotation and T for translation, and the plot shows the distribution of decomposed warping flow magnitudes (px) of randomly sampled points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Distribution of decomposed camera motions</head><p>Inter-frame camera motions and warping flows. <ref type="figure">Fig. 2</ref> shows the camera motion statistics on KITTI <ref type="bibr" target="#b32">[33]</ref> and NYUv2 <ref type="bibr" target="#b35">[36]</ref> datasets. KITTI is pre-processed by removing static images, as done in <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b31">[32]</ref>. We pick one image of every 10 frames in NYUv2, which is denoted as Original NYUv2. Then we apply the proposed pre-processing (Sec. 4) to obtain Rectified NYUv2. For all datasets, we compare the decomposed camera pose of their training image pairs w.r.t. the absolute magnitude and inter-frame warping flow 1 . Specifically, we compute the averaged warping flow of randomly sample points in the first image using the ground-truth depth and pose. For each point ( <ref type="figure">Fig. 2</ref> shows that the rotational flows dominates the flows in Original NYUv2 but it is opposite in KITTI. Along with the conclusion in Sec. 3.2 that the depth is supervised by the translation while the rotation behaves as the noise, this answers the question why unsupervised depth learning methods that obtain state-of-the-art results in driving scenes often fail in indoor videos. Besides, the results on Rectified NYUv2 demonstrate that our proposed data pre-processing can effectively address the issue. <ref type="bibr" target="#b0">1</ref>. We first compute the rotational flow using Eqn. 5, and then we get the translational flow by subtracting rotational flow from the overall warping flow. Here, the translational flow is also called residual parallax in <ref type="bibr" target="#b51">[52]</ref>, where it is used to compute depth from correspondences and relative camera poses.</p><formula xml:id="formula_7">u 1 , v 1 ) that is warped to (u 2 , v 2 ), the flow magnitude is (u 2 ? u 1 ) 2 + (v 2 ? v 1 ) 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PROPOSED DATA PROCESSING</head><p>The above analysis suggests that unsupervised depth learning favours image pairs those have small rotational and moderate translational motions for training. However, unlike driving sequences, videos captured by handheld cameras tend to have significant rotational motions, as shown in <ref type="figure">Fig. 2</ref>. In this section, we describe the proposed method to rectify images for effective learning in Sec. 4.1, and discuss the method in Sec. 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Weak Rectification</head><p>Sampling image pair candidates. Given a high frame rate video, e.g., 30fps, we first downsample it temporally to remove redundant images, i.e., extract one key (1st) frame from every m frames. Then, instead of only considering adjacent frames as a pair, we pair up each image with its following k frames as pair candidates.</p><p>Here, we use m = 10 and k = 10 in NYUv2 <ref type="bibr" target="#b35">[36]</ref>.</p><p>Two-view relative pose estimation. For each pair candidate, we first generate correspondences across images by using SIFT <ref type="bibr" target="#b4">[5]</ref> features, and we then apply the ratio test <ref type="bibr" target="#b4">[5]</ref> with GMS <ref type="bibr" target="#b31">[32]</ref> filter to find good matches. Second, with the selected correspondences, we estimate the essential matrix using the five-point algorithm <ref type="bibr" target="#b39">[40]</ref> within a RANSAC <ref type="bibr" target="#b52">[53]</ref> framework. Then the relative camera pose is decomposed from the essential matrix. This two-view image matching pipeline has a well-established technique in Computer Vision. See more technical details in <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>.</p><p>3-DoF weak rectification. Given an image pair with the precomputed relative pose, we warp both images to a common plane using the their relative rotation matrix R. Specifically, (i) we fist convert R to rotation vector r using Rodrigues formula <ref type="bibr" target="#b53">[54]</ref> to obtain half rotation vectors for two images (i.e., r 2 and ? r 2 ), and then we convert them back to rotation matrices R 1 and R 2 . (ii) Given R 1 , R 2 , and camera intrinsic K, we warp images to a new common image plane according to Eqn. 5. Then in the common plane, we crop their overlapped rectangular regions to obtain the weakly rectified pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Discussion of the proposed weak rectification</head><p>Inspiration from Stereo Rectification <ref type="bibr" target="#b37">[38]</ref>. The image rectification has been standard techniques for pre-processing images in the stereo matching task <ref type="bibr" target="#b38">[39]</ref>, which simplifies the problem by rectifying images so that corresponding points in two images have identical vertical coordinates. Drawing inspiration from it, we propose to weakly rectify images for easing the task of unsupervised depth learning from video. Compared with the standard rectification <ref type="bibr" target="#b37">[38]</ref>, we only consider the rotation for image warping and deliberately ignore the translation, which results in weakly rectified pairs that have 3-DoF translational motions, instead of 1-DoF motions as in <ref type="bibr" target="#b37">[38]</ref>, due to two reasons: (i) the standard rectification is used for imaged pairs that are captured by two horizontal cameras, and using it in video cases, especially the forward-motion pairs, often cause wired results such as extremely deformed images <ref type="bibr" target="#b37">[38]</ref>; (ii) the rigorous 1-DoF rectification is unnecessary for unsupervised depth learning, because the motions in all three directions are associated with the depth during image warping, as shown in Eqn. 7. An experimental example is that the existing methods <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr">[</ref>  <ref type="figure">Fig. 3</ref>. Proposed Auto-Rectify Network (ARN) with loss functions. We use ARN to predict the relative rotation between two input images (Ia, I b ), and warp I b to obtain the I b , which is supposed to be wellaligned with Ia in terms of rotation. Then we use the image pair (Ia, I b ) for subsequent depth learning, as described in <ref type="figure">Fig. 1</ref>. The proposed loss functions are used to regularize the training of ARN.</p><p>motions, can show comparable results to methods those are trained on KITTI stereo pairs <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>.</p><p>How to deal with unstable rectification? This is unavoidable due to the nature of geometric methods. For example, we may obtain significantly deformed images due to the inaccurate rotation estimation, which could be caused by insufficient correspondence search. However, this does not matter in our problem, because our goal is to construct good data for effective training, while we do not require that the rectification on all data is perfect. In practice, we set thresholds to remove invalid pairs, e.g., based on correspondence numbers and aspect ratios of the rectified images.</p><p>Can we do rectification in end-to-end fashion? Although using our pre-processed data can lead to better performance (Tab. 1) and faster convergence ( <ref type="figure" target="#fig_4">Fig. 6</ref>), the end-to-end learning approach is more favorable. For example, it benefits post-processing during inference, as recently discussed in <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b54">[55]</ref>. We here propose the data pre-processing solution mainly to validate our motivation, and we show how to encapsulate this idea into an end-to-end learning framework in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">PROPOSED END-TO-END METHOD</head><p>In this section, we present the proposed Auto-Rectify Network (ARN) with novel loss functions in Sec. 5.1 for end-to-end learning, and we discuss our design in Sec. 5.2. <ref type="figure">Fig. 3</ref> illustrates the proposed methods. ARN takes a concatenation of two images as input and outputs their 3-DoF relative rotation. Then we warp I b to obtain I b using the predicted rotation parameters, which is supposed to be co-planar with I a . Then we pass the image pair (I a , I b ) to our baseline framework for training. <ref type="figure">Fig. 3</ref> shows the pipeline and <ref type="figure" target="#fig_2">Fig. 4</ref> shows the visualized results. Network Architecture. The network architectures of ARN is almost the same as Pose CNN, and the difference is that ARN outputs a 3-DoF rotation instead of full 6-DoF pose. We use ResNet-18 <ref type="bibr" target="#b55">[56]</ref> encoder to extract features from a concatenation of two images, and regress their rotation parameters using several convolution layers. In order to enable two frames as input, we modify the first layer to have six channels instead of three.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Auto-Rectify Network with losses</head><p>Loss Functions. First, as we expect that I b is well-aligned with I a w.r.t. rotation, Rot2 is minimized. However, this leads to a trivial solution (i.e., the network always outputs zero), which inhabits the network ability for learning rotation and thus rectification. As a regularization, we simultaneously maximize Rot1. This relies in the assumption that the warped pairs are more well-aligned than original pairs. This allows us to define a Rotation-Triple loss L RT . Formally,</p><formula xml:id="formula_8">L RT = max(||Rot2|| 1 ? ||Rot1|| 1 + ?, 0),<label>(8)</label></formula><p>where ? is the margin, which we empirically set to be 0.1. Besides, as I b is manually warped from I b using Rot1, which is the ground truth of Rot3, the prediction of ARN on pair (I b , I b ). Therefore, we define an Rotation-Consistent loss L RC :</p><formula xml:id="formula_9">L RC = ||Rot3 ? Rot1|| 1 ,<label>(9)</label></formula><p>where || ? || 1 stands for the L 1 loss. We validate the effects of both losses in Tab. 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Discussion of the proposed ARN</head><p>Inspiration from Spatial Transformer Network <ref type="bibr" target="#b40">[41]</ref>. The STN is proposed to warp a single image (or its feature map) to the canonical view using the automatically learned spatial transformation parameters, and this has shown advantages to supervised learning tasks, e.g., image classification. In this paper, drawing inspiration from STN, we propose ARN to learn the relative rotation between two images and rectify images by warping the second image using the learned rotation parameters to a new view, which is supposed to be better aligned with the first image. We demonstrate that using the rectified pairs leads to significantly Relation to learning homography. Although both methods estimate a relative rotation, homography estimators solve rotation of purely rotated image pairs for accurate registration, while the goal of our proposed ARN is to rectify image pairs with general motions. In the special case where input images are related by only rotational motions, our ARN is equivalent to the homography estimator. Therefore, we propose the L RC (Eqn. 9) for supervising the network training in such case. Specifically, Rot3 is the prediction of ARN on the purely rotated pairs (I b , I b ), which is manually rotated by Rot1, so we minimize the difference between Rot3 and Rot1 to encourage accurate rotation predictions.</p><p>How to deal with purely rotated pairs? As Eqn. 5 shows that the rotation behaves as noises during training, so that purely rotated pairs are invalid for training the depth CNN. Previous methods <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b36">[37]</ref> manually remove that from the dataset. However, we show that in our end-to-end learning framework, the purely rotated images could be automatically masked out. Specifically, for the image pair (I a , I b ) that is related by purely rotational motions, after the ARN-based warping, the new pair (I a , I b ) would be well-aligned and become stationary pairs. In this scenario, the auto-mask <ref type="bibr" target="#b28">[29]</ref> which was proposed to remove stationary points, are used in our framework for removing purely rotational points. We implement this component in our baseline framework <ref type="bibr" target="#b31">[32]</ref>, and the effects of that is shown in Tab. 9.</p><p>Can we reuse the PoseNet as ARN? It sounds possible because the ARN has a very similar network structure with the PoseNet. Specifically, we can use only the rotation output of the PoseNet in the first stage for rectification and the full pose prediction in the second stage for image warping. However, in practice, we are hard to make the network converge during training. We conjecture that  it depends on the model initialization and the knowledge conflicts between two steps. More specifically, in the first stage it needs to learn coarse but big rotations, while in the second stage it needs to learn fine but small pose residuals. Therefore, we suggest not sharing parameters between the ARN and PoseNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Implementation Details</head><p>We use SC-Depth <ref type="bibr" target="#b31">[32]</ref> as our baseline, which is publicly available. We use the default hyper-parameters. To be specific, we use ? = 1, ? = 0.1, ? = 0.5 in Eqn. 1. For training ARN, we use the weights 0.5 for L RT and 0.1 for L RC . We train models for 50 epochs using the Adam <ref type="bibr" target="#b64">[65]</ref> optimizer with the learning rate being 0.0001. The batch size equals to 8 for 3-frame inputs, and it is 16 for pairwise inputs, as the former is used as two pairs during training. Besides, to demonstrate that our proposed preprocessing is universal to different methods, we experiment with Monodepth2 <ref type="bibr" target="#b28">[29]</ref> (ResNet-18) in ablation studies, where we use the default parameters and train models for 50 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Dataset and Metrics</head><p>NYUv2 <ref type="bibr" target="#b35">[36]</ref>. The dataset is composed of indoor video sequences recorded by a handheld Kinect RGB-D camera at 640 ? 480 resolution. The dataset contains 464 scenes taken from three cities. We use the officially provided 654 densely labeled images for testing, and use the rest 335 sequences (no overlap with testing scenes) for training (302) and validation <ref type="bibr" target="#b32">(33)</ref>. The raw training  sequences contain 268K images. It is first downsampled 10 times to remove redundant frames. We train on the pre-processed data (i.e., it generates 67K rectified pairs) and train directly on original data with ARN, respectively. The images are resized to 320 ? 256 resolution for training.</p><p>KITTI <ref type="bibr" target="#b32">[33]</ref>. The dataset contains driving videos in outdoor driving scenes, and we use it to investigate whether the proposed ARN has an adverse impact on motion-simple data. We use Eigen <ref type="bibr" target="#b9">[10]</ref>'s splits and follow SC-Depth <ref type="bibr" target="#b31">[32]</ref> for training and evaluation. The image is re-scaled to 832 ? 256 resolution for training.</p><p>ScanNet <ref type="bibr" target="#b41">[42]</ref>. The dataset provides RGB-D videos of 1513 indoor scans, captured by handheld devices. We use officially released test set, which is originally used to evaluate the semantic labeling task, for evaluating the generalization of our trained model. It contains total 2135 color images (1296 ? 968), and corresponding ground truth depths (640 ? 480). We re-scale the depth prediction to GT resolution for evaluation. <ref type="bibr" target="#b42">[43]</ref>. The dataset contains 7 indoor scenes, and each scene contains several video sequences (500-1000 frames per sequence), which are captured by a Kinect camera at 640 ? 480 resolution. We follow the official train/test split for each scene. For testing, we simply extract the first image from every 10 frames, and for training, we fine-tune the model that is pre-trained on NYUv2 to demonstrate the universality of the proposed ARN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7-Scenes</head><p>Make3D <ref type="bibr" target="#b56">[57]</ref>. Following previous unsupervised methods <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b28">[29]</ref>, we test the zero-shot generalization of our trained models on the Make3D dataset. It contains 134 in-the-wild images for testing. However, caution should be taken, as the ground truth depth and input images are not well aligned, causing potential evaluation issues.</p><p>Evaluation metrics. Following previous methods <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, we use the mean absolute relative error (AbsRel), mean log10 error (Log10), root mean squared error (RMS), root mean  <ref type="bibr" target="#b42">[43]</ref>. The model is pre-trained on NYUv2. We fine-tune models in each scene for 3 epochs, which consumes less than 10 minutes.  squared log error (RMSLog), and the accuracy under threshold (? i &lt; 1.25 i , i = 1, 2, 3). As unsupervised methods cannot recover the metric scale, we multiply the predicted depth maps by a scalar that matches the median with that of the ground truth, as in <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b31">[32]</ref>. The predicted depths are capped at 80m in KITTI, 70m in Make3D, and 10m in all indoor datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Results</head><p>Results on NYUv2. Tab. 1 shows the single-view depth estimation results on NYUv2 <ref type="bibr" target="#b35">[36]</ref>. It shows that our method clearly outperforms previous unsupervised methods <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b46">[47]</ref>. See qualitative results of depth estimation in <ref type="figure" target="#fig_3">Fig. 5</ref> and the visualization of 3D point clouds in <ref type="figure">Fig. 8</ref>. Besides, compared with SC-Depth <ref type="bibr" target="#b31">[32]</ref>, which is our baseline method, the higher performance demonstrates the efficacy of our proposed DP and ARN. Note that NYUv2 dataset is so challenging that many previous unsupervised methods such as GeoNet <ref type="bibr" target="#b25">[26]</ref> fail to get reasonable results, as discussed in <ref type="bibr" target="#b34">[35]</ref>. We find that Monodepth2 <ref type="bibr" target="#b28">[29]</ref> sometimes also meets this issue, and we report its best result of multiple runs. Moreover, our method outperforms a series of fully supervised methods <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b56">[57]</ref>- <ref type="bibr" target="#b62">[63]</ref>. However, there still has a gap to the state-of-the-art supervised method <ref type="bibr" target="#b16">[17]</ref>.</p><p>Results on KITTI. Other than indoor datasets, we also report the results on the KITTI outdoor driving dataset <ref type="bibr" target="#b32">[33]</ref>. Note that in the driving dataset, the image rotation is very small because most image pairs have a simple forward motion, and in this scenario our proposed rotation rectification is not necessary. Therefore, we do not run the data processing (DP) and only evaluate the effects of our proposed ARN. The results are summarized in Tab. 2. It shows that the performance of our method is slightly lower than the state-of-the-art methods <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b45">[46]</ref>. However, compared with SC-Depth <ref type="bibr" target="#b31">[32]</ref>, which is our baseline method, the proposed  <ref type="bibr" target="#b36">[37]</ref>, Monodepth2 <ref type="bibr" target="#b28">[29]</ref>, and Ours. The models are trained on NYUv2 <ref type="bibr" target="#b35">[36]</ref>.</p><p>ARN module can lead to a minor improvement. It demonstrates that the proposed ARN has no adverse impact when training on motion-simple datasets.</p><p>Generalization results on ScanNet. Tab. 3 shows the zero-shot generalization results on ScanNet <ref type="bibr" target="#b41">[42]</ref>, where all models are trained on NYUv2 <ref type="bibr" target="#b35">[36]</ref>. The results demonstrate that our trained models generalize well to new dataset. See qualitative results in <ref type="figure" target="#fig_3">Fig. 5</ref>. Besides, following <ref type="bibr" target="#b46">[47]</ref>, we provide the pose estimation results on ScanNet dataset, where 2000 image pairs from diverse scenes are selected by <ref type="bibr" target="#b65">[66]</ref>. The results demonstrate that our method outperforms other unsupervised alternatives.</p><p>Generalization results on Make3D. Tab. 5 shows the zeroshot generalization results on Make3D <ref type="bibr" target="#b56">[57]</ref>. Note that it is very challenging because our models are trained on indoor NYUv2 dataset but tested on in-the-wild outdoor images. Here, even though other unsupervised methods are trained on KITTI outdoor dataset, in which the images are arguably more similar to Make3D images, the results show that our indoor trained models outperform other unsupervised outdoor trained models.</p><p>Fine-tuned results on 7-Scenes. The nature of unsupervised learning makes our method easy to be fine-tuned in new datasets, i.e., we can fine-tune models there without the requirement for the ground-truth labels. Tab. 6 shows the quantitative fine-tuned results on 7-Scenes <ref type="bibr" target="#b42">[43]</ref>, where the models were pre-trained on NYUv2 <ref type="bibr" target="#b35">[36]</ref>. The results shows that our model not only generalizes well to new datasets, but also a quick fine-tuning can improve the performance significantly. This has important implications to real-world applications. Also, the results demonstrate that our proposed ARN performs well in different scenarios.</p><p>Timing. It takes 28 hours to train the models for 50 epochs on original NYUv2 dataset and 25 hours on the rectified data, measured in a single 16GB NVIDIA V100 GPU. It takes 44 hours when training with the proposed ARN. The inference speed is about 210 FPS in a NVIDIA RTX 2080 GPU, where images are resized to 320 ? 256 before feeding to the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Ablation studies</head><p>Effects of the proposed DP. Tab. 7 shows the ablation study results on NYUv2. For both SC-Depth <ref type="bibr" target="#b31">[32]</ref> and Monodepth2 <ref type="bibr" target="#b28">[29]</ref>   frameworks, training on our pre-processed data leads to significantly improved results than using original dataset. It demonstrates that the proposed data processing method is independent to the method chosen, and our motivation of removing rotation is correct. Moreover, we visualize the quantitative learning curves in <ref type="figure" target="#fig_4">Fig. 6</ref> for more detailed comparison.</p><p>Effects of the proposed ARN. The results in Tab. 1 show that the proposed ARN improves the depth estimation accuracy significantly on NYUv2. Besides, the results in Tab. 2 show that the performance improvement by ARN is minor on KITTI, since the camera rotation in driving scenes is almost small. As shown in <ref type="figure">Fig. 2(a)</ref>, the ego-motion in NYUv2 is more complex than in KITTI. This indicates that using ARN is beneficial to training on motion-complex data (see <ref type="figure" target="#fig_4">Fig. 6</ref>), while it has less impact when training on motion-simple data.</p><p>Effects of the proposed losses. Tab. 8 shows the ablation study results of the proposed loss functions on NYUv2. Note that the ARN could converge well even without applying the proposed loss functions during training, since it can get supervision signals from the photometric loss and geometry consistency loss, contributed to the differentiable warping. This is very important because it makes our system really end-to-end trainable, leading to improved performance against the two-step learning-our data preprocessing solution. Besides, the results show that the proposed L RT and L RC can effectively regularize the network during training and lead to a higher performance. Moreover, Tab. 9 shows the effects of ImageNet pretraining and Auto-Mask, which is used to remove purely rotational motions.</p><p>Rotation removal by ARN. <ref type="figure">Fig. 7</ref> shows that the proposed ARN can effectively reduce the relative image rotations, where the model is trained on NYUv2 without fine-tuning on 7-Scenes. However, when compared with traditional geometry based methods, the accuracy is lower, and the generalization is worse.</p><p>Besides, <ref type="figure" target="#fig_2">Fig. 4</ref> shows several ARN warped results during training, and <ref type="figure" target="#fig_4">Fig. 6</ref> demonstrates that this is effective for improving depth learning. Therefore, we conclude that ARN is effective for assisting the depth learning, while not comparable to geometry- 7-Scenes <ref type="figure">Fig. 7</ref>. Effects of proposed rectification methods. We test the data preprocessing (DP) and ARN on the validation sequence. The ARN model is trained on NYUv2 <ref type="bibr" target="#b35">[36]</ref>.</p><p>based methods in terms of rotation estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper, we analyze the impact of ego-motion on the indoor unsupervised depth learning problem. Our analysis shows that (i) rotational motions dominate the camera motions in videos taken by handheld devices, and (ii) rotation behaves as noises while translation contributes effective signals to training. Based on the analysis, we propose a data pre-processing method to rectify the training image pairs. The results show that training on rectified image pairs can effectively improve the performance. Besides, we propose an Auto-Rectify Network with novel loss functions, which are integrated into the existing self-supervised learning framework for end-to-end learning. The comprehensive evaluation results in different datasets demonstrate the efficacy and universality of our proposed methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was in part supported by the Australian Centre of Excellence for Robotic Vision CE140100016, and the ARC Laureate Fellowship FL130100102 to Prof. Ian Reid. We thank anonymous reviewers for their valuable suggestions. <ref type="figure">Fig. 8</ref>. Visualisation of 3D point clouds on NYUv2 <ref type="bibr" target="#b35">[36]</ref>. Left to right: RGB, TrainFlow <ref type="bibr" target="#b36">[37]</ref>, Monodepth2 <ref type="bibr" target="#b28">[29]</ref>, and Ours.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>?</head><label></label><figDesc>J.-W. Bian, H. Zhan, T.-J. Chin, C. Shen, and I. Reid are with The University of Adelaide and Australian Centre for Robotic Vision, Australia; ? N. Wang is with TuSimple, China.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>KITTI [ 33 Fig. 2 .</head><label>332</label><figDesc>] (R=0.25 ? , T=0.99m) Original NYUv2 [36] (R=2.28 ? , T=0.05m) Distribution of inter-frame camera motions and warping flows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>?Fig. 4 .</head><label>4</label><figDesc>Samples of ARN warped results. Ia, I b are input images, and I b is the warped image by ARN. Note the grey board of I b , which stands for the zero-padding in image warping.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Qualitative results. Left to right: RGB, TrainFlow</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Validation loss during training on NYUv2<ref type="bibr" target="#b35">[36]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 1</head><label>1</label><figDesc>Single-view depth estimation results on NYUv2<ref type="bibr" target="#b35">[36]</ref>.</figDesc><table><row><cell>Methods</cell><cell>Supervision</cell><cell>Error ?</cell><cell></cell><cell></cell><cell>Accuracy ?</cell><cell></cell></row><row><cell></cell><cell cols="2">AbsRel Log10</cell><cell>RMS</cell><cell>?1</cell><cell>?2</cell><cell>?3</cell></row><row><cell>Make3D [57]</cell><cell>0.349</cell><cell>-</cell><cell>1.214</cell><cell cols="3">0.447 0.745 0.897</cell></row><row><cell>Depth Transfer [58]</cell><cell>0.349</cell><cell>0.131</cell><cell>1.21</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Liu et al. [59]</cell><cell>0.335</cell><cell>0.127</cell><cell>1.06</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ladicky et al. [60]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">0.542 0.829 0.941</cell></row><row><cell>Li et al. [61]</cell><cell>0.232</cell><cell>0.094</cell><cell>0.821</cell><cell cols="3">0.621 0.886 0.968</cell></row><row><cell>Roy et al. [62]</cell><cell>0.187</cell><cell>0.078</cell><cell>0.744</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Liu et al. [11]</cell><cell>0.213</cell><cell>0.087</cell><cell>0.759</cell><cell cols="3">0.650 0.906 0.976</cell></row><row><cell>Wang et al. [63]</cell><cell>0.220</cell><cell>0.094</cell><cell>0.745</cell><cell cols="3">0.605 0.890 0.970</cell></row><row><cell>Eigen et al. [12]</cell><cell>0.158</cell><cell>-</cell><cell>0.641</cell><cell cols="3">0.769 0.950 0.988</cell></row><row><cell>Chakrabarti et al. [13]</cell><cell>0.149</cell><cell>-</cell><cell>0.620</cell><cell cols="3">0.806 0.958 0.987</cell></row><row><cell>Laina et al. [14]</cell><cell>0.127</cell><cell>0.055</cell><cell>0.573</cell><cell cols="3">0.811 0.953 0.988</cell></row><row><cell>Li et al. [15]</cell><cell>0.143</cell><cell>0.063</cell><cell>0.635</cell><cell cols="3">0.788 0.958 0.991</cell></row><row><cell>DORN [16]</cell><cell>0.115</cell><cell>0.051</cell><cell>0.509</cell><cell cols="3">0.828 0.965 0.992</cell></row><row><cell>VNL [17]</cell><cell>0.108</cell><cell>0.048</cell><cell>0.416</cell><cell cols="3">0.875 0.976 0.994</cell></row><row><cell>MovingIndoor [35]</cell><cell>0.208</cell><cell>0.086</cell><cell>0.712</cell><cell cols="3">0.674 0.900 0.968</cell></row><row><cell>TrainFlow [37]</cell><cell>0.189</cell><cell>0.079</cell><cell>0.686</cell><cell cols="3">0.701 0.912 0.978</cell></row><row><cell>Monodepth2 [29]</cell><cell>0.176</cell><cell>0.074</cell><cell>0.639</cell><cell cols="3">0.734 0.937 0.983</cell></row><row><cell>P 2 Net (3-frame) [47]</cell><cell>0.159</cell><cell>0.068</cell><cell>0.599</cell><cell cols="3">0.772 0.942 0.984</cell></row><row><cell>P 2 Net (5-frame) [47]</cell><cell>0.147</cell><cell>0.062</cell><cell>0.553</cell><cell cols="3">0.801 0.951 0.987</cell></row><row><cell>Baseline (SC-Depth [32])</cell><cell>0.159</cell><cell>0.068</cell><cell>0.608</cell><cell cols="3">0.772 0.939 0.982</cell></row><row><cell>Ours-DP</cell><cell>0.143</cell><cell>0.060</cell><cell>0.538</cell><cell cols="3">0.812 0.951 0.986</cell></row><row><cell>Ours-ARN</cell><cell>0.138</cell><cell>0.059</cell><cell>0.532</cell><cell cols="3">0.820 0.956 0.989</cell></row></table><note>better performance than original pairs for unsupervised depth learning in motion-complex dataset-see Tab. 1.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2</head><label>2</label><figDesc>Unsupervised single-view depth estimation results on KITTI<ref type="bibr" target="#b32">[33]</ref>.</figDesc><table><row><cell>Methods</cell><cell></cell><cell cols="2">Error ?</cell><cell></cell><cell></cell><cell>Accuracy ?</cell></row><row><cell></cell><cell cols="2">AbsRel SqRel</cell><cell>RMS</cell><cell>RMSLog</cell><cell>?1</cell><cell>?2</cell><cell>?3</cell></row><row><cell>SfMLearner [19]</cell><cell>0.208</cell><cell>1.768</cell><cell>6.856</cell><cell>0.283</cell><cell cols="2">0.678 0.885 0.957</cell></row><row><cell>Vid2Depth [25]</cell><cell>0.163</cell><cell>1.240</cell><cell>6.220</cell><cell>0.250</cell><cell cols="2">0.762 0.916 0.968</cell></row><row><cell>DDAD [24]</cell><cell>0.151</cell><cell>1.257</cell><cell>5.583</cell><cell>0.228</cell><cell cols="2">0.810 0.936 0.974</cell></row><row><cell>GeoNet [26]</cell><cell>0.155</cell><cell>1.296</cell><cell>5.857</cell><cell>0.233</cell><cell cols="2">0.793 0.931 0.973</cell></row><row><cell>DF-Net [27]</cell><cell>0.150</cell><cell>1.124</cell><cell>5.507</cell><cell>0.223</cell><cell cols="2">0.806 0.933 0.973</cell></row><row><cell>Struct2Depth [44]</cell><cell>0.141</cell><cell>1.026</cell><cell>5.291</cell><cell>0.215</cell><cell cols="2">0.816 0.945 0.979</cell></row><row><cell>DW [30]</cell><cell>0.128</cell><cell>0.959</cell><cell>5.230</cell><cell>0.212</cell><cell cols="2">0.845 0.947 0.976</cell></row><row><cell>GL-Net [31]</cell><cell>0.135</cell><cell>1.070</cell><cell>5.230</cell><cell>0.210</cell><cell cols="2">0.841 0.948 0.980</cell></row><row><cell>CC [28]</cell><cell>0.140</cell><cell>1.070</cell><cell>5.326</cell><cell>0.217</cell><cell cols="2">0.826 0.941 0.975</cell></row><row><cell>EPC++ [64]</cell><cell>0.141</cell><cell>1.029</cell><cell>5.350</cell><cell>0.216</cell><cell cols="2">0.816 0.941 0.976</cell></row><row><cell>Monodepth2 [29]</cell><cell>0.115</cell><cell>0.882</cell><cell>4.701</cell><cell>0.190</cell><cell cols="2">0.879 0.961 0.982</cell></row><row><cell>TrainFlow [37]</cell><cell>0.113</cell><cell>0.704</cell><cell>4.581</cell><cell>0.184</cell><cell cols="2">0.871 0.961 0.984</cell></row><row><cell>Insta-DM [45]</cell><cell>0.112</cell><cell>0.777</cell><cell>4.772</cell><cell>0.191</cell><cell cols="2">0.872 0.959 0.982</cell></row><row><cell>PackNet [46]</cell><cell>0.107</cell><cell>0.802</cell><cell>4.538</cell><cell>0.186</cell><cell cols="2">0.889 0.962 0.981</cell></row><row><cell>Baseline (SC-Depth [32])</cell><cell>0.119</cell><cell>0.857</cell><cell>4.950</cell><cell>0.197</cell><cell cols="2">0.863 0.957 0.981</cell></row><row><cell>Ours</cell><cell>0.118</cell><cell>0.861</cell><cell>4.803</cell><cell>0.193</cell><cell cols="2">0.866 0.958 0.981</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 3</head><label>3</label><figDesc>Zero-shot generalization results on ScanNet<ref type="bibr" target="#b41">[42]</ref>.</figDesc><table><row><cell>Methods</cell><cell>Supervision</cell><cell>Error ?</cell><cell></cell><cell></cell><cell>Accuracy ?</cell></row><row><cell></cell><cell cols="2">AbsRel Log10</cell><cell>RMS</cell><cell>?1</cell><cell>?2</cell><cell>?3</cell></row><row><cell>Laina et al. [14]</cell><cell>0.141</cell><cell>0.059</cell><cell>0.339</cell><cell cols="2">0.811 0.958 0.990</cell></row><row><cell>VNL [17]</cell><cell>0.123</cell><cell>0.052</cell><cell>0.306</cell><cell cols="2">0.848 0.964 0.991</cell></row><row><cell>TrainFlow [37]</cell><cell>0.179</cell><cell>0.076</cell><cell>0.415</cell><cell cols="2">0.726 0.927 0.980</cell></row><row><cell>SC-Depth [32]</cell><cell>0.169</cell><cell>0.072</cell><cell>0.392</cell><cell cols="2">0.749 0.938 0.983</cell></row><row><cell>Ours</cell><cell>0.156</cell><cell>0.066</cell><cell>0.361</cell><cell cols="2">0.781 0.947 0.987</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 4</head><label>4</label><figDesc>Camera pose estimation results on ScanNet<ref type="bibr" target="#b41">[42]</ref>.</figDesc><table><row><cell>Method</cell><cell cols="3">rot (deg) tr (deg) tr (cm)</cell></row><row><cell>MovingIndoor [35]</cell><cell>1.96</cell><cell>39.17</cell><cell>1.40</cell></row><row><cell>Monodepth2 [29]</cell><cell>2.03</cell><cell>41.12</cell><cell>0.83</cell></row><row><cell>P 2 Net [47]</cell><cell>1.86</cell><cell>35.11</cell><cell>0.89</cell></row><row><cell>Ours</cell><cell>1.82</cell><cell>39.41</cell><cell>0.55</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 5</head><label>5</label><figDesc>Zero-shot generalization results on Make3D<ref type="bibr" target="#b56">[57]</ref>.</figDesc><table><row><cell>Methods</cell><cell>Supervision</cell><cell cols="2">AbsRel SqRel</cell><cell>RMS</cell></row><row><cell>Karsch [58]</cell><cell></cell><cell>0.428</cell><cell>5.079</cell><cell>8.389</cell></row><row><cell>Liu [59]</cell><cell></cell><cell>0.475</cell><cell>6.562</cell><cell>10.05</cell></row><row><cell>Laina [14]</cell><cell></cell><cell>0.204</cell><cell>1.840</cell><cell>5.683</cell></row><row><cell>SfMLearner [19]</cell><cell></cell><cell>0.383</cell><cell>5.321</cell><cell>10.470</cell></row><row><cell>DDVO [24]</cell><cell></cell><cell>0.387</cell><cell>4.720</cell><cell>8.090</cell></row><row><cell>Monodepth2 [29]</cell><cell></cell><cell>0.322</cell><cell>3.589</cell><cell>7.417</cell></row><row><cell>Ours</cell><cell></cell><cell>0.305</cell><cell>2.869</cell><cell>7.320</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 6</head><label>6</label><figDesc></figDesc><table><row><cell>Fine-tuned results on 7-Scenes</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 7</head><label>7</label><figDesc>Effects of our proposed data processing (DP) on NYUv2<ref type="bibr" target="#b35">[36]</ref>.</figDesc><table><row><cell>Methods</cell><cell>With</cell><cell></cell><cell>Error ?</cell><cell></cell><cell></cell><cell>Accuracy ?</cell></row><row><cell></cell><cell>DP</cell><cell cols="2">AbsRel Log10</cell><cell>RMS</cell><cell>?1</cell><cell>?2</cell><cell>?3</cell></row><row><cell>SC-Depth [32]</cell><cell></cell><cell>0.159</cell><cell>0.068</cell><cell>0.608</cell><cell cols="2">0.772 0.939 0.982</cell></row><row><cell></cell><cell></cell><cell>0.143</cell><cell>0.060</cell><cell>0.538</cell><cell cols="2">0.812 0.951 0.986</cell></row><row><cell>Monodepth2 [29]</cell><cell></cell><cell>0.176</cell><cell>0.074</cell><cell>0.639</cell><cell cols="2">0.734 0.937 0.983</cell></row><row><cell></cell><cell></cell><cell>0.151</cell><cell>0.064</cell><cell>0.559</cell><cell cols="2">0.795 0.947 0.985</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 8</head><label>8</label><figDesc>Effects of the proposed loss functions on NYUv2<ref type="bibr" target="#b35">[36]</ref>.</figDesc><table><row><cell>With</cell><cell>With</cell><cell></cell><cell>Error ?</cell><cell></cell><cell></cell><cell>Accuracy ?</cell></row><row><cell>L RT</cell><cell>L RC</cell><cell cols="2">AbsRel Log10</cell><cell>RMS</cell><cell>?1</cell><cell>?2</cell><cell>?3</cell></row><row><cell></cell><cell></cell><cell>0.150</cell><cell>0.064</cell><cell>0.564</cell><cell cols="3">0.797 0.946 0.985</cell></row><row><cell></cell><cell></cell><cell>0.145</cell><cell>0.062</cell><cell>0.560</cell><cell cols="3">0.802 0.948 0.987</cell></row><row><cell></cell><cell></cell><cell>0.148</cell><cell>0.063</cell><cell>0.562</cell><cell cols="3">0.798 0.950 0.987</cell></row><row><cell></cell><cell></cell><cell>0.138</cell><cell>0.059</cell><cell>0.532</cell><cell cols="3">0.820 0.956 0.989</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 9</head><label>9</label><figDesc>Effects of Auto-Mask (AM) and ImageNet Pretrain (IP) on NYUv2<ref type="bibr" target="#b35">[36]</ref>.</figDesc><table><row><cell>With</cell><cell>With</cell><cell></cell><cell>Error ?</cell><cell></cell><cell></cell><cell>Accuracy ?</cell></row><row><cell>AM</cell><cell>IP</cell><cell cols="2">AbsRel Log10</cell><cell>RMS</cell><cell>? 1</cell><cell>? 2</cell><cell>? 3</cell></row><row><cell></cell><cell></cell><cell>0.164</cell><cell>0.069</cell><cell>0.596</cell><cell cols="3">0.768 0.937 0.982</cell></row><row><cell></cell><cell></cell><cell>0.153</cell><cell>0.065</cell><cell>0.571</cell><cell cols="3">0.790 0.945 0.986</cell></row><row><cell></cell><cell></cell><cell>0.138</cell><cell>0.059</cell><cell>0.532</cell><cell cols="3">0.820 0.956 0.989</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Jia-Wang Bian is a PhD candidate at the University of Adelaide and an Associated PhD researcher with the Australian Centre for Robotic Vision (ACRV). He is advised by Prof. Ian Reid and Prof. Chunhua Shen. His research interests lie in the field of computer vision, deep learning, and robotics. Jiawang received his B.Eng. degree from Nankai University, where he was advised by Prof. Ming-Ming Cheng. He was a research assistant at the Singapore University of Technology and Design (SUTD). Also, Jiawang did research intern jobs in many companies, including Advanced Digital Sciences Center (ADSC), Huawei, TuSimple, Amazon, and Meta. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Huangying</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Structure-from-motion revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Schonberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Monoslam: Realtime single camera slam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Molton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Stasse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Recognition and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dtam: Dense tracking and mapping in real-time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Lovegrove</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ORB-SLAM: a versatile and accurate monocular slam system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mur-Artal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M M</forename><surname>Montiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Tardos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Computer Vision (IJCV)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">GMS: Grid-based motion statistics for fast, ultra-robust feature correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Computer</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Determining the epipolar geometry and its uncertainty: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Computer Vision (IJCV)</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An evaluation of feature matchers for fundamental matrix estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images using deep convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Recognition and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Depth from a single image by harmonizing overcomplete local network predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A two-streamed network for estimating fine-scaled depth maps from single rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Enforcing geometric constraints of virtual normal for depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised cnn for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Bg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised learning of monocular depth estimation and visual odometry with deep feature reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Weerasekera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Self-supervised learning for single view depth and surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Weerasekera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Selfsupervised monocular depth hints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Turmukhambetov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning depth from monocular videos using direct methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from monocular video using 3d geometric constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">GeoNet: Unsupervised learning of dense depth, optical flow and camera pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">DF-Net: Unsupervised joint learning of depth and flow using cross-task consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Competitive Collaboration: Joint unsupervised learning of depth, camera motion, optical flow and motion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Digging into self-supervised monocular depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Depth from videos in the wild: Unsupervised monocular depth learning from unknown cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jonschkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Self-supervised learning with geometric constraints in monocular video: Connecting flow, depth, and camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised scale-consistent depth learning from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Computer</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Vision meets Robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">IJRR</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Moving indoor: Unsupervised video depth learning in challenging environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Towards better generalization: Joint depth-pose learning without posenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A compact algorithm for rectification of stereo pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fusiello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trucco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Verri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Vision and Applications</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Stereo processing by semiglobal matching and mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hirschmuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Recognition and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">An efficient solution to the five-point relative pose problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nist?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Recognition and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Scene coordinate regression forests for camera relocalization in rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Depth prediction without the sensors: Leveraging structure for unsupervised learning from monocular videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Casser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning monocular depth in dynamic scenes via instance-aware projection consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Im</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">3d packing for self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ambrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raventos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">P 2 net: Patch-match and plane-regularization for unsupervised indoor depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Pixelwise view selection for unstructured multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Sch?nberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Surface normal estimation of tilted images via spatial rectifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vuong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Roumeliotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Deep image homography estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03798</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">MannequinChallenge: Learning the depths of moving people by watching frozen people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Recognition and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Introductory techniques for 3-D computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trucco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Verri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Prentice Hall Englewood Cliffs</publisher>
			<biblScope unit="volume">201</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Consistent video depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Matzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Depth transfer: Depth extraction from video using non-parametric sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Karsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Recognition and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Discrete-continuous depth estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Pulling things out of perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Depth and surface normal estimation from monocular images using regression on deep features and hierarchical crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Monocular depth estimation using neural regression forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Towards unified depth and semantic prediction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Every pixel counts++: Joint learning of geometry and motion with 3d holistic understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Recognition and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2624" to="2641" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">ADAM: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Deepv2d: Video to depth with differentiable structure from motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
