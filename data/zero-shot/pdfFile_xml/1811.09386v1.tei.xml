<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Explicit Interaction Model towards Text Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cunxiao</forename><surname>Du</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shandong University</orgName>
								<address>
									<addrLine>No.72 Binhai Road</addrLine>
									<postCode>266237</postCode>
									<settlement>Jimo, Qingdao</settlement>
									<region>Shandong</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaozheng</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shandong University</orgName>
								<address>
									<addrLine>No.72 Binhai Road</addrLine>
									<postCode>266237</postCode>
									<settlement>Jimo, Qingdao</settlement>
									<region>Shandong</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuli</forename><surname>Feng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<addrLine>13 Computing Drive</addrLine>
									<postCode>117417</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Changqing Dist</orgName>
								<orgName type="institution">Shandong Normal University</orgName>
								<address>
									<addrLine>No.1 University Road</addrLine>
									<postCode>250358</postCode>
									<settlement>Ji&apos;nan, Shandong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Gan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shandong University</orgName>
								<address>
									<addrLine>No.72 Binhai Road</addrLine>
									<postCode>266237</postCode>
									<settlement>Jimo, Qingdao</settlement>
									<region>Shandong</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
							<email>nieliqiang@sdu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shandong University</orgName>
								<address>
									<addrLine>No.72 Binhai Road</addrLine>
									<postCode>266237</postCode>
									<settlement>Jimo, Qingdao</settlement>
									<region>Shandong</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Explicit Interaction Model towards Text Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Text classification is one of the fundamental tasks in natural language processing. Recently, deep neural networks have achieved promising performance in the text classification task compared to shallow models. Despite of the significance of deep models, they ignore the fine-grained (matching signals between words and classes) classification clues since their classifications mainly rely on the text-level representations. To address this problem, we introduce the interaction mechanism to incorporate word-level matching signals into the text classification task. In particular, we design a novel framework, EXplicit interAction Model (dubbed as EXAM), equipped with the interaction mechanism. We justified the proposed approach on several benchmark datasets including both multilabel and multi-class text classification tasks. Extensive experimental results demonstrate the superiority of the proposed method. As a byproduct, we have released the codes and parameter settings to facilitate other researches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Text classification is one of the fundamental tasks in natural language processing, targeting at classifying a piece of text content into one or multiple categories. According to the number of desired categories, text classification can be divided into two groups, namely, multi-label (multiple categories) and multi-class (unique category). For instance, classifying an article into different topics (e.g., machine learning or data mining) falls into the former one since an article could be under several topics simultaneously. By contrast, classifying a comment of a movie into its corresponding rating level lies into the multi-class group. Both multi-label and multi-class text classifications have been widely applied in many fields like sentimental analysis <ref type="bibr" target="#b1">(Cambria, Olsher, and Rajagopal 2014)</ref>, topic tagging <ref type="bibr" target="#b6">(Grave et al. 2017)</ref>, and document classification <ref type="bibr" target="#b21">(Yang et al. 2016)</ref>.</p><p>Feature engineering dominates the performance of traditional shallow text classification methods for a very long time. Various rule-based and statistical features like bag-of-words <ref type="bibr">(Wallach 2006)</ref> and N-grams <ref type="bibr" target="#b0">(Brown et al. 1992</ref>) are designed to describe the text, and fed into the shallow machine learning models such as Linear Regression <ref type="bibr" target="#b25">(Zhu and Hastie 2001)</ref> and Support Vector Machine <ref type="bibr" target="#b4">(Cortes and Vapnik 1995)</ref> to make the judgment. Traditional solutions suffer from two defects: 1) High labor intensity for the manually crafted features, and 2) data sparsity (a N-grams could occur only several times in a given dataset).</p><p>Recently, owing to the ability of tackling the aforementioned problems, deep neural networks <ref type="bibr" target="#b8">(Kim 2014;</ref><ref type="bibr" target="#b7">Iyyer et al. 2015;</ref><ref type="bibr" target="#b16">Schwenk et al. 2017</ref>; Liu, Qiu, and Huang 2016; Grave et al. <ref type="bibr">2017</ref>) have become the promising solutions for the text classification. Deep neural networks typically learn a word-level representation for the input text, which is usually a matrix with each row/column as an embedding of a word in the text. They then compress the word-level representation into a text-level representation (vector) with aggregation operations (e.g., pooling). Thereafter, a fullyconnected (FC) layer at the topmost of the network is appended to make the final decision. Note that these solutions are also called encoding-based methods , since they encode the textual content into a latent vector representation.</p><p>Although great success has been achieved, these deep neural network based solutions naturally ignore the finegrained classification clues (i.e., matching signals between words and classes), since their classifications are based on text-level representations. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the classification (i.e., FC) layer of these solutions matches the text-level representation with class representations via a dotproduct operation. Mathematically, it interprets the parameter matrix of the FC layer as a set of class representations (each column is associated with a class) (Press and Wolf 2017). As such, the probability of the text belonging to a class is largely determined by their overall matching score regardless of word-level matching signals, which would provide explicit signals for classification (e.g., missile strongly indicates the topic of military).</p><p>To address the aforementioned problems, we introduce the interaction mechanism (Wang and Jiang 2016b), which is capable of incorporating the word-level matching signals for text classification. The key idea behind the interaction mechanism is to explicitly calculate the matching scores between the words and classes. From the word-level representation, it computes an interaction matrix, in which each entry is the matching score between a word and a class (dot-product between their representations), illustrating the word-level matching signals. By taking the interaction matrix as a text representation, the later classification layer could incorporate fine-grained word level signals for the finer classification rather than simply making the text-level matching.</p><p>Based upon the interaction mechanism, we devise an EXplicit interAction Model (dubbed as EXAM). Specifically, the proposed framework consists of three main components: word-level encoder, interaction layer, and aggregation layer. The word-level encoder projects the textual contents into the word-level representations. Hereafter, the interaction layer calculates the matching scores between the words and classes (i.e., constructs the interaction matrix). Then, the last layer aggregates those matching scores into predictions over each class, respectively. We justify our proposed EXAM model over both the multi-label and multi-class text classifications. Extensive experiments on several benchmarks demonstrate the effectiveness of the proposed method, surpassing the corresponding state-of-the-art methods remarkably.</p><p>In summary, the contributions of this work are threefold: ? We present a novel framework, EXAM, which leverages the interaction mechanism to explicitly compute the wordlevel interaction signals for the text classification. ? We justify the proposed EXAM model over both multilabel and multi-class text classifications. Extensive experimental results demonstrate the effectiveness of the proposed method. ? We release the implementation of our method (including some baselines) and the involved parameter settings to facilitate later researchers 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preliminaries</head><p>In this section, we introduce two widely-used word-level encoders: Gated Recurrent Units notations in this paper, we use bold capital letters (e.g., X) and bold lowercase letters (e.g., x) to denote matrices and vectors, respectively. We employ non-bold letters (e.g., x) to represent scalars, and Greek letters (e.g., ? ) as parameters. X i,: is used to refer the i-th row of the matrix X, X :,j to represent the j-th column vector and X i,j to denote the element in the i-th row and j-th column.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gated Recurrent Units</head><p>Owing to the ability of capturing the sequential dependencies and being easily optimized (i.e., avoid the gradient vanishing and explosion problems), Gated Recurrent Units (GRU) becomes a widely used word-level encoder <ref type="bibr" target="#b10">(Liu, Qiu, and Huang 2016;</ref><ref type="bibr" target="#b22">Yogatama et al. 2017)</ref>. Typically, a GRU generates word-level representations in two phases: 1) mapping each word in the text into an embedding (a real-valued vector), and 2) projecting the sequence of word embeddings into a sequence of hidden representations, which encodes the sequential dependencies. Word embedding. Word embedding is a general method to map a word from one hot vector to a low dimensional and real-valued vector. With enough data, word embedding can capture high-level representations of words. Hidden representation. Given an embedding feature sequence E = [E 1,: , E 2,: , ? ? ?, E n,: ], GRU will compute a vector H i,: at the i-th time-step for each E i,: , and H i,: is defined as:</p><formula xml:id="formula_0">? ? ? ? ? ? ? r i = ?(M r ? [H i?1,: , E i,: ]), z i = ?(M z ? [H i?1,: , E i,: ]), H i,: = tanh(M r ? [H i?1,: , E i,: ]), H i,: = (1 ? z i ) * H i?1,: + z i * H i,: ,<label>(1)</label></formula><p>where M r and M z are trainable parameters in the GRU, and ? and tanh are sigmoid and tanh activation functions, respectively. The sequence of hidden representations H = [H 1,: , ? ? ? , H n,: ] is denoted as the word-level representation of the input text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Region Embedding</head><p>Although word embedding is a good representation for the word, it can only compute the feature vector for the single word. Qiao et al. <ref type="bibr" target="#b15">(2018)</ref> proposed region embedding to learn and utilize task-specific distributed representations of Ngrams. In the region embedding layer, the representation of a word has two parts, the embedding of the word itself and a weighting matrix to interact with the local context. For the word w i , the first part e w i is learned by an embedding matrix E ? R k?v and the second part</p><formula xml:id="formula_1">K w i ? R k?(2?s+1) is looked up in the tensor U ? R k?(2?s+1)?v by w i 's index in the vocabulary,</formula><p>where v is the size of the vocabulary, 2 ? s + 1 the region size and k the embedding size. And then, each column in K w i is used to interact with the context word in the corresponding relative position of w i to get the contextaware p t w i+t for each word w i+t in the region. Formally it is computed by the following function: where denotes element-wise multiply. And the final representation r i,s of the middle word w i is computed as follows:</p><formula xml:id="formula_2">p i w i+t = K w i ,t e w i+t ,<label>(2)</label></formula><formula xml:id="formula_3">r i,s = max([p i w i?s , p i w i?s+1 , ? ? ?, p i w i+s?1 , p i w i+s ]). (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Problem Formulation</head><p>? Multi-Class Classification. In this task, we should categorize each text instance to precisely one of c classes. Suppose that we have a data set D = {d i , l i } N , where d i denotes the text and the one-hot vector l i ? R c represents the label for d i , our goal is to learn a neural network N to classify the text. ? Multi-Label Classification. In this task, each text instance belongs to a set of c target labels. Formally, suppose that we have a dataset D = {d i , l i } N i=1 , where d i denotes the text and the multi-hot vector l i represents the label for the text d i . Our goal is to learn a neural network N to classify the text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Overview</head><p>Motivated by the limitation of encoding-based models for text classification, which is lacking the fine-grained classification clue, we propose a novel framework, named EXplicit interAction Model (EXAM), leveraging the interaction mechanism to incorporate word-level matching signals. As can be seen from <ref type="figure" target="#fig_2">Figure 2</ref>, EXAM mainly contains three components:</p><p>? A word-level encoder to project the input text d i into a word-level representation H. ? An interaction layer to compute the interaction signals between the words and classes. ? An aggregation layer to aggregate the interaction signals for each class and make the final predictions.</p><p>Considering that word-level encoders are well investigated in previous studies (as mentioned in the Section 2), and the target of this work is to learn the fine-grained classification signals, we only elaborate the interaction layer and aggregation layer in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interaction Layer</head><p>Interaction mechanism is widely used in tasks of matching source and target textual contents, such as natural language inference (Wang and Jiang 2016b) and retrieve-based chatbot <ref type="bibr" target="#b20">(Wu et al. 2017)</ref>. The key idea of interaction mechanism is to use the interaction features between the small units (e.g., words in the textual contents) to infer fine-grained clues whether two contents are matching. Inspired by the success of methods equipped with interaction mechanism over encodebased methods in matching the textual contents, we introduce the interaction mechanism into the task of matching textual contents with their classes (i.e., text classification).</p><p>Specifically, we devise an interaction layer which aims to compute the matching score between the word and class. Different from conventional interaction layer, where the wordlevel representations of both source and target are extracted with encoders like GRU, here we first project classes into real-valued latent representations. In other words, we employ a trainable representation matrix T ? R c?k to encode classes (each row represents a class), where c denotes the amount of classes and k is the embedding size equals to that of words. We then adopt dot product as the interaction function to estimate the matching score between the target word t and class s, of which the formulation is,</p><formula xml:id="formula_4">I st = T s,: H T t,: ,<label>(4)</label></formula><p>where H ? R n?k denotes word-level representation of the text, extracted by the encoder with n denoting the length of the text. In this way, we can compute the interaction matrix I ? R c?n by following:</p><formula xml:id="formula_5">I = TH T .<label>(5)</label></formula><p>Note that we reject more complex interaction functions like element-wise multiply (Gong, Luo, and <ref type="bibr" target="#b5">Zhang 2017)</ref> and cosine similarity (Wang, Hamza, and Florian 2017) for the consideration of efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Aggregation Layer</head><p>This layer is devised to aggregate the interaction features for each class s into a logits o s i , which denotes the matching score between class s and the input text d i . The aggregation layer can be implemented in different ways such as CNN (Gong, Luo, and <ref type="bibr" target="#b5">Zhang 2017)</ref> and LSTM (Wang, Hamza, and Florian 2017). However, to keep the simplicity and efficiency of EXAM, here we only use a MLP with two FC layers, where ReLU is employed as the activation function of the first layer. Formally, the MLP aggregates the interaction features I s,: for class s, and compute its associated logits as following:</p><formula xml:id="formula_6">A s,: = ReLU (I s,: W 1 + b), o s i = A s,: W 2 ,<label>(6)</label></formula><p>where W 1 and W 2 are trainable parameters and b is the bias in the first layer. We then normalize the logits o i = [o 1 i , ? ? ? , o c i ] into probabilities p i . Note that we follow previous work <ref type="bibr" target="#b6">(Grave et al. 2017)</ref> and employ softmax and sigmoid for multi-class and multi-label classifications, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Function</head><p>Similar to previous studies <ref type="bibr" target="#b16">(Schwenk et al. 2017)</ref>, in the multi-class text classification, we use cross entorpy loss as our loss function:</p><formula xml:id="formula_7">L loss = ? N i=1 c j=1 (l j i log(p j i )).<label>(7)</label></formula><p>Following previous researchers (Grave et al. <ref type="bibr">2017</ref>), we choose binary classification loss as our loss function for the multi-label one:</p><formula xml:id="formula_8">L loss = ? N i=1 c j=1 (l j i log(p j i ) + (1 ? l j i )log(1 ? p j i )). (8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generalized Encoding-Based Model</head><p>In this section, we elaborate how the encoding-based model can be interpreted as a special case of our EXAM framework. As FastText <ref type="bibr" target="#b6">(Grave et al. 2017)</ref> is the most popular model for text classification and has been investigated extensively in the literature, being able to recover it allows EXAM to mimic a large family of text classification models. FastText contains three layers: 1) an embedding layer to get the word-level representation H t,: for the word t, 2) an average pooling layer to get the text-level representation f ? R 1?k , and 3) a FC layer to get the final logits p ? R 1?c , where k denotes the embedding size and c means the number of classes. Note that we omit the subscript of the document ID for conciseness. Formally, it computes the logits p s of s-th class as follows:</p><formula xml:id="formula_9">f = 1 n n t=1 H t,: , p s = fW :,s + b s ,<label>(9)</label></formula><p>where W ? R k?c and b ? R 1?c are the trainable parameters in the last FC layer, and n denotes the length of the text. The Eqn.(9) has an equivalent form as following:</p><formula xml:id="formula_10">p s = 1 n n t=1 (H t,: W :,s ) + b s .<label>(10)</label></formula><p>It is worth noting that H t,: W :,s is exactly the interaction feature between word t and class s. Therefore, the FastText is a special case of EXAM with an average pooling as the aggregation layer. In EXAM, we use a non-linear MLP to be the aggregation layer, and it will generalize FastText to a non-linear setting which might be more expressive than the original one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Class Classification</head><p>Datasets We used publicly available benchmark datasets from (Zhang, Zhao, and LeCun 2015) to evaluate EXAM. There are in total 6 text classification datasets, corresponding to sentiment analysis, news classification, question-answer and ontology extraction tasks, respectively. <ref type="table" target="#tab_0">Table 1</ref> shows the descriptive statistics of datasets used in our experiments. Stanford tokenizer is used to tokenize the text and all words are converted to lower case. We used padding to handle the various lengths of the text, and different maximum lengths are set for each dataset, respectively. If the length of the text is less than the corresponding predefined value, we padded it with zero; otherwise we truncated the original text. To guarantee a fair comparison, the same evaluation protocol of (Zhang, Zhao, and LeCun 2015) is employed. We split 10% samples from the training set as the validation set to perform early stop for our models.</p><p>Hyperparameters For the multi-class task, we chose region embedding as the Encoder in EXAM. The region size is 7 and embedding size is 128. We used adam (Kingma and Ba 2014) as the optimizer with the initial learning rate 0.0001 and the batch size is set to 16. As for the aggregation MLP, we set the size of the hidden layer as 2 times interaction feature length. Our models are implemented and trained by MXNet (Chen et al. <ref type="bibr">2015</ref>) with a single NVIDIA TITAN Xp.</p><p>Baselines To demonstrate the effectiveness of our proposed EXAM, we compared it with several state-of-the-art baselines. The baselines are mainly in three variants: 1) models based on feature engineering; 2) Char-based deep models, and 3) Word-based deep models. The first category uses the feature from the text to conduct the classification, and we reported the results from BoW <ref type="bibr">(</ref>  Overall Performance We compared our EXAM to several state-of-the-art baselines with respect to accuracy. All results are summarized in <ref type="table" target="#tab_2">Table 2</ref>. Four points are observed as following:</p><p>? Models based on feature engineering get the worst results on all the five datasets compared to the other methods. The main reason is that the feature engineering cannot take full advantage of the supervision from the training set and it also suffers from the data sparsity.</p><p>? Char-based models get the highest overall scores on the two Amazon datasets. There are possibly two reasons, 1) compared to the word-based models, char-based models enrich the supervision from characters and the characters are combined to form N-grams, stems, words and phrase which are helpful in the sentimental classification.</p><p>2) The two Amazon datasets contain millions of training samples, perfectly fitting the deep residual architecture for the VDCNN. For the three char-based baselines, VDCNN gets the best performance on almost all the datasets because it has 29 convolutional layers allowing the model to learn more combinations of characters.</p><p>? Word-based baselines exceed the other variants on three datasets and lose on the two Amazon datasets. The main reason is that the three tasks like news classification conduct categorization mainly via key words, and the wordbased models are able to directly use the word embedding without combining the characters. For the five baselines, W.C RegionEmb performs the best, because it learns the region embedding to utilize the N-grams feature from the text.</p><p>? It is clear to see that EXAM achieves the best performance over the three datasets: AG, Yah. A. and DBP. For the Yah.A., EXAM improves the best performance by 1.1%. Additionally, as a word-based model, EXAM beats all the word-based baselines on the other two Amazon datasets with a performance gain of 1.0% on the Amazon Full, because our EXAM considers more fine-grained interaction features between classes and words, which is quite helpful in this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Component-wise Evaluation</head><p>We studied the variant of our model to further investigate the effectiveness of the interaction layer and aggregation layer. We built a model called EXAM Encoder to preserve only the Encoder component with a max pooling layer and FC layer to derive the final probabilities. EXAM Encoder does not consider the interaction features between the classes and words, so it will automatically be degenerated into the Encoding-Based model. We reported the results of the two models on all the datasets at <ref type="table" target="#tab_3">Table 3</ref>, and it is clear to see that EXAM Encoder is not a patch on the original EXAM, verifying the effectiveness of interaction mechanism. We also drew the convergence lines for EXAM and the EXAM Encoder for the datasets. From the <ref type="figure" target="#fig_4">Figure 3</ref>, where the red lines represent EXAM and the blue is EXAM Encoder , we observed that EXAM converges faster than EXAM Encoder with respect to all the datasets. Therefore, the interaction brings not only performance improvement but also faster convergence. The possible reason is that a non-linear aggregation layer introduces more parameters to fit the interaction features compared to the average pooling layer as mentioned in Section 4.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Label Classification</head><p>Datasets We conducted experiments on two different multi-label text classification datasets, named KanShan-Cup dataset 2 (a benchmark) and Zhihu dataset 3 , respectively.</p><p>? KanShan-Cup dataset. This dataset is released by a competition of tagging topics for questions (multi-label classification) posted in the largest Chinese community question answering platform, Zhihu. The dataset contains 3,000,000 questions and 1,999 topics (classes), where one question may belong to one to five topics. For questions with more than 30 words, we kept the last 30 words, otherwise, we padded zeros. We separated the dataset into training, validation, and testing with 2,800,000, 20,000, and 180,000 questions, respectively. ? Zhihu dataset. Considering the user privacy and data security, KanShan-Cup does not provide the original texts of the questions and topics, but uses numbered codes and numbered segmented words to represent text messages. Therefore, it is inconvenient for researchers to perform analyses like visualization and case study. To solve this problem, we constructed a dataset named Zhihu dataset. We chose the top 1,999 frequent topics from Zhihu and crawled all the questions relevant to these topics. Finally, we acquired 3,300,000 questions, with less than 5 topics for each question. We adopted 3,000,000 samples as the training set, 30,000 samples as validation and 300,000 samples as testing.</p><p>Baselines We applied the following models as baselines to evaluate the effectiveness of EXAM.  <ref type="bibr">2017)</ref>. The three models got the best performance in the KanShan-Cup competition, so we applied them as the word-based baselines.</p><p>Hyperparameters We implemented the baseline models and EXAM by MXNet <ref type="bibr" target="#b2">(Chen et al. 2015)</ref>. We used the matrix trained by word2vec <ref type="bibr" target="#b11">(Mikolov et al. 2013)</ref> to initialize the embedding layer, and the embedding size is 256. We adopted GRU as the Encoder, and each GRU Cell has 1,024 hidden states. The accumulated MLP has 60 hidden units. We applied Adam <ref type="bibr" target="#b9">(Kingma and Ba 2014)</ref> to optimize models on one NVIDIA TITAN Xp with the batch size of 1000 and the initial learning rate is 0.001. The validation set is applied for early-stopping to avoid overfitting. All hyperparameters are chosen empirically.</p><p>Metrics We used the following metrics to evaluate the performance of our model and baseline models.</p><p>? Precision: Different from the traditional precision metric (Precision@5) which is set as the fraction of the relevant topic tags among the five returned tags, we utilized weighted precision to encourage the relevant topic tags to be ranked higher in the returned list. Formally, the Precision is computed as following, P recision = pos?{1,2,3,4,5} P recision@pos log(pos + 1) .  <ref type="figure">Figure 4</ref>: The visualization of interaction features of EXAM.</p><p>? Recall@5: Recall is the fraction of relevant topic tags that have been retrieved over the total amount of five relevant topic tags, high recall means that the model returns most of the relevant topic tags. ? F 1 : F 1 is the harmonic average of the precision and recall, we computed it as following,</p><formula xml:id="formula_12">F 1 = P recision * Recall@5 P recision + Recall@5 .<label>(12)</label></formula><p>Performance Comparison <ref type="table" target="#tab_4">Table 4</ref> gives the performance of our model and baselines over two different datasets with respect to Precision, Recall@5 and F 1 . We observed the following from the <ref type="table" target="#tab_4">Table 4</ref>:</p><p>? Word-based models are better than char-based models in Kanshan-Cup dataset. That may be because in Chinese the words can offer more supervisions than characters and the question tagging task needs more word supervision. ? For word-based baseline models, all the baselines have similar performance which corroborates the conclusion in FastText <ref type="bibr" target="#b6">(Grave et al. 2017</ref>) that simple network is on par with deep learning classifiers in text classification. ? Our models achieve the state-of-the-art performance over two different datasets though we only slightly modified TextRNN to build EXAM. Different from the traditional models which encode the whole text into a vector, in EXAM, the representations of classes firstly interact with words to get more fine-grained features as shown in <ref type="figure">Figure 4</ref>. The results suggest that word-level interaction features are relatively more important than global text-level representations in this task.</p><p>Interaction Visualization To illustrate the effectiveness of explicit interaction, we visualized an interaction feature I of the question "Second-hand TIDDA 1.6 T Mannual gear has gotten some problems, please everybody help me to solve it ?". This question has 5 topics: Car, Second-hand Car, Motor Dom, Autocar Conversation and Autocar Service. EXAM only misclassified the last topic. In <ref type="figure">Figure 4</ref>, we observed that when classifying different topics, the interaction features are different. The topics "Car" and "Second-hand Car" pay much attention to the words like "Second-hand TIIDA" and the other topic like "Autocar Conversation" focuses more on "got some problems". The results clearly signify that the interaction feature between the word and class is well-learned and highly meaningful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Text Classification Existing researches on text classification can be categorized into two groups: feature-based and deep neural models. The former focuses on handcraft features and uses machine learning algorithms as the classifier. Bag-of-words (Wallach 2006) is a very efficient way to conduct the feature engineering. SVM and Naive Bayes are constantly the classifier. The latter, deep neural models, taking advantage of neural networks to accomplish the model learning from data, have become the promising solution for the text classification. For instance, Iyyer et al. <ref type="bibr">(2015)</ref> proposed Deep Averaging Networks (DAN) and Grave et al. <ref type="bibr">(2017)</ref> proposed the FastText, and both are simple but efficient. To get the temporal features between the words in the text, some models like TextCNN <ref type="bibr" target="#b8">(Kim 2014)</ref> and Char-CNN (Zhang, Zhao, and LeCun 2015) exploit the convolutional neural network, and there are also some models based on Recurrent Neural Network (RNN). Recently, Johnson et al. <ref type="bibr">(2017)</ref> investigated the residual architecture and built a model called VD-CNN and Qiao et al. <ref type="bibr" target="#b15">(2018)</ref> proposed a new method of region embedding for the text classification. However, as mentioned in the Introduction, all these methods are text-level models while EXAM conducts the matching at the word level.</p><p>Interaction Mechanism Interaction Mechanism is widely used in Natural Language Sentence Matching (NLSM). The key idea of interaction mechanism is to use the interaction features between the small units (like words in sentence) to make the matching. </p><p>proposed a densely interactive inference network to use DenseNet to aggregate dense interaction features. Our work is different from them since they mainly apply this mechanism in text matching instead of the classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this work, we present a novel framework named EXAM which employs the interaction mechanism to explicitly compute the word-level interaction signals for the text classification. We apply the proposed EXAM on multi-class and multi-label text classifications. Experiments over several benchmark datasets verify the effectiveness of our proposed mechanism. In the future, we plan to investigate the effect of different interaction functions in the interaction mechanism. Besides, we are interested in extend EXAM by introducing more complex aggregation layers like ResNet or DenseNet.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of encoding-based methods for text classification with text-level matching.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(</head><label></label><figDesc><ref type="bibr" target="#b3">Chung et al. 2014)</ref> and Region Embedding<ref type="bibr" target="#b15">(Qiao et al. 2018)</ref>. These encoders project a piece of input text into a word-level representation, serving as the building blocks of the proposed method. For the 1 https://github.com/NonvolatileMemory/AAAI 2019 EXAM .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of proposed EXAM method with wordlevel matching.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>i n i n g L o s s E X A M E n c o d e r E X A M Y a h . A .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Convergence lines on the four dataset DBP, Amz. F., Amz. P. and Yah. A., respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Wang et al. (2016b) proposed a "matching-aggregation" framework to perform the interaction in Natural Language Inference. Following this work, Parikh et al. (2016) integrated the attention mechanism into this framework, called Decomposable Attention Model. Then Wang et al. (2016a) discussed different interaction functions in Text Matching. Yu et al. (2017) adopted tree-LSTM to get different level units to perform the interaction. Gong et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of Datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="4">Classes Average Lengths Train Samples Test Samples</cell><cell>Tasks</cell></row><row><cell>Amazon Review Polarity</cell><cell>2</cell><cell>91</cell><cell>3,600,000</cell><cell>400,000</cell><cell>Sentiment</cell></row><row><cell>Amazon Review Full</cell><cell>5</cell><cell>93</cell><cell>3,000,000</cell><cell>650,000</cell><cell>Analysis</cell></row><row><cell>AG's News</cell><cell>4</cell><cell>44</cell><cell>120,000</cell><cell>7,600</cell><cell>News Classification</cell></row><row><cell>Yahoo! Answers</cell><cell>10</cell><cell>112</cell><cell>1,400,000</cell><cell>60,000</cell><cell>Question Answer</cell></row><row><cell>DBPedia</cell><cell>14</cell><cell>55</cell><cell>560,000</cell><cell>70,000</cell><cell>Ontology Extraction</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Test Set Accuracy [%] on multi-class document classification tasks, and all the results of baselines are directly cited from the respective papers. The three different models are separated by lines. The best scores for the category are marked with underline and the overall best scores are highlight with bold font.</figDesc><table><row><cell>Model</cell><cell cols="5">Amz. P. Amz. F. AG Yah. A. DBP</cell></row><row><cell>BoW (Zhang, Zhao, and LeCun 2015)</cell><cell>90.4</cell><cell>54.6</cell><cell>88.8</cell><cell>68.9</cell><cell>96.6</cell></row><row><cell>N-grams (Zhang, Zhao, and LeCun 2015)</cell><cell>92.0</cell><cell>54.3</cell><cell>92.0</cell><cell>68.5</cell><cell>98.6</cell></row><row><cell>N-grams TFIDF (Zhang, Zhao, and LeCun 2015)</cell><cell>91.5</cell><cell>52.4</cell><cell>92.4</cell><cell>68.5</cell><cell>98.7</cell></row><row><cell>Char-CNN (Zhang, Zhao, and LeCun 2015)</cell><cell>94.5</cell><cell>59.6</cell><cell>87.2</cell><cell>71.2</cell><cell>98.3</cell></row><row><cell>Char-CRNN (Zhang, Zhao, and LeCun 2015)</cell><cell>94.1</cell><cell>59.2</cell><cell>91.4</cell><cell>71.7</cell><cell>98.6</cell></row><row><cell>VDCNN (Schwenk et al. 2017)</cell><cell>95.7</cell><cell>63.0</cell><cell>91.3</cell><cell>73.4</cell><cell>98.7</cell></row><row><cell>Small word CNN (Zhang, Zhao, and LeCun 2015)</cell><cell>94.2</cell><cell>56.3</cell><cell>89.1</cell><cell>70.0</cell><cell>98.2</cell></row><row><cell>Large word CNN (Zhang, Zhao, and LeCun 2015)</cell><cell>94.2</cell><cell>54.1</cell><cell>91.5</cell><cell>71.0</cell><cell>98.3</cell></row><row><cell>LSTM (Zhang, Zhao, and LeCun 2015)</cell><cell>93.9</cell><cell>59.4</cell><cell>86.1</cell><cell>70.8</cell><cell>98.6</cell></row><row><cell>Bigram-FastText (Grave et al. 2017)</cell><cell>94.6</cell><cell>60.2</cell><cell>92.5</cell><cell>72.3</cell><cell>98.6</cell></row><row><cell>W.C RegionEmb (Qiao et al. 2018)</cell><cell>95.1</cell><cell>60.9</cell><cell>92.8</cell><cell>73.7</cell><cell>98.9</cell></row><row><cell>EXAM (Ours)</cell><cell>95.5</cell><cell>61.9</cell><cell>93.0</cell><cell>74.8</cell><cell>99.0</cell></row><row><cell>that all the baselines and our EXAM do not use pre-trained</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>word embedding over other corpus like glove.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Component-wise evaluation.</figDesc><table><row><cell cols="3">Dataset EXAM EXAM Encoder</cell></row><row><cell>Amz. P.</cell><cell>95.5</cell><cell>95.1</cell></row><row><cell>Amz. F.</cell><cell>61.9</cell><cell>60.9</cell></row><row><cell>AG</cell><cell>93.0</cell><cell>92.8</cell></row><row><cell>Yah. A.</cell><cell>74.8</cell><cell>73.1</cell></row><row><cell>DBP</cell><cell>99.0</cell><cell>98.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Performance comparison between EXAM and baselines. The best scores are highlight in bold font.</figDesc><table><row><cell>Kanshan-Cup Dataset</cell><cell>Zhihu Dataset</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Class-based n-gram models of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">J D</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="479" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Senticnet 3: A common and common-sense knowledge base for cognition-driven sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Olsher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rajagopal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1515" to="1521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/1512.01274</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>G?l?ehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Natural language inference over interaction space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/1709.04348</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="427" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep unordered composition rivals syntactic methods for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Manjunatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Iii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1681" to="1691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<idno>abs/1408.5882</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recurrent neural network for text classification with multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence</title>
		<meeting>the International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2873" to="2879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural semantic encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">397</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A decomposable attention model for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>T?ckstr?m</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2249" to="2255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Using the output embedding to improve language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="157" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A new method of region embedding for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="977" to="984" />
		</imprint>
	</monogr>
	<note>Proceedings of the International Conference on Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A compare-aggregate model for matching text sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<idno>abs/1611.01747</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning natural language inference with LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1442" to="1451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bilateral multi-perspective matching for natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Florian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence</title>
		<meeting>the International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4144" to="4150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sequential matching network: A new architecture for multi-turn response selection in retrieval-based chatbots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="496" to="505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Generative and discriminative text classification with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<idno>CoRR abs/1703.01898</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural tree indexers for text understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munkhdalai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="11" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Characterlevel convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Kernel logistic regression and the import vector machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1081" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
