<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Progressively Normalized Self-Attention Network for Video Polyp Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge-Peng</forename><surname>Ji</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of AI (IIAI)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Wuhan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Cheng</forename><surname>Chou</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Wuhan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of AI (IIAI)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geng</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of AI (IIAI)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of AI (IIAI)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debesh</forename><surname>Jha</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">SimulaMet</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of AI (IIAI)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Progressively Normalized Self-Attention Network for Video Polyp Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Normalized self-attention ? Polyp segmentation ? Colonoscopy</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing video polyp segmentation (VPS) models typically employ convolutional neural networks (CNNs) to extract features. However, due to their limited receptive fields, CNNs cannot fully exploit the global temporal and spatial information in successive video frames, resulting in false positive segmentation results. In this paper, we propose the novel PNS-Net (Progressively Normalized Self-attention Network), which can efficiently learn representations from polyp videos with real-time speed (?140fps) on a single RTX 2080 GPU and no postprocessing. Our PNS-Net is based solely on a basic normalized selfattention block, equipping with recurrence and CNNs entirely. Experiments on challenging VPS datasets demonstrate that the proposed PNS-Net achieves state-of-the-art performance. We also conduct extensive experiments to study the effectiveness of the channel split, soft-attention, and progressive learning strategy. We find that our PNS-Net works well under different settings, making it a promising solution to the VPS task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Early diagnosis of colorectal cancer (CRC) plays a vital role in improving the survival rate of CRC patients. In fact, the survival rate in the first stage of CRC is over 95%, decreasing to below 35% in the fourth and fifth stages <ref type="bibr" target="#b3">[4]</ref>. Currently, colonoscopy is widely adopted in clinical practice and has become a standard method for screening CRC. During the colonoscopy, physicians visually inspect the bowel with an endoscope to identify polyps, which can develop into CRC if left untreated. In practice, colonoscopy is highly dependent on the physicians' level of experience and suffers from a high polyp miss rate <ref type="bibr" target="#b17">[18]</ref>. These limitations can be resolved with automatic polyp segmentation techniques, which segment polyps from colonoscopy images/videos without intervention from physicians. However, accurate and real-time polyp segmentation is a challenging task due to the low boundary contrast between a polyp and its surroundings and the large shape variation of polyps <ref type="bibr" target="#b7">[8]</ref>. Significant efforts have been dedicated to overcoming these challenges. In early studies, learning-based methods turned to handcrafted features <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b19">20]</ref>, such as color, shape, texture, appearance, or some combination. These methods train a classifier to separate the polyps from the background. However, they usually suffer from low accuracy due to the limited representation capability of handcrafted features in depicting heterogeneous polyps, as well as the close resemblance between polyps and hard mimics <ref type="bibr" target="#b23">[24]</ref>. In more recent studies, deep learning methods have been used for polyp segmentation <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26]</ref>. Although these methods have made some progress, they only use bounding boxes to detect polyps, and therefore cannot accurately locate the boundaries. To solve this, Brandao et al. <ref type="bibr" target="#b4">[5]</ref> adopted a fully convolutional networks (FCN) with a pre-trained model to recognize and segment polyps. Later, Akbari et al. <ref type="bibr" target="#b0">[1]</ref> introduced a modified FCN to increase the accuracy of polyp segmentation. Inspired by the success of UNet <ref type="bibr" target="#b18">[19]</ref> in biomedical image segmentation, UNet++ <ref type="bibr" target="#b27">[28]</ref> and ResUNet <ref type="bibr" target="#b12">[13]</ref> are employed for polyp segmentation and achieved good results. Some methods also focus on area-boundary constraints. For instance, Psi-Net <ref type="bibr" target="#b16">[17]</ref> makes use of polyp boundary and area information simultaneously. Fang et al. <ref type="bibr" target="#b8">[9]</ref> introduced a three-step selective feature aggregation network. ACSNet <ref type="bibr" target="#b24">[25]</ref> utilized an adaptive context selection based encoder-decoder framework. Zhong et al. <ref type="bibr" target="#b26">[27]</ref> propose a context-aware network based on adaptive scale and global semantic context. Introduced more recently, the current golden standard for image polyp segmentation, PraNet <ref type="bibr" target="#b7">[8]</ref>, applies area and boundary cues in a reverse attention module, achieving the cutting-edge performance. However, these methods have only been trained and evaluated on still images and focus on static information, ignoring the temporal information in endoscopic videos which can be exploited for better results. To this end, Puyal et al. <ref type="bibr" target="#b17">[18]</ref> propose a hybrid 2D/3D CNN architecture. Their model aggregates spatial and temporal correlations and achieves better segmentation results. However, the spatial correlation between frames is restricted by the size of the kernel, preventing the accurate segmentation of fast videos.</p><p>Recently, the self-attention network <ref type="bibr" target="#b21">[22]</ref> has shown superior performance in computer vision tasks such as video object segmentation <ref type="bibr" target="#b9">[10]</ref>, image superresolution <ref type="bibr" target="#b22">[23]</ref>, and others. Inspired by this, in this paper, we propose a novel selfattention framework, called the Progressively Normalized Self-attention Network (PNS-Net), for the video polyp segmentation (VPS) task. Our contributions are as follows:</p><p>? Different from existing CNN-based models, the proposed PNS-Net framework is a self-attention model for VPS, introducing a new perspective for addressing this task. ? To fully utilize the temporal and spatial cues, we propose a simple normalized self-attention (NS) block. The NS block is flexible (backbone-free) and efficient, enabling it to easily be embedded into current CNN-based encoderdecoder architectures for better performance. ? We evaluate the proposed PNS-Net on challenging VPS datasets and compare it with two classical methods (i.e., UNet <ref type="bibr" target="#b18">[19]</ref> and UNet++ <ref type="bibr" target="#b27">[28]</ref>) and three cutting-edge models (i.e., ResUNet <ref type="bibr" target="#b12">[13]</ref>, ACSNet <ref type="bibr" target="#b24">[25]</ref>, and PraNet <ref type="bibr" target="#b7">[8]</ref>). Experimental results show that PNS-Net achieves state-of-the-art performance with real-time speed. All the training data, models, results, and evaluation tools will be released to advance the development of this field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Normalized Self-attention (NS)</head><p>Motivation. Recently, the self-attention mechanism <ref type="bibr" target="#b21">[22]</ref> has been widely exploited in many popular computer vision tasks. However, in our initial studies, we found that introducing the original self-attention mechanism to the VPS task does not achieve satisfactory results (i.e., high accuracy and speed). Analysis. For the VPS task, multi-scale polyps move at various speeds. Thus, dynamically updating the receptive field of the network is important. Further, the self-attention, such as the non-local network <ref type="bibr" target="#b21">[22]</ref>, incurs a high computational and memory cost, which limits the inference speed for our fast and dense prediction task. Motivated by the recent video salient object detection model <ref type="bibr" target="#b9">[10]</ref>, we utilize the channel split, query-dependent, and normalization rules to reduce the computational cost and improve the accuracy, respectively. Channel Split Rule. Specifically, given an input feature (i.e., X ? R T ?H?W ?C ) extracted from T video frames with a size of H ? W and C channels, we first utilize three linear embedding functions ?(?), ?(?), and g(?) to generate the corresponding attention features, which are implemented by a 1 ? 1 ? 1 convolutional layer <ref type="bibr" target="#b21">[22]</ref>. This can be expressed as:</p><formula xml:id="formula_0">Q = ?(X), K = ?(X), V = g(X).<label>(1)</label></formula><p>Then we split each attention feature {Q, K, V } ? R T ?H?W ?C into N groups along the channel dimension and generate query, key, and value features, i.e.,</p><formula xml:id="formula_1">{Q i , K i , V i } ? R T ?H?W ? C N , where i = {1, 2, ? ? ? , N }. Query-Dependent Rule.</formula><p>To extract the spatial-temporal relationship between successive video frames, we need to measure the similarity between query features Q i and key features K i . Inspired by <ref type="bibr" target="#b9">[10]</ref>, we introduce N relevance measuring (i.e., query-dependent rule) blocks to compute the spatial-temporal affinity matrix for the constrained neighborhood of the target pixel. Rather than computing the response between a query position and the feature at all positions, as done in <ref type="bibr" target="#b21">[22]</ref>, the relevance measuring block can capture more relevance regarding the target object within T frames. More specifically, given a sliding window with fixed kernel size k and dilation rate d i = 2i ? 1, we get the corresponding constrained neighborhood in K i for query pixel X q of Q i in position (x, y, z), which can be obtained by a sampling function F S . This is computed by:</p><formula xml:id="formula_2">F S X q , K i ? R T (2k+1) 2 ? C N = ? x+kdi m=x?kdi ? y+kdi n=y?kdi ? T t=1 K i (m, n, t),<label>(2)</label></formula><formula xml:id="formula_3">where 1 ? x ? H, 1 ? y ? W , and 1 ? z ? T .</formula><p>Thus, the size of the constrained neighborhood depends on the various spatial-temporal receptive fields with different kernel size k, dilation rate d i , and frame number T , respectively. Normalization Rule. However, the internal covariate shift problem <ref type="bibr" target="#b10">[11]</ref> exists in the feed-forward of input Q i , incurring that the layer parameters cannot dynamically adapt the next mini-batch. Therefore, we maintain a fixed distribution for Q i via:Q</p><formula xml:id="formula_4">i = Norm(Q i ),<label>(3)</label></formula><p>where Norm is implemented by layer normalization <ref type="bibr" target="#b1">[2]</ref> along temporal dimension. Relevance Measuring. Finally, the affinity matrix is computed as:</p><formula xml:id="formula_5">M A i ? R T HW ?T (2k+1) 2 = Softmax(Q i F S X q , K i T C/N ), whenX q ?Q i ,<label>(4)</label></formula><p>where C/N is a scaling factor to balance the multi-head attention <ref type="bibr" target="#b20">[21]</ref>. Spatial-Temporal Aggregation. Similar to relevance measuring, we also compute the spatial-temporally aggregated features M T i within the constrained neighborhood during temporal aggregation. This can be formulated as:</p><formula xml:id="formula_6">M T i ? R T HW ? C N = M A i F S X a , V i , when X a ? M A i ,<label>(5)</label></formula><p>Soft-Attention. We use a soft-attention block to synthesize features from the group of affinity matrices M A i and aggregated features M T i . During the synthesis process, relevant spatial-temporal patterns should be enhanced while less relevant ones should be suppressed. We first concatenate a group of affinity matrices M A i along the channel dimension to generate M A . Thus, the softattention map M S is computed by:</p><formula xml:id="formula_7">M S ? R T HW ?1 ? maxM A , when M A ? R T HW ?T (2k+1) 2 N ,<label>(6)</label></formula><p>where the max function computes the channel-wise maximum value. Then we concatenate a group of the spatial-temporally aggregated features M T i along the channel dimension to generate M T . Normalized Self-attention. Finally, our NS block can be computed as:</p><formula xml:id="formula_8">Z ? R T ?H?W ?C = X + Y = X + (M T W T ) M S ,<label>(7)</label></formula><p>where W T is the learnable weight and is the channel-wise Hadamard product.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Progressive Learning Strategy</head><p>Encoder. For fair comparison, we use the same backbone (i.e., Res2Net-50) as in PraNet <ref type="bibr" target="#b7">[8]</ref>. Given a polyp video clip with T frames as input (i.e., {I} T t=1 ? R H ?W ?3 ), we first feed it into a spatial encoder to extract two spatial features from the conv3 4 and conv4 6 layers, respectively. To alleviate the computational burden, we adopt an RFB-like <ref type="bibr" target="#b14">[15]</ref> module to reduce the feature channel. Thus, we generate two spatial features, including low-level (i.e.,</p><formula xml:id="formula_9">{X l t } T t=1 ? R H l ?W l ?C l ) and high-level (i.e., {X h t } T t=1 ? R H h ?W h ?C h ) 1</formula><p>. Progressively Normalized Self-attention (PNS). Most attention strategies aim to refine candidate features, such as first-order <ref type="bibr" target="#b7">[8]</ref> and second-order <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b20">21]</ref> functions. As such, the strong semantic information in high-level features might be diffused gradually during the forward pass of the network. To alleviate this, we introduce a progressive residual learning strategy in our NS block. Specifically, we first reshape the corresponding high-level features {X h t } T t=1 of consecutive input frames into a temporal feature, which can be viewed as a four-dimensional tensor (i.e., X h ? R T ?H h ?W h ?C h ). Then we refine X h via stacked normalized self-attention in a progressive manner:</p><formula xml:id="formula_10">X r ? R T ?H h ?W h ?C h = NS ?R ( X h ) = NS ?R (F R ({X h t } T t=1 )),<label>(8)</label></formula><p>where NS ?R means that R normalized self-attention blocks are stacked in the refinement process. F R is the reshaping function for the temporal dimension. To allow this block to easily be plugged into pre-trained networks, the commonly adopted solution is to add a residual learning process. Finally, the refined spatialtemporal feature is generated by:</p><formula xml:id="formula_11">{X r t } T t=1 ? R H h ?W h ?C h = F R ( X h + X r ).<label>(9)</label></formula><p>Decoder and Learning Strategy. We combine the low-level feature {X l t } T t=1 from the spatial decoder and the spatial-temporal feature {X r t } T t=1 from the PNS block via a two-stage UNet-like decoder F D . Thus, the output of our method is computed by</p><formula xml:id="formula_12">{P t } T t=1 = F D ({X l t } T t=1 , {X r t } T t=1</formula><p>). We adopt the standard crossentropy loss function in the learning process. <ref type="bibr" target="#b0">1</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Implementation Details</head><p>Datasets. We adopt four widely used polyp datasets in our experiments, including image-based (i.e., Kvasir <ref type="bibr" target="#b11">[12]</ref>) and video-based (i.e., CVC-300 <ref type="bibr" target="#b3">[4]</ref>, CVC-612 <ref type="bibr" target="#b2">[3]</ref>, and ASU-Mayo <ref type="bibr" target="#b19">[20]</ref>) ones. Kvasir is a large-scale and challenging dataset, which consists of 1,000 polyp images with fully annotated pixel-level ground truths (GTs). The whole Kvasir is used for training. ASU-Mayo contains 10 negative video samples from normal subjects and 10 positive samples from patients. We only adopt the positive part for training. Following the same protocol as <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3]</ref>, we split the videos from CVC-300 (12 clips) and CVC-612 (29 clips) into 60% for training, 20% for validation, and 20% for testing.</p><p>Training. Due to the limited video training data, we try to fully utilize largescale image data to capture more appearances of the polyp and scene. Thus, we train our model in two steps: i) Pre-training phase. We remove the normalized self-attention (NS) block from PNS-Net and pre-train the static backbone using an image-based polyp dataset (i.e., Kvasir <ref type="bibr" target="#b11">[12]</ref>) and the training set of videobased polyp datasets (i.e., CVC-300 <ref type="bibr" target="#b3">[4]</ref>, CVC-612 <ref type="bibr" target="#b2">[3]</ref>, and ASU-Mayo <ref type="bibr" target="#b19">[20]</ref>). The initial learning rate of the Adam algorithm and the weight decay are both 1e-4. The static part of our PNS-Net convergences after 100 epochs. ii) Fine-tuning phase. We plug the NS block into our PNS-Net and fine-tune the whole network using the video polyp datasets, including the ASU-Mayo and the training sets of CVC-300 and CVC-612. We set the number of attention groups N = 4 and the number of stacked normalized self-attention blocks R = 2, along with a kernel size of k = 3. The initial learning is set to 1e-4, and the whole model is fine-tuned over one epoch. In this way, although the densely labeled VPS data is scarce, our PNS-Net still achieves good generalization performance. Testing and Runtime. To test the performance of our PNS-Net, we validate it on challenging datasets, including the test set of CVC-612 (i.e., CVC-612-T), the validation set of CVC-612 (i.e., CVC-612-V), and the test/validation set of CVC-300 (i.e., CVC-300-TV). During inference, we sample T =5 frames from a polyp clip and resize them to 256?448 as the input. For final prediction, we use the output P t of the network followed by a sigmoid function. Our PNS-Net achieves a speed of ?140fps on a single RTX 2080 GPU without any post-processing (e.g., CRF <ref type="bibr" target="#b13">[14]</ref>). The speeds of the compared methods are listed in Tab. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation on Video Polyp Segmentation</head><p>Baselines. We re-train five cutting-edge polyp segmentation baselines (i.e., UNet <ref type="bibr" target="#b18">[19]</ref>, UNet++ <ref type="bibr" target="#b27">[28]</ref>, ResUNet <ref type="bibr" target="#b12">[13]</ref>, ACSNet <ref type="bibr" target="#b24">[25]</ref>, and PraNet <ref type="bibr" target="#b7">[8]</ref>) with the same data used by our PNS-Net, under their default settings, for fair comparison.</p><p>Metrics. The metrics used included: (1) maximum Dice (maxDice), which measures the similarity between two sets of data; (2) maximum specificity (maxSpe), which refers to the percentage of the samples that are negative and are judged as such; (3) maximum IoU (maxIoU), which measures the overlap between two  masks; (4) S-measure <ref type="bibr" target="#b5">[6]</ref> (S ? ), which evaluates region-and object-aware structural similarity; (5) enhanced-alignment measure <ref type="bibr" target="#b6">[7]</ref> (E ? ), which measures pixellevel matching and image-level statistics; and (6) mean absolute error (M ), which measures the pixel-level error between the prediction and GT. Qualitative Comparison. In <ref type="figure" target="#fig_2">Fig. 2</ref>, We provide the polyp segmentation results of our PNS-Net on CVC-612-T. Our model can accurately locate and segment polyps in many difficult situations, such as different sizes, homogeneous areas, different textures, etc. Quantitative Comparison. Quantitative comparison results are summarized in Tab. 1. We conduct three experiments on test datasets to verify the model's performance. CVC-300-TV consists of both validation set and test set, which include six videos in total. CVC-612-V and CVC-612-T each contain five videos. On CVC-300, where all the baseline methods perform poorly, our PNS-Net achieves remarkable performance in all metrics and outperforms all SOTA methods by </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Study</head><p>Effectiveness of Channel Split. We investigate the contribution of channel split rule under different scales. The results are listed in rows #2 to #5 in Tab.</p><p>2. We observe that #4 (N=4) outperforms other settings (i.e., #2, #3, and #5) on CVC-300-TV, in all metrics. This improvement shows that an improper receptive field (RF) harms the ability to excavate temporal information, since a large RF will pay more attention to the global environment rather than local motion information. On the other hand, when the split number is too small, the model fails to capture multi-scale polyps moving at various speeds. Effectiveness of Soft-attention. We further investigate the contribution of the soft-attention mechanism. As shown in Tab. 2, #6 is generally better than #4 with the soft-attention block on CVC-612-T. This improvement suggests that introducing the soft-attention block to synthesize the aggregation feature and affinity matrix is necessary for increasing performance.</p><p>Effectiveness of the Number of NS Blocks. To access the number of normalized self-attention blocks under different settings, we derive three variants as #7, #8, and #9. We observe that #8 (PNS-Net setting) is significantly better than #7 and #9, with R = 2, in all metrics on CVC-300-TV and CVC-612-T. This improvement illustrates that too many iterations of NS blocks may cause overfitting on small datasets (#9). In contrast, the model fails to alleviate the diffusion issue of high-level features with a single residual block. Empirically, we recommend increasing the number of NS blocks when training on larger datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We have proposed a self-attention based framework, PNS-Net, to accurately segment polyps from colonoscopy videos with super high speed (?140fps). Our basic normalized self-attention blocks can be easily plugged into existing CNNbased architectures. We experimentally show that our PNS-Net achieves the best performance on all existing publicly available datasets under six metrics. Further, extensive ablation studies demonstrate that the core components in our PNS-Net are all effective. We hope that the proposed PNS-Net can serve as a catalyst for progressing both in VPS as well as other closely related video-based medical segmentation tasks. Exploring the performance of PNS-Net on a larger VPS dataset will be left to our future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>G.-P. Ji and Y.-C. Chou contributed equally. Code: http://dpfan.net/pranet/ arXiv:2105.08468v2 [cs.CV] 24 May 2021</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>Pipeline of the proposed PNS-Net, including the normalized selfattention block (see ? 2.1) with a stacked (?R) learning strategy (see ? 2.2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Qualitative results on CVC-612-T<ref type="bibr" target="#b2">[3]</ref>. For more visualization results please refer to the supplementary material (i.e., PDF file and videos).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>We set H l = H 4 , W l = W 4 , C l = 24, H h = H 8 , W h = W 8 , and C h = 32.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Quantitative results on different datasets.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>2018?2019</cell><cell></cell><cell>2020</cell><cell></cell><cell>2021</cell></row><row><cell></cell><cell></cell><cell>UNet</cell><cell cols="2">UNet++ ResUNet</cell><cell>ACSNet</cell><cell>PraNet</cell><cell>PNS-Net</cell></row><row><cell></cell><cell>Metrics</cell><cell cols="3">MICCAI [19] TMI [28] ISM [13]</cell><cell cols="2">MICCAI [25] MICCAI [8]</cell><cell>(OUR)</cell></row><row><cell></cell><cell>Speed</cell><cell>108fps</cell><cell>45fps</cell><cell>20fps</cell><cell>35fps</cell><cell>97fps</cell><cell>140fps</cell></row><row><cell>CVC-300-TV</cell><cell>maxDice? maxSpe? maxIoU? S? ? E ? ? M ?</cell><cell>0.639 0.963 0.525 0.793 0.826 0.027</cell><cell>0.649 0.944 0.539 0.796 0.831 0.024</cell><cell>0.535 0.852 0.412 0.703 0.718 0.052</cell><cell>0.738 0.987 0.632 0.837 0.871 0.016</cell><cell>0.739 0.993 0.645 0.833 0.852 0.016</cell><cell>0.840 0.996 0.745 0.909 0.921 0.013</cell></row><row><cell>CVC-612-V</cell><cell>maxDice? maxSpe? maxIoU? S? ? E ? ? M ?</cell><cell>0.725 0.971 0.610 0.826 0.855 0.023</cell><cell>0.684 0.952 0.570 0.805 0.830 0.025</cell><cell>0.752 0.939 0.648 0.829 0.877 0.023</cell><cell>0.804 0.929 0.712 0.847 0.887 0.054</cell><cell>0.869 0.983 0.799 0.915 0.936 0.013</cell><cell>0.873 0.991 0.800 0.923 0.944 0.012</cell></row><row><cell>CVC-612-T</cell><cell>maxDice? maxSpe? maxIoU? S? ? E ? ? M ?</cell><cell>0.729 0.971 0.635 0.810 0.836 0.058</cell><cell>0.740 0.975 0.635 0.800 0.817 0.059</cell><cell>0.617 0.950 0.514 0.727 0.758 0.084</cell><cell>0.782 0.975 0.700 0.838 0.864 0.053</cell><cell>0.852 0.986 0.786 0.886 0.904 0.038</cell><cell>0.860 0.992 0.795 0.903 0.903 0.038</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation studies. See ? 3.3 for more details. Soft Norm R maxDice? maxIoU? S? ? E ? ? maxDice? maxIoU? S? ? E ? ?</figDesc><table><row><cell></cell><cell>Variants</cell><cell></cell><cell></cell><cell cols="2">CVC-300-TV</cell><cell></cell><cell cols="2">CVC-612-T</cell></row><row><cell cols="2">No. Base N #1</cell><cell></cell><cell>0.778</cell><cell>0.665</cell><cell>0.850 0.858</cell><cell>0.850</cell><cell>0.778</cell><cell>0.896 0.885</cell></row><row><cell>#2</cell><cell>1</cell><cell>1</cell><cell>0.755</cell><cell>0.650</cell><cell>0.865 0.844</cell><cell>0.850</cell><cell>0.779</cell><cell>0.896 0.891</cell></row><row><cell>#3</cell><cell>2</cell><cell>1</cell><cell>0.790</cell><cell>0.679</cell><cell>0.876 0.872</cell><cell>0.825</cell><cell>0.746</cell><cell>0.870 0.856</cell></row><row><cell>#4</cell><cell>4</cell><cell>1</cell><cell>0.809</cell><cell>0.709</cell><cell>0.893 0.884</cell><cell>0.834</cell><cell>0.760</cell><cell>0.881 0.867</cell></row><row><cell>#5</cell><cell>8</cell><cell>1</cell><cell>0.763</cell><cell>0.663</cell><cell>0.867 0.842</cell><cell>0.787</cell><cell>0.702</cell><cell>0.841 0.829</cell></row><row><cell>#6</cell><cell>4</cell><cell>1</cell><cell>0.829</cell><cell>0.729</cell><cell>0.896 0.903</cell><cell>0.852</cell><cell>0.784</cell><cell>0.895 0.897</cell></row><row><cell>#7</cell><cell>4</cell><cell>1</cell><cell>0.827</cell><cell>0.732</cell><cell>0.897 0.898</cell><cell>0.856</cell><cell>0.792</cell><cell>0.898 0.896</cell></row><row><cell>#8</cell><cell>4</cell><cell>2</cell><cell>0.840</cell><cell cols="2">0.745 0.909 0.921</cell><cell>0.860</cell><cell cols="2">0.795 0.903 0.903</cell></row><row><cell>#9</cell><cell>4</cell><cell>3</cell><cell>0.737</cell><cell>0.609</cell><cell>0.793 0.751</cell><cell>0.732</cell><cell>0.613</cell><cell>0.776 0.728</cell></row><row><cell cols="9">a large margin (max Dice: ?10%). On CVC-612-V and CVC-612-T, our PNS-</cell></row><row><cell cols="6">Net consistently outperforms other SOTAs.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Polyp segmentation in colonoscopy images using fully convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Akbari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohrekesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nasr-Esfahani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Soroushmehr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Najarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE EMBC</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="69" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer normalization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Wm-dova maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from physicians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>S?nchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fern?ndez-Esparrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rodr?guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Vilari?o</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CMIG</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="99" to="111" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards automatic polyp detection with a polyp appearance model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>S?nchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Vilarino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PR</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3166" to="3182" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fully convolutional neural networks for polyp segmentation in colonoscopy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Brandao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mazomenos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ciuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cali?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Menciassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dario</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Koulaouzidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arezzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: MICAD</title>
		<imprint>
			<biblScope unit="volume">10134</biblScope>
			<biblScope unit="page">101340</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Structure-measure: A new way to evaluate foreground maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4548" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Cognitive vision inspired object segmentation metric and loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>SSI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pranet: Parallel reverse attention network for polyp segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MICCAI. pp</title>
		<imprint>
			<biblScope unit="page" from="263" to="273" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Selective feature aggregation network with area-boundary constraints for polyp segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Y</forename><surname>Tong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="302" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pyramid constrained self-attention network for fast video salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: AAAI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="10869" to="10876" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Normalized and geometry-aware self-attention network for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10327" to="10336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Kvasir-seg: A segmented polyp dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Smedsrud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>De Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Johansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MMM</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="451" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Resunet++: An advanced architecture for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Smedsrud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>De Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Johansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ISM</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="225" to="2255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="109" to="117" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Receptive field block net for accurate and fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="385" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automated polyp detection in colon capsule endoscopy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Mamonov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">N</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H R</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1488" to="1502" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Psi-Net: Shape and boundary aware joint multi-task deep network for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Murugesan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sarveswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Shankaranarayana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sivaprakasam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE EMBC</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7223" to="7226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Endoscopic polyp segmentation using a hybrid 2d/3d cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G B</forename><surname>Puyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Brandao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">F</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Toth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lovat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mountney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="295" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automated polyp detection in colonoscopy videos using shape and context information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Gurudu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="630" to="644" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning texture transformer network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5791" to="5800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Integrating online and offline threedimensional deep learning for automated polyp detection in colonoscopy videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE JBHI</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="75" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Adaptive context selection for polyp segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="253" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Polyp detection during colonoscopy using a regression-based convolutional neural network with a tracker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PR</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="209" to="219" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Polypseg: An efficient contextaware network for polyp segmentation from colonoscopy videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="285" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unet++: A nested u-net architecture for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M R</forename><surname>Siddiquee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMI</title>
		<imprint>
			<biblScope unit="page" from="3" to="11" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
