<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Supervised Monocular Depth Estimation with Internal Feature Fusion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhou</surname></persName>
							<email>hang.zhou@uea.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing Sciences</orgName>
								<orgName type="institution">University of East Anglia Norwich</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Greenwood</surname></persName>
							<email>david.greenwood@uea.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing Sciences</orgName>
								<orgName type="institution">University of East Anglia Norwich</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Taylor</surname></persName>
							<email>s.l.taylor@uea.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing Sciences</orgName>
								<orgName type="institution">University of East Anglia Norwich</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Supervised Monocular Depth Estimation with Internal Feature Fusion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>ZHOU AT EL: DIFFNET FOR SELF-SUPERVISED MONOCULAR DEPTH ESTIMATION 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-supervised learning for depth estimation uses geometry in image sequences for supervision and shows promising results. Like many computer vision tasks, depth network performance is determined by the capability to learn accurate spatial and semantic representations from images. Therefore, it is natural to exploit semantic segmentation networks for depth estimation. In this work, based on a well-developed semantic segmentation network HRNet, we propose a novel depth estimation network DIFFNet, which can make use of semantic information in down and up sampling procedures. By applying feature fusion and an attention mechanism, our proposed method outperforms the state-of-the-art monocular depth estimation methods on the KITTI benchmark. Our method also demonstrates greater potential on higher resolution training data. We propose an additional extended evaluation strategy by establishing a test set of challenging cases, empirically derived from the standard benchmark. The code and trained models are available at https://github.com/brandleyzhou/DIFFNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Monocular depth estimation methods predict the depth of a scene from a single image. As an upstream task for scene understanding, it has a wide range of practical applications including autonomous vehicles, robotics and 3D reconstruction. While specialist hardware such as LiDAR or RGB-D cameras can be employed in such applications, deriving 3D geometry from a monocular RGB camera remains compelling. Supervised depth estimation methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b36">37]</ref> can produce dense depth maps but require large amounts of labelled data, which can be costly and time consuming even with the help of depth sensors. Self-supervised methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref> only rely on the scene's geometry in sequential images, and can take advantage of large scale unlabelled training resources to gain an advantage over supervised approaches.</p><p>As a result of using Structure from Motion (SfM) to construct the supervisory signal, most self-supervised methods suffer from those pixels which violate the assumptions of SfM, for example low-texture, occlusion and moving objects. To alleviate this intrinsic problem, prior works seek further auxiliary constraints such as optical flow <ref type="bibr" target="#b0">[1]</ref> and semantics <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b39">40]</ref> to collaborate with geometry information. In contrast with optical flow, semantic segmentation has a closer relationship to depth estimation. Intuitively, when human vision systems estimate scene depth, extracting semantic information is a critical part of this procedure; objects belonging to different categories have corresponding cues in depth perception. Visually, outputs from semantic networks and depth networks both need accurate object boundaries, which mainly determine the performance of a model <ref type="bibr" target="#b24">[25]</ref>.</p><p>With the goal of improving depth estimation by semantic segmentation, most related works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref> require a separate well-trained semantic network either to guide representation learning in depth networks or generate masks to filter those pixels belonging to non-rigid objects. When training a self-supervised depth network with an extra supervised semantic network that requires ground truth labels, the most attractive advantage of self-supervised learning disappears, and other problems are introduced such as domain gap.</p><p>When we look inside depth networks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b38">39]</ref>, we find that all of them are based on an encoder-decoder architecture <ref type="bibr" target="#b23">[24]</ref>, which uses skip connections to restore semantic and spatial information. We propose a new representation learning network, DIFFNet, to explicitly utilize built-in semantic information effectively, based on a well-developed semantic network <ref type="bibr" target="#b32">[33]</ref>. Our contributions are: <ref type="bibr" target="#b0">(1)</ref> We apply a novel internal feature fusion mechanism to a semantic network for depth estimation, to bridge the semantic gap between encoder and decoder feature maps. <ref type="bibr" target="#b1">(2)</ref> We propose an effective attention module in the decoder to process skip connections. (3) Our proposed method advances the state-of-the-art on the KITTI benchmark and outperforms other methods on a customised benchmark. <ref type="bibr" target="#b3">(4)</ref> We propose an extended evaluation strategy where methods can be further tested using difficult cases in the benchmark data, formed in a self-established manner.</p><p>2 Related Work 2.1 Self-supervised Monocular Depth Estimation Inferring depth from a single image is an ill-posed problem as 3D points from multiple depth planes can be projected onto the same 2D pixel of an image. Inspired by a classic computer vision algorithm SfM, the seminal work of <ref type="bibr" target="#b38">[39]</ref> proposed a fundamental framework consisting of a depth network and a pose network which are trained simultaneously with sequential video frames. Many works <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38]</ref> have further developed this idea in terms of the objective functions or model architectures. Monocular depth estimation is now one of the most successful applications of self-supervised learning, and even outperforms supervised methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Semantic Information and Depth Estimation</head><p>Semantic information has been introduced as an additional source for improving depth estimation. Prior works can be divided into two categories. The first uses a separate semantic segmentation model to either add constraints to a photometric loss or to distinguish pixels belonging to categories which violate the static-world assumption (e.g. pedestrians, moving vehicles). A schema to deal with moving dynamic-class objects to avoid contamination to the photometric loss is found in <ref type="bibr" target="#b19">[20]</ref>. Motivated by the observation that semantic segmentation networks trained with limited ground truth can generate more defined object borders than that of depth estimation, Zhu et al. <ref type="bibr" target="#b39">[40]</ref> proposed a measurement of border consistency between segmentation and depth, and minimized it to push a depth network towards more accurate edges. The second category of models to exploit semantic information contain those that use it for representation learning, rather than in the photometric loss. Chen et al. <ref type="bibr" target="#b1">[2]</ref> measured the content consistency between depth and semantic maps to propose an additional supervisory signal which guides networks to learn semantic-rich features. Several prior works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21]</ref> used a pretrained semantic network to guide the feature extraction of a depth network. Generally, most methods in this direction require an extra semantic network trained with labeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Representation Learning for Monocular Depth Estimation</head><p>For extracting features from the input images <ref type="bibr" target="#b38">[39]</ref> proposed DispNet which was based on U-Net <ref type="bibr" target="#b29">[30]</ref>, a typical encoder-decoder architecture. Monodepth2 <ref type="bibr" target="#b10">[11]</ref> proposed a feature encoder based on ResNet <ref type="bibr" target="#b14">[15]</ref> which has since become the standard approach. To increase the robustness of the photometric loss, <ref type="bibr" target="#b30">[31]</ref> used an external network to transform a reference frame and target frames into another domain in which there are better alternative representations for texture-less regions. Guizilini et al. <ref type="bibr" target="#b12">[13]</ref> introduced 3D convolutions to construct packing and unpacking blocks, which are the replacement of standard downsample and upsample operations and preserve more details in feature maps than those of 2D convolutions.</p><p>To bridge the semantic gap between the encoder and decoder in the depth network, <ref type="bibr" target="#b24">[25]</ref> redesigned the skip connections in a U-Net architecture by fusing features at different scales. Kendall et al. <ref type="bibr" target="#b17">[18]</ref> proposed a multi-task training framework in which geometry and semantic representations are learned with a shared encoder. Under this schema, their models for depth estimation, semantic segmentation and instance segmentation all outperform the competitors which were trained individually on each task. Inspired by these ideas, we investigate a network architecture which has two key attributes: an architecture suitable for semantic segmentation and depth estimation and an internal mechanism which evolves multiple chances for feature fusion. Therefore, we choose HRNet <ref type="bibr" target="#b32">[33]</ref> as our new encoder blueprint. HRNet is able to learn high-resolution representations from images that are both semantically and spatially descriptive, and has been successfully applied to human pose estimation, semantic segmentation and object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Self-supervised Monocular Depth Estimation Framework</head><p>Our general framework is based on the SfM paradigm that is followed by all other selfsupervised monocular depth estimation approaches. It requires a depth model ? depth and a pose model ? pose trained simultaneously with a triplet of sequential RGB frames I t ? R H?W ?3 ,t ? {?1, 0, 1}. At training time ? depth takes a target frame I 0 as input and predicts a depth map d = ? depth (I 0 ), while a relative pose change between the target frame and a source frame is estimated, T 0?t = ? pose (I 0 , I t ),t ? {?1, 1}. Based on the assumption that the world is static and the view change is only caused by a moving camera, a synthesized counterpart to target frame I 0 can be generated using only pixels from the source frames I t ,t ? {?1, 1}:</p><formula xml:id="formula_0">I t ?0 = I t [pro j(repro j(I 0 , d, T 0?t ), K)]<label>(1)</label></formula><p>where K are known camera intrinsics, [] is the sampling operator, repro j returns a 3D point cloud of camera t , and pro j outputs the 2D coordinates when projecting the point cloud onto I t . Using the predicted depth map d, the generated view I t ?0 and the corresponding target frame I 0 , we build a supervisory signal consisting of two items: Photometric Loss, p , is an appearance matching loss which calculates the difference between I 0 and I t ?0 . Following <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, the similarity between a synthesized frame and a target frame is computed using a Structural Similarity term (SSIM) <ref type="bibr" target="#b33">[34]</ref>. Then combining with the L1 norm, the final photometric loss function is defined:</p><formula xml:id="formula_1">p (I 0 , I t ?0 ) = ? 1 ? SSIM(I 0 , I t ?0 ) 2 + (1 ? ?)|I 0 ? I t ?0 |<label>(2)</label></formula><p>Edge-aware Smoothness <ref type="bibr" target="#b9">[10]</ref>, s , regularizes the depth in low gradient regions:</p><formula xml:id="formula_2">s (d) = | ?d ? x |e ?| ?I 0 ? x | + | ?d ? y |e ?| ?I 0 ? y |<label>(3)</label></formula><p>We also employ the minimum photometric error, auto-masking and multi-scale depth loss techniques which were introduced in <ref type="bibr" target="#b10">[11]</ref>. The final self-supervised loss function is defined:</p><formula xml:id="formula_3">f inal = min( p (I 0 , I t ?0 )) + ? s (d),t ? {?1, 1}<label>(4)</label></formula><p>Where ? is a weighting coefficient between the photometric loss p and depth smoothness s . The objective loss is averaged per pixel, pyramid scale and image batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DIFFNet</head><p>DIFFNet introduces a novel depth network which combines multiple resolution feature fusion and a spatial attention mechanism. In this section we provide details on our proposed network, which is built on an encoder-decoder architecture and is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">High-Resolution Depth Encoder</head><p>Low level but high resolution features are spatially precise, and, conversely, high level but low resolution features are not spatially precise but are semantically rich. Many existing depth estimation approaches <ref type="bibr" target="#b10">[11]</ref> are built on ResNet which encodes the input image as a low-resolution feature map. Instead, we investigate an effective architecture that is capable of fusing semantically-rich and spatially-precise features. High-Resolution Network (HRNet) <ref type="bibr" target="#b32">[33]</ref> maintains high resolution representations by the feature extraction process, with two key design characteristics: multiple streams with every feature map in the stream having the same resolution, and multiple stages having different resolution exchanging information in each stage. HRNet is illustrated in <ref type="figure">Figure 3</ref>(a) showing each stage as a red box and each stream as a row. Let x e r,s denote the feature map from an HRNet encoder node located in the rth sub-stream and at the sth stage. The resolution of sub-stream r is 1 2 r?1 of the resolution of the first stream. As r increments, the number of channels in the feature maps doubles.</p><p>When we use an HRNet as the encoder for our depth network, we observe significant improvements over other approaches that use ResNet as the encoder. An HRNet has four streams and four stages, and outputs five feature maps at different scales from the final stage,  <ref type="figure" target="#fig_0">Figure 1</ref>: An overview of the DIFFNet depth network. The encoder uses feature fusion to generate stacks of multi-stage feature maps. The decoder uses an attention module and a 3 ? 3 convolution layer to restore compressed feature maps at different scales.</p><p>x e 0,0 and x e r,4 , r = 1, 2, 3, 4. Information from features in previous stages is ignored. We augment this module with internal feature fusion to further exploit the potential of the HRNet architecture: Multi-stage Internal Feature Fusion Based on the relationship between feature resolution and spatial information, we assume that feature maps with more channels contain more semantic information and vice versa. To get a semantically-rich intermediate feature map without changing the scale we could increase the number of convolution kernels. However, this would dramatically increase the computational complexity. For example, given a C in dimensional feature and a kernel with a size 3 ? 3 to output a C out dimensional feature, the number of trainable parameters is C in ? C out ? 3 ? 3. If we need double C out , the number of parameters also doubles. HRNet contains a multi-stage convolution strategy ( <ref type="figure">Figure 3a)</ref>, and so increasing the convolution kernels leads to a large increase in parameters. However, DIFFNet forces feature maps from different stages to contain different semantic information but fuses outputs from all intermediate stages using a concatenation strategy before decoding. Without additional parameters, this strategy is capable of extracting richer feature maps -see column four in <ref type="figure">Figure 2</ref>, which shows a smaller semantic gap between DIFFNet encoded features and decoded outputs.</p><p>The stack of feature maps for stream r is computed as:</p><formula xml:id="formula_4">x e r = [x e r,s ], s = r, ? ? ? , 4<label>(5)</label></formula><p>where [?] is the concatenation layer. The modified architecture is illustrated in <ref type="figure">Figure 3b</ref> in which the red arrows denote a concatenation of feature maps. The advantages of giving low level feature maps more semantic information (stacking multi-stage features) is explored in Section 5.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Attention-based Depth Decoder</head><p>Our decoder is based on a U-Net architecture with further inspiration taken from <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b35">36]</ref>. Specifically, we introduce an attention mechanism to process the skip-connections from the encoder. An illustration of the decoder can be seen in <ref type="figure" target="#fig_0">Figure 1</ref> with an outline of each decoder node, D i , shown bottom right. Let x d i denote the output of decoder node D i ,         calculated as:</p><formula xml:id="formula_5">i 0 + V F w Q O U = " &gt; A A A B 8 H i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C R b B g 5 T d K u q x 6 M V j B f s h 7 V q y a b Y N T b J L k h X L 0 l / h x Y M i X v 0 5 3 v w 3 p u 0 e t P X B w O O 9 G W b m B T F n 2 r j u t 5 N b W l 5 Z X c u v F z Y 2 t 7 Z 3 i r t 7 D R 0 l i t A 6 i X i k W g H W l D N J 6 4 Y Z T l u x o l g E n D a D 4 f X E b z 5 S p V k k 7 8 w o p r 7 A f c l C R r C x 0 v 3 T A + 2 m 3 k l l 3 C 2 W 3 L I 7 B V o k X k Z K k K H W L X 5 1 e h F J B J W G c K x 1 2 3 N j 4 6 d Y G U Y 4 H R c 6 i a Y x</formula><formula xml:id="formula_6">P G w Y G 2 g 7 l H S q + S 0 H O r b v X q J B E = " &gt; A A A B 8 H i c b V B N S w M x E M 3 W r 1 q / q h 6 9 B I v g Q c p u F f V Y 9 O K x g v 2 Q d i 3 Z d L Y N T b J L k h X L 0 l / h x Y M i X v 0 5 3 v w 3 p u 0 e t P X B w O O 9 G W b m B T F n 2 r j u t 5 N b W l 5 Z X c u v F z Y 2 t 7 Z 3 i r t 7 D R 0 l i k K d R j x S r Y B o 4 E x C 3 T D D o R U r I C L g 0 A y G 1 x O / + Q h K s 0 j e m V E M v i B 9 y U J G i b H S / d M D d N P K S W X c L Z b c s j s F X i R e R k o</formula><formula xml:id="formula_7">X I w x g D o = " &gt; A A A B 8 H i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C R b B g 5 R d K + q x 6 M V j B f s h 7 V q y a b Y N T b J L k h X L 0 l / h x Y M i X v 0 5 3 v w 3 p u 0 e t P X B w O O 9 G W b m B T F n 2 r j u t 5 N b W l 5 Z X c u v F z Y 2 t 7 Z 3 i r t 7 D R 0 l i t A 6 i X i k W g H W l D N J 6 4 Y Z T l u x o l g E n D a D 4 f X E b z 5 S p V k k 7 8 w o p r 7 A f c l C R r C x 0 v 3 T A + 2 m 3 k l l 3 C 2 W 3 L I 7 B V o k X k Z K k K H W L X 5 1 e h F J B J W G c K x 1 2 3 N j 4 6 d Y G U Y 4 H R c 6 i a Y x</formula><formula xml:id="formula_8">V R k g = " &gt; A A A B 8 H i c b V B N S w M x E M 3 W r 1 q / q h 6 9 B I v g Q c q u F f V Y 9 O K x g v 2 Q d i 3 Z d L Y N T b J L k h X L 0 l / h x Y M i X v 0 5 3 v w 3 p u 0 e t P X B w O O 9 G W b m B T F n 2 r j u t 5 N b W l 5 Z X c u v F z Y 2 t 7 Z 3 i r t 7 D R 0 l i k K d R j x S r Y B o 4 E x C 3 T D D o R U r I C L g 0 A y G 1 x O / + Q h K s 0 j e m V E M v i B 9 y U J G i b H S / d M D d N P K S W X c L Z b c s j s F X i R e R k o</formula><formula xml:id="formula_9">U = " &gt; A A A B 8 H i c b V B N S w M x E M 3 W r 1 q / q h 6 9 B I v g Q c q u F v V Y 9 O K x g v 2 Q d i 3 Z d L Y N T b J L k h X L 0 l / h x Y M i X v 0 5 3 v w 3 p u 0 e t P X B w O O 9 G W b m B T F n 2 r j u t 5 N b W l 5 Z X c u v F z Y 2 t 7 Z 3 i r t 7 D R 0 l i k K d R j x S r Y B o 4 E x C 3 T D D o R U r I C L g 0 A y G 1 x O / + Q h K s 0 j e m V E M v i B 9 y U J G i b H S / d M D d F P v p D L u F k t u 2 Z 0 C L x I v I y W U o d Y t f n V 6 E U 0 E S E M 5 0 b r t u b H x U 6 I M o x z G h U 6 i I S Z 0 S P r Q t l Q S A d</formula><formula xml:id="formula_10">= " &gt; A A A B 8 H i c b V B N S w M x E M 3 W r 1 q / q h 6 9 B I v g Q c q u F v V Y 9 O K x g v 2 Q d i 3 Z d L Y N T b J L k h X L 0 l / h x Y M i X v 0 5 3 v w 3 p u 0 e t P X B w O O 9 G W b m B T F n 2 r j u t 5 N b W l 5 Z X c u v F z Y 2 t 7 Z 3 i r t 7 D R 0 l i k K d R j x S r Y B o 4 E x C 3 T D D o R U r I C L g 0 A y G 1 x O / + Q h K s 0 j e m V E M v i B 9 y U J G i b H S / d M D d N P K S W X c L Z b c s j s F X i R e R k o</formula><formula xml:id="formula_11">i 0 + V F w Q O U = " &gt; A A A B 8 H i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C R b B g 5 T d K u q x 6 M V j B f s h 7 V q y a b Y N T b J L k h X L 0 l / h x Y M i X v 0 5 3 v w 3 p u 0 e t P X B w O O 9 G W b m B T F n 2 r j u t 5 N b W l 5 Z X c u v F z Y 2 t 7 Z 3 i r t 7 D R 0 l i t A 6 i X i k W g H W l D N J 6 4 Y Z T l u x o l g E n D a D 4 f X E b z 5 S p V k k 7 8 w o p r 7 A f c l C R r C x 0 v 3 T A + 2 m 3 k l l 3 C 2 W 3 L I 7 B V o k X k Z K k K H W L X 5 1 e h F J B J W G c K x 1 2 3 N j 4 6 d Y G U Y 4 H R c 6 i a Y x</formula><formula xml:id="formula_12">P G w Y G 2 g 7 l H S q + S 0 H O r b v X q J B E = " &gt; A A A B 8 H i c b V B N S w M x E M 3 W r 1 q / q h 6 9 B I v g Q c p u F f V Y 9 O K x g v 2 Q d i 3 Z d L Y N T b J L k h X L 0 l / h x Y M i X v 0 5 3 v w 3 p u 0 e t P X B w O O 9 G W b m B T F n 2 r j u t 5 N b W l 5 Z X c u v F z Y 2 t 7 Z 3 i r t 7 D R 0 l i k K d R j x S r Y B o 4 E x C 3 T D D o R U r I C L g 0 A y G 1 x O / + Q h K</formula><formula xml:id="formula_13">? ? ? x d 4 = D(? ([?(x e 4 ), x e 3 ])), x d i = D(? ([?(x d i+1 ), x e i?1 ])), i = 1, 2, 3 x d 0 = D(? (?(x d 1 )))<label>(6)</label></formula><p>where ?(?) is an upsampling operator, ? (?) is an attention module, [?] is concatenation layer and D(?) is a 3 ? 3 convolution layer. Attention Module. We explore three strategies for incorporating attention into the decoder: channel-wise attention, spatial attention and channel-spatial attention. Given a feature map F ? R C?H?W , the attention aggregated maps F c,s,cs ? R C?H?W are computed as:</p><formula xml:id="formula_14">F c = M c (F) F, F s = M s (F) F, F cs = M s (F c ) F c .<label>(7)</label></formula><p>where M c (?) and M s (?) are attention map generators which output a 1D channel attention map m c ? R C?1?1 and a 2D spatial attention map m s ? R 1?H?W respectively, and denotes element-wise multiplication. During multiplication, the attention values are copied accordingly with channel attention values being broadcast along the spatial dimension, and vice versa (see <ref type="bibr" target="#b35">[36]</ref> for details). We compare these three attention strategies in Section 5.4 and identify that channel-wise attention gives the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we validate that our proposed network can output semantically-rich and spatially-precise depth maps, and our contributions improve the representation learning ability of HRNet while outperforming other published methods on the KITTI benchmark <ref type="bibr" target="#b8">[9]</ref>. Furthermore, we analyse the characteristics of the more challenging scenes from the test partition of the KITTI dataset, and publish identifying information for the high error images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset</head><p>KITTI <ref type="bibr" target="#b8">[9]</ref> is a dataset that contains stereo images and corresponding 3D laser scans of outdoor scenes captured by imaging equipment mounted on a moving vehicle <ref type="bibr" target="#b18">[19]</ref>. The RGB images have a resolution of ? 1241 ? 376 and the corresponding depth maps are sparse with a large amount of missing data. For training, we adopt the dataset split proposed by <ref type="bibr" target="#b5">[6]</ref>. After removing the static frames by a pre-processing step suggested by <ref type="bibr" target="#b38">[39]</ref>, this results in 39,810 monocular frame triplets for training and 4,424 frame triplets for validation. To simplify the training process, the camera intrinsic matrices are assumed identical for all the frames in different scenes. To obtain this "universal" intrinsic matrix, we offset the principal point of the camera to the image centre and reset the focal length as the average of all the focal lengths in KITTI. This assumption is only valid when the capturing cameras are similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Implementation Details</head><p>Our models are trained and tested on a single NVidia RTX 6000 GPU using Pytorch <ref type="bibr" target="#b26">[27]</ref>. A depth network and a pose network are trained for 20 epochs using the Adam optimizer <ref type="bibr" target="#b18">[19]</ref> with the default betas 0.9 and 0.999. They were trained with a batch size of 16 and an input and output resolution of 640 ? 192. We set the initial learning rate as 10 ?4 for the first 14 epochs and then 10 ?5 for fine-tuning the remainder. In the objective function f inal <ref type="figure" target="#fig_10">(Equation 4</ref>), we let the SSIM weight ? = 0.85 and the edge-aware smoothness weight ? = 1 ? 10 ?3 .</p><p>Depth Network. We implement our proposed DIFFNet as described in Section 4 as our backbone. We use HRNet pre-trained only on ImageNet <ref type="bibr" target="#b4">[5]</ref> to initialize DIFFNet (the effect of pre-training is shown in <ref type="table" target="#tab_1">Table 2</ref>). At training, losses from four scaled depth maps are averaged. When testing, only the maximum resolution depth map is output by the model.</p><p>Pose Network. We implement the architecture proposed in <ref type="bibr" target="#b10">[11]</ref> for pose estimation, which is built on ResNet-18. The pose network takes the two adjacent frames as input and outputs the relative pose which is parameterized with a 6-DOF vector. We experimented with replacing the pose encoder with HRNet, but did not achieve the same performance gains that we observe with the depth network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation on KITTI</head><p>Using metrics described in <ref type="bibr" target="#b5">[6]</ref>, we evaluate the performance of DIFFNet on KITTI. The quantitative results are summarized in <ref type="table" target="#tab_0">Table 1</ref>. Our method outperforms state-of-the-art approaches in terms of Absolute Relative Error and RMSE. When trained on the stereo examples in KITTI, our method achieves best results on all metrics. Given a higher image resolution of 1024 ? 320, the accuracy of DIFFNet further increases while continuing to outperform competing methods (see in supplementary material for more details). In <ref type="figure" target="#fig_10">Figure 4</ref> we illustrate the qualitative performance of DIFFNet against PackNet <ref type="bibr" target="#b12">[13]</ref>, HR-depth <ref type="bibr" target="#b24">[25]</ref> and Monodepth2 <ref type="bibr" target="#b10">[11]</ref>. DIFFNet outperforms all self-supervised approaches and even those which use semantic labels as an external supervision resource. We draw attention to the second row that shows our method, where we have used a dashed outline to illustrate the benefits of our semantic backbone when compared with other methods. We achieve greater detail in a number of roadside items, while holding the advantage of fewer trainable parameters than the other techniques (see <ref type="table" target="#tab_2">Table 3</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Study</head><p>To validate the performance improvements that our contributions provide, we conduct an ablative analysis. We establish a baseline by replacing the original ResNet-based depth encoder in Monodepth2 <ref type="bibr" target="#b10">[11]</ref> with HRNet-18. <ref type="table" target="#tab_1">Table 2</ref> shows the results of the analysis, with the progressive addition of pre-training the encoder on ImageNet, multi-stage fusion (MF), channel-wise attention (CA) and space-wise attention (SA). The largest performance gain is achieved by pre-training the encoder rather than training from scratch. We observe that channel-wise attention yields increased accuracy compared with spatial attention. Furthermore, feature fusion improves baseline performance for all attention configurations with the exception of channel-spatial. A qualitative comparison of DIFFNet and the baseline model is shown in <ref type="figure">Figure 5</ref>. <ref type="table" target="#tab_0">Table 1</ref> reveals the relative performance gap between contemporary methods on KITTI is diminishing. From empirical testing, we observe that the 10 images that give the highest error from each of these methods represents ? 1.4% of the KITTI test set, but contributes &gt; 3% of error when evaluating. Hence, error is not uniformly distributed throughout the test set, but certain images are more challenging than others. A model's performance on its own top 10 hard cases is a key factor in measuring its robustness and stability. For a The second row shows the result from DIFFNet, and the remaining rows are from other contemporary methods. Note the improvement in detail for many roadside items, that our semantic backbone provides. Hotter colours indicate closer objects. fair comparison, we propose that the difficult cases from competing methods form a single challenge set. It is our hope that future authors will accept this strategy when they evaluate their models and compare against others. In our case, we create a challenging test set that is the union of the 10 images with highest error from the four approaches shown in <ref type="table" target="#tab_2">Table 3</ref>, including a baseline method discussed in Section 5.4. The union set comprises 23 images 1 , and 3 images are common to all sets. In <ref type="table" target="#tab_2">Table 3</ref> it is clear that our method performs competitively under this most difficult test, resulting in the lowest Absolute Relative Error. We can hypothesise these are the most challenging images due to the large regions of foliage in combination with difficult lighting.  <ref type="figure">Figure 5</ref>: Visualisation of the ablation study. Row one shows that with more semantic information fed into depth decoder, the predicted depth map will more precise. Row two shows that DIFFNet produces a depth map with fewer artefacts than the baseline method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Extended Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we have proposed DIFFNet for self-supervised monocular depth estimation. Based on HRNet, which is designed for other computer vision tasks, we adopt it and improve it with two simple but effective strategies. Specifically, we incorporate multiple resolution feature fusion and a channel attention mechanism. With fewer parameters to learn, DIFFNet outperforms other state-of-the-art self-supervised methods, especially when high resolution input is available. We have shown that the DIFFNet encoder computes semantically rich feature maps, and our ablation study demonstrates the performance gain from each proposed modification. Finally, we introduced a creative strategy for evaluating models by investigating difficult test cases, and we invite authors to adopt the same approach going forward.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 &lt;</head><label>1</label><figDesc>l a t e x i t s h a 1 _ b a s e 6 4 = " + R v Y 4 J S 9 x 8 K 3 i a Q G 6 Z W g Y E 8 d y K 4 = " &gt; A A A B 8 H i c b V D L S g N B E O z 1 G e M r 6 t H L Y h C 8 G H Y l o s e g F 4 8 R z E O S N c x O e p M h M 7 P L z K w Y l n y F F w + K e P V z v P k 3 T h 4 H T S x o K K q 6 6 e 4 K E 8 6 0 8 b x v Z 2 l 5 Z X V t P b e R 3 9 z a 3 t k t 7 O 3 X d Z w q i j U a 8 1 g 1 Q 6 K R M 4 k 1 w w z H Z q K Q i J B j I x x c j / 3 G I y r N Y n l n h g k G g v Q k i x g l x k r 3 T w / Y y d i p P + o U i l 7 J m 8 B d J P 6 M F G G G a q f w 1 e 7 G N B U o D e V E 6 5 b v J S b I i D K M c h z l 2 6 n G h N A B 6 W H L U k k E 6 i C b H D x y j 6 3 S d a N Y 2 Z L G n a i / J z I i t B 6 K 0 H Y K Y v p 6 3 h u L / 3 m t 1 E S X Q c Z k k h q U d L o o S r l r Y n f 8 v d t l C q n h Q 0 s I V c z e 6 t I + U Y Q a m 1 H e h u D P v 7 x I 6 m c l v 1 w 6 v y 0 X K 1 e z O H J w C E d w A j 5 c Q A V u o A o 1 o C D g G V 7 h z V H O i / P u f E x b l 5 z Z z A H 8 g f P 5 A 3 v E k D U = &lt; / l a t e x i t &gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :e 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 =e 2 &lt; l a t e x i t s h a 1 _e 3 &lt; l a t e x i t s h a 1 _ b a s e 6 4 =e 4 &lt; l a t e x i t s h a 1 _e 1 , 1 &lt;</head><label>2114213144111</label><figDesc>Visualisation of intermediate feature maps. We show four intermediate feature maps from stream r = 1 and stages s = 1, 2, 3, 4 in the HRNet [33] (top) and DIFFNet (bottom) encoders. The final column shows the RGB input and DIFFNet predicted depth map. &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " R z 2 q 4 W / U C d c l W V d 8 0 l G F e h 3 Z 5 Q A = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o m K e i x 6 8 V j B t I U 2 l s 1 2 0 i 7 d b M L u R i y l v 8 G L B 0 W 8 + o O 8 + W / c t j l o 6 4 O B x 3 s z z M w L U 8 G 1 c d 1 v Z 2 l 5 Z X V t v b B R 3 N z a 3 t k t 7 e 3 X d Z I p h j 5 L R K K a I d U o u E T f c C O w m S q k c S i w E Q 5 u J n 7 j E Z X m i b w 3 w x S D m P Y k j z i j x k r + 0 w N 2 v E 6 p 7 F b c K c g i 8 X J S h h y 1 T u m r 3 U 1 Y F q M 0 T F C t W 5 6 b m m B E l e F M 4 L j Y z j S m l A 1 o D 1 u W S h q j D k b T Y 8 f k 2 C p d E i X K l j R k q v 6 e G N F Y 6 2 E c 2 s 6 Y m r 6 e 9 y b i f 1 4 r M 9 F V M O I y z Q x K N l s U Z Y K Y h E w + J 1 2 u k B k x t I Q y x e 2 t h P W p o s z Y f I o 2 B G / + 5 U V S P 6 1 4 F 5 W z u / N y 9 T q P o w C H c A Q n 4 M E l V O E W a u A D A w 7 P 8 A p v j n R e n H f n Y 9 a 6 5 O Q z B / A H z u c P g o 2 O f w = = &lt; / l a t e x i t &gt;x " f H K G s A b I S R d z 7 V M T h c h 8 W v F G A p A = " &gt; A A A B 7 H i c b V B N T 8 J A E J 3 i F + I X 6 t F L I z H x R F o 0 6 p H o x S M m F k i g k u 0 y h Q 3 b b b O 7 N Z K G 3 + D F g 8 Z 4 9 Q d 5 8 9 + 4 Q A 8 K v m S S l / d m M j M v S D h T 2 n G + r c L K 6 t r 6 R n G z t L W 9 s 7 t X 3 j 9 o q j i V F D 0 a 8 1 i 2 A 6 K Q M 4 G e Z p p j O 5 F I o o B j K x j d T P 3 W I 0 r F Y n G v x w n 6 E R k I F j J K t J G 8 p w f s 1 X r l i l N 1 Z r C X i Z u T C u R o 9 M p f 3 X 5 M 0 w i F p p w o 1 X G d R P s Z k Z p R j p N S N 1 W Y E D o i A + w Y K k i E y s 9 m x 0 7 s E 6 P 0 7 T C W p o S 2 Z + r v i Y x E S o 2 j w H R G R A / V o j c V / / M 6 q Q 6 v / I y J J N U o 6 H x R m H J b x / b 0 c 7 v P J F L N x 4 Y Q K p m 5 1 a Z D I g n V J p + S C c F d f H m Z N G t V 9 6 J 6 d n d e q V / n c R T h C I 7 h F F y 4 h D r c Q g M 8 o M D g G V 7 h z R L W i / V u f c x b C 1 Y + c w h / Y H 3 + A I Q R j o A = &lt; /l a t e x i t &gt; x b a s e 6 4 = " K r x K A k I T 2 a q 0 s k V P u G + 3 F G o R p r U = " &gt; A A A B 7 H i c b V B N T 8 J A E J 3 i F + I X 6 t F L I z H x R F o x 6 p H o x S M m F k i g k u 0 y h Q 3 b b b O 7 N Z K G 3 + D F g 8 Z 4 9 Q d 5 8 9 + 4 Q A 8 K v m S S l / d m M j M v S D h T 2 n G + r c L K 6 t r 6 R n G z t L W 9 s 7 t X 3 j 9 o q j i V F D 0 a 8 1 i 2 A 6 K Q M 4 G e Z p p j O 5 F I o o B j K x j d T P 3 W I 0 r F Y n G v x w n 6 E R k I F j J K t J G 8 p w f s 1 X r l i l N 1 Z r C X i Z u T C u R o 9 M p f 3 X 5 M 0 w i F p p w o 1 X G d R P s Z k Z p R j p N S N 1 W Y E D o i A + w Y K k i E y s 9 m x 0 7 s E 6 P 0 7 T C W p o S 2 Z + r v i Y x E S o 2 j w H R G R A / V o j c V / / M 6 q Q 6 v / I y J J N U o 6 H x R m H J b x / b 0 c 7 v P J F L N x 4 Y Q K p m 5 1 a Z D I g n V J p + S C c F d f H m Z N M + q 7 k W 1 d n d e q V / n c R T h C I 7 h F F y 4 h D r c Q g M 8 o M D g G V 7 h z R L W i / V u f c x b C 1 Y + c w h / Y H 3 + A I W V j o E = &lt; / l a t e x i t &gt; x " 8 z u M T E l D L D G M h z + 9 q Y V d w c / i V z c = " &gt; A A A B 7 H i c b V B N T 8 J A E J 3 i F + I X 6 t F L I z H x R F o l 6 p H o x S M m F k i g k u 0 y h Q 3 b b b O 7 N Z K G 3 + D F g 8 Z 4 9 Q d 5 8 9 + 4 Q A 8 K v m S S l / d m M j M v S D h T 2 n G + r c L K 6 t r 6 R n G z t L W 9 s 7 t X 3 j 9 o q j i V F D 0 a 8 1 i 2 A 6 K Q M 4 G e Z p p j O 5 F I o o B j K x j d T P 3 W I 0 r F Y n G v x w n 6 E R k I F j J K t J G 8 p w f s 1 X r l i l N 1 Z r C X i Z u T C u R o 9 M p f 3 X 5 M 0 w i F p p w o 1 X G d R P s Z k Z p R j p N S N 1 W Y E D o i A + w Y K k i E y s 9 m x 0 7 s E 6 P 0 7 T C W p o S 2 Z + r v i Y x E S o 2 j w H R G R A / V o j c V / / M 6 q Q 6 v / I y J J N U o 6 H x R m H J b x / b 0 c 7 v P J F L N x 4 Y Q K p m 5 1 a Z D I g n V J p + S C c F d f H m Z N M + q 7 k X 1 / K 5 W q V / n c R T h C I 7 h F F y 4 h D r c Q g M 8 o M D g G V 7 h z R L W i / V u f c x b C 1 Y + c w h / Y H 3 + A I c Z j o I = &lt; / l a t e x i t &gt; x b a s e 6 4 = " O n T I S p M g 0 L D n T x w O L 8 m w b X v 2 8 a k = " &gt; A A A B 8 H i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 S N h V U Y 9 B L x 4 j m I c k a 5 i d d J I h s 7 P L z K w Y l n y F F w + K e P V z v P k 3 T p I 9 a G J B Q 1 H V T X d X E A u u j e t + O w u L S 8 s r q 7 m 1 / P r G 5 t Z 2 Y W e 3 p q N E M a y y S E S q E V C N g k u s G m 4 E N m K F N A w E 1 o P B 9 d i v P 6 L S P J J 3 Z h i j H 9 K e 5 F 3 O q L H S / d M D t l P v 2 B u 1 C 0 W 3 5 E 5 A 5 o m X k S J k q L Q L X 6 1 O x J I Q p W G C a t 3 0 3 N j 4 K V W G M 4 G j f C v R G F M 2 o D 1 s W i p p i N p P J w e P y K F V O q Q b K V v S k I n 6 e y K l o d b D M L C d I T V 9 P e u N x f + 8 Z m K 6 l 3 7 K Z Z w Y l G y 6 q J s I Y i I y / p 5 0 u E J m x N A S y h S 3 t x L W p 4 o y Y z P K 2 x C 8 2 Z f n S e 2 k 5 J 2 X T m / P i u W r L I 4 c 7 M M B H I E H F 1 C G G 6 h A F R i E 8 A y v 8 O Y o 5 8 V 5 d z 6 m r Q t O N r M H f + B 8 / g A k u o / 8 &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " K B q 4 C E p D F 0 m N 5 n k F i u</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>e 1 , 2 &lt;</head><label>12</label><figDesc>J k P c p 2 1 L J R Z U + + n 0 4 D E 6 s k o P h Z G y J Q 2 a q r 8 n U i y 0 H o n A d g p s B n r e m 4 j / e e 3 E h J d + y m S c G C r J b F G Y c G Q i N P k e 9 Z i i x P C R J Z g o Z m 9 F Z I A V J s Z m V L A h e P M v L 5 J G p e y d l 0 9 v z 0 r V q y y O P B z A I R y D B x d Q h R u o Q R 0 I C H i G V 3 h z l P P i v D s f s 9 a c k 8 3 s w x 8 4 n z 8 m P 4 / 9 &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " f C</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>e 2 , 2 &lt;</head><label>22</label><figDesc>o Q 6 1 b / O r 0 I p o I k I Z y o n X b c 2 P j p 0 Q Z R j m M C 5 1 E Q 0 z o k P S h b a k k A r S f T g 8 e 4 y O r 9 H A Y K V v S 4 K n 6 e y I l Q u u R C G y n I G a g 5 7 2 J + J / X T k x 4 6 a d M x o k B S W e L w o R j E + H J 9 7 j H F F D D R 5 Y Q q p i 9 F d M B U Y Q a m 1 H B h u D N v 7 x I G p W y d 1 4 + v T 0 r V a + y O P L o A B 2 i Y + S h C 1 R F N 6 i G 6 o g i g Z 7 R K 3 p z l P P i v D s f s 9 a c k 8 3 s o z 9 w P n 8 A J 8 a P / g = = &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " m e e 9 T K R j O N N l z r B C 7 o b t</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>e 1 , 3 &lt;e 2 , 3 &lt;</head><label>1323</label><figDesc>J k P c p 2 1 L J R Z U + + n 0 4 D E 6 s k o P h Z G y J Q 2 a q r 8 n U i y 0 H o n A d g p s B n r e m 4 j / e e 3 E h J d + y m S c G C r J b F G Y c G Q i N P k e 9 Z i i x P C R J Z g o Z m 9 F Z I A V J s Z m V L A h e P M v L 5 L G a d k 7 L 1 d u z 0 r V q y y O P B z A I R y D B x d Q h R u o Q R 0 I C H i G V 3 h z l P P i v D s f s 9 a c k 8 3 s w x 8 4 n z 8 n x I / + &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " E 0 X Q 3 C p a q I X Z t A 1 y A k x y c m j l f X o = " &gt; A A A B 8 H i c b V B N S w M x E J 3 1 s 9 a v q k c v w S J 4 k L L b i n o s e v F Y w X 5 I u 5 Z s m m 1 D k + y S Z M W y 9 F d 4 8 a C I V 3 + O N / + N a b s H b X 0 w 8 H h v h p l 5 Q c y Z N q 7 7 7 S w t r 6 y u r e c 2 8 p t b 2 z u 7 h b 3 9 h o 4 S R W i d R D x S r Q B r y p m k d c M M p 6 1 Y U S w C T p v B 8 H r i N x + p 0 i y S d 2 Y U U 1 / g v m Q h I 9 h Y 6 f 7 p g X b T 8 m l l 3 C 0 U 3 Z I 7 B V o k X k a K k K H W L X x 1 e h F J B J W G c K x 1 2 3 N j 4 6 d Y G U Y 4 H e c 7 i a Y x J k P c p 2 1 L J R Z U + + n 0 4 D E 6 t k o P h Z G y J Q 2 a q r 8 n U i y 0 H o n A d g p s B n r e m 4 j / e e 3 E h J d + y m S c G C r J b F G Y c G Q i N P k e 9 Z i i x P C R J Z g o Z m 9 F Z I A V J s Z m l L c h e P M v L 5 J G u e S d l y q 3 Z 8 X q V R Z H D g 7 h C E 7 A g w u o w g 3 U o A 4 E B D z D K 7 w 5 y n l x 3 p 2 P W e u S k 8 0 c w B 8 4 n z 8 p S 4 / / &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " t Q C R + V l i 6 u 0 N Z e 8 8 L Z n G + 1 9</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>e 3 , 3 &lt;</head><label>33</label><figDesc>o Q 6 1 b / O r 0 I p o I k I Z y o n X b c 2 P j p 0 Q Z R j m M C 5 1 E Q 0 z o k P S h b a k k A r S f T g 8 e 4 y O r 9 H A Y K V v S 4 K n 6 e y I l Q u u R C G y n I G a g 5 7 2 J + J / X T k x 4 6 a d M x o k B S W e L w o R j E + H J 9 7 j H F F D D R 5 Y Q q p i 9 F d M B U Y Q a m 1 H B h u D N v 7 x I G q d l 7 7 x c u T 0 r V a + y O P L o A B 2 i Y + S h C 1 R F N 6 i G 6 o g i g Z 7 R K 3 p z l P P i v D s f s 9 a c k 8 3 s o z 9 w P n 8 A K t K Q A A = = &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " 6 u L A q 7 J / p V L + 4 7 Q 3 h N I t 9 Q d V g p</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>1 , 4 &lt; 2 , 4 &lt; 3 , 4 &lt;</head><label>142434</label><figDesc>p P p w e P 8 Z F V e j i M l C 1 p 8 F T 9 P Z E S o f V I B L Z T E D P Q 8 9 5 E / M 9 r J y a 8 9 F M m 4 8 S A p L N F Y c K x i f D k e 9 x j C q j h I 0 s I V c z e i u m A K E K N z a h g Q / D m X 1 4 k j d O y d 1 4 + u 6 2 U q l d Z H H l 0 g A 7 R M f L Q B a q i G 1 R D d U S R Q M / o F b 0 5 y n l x 3 p 2 P W W v O y W b 2 0 R 8 4 n z 8 p S Y / / &lt; / l a t e x i t &gt; x e l a t e x i t s h a 1 _ b a s e 6 4 = " 5 l K Q I 9 B R 8 / v S p u u Q 7 z e w Y p i i w 4 s = " &gt; A A A B 8 H i c b V B N S w M x E J 3 1 s 9 a v q k c v w S J 4 k L J b i 3 o s e v F Y w X 5 I u 5 Z s m m 1 D k + y S Z M W y 9 F d 4 8 a C I V 3 + O N / + N a b s H b X 0 w 8 H h v h p l 5 Q c y Z N q 7 7 7 S w t r 6 y u r e c 2 8 p t b 2 z u 7 h b 3 9 h o 4 S R W i d R D x S r Q B r y p m k d c M M p 6 1 Y U S w C T p v B 8 H r i N x + p 0 i y S d 2 Y U U 1 / g v m Q h I 9 h Y 6 f 7 p g X b T 8 m l l 3 C 0 U 3 Z I 7 B V o k X k a K k K H W L X x 1 e h F J B J W G c K x 1 2 3 N j 4 6 d Y G U Y 4 H e c 7 i a Y x J k P c p 2 1 L J R Z U + + n 0 4 D E 6 t k o P h Z G y J Q 2 a q r 8 n U i y 0 H o n A d g p s B n r e m 4 j / e e 3 E h J d + y m S c G C r J b F G Y c G Q i N P k e 9 Z i i x P C R J Z g o Z m 9 F Z I A V J s Z m l L c h e P M v L 5 J G u e S d l 8 5 u K 8 X q V R Z H D g 7 h C E 7 A g w u o w g 3 U o A 4 E B D z D K 7 w 5 y n l x 3 p 2 P W e u S k 8 0 c w B 8 4 n z 8 q 0 J A A &lt; / l a t e x i t &gt; x e l a t e x i t s h a 1 _ b a s e 6 4 = " U o R J Y 2 i O 1 B a F V C N B u w r 3 R h A R A g c = " &gt; A A A B 8 H i c b V B N S w M x E J 3 1 s 9 a v q k c v w S J 4 k L J r i 3 o s e v F Y w X 5 I u 5 Z s m m 1 D k + y S Z M W y 9 F d 4 8 a C I V 3 + O N / + N a b s H b X 0 w 8 H h v h p l 5 Q c y Z N q 7 7 7 S w t r 6 y u r e c 2 8 p t b 2 z u 7 h b 3 9 h o 4 S R W i d R D x S r Q B r y p m k d c M M p 6 1 Y U S w C T p v B 8 H r i N x + p 0 i y S d 2 Y U U 1 / g v m Q h I 9 h Y 6 f 7 p g X b T 8 m l l 3 C 0 U 3 Z I 7 B V o k X k a K k K H W L X x 1 e h F J B J W G c K x 1 2 3 N j 4 6 d Y G U Y 4 H e c 7 i a Y x J k P c p 2 1 L J R Z U + + n 0 4 D E 6 t k o P h Z G y J Q 2 a q r 8 n U i y 0 H o n A d g p s B n r e m 4 j / e e 3 E h J d + y m S c G C r J b F G Y c G Q i N P k e 9 Z i i x P C R J Z g o Z m 9 F Z I A V J s Z m l L c h e P M v L 5 L G W c k 7 L 5 V v K 8 X q V R Z H D g 7 h C E 7 A g w u o w g 3 U o A 4 E B D z D K 7 w 5 y n l x 3 p 2 P W e u S k 8 0 c w B 8 4 n z 8 s V 5 A B &lt; / l a t e x i t &gt; x e l a t e x i t s h a 1 _ b a s e 6 4 = " m t V N u 8 S k y q D a y p q r P j / N Q 4 M P R l 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>4 , 4 &lt; 1 , 1 &lt;</head><label>4411</label><figDesc>o Q 6 1 b / O r 0 I p o I k I Z y o n X b c 2 P j p 0 Q Z R j m M C 5 1 E Q 0 z o k P S h b a k k A r S f T g 8 e 4 y O r 9 H A Y K V v S 4 K n 6 e y I l Q u u R C G y n I G a g 5 7 2 J + J / X T k x 4 6 a d M x o k B S W e L w o R j E + H J 9 7 j H F F D D R 5 Y Q q p i 9 F d M B U Y Q a m 1 H B h u D N v 7 x I G q d l 7 7 x 8 d l s p V a + y O P L o A B 2 i Y + S h C 1 R F N 6 i G 6 o g i g Z 7 R K 3 p z l P P i v D s f s 9 a c k 8 3 s o z 9 w P n 8 A L d 6 Q A g = = &lt; / l a t e x i t &gt; x e l a t e x i t s h a 1 _ b a s e 6 4 = " O n T I S p M g 0 L D n T x w O L 8 m w b X v 2 8 a k = " &gt; A A A B 8 H i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 S N h V U Y 9 B L x 4 j m I c k a 5 i d d J I h s 7 P L z K w Y l n y F F w + K e P V z v P k 3 T p I 9 a G J B Q 1 H V T X d X E A u u j e t + O w u L S 8 s r q 7 m 1 / P r G 5 t Z 2 Y W e 3 p q N E M a y y S E S q E V C N g k u s G m 4 E N m K F N A w E 1 o P B 9 d i v P 6 L S P J J 3 Z h i j H 9 K e 5 F 3 O q L H S / d M D t l P v 2 B u 1 C 0 W 3 5 E 5 A 5 o m X k S J k q L Q L X 6 1 O x J I Q p W G C a t 3 0 3 N j 4 K V W G M 4 G j f C v R G F M 2 o D 1 s W i p p i N p P J w e P y K F V O q Q b K V v S k I n 6 e y K l o d b D M L C d I T V 9 P e u N x f + 8 Z m K 6 l 3 7 K Z Z w Y l G y 6 q J s I Y i I y / p 5 0 u E J m x N A S y h S 3 t x L W p 4 o y Y z P K 2 x C 8 2 Z f n S e 2 k 5 J 2 X T m / P i u W r L I 4 c 7 M M B H I E H F 1 C G G 6 h A F R i E 8 A y v 8 O Y o 5 8 V 5 d z 6 m r Q t O N r M H f + B 8 / g A k u o / 8 &lt; / l a t e x i t &gt; x e l a t e x i t s h a 1 _ b a s e 6 4 = " K B q 4 C E p D F 0 m N 5 n k F i u</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>1 , 2 &lt;</head><label>12</label><figDesc>J k P c p 2 1 L J R Z U + + n 0 4 D E 6 s k o P h Z G y J Q 2 a q r 8 n U i y 0 H o n A d g p s B n r e m 4 j / e e 3 E h J d + y m S c G C r J b F G Y c G Q i N P k e 9 Z i i x P C R J Z g o Z m 9 F Z I A V J s Z m V L A h e P M v L 5 J G p e y d l 0 9 v z 0 r V q y y O P B z A I R y D B x d Q h R u o Q R 0 I C H i G V 3 h z l P P i v D s f s 9 a c k 8 3 s w x 8 4 n z 8 m P 4 / 9 &lt; / l a t e x i t &gt; x e l a t e x i t s h a 1 _ b a s e 6 4 = " f C</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>e 2 , 2 &lt;e 1 , 3 &lt;e 2 , 3 &lt;e 3 , 3 &lt;e 1 , 4 &lt;e 2 , 4 &lt;e 3 , 4 &lt; l a t e x i t s h a 1 _ b a s e 6 4 =e 4 , 4 &lt; l a t e x i t s h a 1 _e 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 =e 2 &lt; l a t e x i t s h a 1 _e 3 &lt; l a t e x i t s h a 1 _ b a s e 6 4 =e 4 &lt; l a t e x i t s h a 1 _e 1 , 1 &lt;e 1 , 2 &lt;e 2 , 2 &lt;e 1 , 3 &lt;e 2 , 3 &lt;e 3 , 3 &lt;e 1 , 4 &lt;e 2 , 4 &lt;e 3 , 4 &lt;e 4 , 4 &lt;e 1 , 1 &lt;e 1 , 2 &lt;e 2 , 2 &lt;e 1 , 3 &lt;e 2 , 3 &lt;e 3 , 3 &lt;e 1 , 4 &lt;e 2 , 4 &lt;e 3 , 4 &lt;</head><label>2213233314243414441114213144111122213233314243444111222132333142434</label><figDesc>s 0 j e m V E M v i B 9 y U J G i b H S / d M D d N P K S W X c L Z b c s j s F X i R e R k o o Q 6 1 b / O r 0 I p o I k I Z y o n X b c 2 P j p 0 Q Z R j m M C 5 1 E Q 0 z o k P S h b a k k A r S f T g 8 e 4 y O r 9 H A Y K V v S 4 K n 6 e y I l Q u u R C G y n I G a g 5 7 2 J + J / X T k x 4 6 a d M x o k B S W e L w o R j E + H J 9 7 j H F F D D R 5 Y Q q p i 9 F d M B U Y Q a m 1 H B h u D N v 7 x I G p W y d 1 4 + v T 0 r V a + y O P L o A B 2 i Y + S h C 1 R F N 6 i G 6 o g i g Z 7 R K 3 p z l P P i v D s f s 9 a c k 8 3 s o z 9 w P n 8 A J 8 a P / g = = &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " m e e 9 T K R j O N N l z r B C 7 o b t X I w x g D o = " &gt; A A A B 8 H i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C R b B g 5 R d K + q x 6 M V j B f s h 7 V q y a b Y N T b J L k h X L 0 l / h x Y M i X v 0 5 3 v w 3 p u 0 e t P X B w O O 9 G W b m B T F n 2 r j u t 5 N b W l 5 Z X c u v F z Y 2 t 7 Z 3 i r t 7 D R 0 l i t A 6 i X i k W g H W l D N J 6 4 Y Z T l u x o l g E n D a D 4 f X E b z 5 S p V k k 7 8 w o p r 7 A f c l C R r C x 0 v 3 T A + 2 m 3 k l l 3 C 2 W 3 L I 7 B V o k X k Z K k K H W L X 5 1 e h F J B J W G c K x 1 2 3 N j 4 6 d Y G U Y 4 H R c 6 i a Y x J k P c p 2 1 L J R Z U + + n 0 4 D E 6 s k o P h Z G y J Q 2 a q r 8 n U i y 0 H o n A d g p s B n r e m 4 j / e e 3 E h J d + y m S c G C r J b F G Y c G Q i N P k e 9 Z i i x P C R J Z g o Z m 9 F Z I A V J s Z m V L A h e P M v L 5 L G a d k 7 L 1 d u z 0 r V q y y O P B z A I R y D B x d Q h R u o Q R 0 I C H i G V 3 h z l P P i v D s f s 9 a c k 8 3 s w x 8 4 n z 8 n x I / + &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " E 0 X Q 3 C p a q I X Z t A 1 y A k x y c m j l f X o = " &gt; A A A B 8 H i c b V B N S w M x E J 3 1 s 9 a v q k c v w S J 4 k L L b i n o s e v F Y w X 5 I u 5 Z s m m 1 D k + y S Z M W y 9 F d 4 8 a C I V 3 + O N / + N a b s H b X 0 w 8 H h v h p l 5 Q c y Z N q 7 7 7 S w t r 6 y u r e c 2 8 p t b 2 z u 7 h b 3 9 h o 4 S R W i d R D x S r Q B r y p m k d c M M p 6 1 Y U S w C T p v B 8 H r i N x + p 0 i y S d 2 Y U U 1 / g v m Q h I 9 h Y 6 f 7 p g X b T 8 m l l 3 C 0 U 3 Z I 7 B V o k X k a K k K H W L X x 1 e h F J B J W G c K x 1 2 3 N j 4 6 d Y G U Y 4 H e c 7 i a Y x J k P c p 2 1 L J R Z U + + n 0 4 D E 6 t k o P h Z G y J Q 2 a q r 8 n U i y 0 H o n A d g p s B n r e m 4 j / e e 3 E h J d + y m S c G C r J b F G Y c G Q i N P k e 9 Z i i x P C R J Z g o Z m 9 F Z I A V J s Z m l L c h e P M v L 5 J G u e S d l y q 3 Z 8 X q V R Z H D g 7 h C E 7 A g w u o w g 3 U o A 4 E B D z D K 7 w 5 y n l x 3 p 2 P W e u S k 8 0 c w B 8 4 n z 8 p S 4 / / &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " t Q C R + V l i 6 u 0 N Z e 8 8 L Z n G + 1 9V R k g = " &gt; A A A B 8 H i c b V B N S w M x E M 3 W r 1 q / q h 6 9 B I v g Q c q u F f V Y 9 O K x g v 2 Q d i 3 Z d L Y N T b J L k h X L 0 l / h x Y M i X v 0 5 3 v w 3 p u 0 e t P X B w O O 9 G W b m B T F n 2 r j u t 5 N b W l 5 Z X c u v F z Y 2 t 7 Z 3 i r t 7 D R 0 l i k K d R j x S r Y B o 4 E x C 3 T D D o R U r I C L g 0 A y G 1 x O / + Q h K s 0 j e m V E M v i B 9 y U J G i b H S / d M D d N P K S W X c L Z b c s j s F X i R e Rk o o Q 6 1 b / O r 0 I p o I k I Z y o n X b c 2 P j p 0 Q Z R j m M C 5 1 E Q 0 z o k P S h b a k k A r S f T g 8 e 4 y O r 9 H A Y K V v S 4 K n 6 e y I l Q u u R C G y n I G a g 5 7 2 J + J / X T k x 4 6 a d M x o k B S W e L w o R j E + H J 9 7 j H F F D D R 5 Y Q q p i 9 F d M B U Y Q a m 1 H B h u D N v 7 x I G q d l 7 7 x c u T 0 r V a + y O P L o A B 2 i Y + S h C 1 R F N 6 i G 6 o g i g Z 7 R K 3 p z l P P i v D s f s 9 a c k 8 3 s o z 9 w P n 8 A K t K Q A A = = &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " 6 u L A q 7 J / p V L + 4 7 Q 3 h N I t 9 Q d V g pU = " &gt; A A A B 8 H i c b V B N S w M x E M 3 W r 1 q / q h 6 9 B I v g Q c q u F v V Y 9 O K x g v 2 Q d i 3 Z d L Y N T b J L k h X L 0 l / h x Y M i X v 0 5 3 v w 3 p u 0 e t P X B w O O 9 G W b m B T F n 2 r j u t 5 N b W l 5 Z X c u v F z Y 2 t 7 Z 3 i r t 7 D R 0 l i k K d R j x S r Y B o 4 E x C 3 T D D o R U r I C L g 0 A y G 1 x O / + Q h K s 0 j e m V E M v i B 9 y U J G i b H S / d M D d F P v p D L u F k t u 2 Z 0 C L x I v I y W U o d Y t f n V 6 E U 0 E S E M5 0 b r t u b H x U 6 I M o x z G h U 6 i I S Z 0 S P r Q t l Q S A d p P p w e P 8 Z F V e j i M l C 1 p 8 F T 9 P Z E S o f V I B L Z T E D P Q 8 9 5 E / M 9 r J y a 8 9 F M m 4 8 S A p L N F Y c K x i f D k e 9 x j C q j h I 0 s I V c z e i u m A K E K N z a h g Q / D m X 1 4 k j d O y d 1 4 + u 6 2 U q l d Z H H l 0 g A 7 R M f L Q B a q i G 1 R D d U S R Q M / o F b 0 5 y n l x 3 p 2 P W W v O y W b 2 0 R 8 4 n z 8 p S Y / / &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " 5 l K Q I 9 B R 8 / v S p u u Q 7 z e w Y p i i w 4 s = " &gt; A A A B 8 H i c b V B N S w M x E J 3 1 s 9 a v q k c v w S J 4 k L J b i 3 o s e v F Y w X 5 I u 5 Z s m m 1 D k + y S Z M W y 9 F d 4 8 a C I V 3 + O N / + N a b s H b X 0 w 8 H h v h p l 5 Q c y Z N q 7 7 7 S w t r 6 y u r e c 2 8 p t b 2 z u 7 h b 3 9 h o 4 S R W i d R D x S r Q B r y p m k d c M M p 6 1 Y U S w C T p v B 8 H r i N x + p 0 i y S d 2 Y U U 1 / g v m Q h I 9 h Y 6 f 7 p g X b T 8 m l l 3 C 0 U 3 Z I 7 B V o k X k a K k K H W L X x 1 e h F J B J W G c K x 1 2 3 N j 4 6 d Y G U Y 4 H e c 7 i a Y x J k P c p 2 1 L J R Z U + + n 0 4 D E 6 t k o P h Z G y J Q 2 a q r 8 n U i y 0 H o n A d g p s B n r e m 4 j / e e 3 E h J d + y m S c G C r J b F G Y c G Q i N P k e 9 Z i i x P C R J Z g o Z m 9 F Z I A V J s Z m l L c h e P M v L 5 J G u e S d l 8 5 u K 8 X q V R Z H D g 7 h C E 7 A g w u o w g 3 U o A 4 E B D z D K 7 w 5 y n l x 3 p 2 P W e u S k 8 0 c w B 8 4 n z 8 q 0 J A A &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " U o R J Y 2 i O 1 B a F V C N B u w r 3 R h A R A g c = " &gt; A A A B 8 H i c b V B N S w M x E J 3 1 s 9 a v q k c v w S J 4 k L J r i 3 o s e v F Y w X 5 I u 5 Z s m m 1 D k + y S Z M W y 9 F d 4 8 a C I V 3 + O N / + N a b s H b X 0 w 8 H h v h p l 5 Q c y Z N q 7 7 7 S w t r 6 y u r e c 2 8 p t b 2 z u 7 h b 3 9 h o 4 S R W i d R D x S r Q B r y p m k d c M M p 6 1 Y U S w C T p v B 8 H r i N x + p 0 i y S d 2 Y U U 1 / g v m Q h I 9 h Y 6 f 7 p g X b T 8 m l l 3 C 0 U 3 Z I 7 B V o k X k a K k K H W L X x 1 e h F J B J W G c K x 1 2 3 N j 4 6 d Y G U Y 4 H e c 7 i a Y x J k P c p 2 1 L J R Z U + + n 0 4 D E 6 t k o P h Z G y J Q 2 a q r 8 n U i y 0 H o n A d g p s B n r e m 4 j / e e 3 E h J d + y m S c G C r J b F G Y c G Q i N P k e 9 Z i i x P C R J Z g o Z m 9 F Z I A V J s Z m l L c h e P M v L 5 L G W c k 7 L 5 V v K 8 X q V R Z H D g 7 h C E 7 A g w u o w g 3 U o A 4 E B D z D K 7 w 5 y n l x 3 p 2 P W e u S k 8 0 c w B 8 4 n z 8 s V 5 A B &lt; / l a t e x i t &gt; x " m t V N u 8 S k y q D a y p q r P j / N Q 4 M P R l 0 = " &gt; A A A B 8 H i c b V B N S w M x E M 3 W r 1 q / q h 6 9 B I v g Q c q u F v V Y 9 O K x g v 2 Q d i 3 Z d L Y N T b J L k h X L 0 l / h x Y M i X v 0 5 3 v w 3 p u 0 e t P X B w O O 9 G W b m B T F n 2 r j u t 5 N b W l 5 Z X c u v F z Y 2 t 7 Z 3 i r t 7 D R 0 l i k K d R j x S r Y B o 4 E x C 3 T D D o R U r I C L g 0 A y G 1 x O / + Q h K s 0 j e m V E M v i B 9 y U J G i b H S / d M D d N P K S W X c L Z b c s j s F X i R e R k o o Q 6 1 b / O r 0 I p o I k I Z y o n X b c 2 P j p 0 Q Z R j m M C 5 1 E Q 0 z o k P S h b a k k A r S f T g 8 e 4 y O r 9 H A Y K V v S 4 K n 6 e y I l Q u u R C G y n I G a g 5 7 2 J + J / X T k x 4 6 a d M x o k B S W e L w o R j E + H J 9 7 j H F F D D R 5 Y Q q p i 9 F d M B U Y Q a m 1 H B h u D N v 7 x I G q d l 7 7 x 8 d l s p V a + y O P L o A B 2 i Y + S h C 1 R F N 6 i G 6 o g i g Z 7 R K 3 p z l P P i v D s f s 9 a c k 8 3 s o z 9 w P n 8 A L d 6 Q A g = = &lt; / l a t e x i t &gt; x b a s e 6 4 = " R z 2 q 4 W / U C d c l W V d 8 0 l G F e h 3 Z 5 Q A = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o m K e i x 6 8 V j B t I U 2 l s 1 2 0 i 7 d b M L u R i y l v 8 G L B 0 W 8 + o O 8 + W / c t j l o 6 4 O B x 3 s z z M w L U 8 G 1 c d 1v Z 2 l 5 Z X V t v b B R 3 N z a 3 t k t 7 e 3 X d Z I p h j 5 L R K K a I d U o u E T f c C O w m S q k c S i w E Q 5 u J n 7 j E Z X m i b w 3 w x S D m P Y k j z i j x k r + 0 w N 2 v E 6 p 7 F b c K c g i 8 X J S h h y 1 T u m r 3 U 1 Y F q M 0 T F C t W 5 6 b m m B E l e F M 4 L j Y z j S m l A 1 o D 1 u W S h q j D k b T Y 8 f k 2 C p d E i X K l j R k q v 6 e G N F Y 6 2 E c 2 s 6 Y m r 6 e 9 y b i f 1 4 r M 9 F V M O I y z Q x K N l s U Z Y K Y h E w + J 1 2 u k B k x t I Q y x e 2 t h P W p o s z Y f I o 2 B G / + 5 U V S P 6 1 4 F 5 W z u / N y 9 T q P o w C H c A Q n 4 M E l V O E W a u A D A w 7 P 8 A p v j n R e n H f n Y 9 a 6 5 O Q z B / A H z u c P g o 2 O f w = = &lt; / l a t e x i t &gt; x " f H K G s A b I S R d z 7 V M T h c h 8 W v F G A p A = " &gt; A A A B 7 H i c b V B N T 8 J A E J 3 i F + I X 6 t F L I z H x R F o 0 6 p H o x S M m F k i g k u 0 y h Q 3 b b b O 7 N Z K G 3 + D F g 8 Z 4 9 Q d 5 8 9 + 4 Q A 8 K v m S S l / d m M j M v S D h T 2 n G + r c L K 6 t r 6 R n G z t L W 9 s 7 t X 3 j 9 o q j i V F D 0 a 8 1 i 2 A 6 K Q M 4 G e Z p p j O 5 F I o o B j K x j d T P 3 W I 0 r F Y n G v x w n 6 E R k I F j J K t J G 8 p w f s 1 X r l i l N 1 Z r C X i Z u T C u R o 9 M p f 3 X 5 M 0 w i F p p w o 1 X G d R P s Z k Z p R j p N S N 1 W Y E D o i A + w Y K k i E y s 9 m x 0 7 s E 6 P 0 7 T C W p o S 2 Z + r v i Y x E S o 2 j w H R G R A / V o j c V / / M 6 q Q 6 v / I y J J N U o 6 H x R m H J b x / b 0 c 7 v P J F L N x 4 Y Q K p m 5 1 a Z D I g n V J p + S C c F d f H m Z N G t V 9 6 J 6 d n d e q V / n c R T h C I 7 h F F y 4 h D r c Q g M 8 o M D g G V 7 h z R L W i / V u f c x b C 1 Y + c w h / Y H 3 + A I Q R j o A = &lt; / l a t e x i t &gt; x b a s e 6 4 = " K r x K A k I T 2 a q 0 s k V P u G + 3 F G o R p r U = " &gt; A A A B 7 H i c b V B N T 8 J A E J 3 i F + I X 6 t F L I z H x R F o x 6 p H o x S M m F k i g k u 0 y h Q 3 b b b O 7 N Z K G 3 + D F g 8 Z 4 9 Q d 5 8 9 + 4 Q A 8 K v m S S l / d m M j M v S D h T 2 n G + r c L K 6 t r 6 R n G z t L W 9 s 7 t X 3 j 9 o q j i V F D 0 a 8 1 i 2 A 6 K Q M 4 G e Z p p j O 5 F I o o B j K x j d T P 3 W I 0 r F Y n G v x w n 6 E R k I F j J K t J G 8 p w f s 1 X r l i l N 1 Z r C X i Z u T C u R o 9 M p f 3 X 5 M 0 w i F p p w o 1 X G d R P s Z k Z p R j p N S N 1 W Y E D o i A + w Y K k i E y s 9 m x 0 7 s E 6 P 0 7 T C W p o S 2 Z + r v i Y x E S o 2 j w H R G R A / V o j c V / / M 6 q Q 6 v / I y J J N U o 6 H x R m H J b x / b 0 c 7 v P J F L N x 4 Y Q K p m 5 1 a Z D I g n V J p + S C c F d f H m Z N M + q 7 k W 1 d n d e q V / n c R T h C I 7 h F F y 4 h D r c Q g M 8 o M D g G V 7 h z R L W i / V u f c x b C 1 Y + c w h / Y H 3 + A I W V j o E = &lt; / l a t e x i t &gt; x " 8 z u M T E l D L D G M h z + 9 q Y V d w c / i V z c = " &gt; A A A B 7 H i c b V B N T 8 J A E J 3 i F + I X 6 t F L I z H x R F o l 6 p H o x S M m F k i g k u 0 y h Q 3 b b b O 7 N Z K G 3 + D F g 8 Z 4 9 Q d 5 8 9 + 4 Q A 8 K v m S S l / d m M j M v S D h T 2 n G + r c L K 6 t r 6 R n G z t L W 9 s 7 t X 3 j 9 o q j i V F D 0 a 8 1 i 2 A 6 K Q M 4 G e Z p p j O 5 F I o o B j K x j d T P 3 W I 0 r F Y n G v x w n 6 E R k I F j J K t J G 8 p w f s 1 X r l i l N 1 Z r C X i Z u T C u R o 9 M p f 3 X 5 M 0 w i F p p w o 1 X G d R P s Z k Z p R j p N S N 1 W Y E D o i A + w Y K k i E y s 9 m x 0 7 s E 6 P 0 7 T C W p o S 2 Z + r v i Y x E S o 2 j w H R G R A / V o j c V / / M 6 q Q 6 v / I y J J N U o 6 H x R m H J b x / b 0 c 7 v P J F L N x 4 Y Q K p m 5 1 a Z D I g n V J p + S C c F d f H m Z N M + q 7 k X 1 / K 5 W q V / n c R T h C I 7 h F F y 4 h D r c Q g M 8 o M D g G V 7 h z R L W i / V u f c x b C 1 Y + c w h / Y H 3 + A I c Z j o I = &lt; / l a t e x i t &gt; x b a s e 6 4 = " O n T I S p M g 0 L D n T x w O L 8 m w b X v 2 8 a k = " &gt; A A A B 8 H i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 S N h V U Y 9 B L x 4 j m I c k a 5 i d d J I h s 7 P L z K w Y l n y F F w + K e P V z v P k 3 T p I 9 a G J B Q 1 H V T X d X E A u u j e t + O w u L S 8 s r q 7 m 1 / P r G 5 t Z 2 Y W e 3 p q N E M a y y S E S q E V C N g k u s G m 4 E N m K F N A w E 1 o P B 9 d i v P 6 L S P J J 3 Z h i j H 9 K e 5 F 3 O q L H S / d M D t l P v 2 B u 1 C 0 W 3 5 E 5 A 5 o m X k S J k q L Q L X 6 1 O x J I Q p W G C a t 3 0 3 N j 4 K V W G M 4 G j f C v R G F M 2 o D 1 s W i p p i N p P J w e P y K F V O q Q b K V v S k In 6 e y K l o d b D M L C d I T V 9 P e u N x f + 8 Z m K 6 l 3 7 K Z Z w Y l G y 6 q J s I Y i I y / p 5 0 u E J m x N A S y h S 3 t x L W p 4 o y Y z P K 2 x C 8 2 Z f n S e 2 k 5 J 2 X T m / P i u W r L I 4 c 7 M M B H I E H F 1 C G G 6 h A F R i E 8 A y v 8 O Y o 5 8 V 5 d z 6 m r Q t O N r M H f + B 8 / g A k u o / 8 &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " K B q 4 C E p D F 0 m N 5 n k F i ui 0 + V F w Q O U = " &gt; A A A B 8 H i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C R b B g 5 T d K u q x 6 M V j B f s h 7 V q y a b Y N T b J L k h X L 0 l / h x Y M i X v 0 5 3 v w 3 p u 0 e t P X B w O O 9 G W b m B T F n 2 r j u t 5 N b W l 5 Z X c u v F z Y 2 t 7 Z 3 i r t 7 D R 0 l i t A 6 i X i k W g H W l D N J 6 4 Y Z T l u x o l g E n D a D 4 f X E b z 5 S p V k k 7 8 w o p r 7 A f c l C R r C x 0 v 3 T A + 2 m 3 k l l 3 C 2 W 3 L I 7 B V o k X k Z K k K H W L X 5 1 e h F J B J W G c K x 1 2 3 N j 4 6 d Y G U Y 4 H R c 6 i a Y xJ k P c p 2 1 L J R Z U + + n 0 4 D E 6 s k o P h Z G y J Q 2 a q r 8 n U i y 0 H o n A d g p s B n r e m 4 j / e e 3 E h J d + y m S c G C r J b F G Y c G Q i N P k e 9 Z i i x P C R J Z g o Z m 9 F Z I A V J s Z m V L A h e P M v L 5 J G p e y d l 0 9 v z 0 r V q y y O P B z A I R y D B x d Q h R u o Q R 0 I C H i G V 3 h z l P P i v D s f s 9 a c k 8 3 s w x 8 4 n z 8 m P 4 / 9 &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " f C P G w Y G 2 g 7 l H S q + S 0 H O r b v X q J B E = " &gt; A A A B 8 H i c b V B N S w M x E M 3 W r 1 q / q h 6 9 B I v g Q c p u F f V Y 9 O K x g v 2 Q d i 3 Z d L Y N T b J L k h X L 0 l / h x Y M i X v 0 5 3 v w 3 p u 0 e t P X B w O O 9 G W b m B T F n 2 r j u t 5 N b W l 5 Z X c u v F z Y 2 t 7 Z 3 i r t 7 D R 0 l i k K d R j x S r Y B o 4 E x C 3 T D D o R U r I C L g 0 A y G 1 x O / + Q h K s 0 j e m V E M v i B 9 y U J G i b H S / d M D d N P K S W X c L Z b c s j s F X i R e R k o o Q 6 1 b / O r 0 I p o I k I Z y o n X b c 2 P j p 0 Q Z R j m M C 5 1 E Q 0 z o k P S h b a k k A r S f T g 8 e 4 y O r 9 H A Y K V v S 4 K n 6 e y I l Q u u R C G y n I G a g 5 7 2 J + J / X T k x 4 6 a d M x o k B S W e L w o R j E + H J 9 7 j H F F D D R 5 Y Q q p i 9 F d M B U Y Q a m 1 H B h u D N v 7 x I G p W y d 1 4 + v T 0 r V a + y O P L o A B 2 i Y + S h C 1 R F N 6 i G 6 o g i g Z 7 R K 3 p z l P P i v D s f s 9 a c k 8 3 s o z 9 w P n 8 A J 8 a P / g = = &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " m e e 9 T K R j O N N l z r B C 7 o b t X I w x g D o = " &gt; A A A B 8 H i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C R b B g 5 R d K + q x 6 M V j B f s h 7 V q y a b Y N T b J L k h X L 0 l / h x Y M i X v 0 5 3 v w 3 p u 0 e t P X B w O O 9 G W b m B T F n 2 r j u t 5 N b W l 5 Z X c u v F z Y 2 t 7 Z 3 i r t 7 D R 0 l i t A 6 i X i k W g H W l D N J 6 4 Y Z T l u x o l g E n D a D 4 f X E b z 5 S p V k k 7 8 w o p r 7 A f c l C R r C x 0 v 3 T A + 2 m 3 k l l 3 C 2 W 3 L I 7 B V o k X k Z K k K H W L X 5 1 e h F J B J W G c K x 1 2 3 N j 4 6 d Y G U Y 4 H R c 6 i a Y x J k P c p 2 1 L J R Z U + + n 0 4 D E 6 s k o P h Z G y J Q 2 a q r 8 n U i y 0 H o n A d g p s B n r e m 4 j / e e 3 E h J d + y m S c G C r J b F G Y c G Q i N P k e 9 Z i i x P C R J Z g o Z m 9 F Z I A V J s Z m V L A h e P M v L 5 L G a d k 7 L 1 d u z 0 r V q y y O P B z A I R y D B x d Q h R u o Q R 0 I C H i G V 3 h z l P P i v D s f s 9 a c k 8 3 s w x 8 4 n z 8 n x I / + &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " E 0 X Q 3 C p a q I X Z t A 1 y A k x y c m j l f X o = " &gt; A A A B 8 H i c b V B N S w M x E J 3 1 s 9 a v q k c v w S J 4 k L L b i n o s e v F Y w X 5 I u 5 Z s m m 1 D k + y S Z M W y 9 F d 4 8 a C I V 3 + O N / + N a b s H b X 0 w 8 H h v h p l 5 Q c y Z N q 7 7 7 S w t r 6 y u r e c 2 8 p t b 2 z u 7 h b 3 9 h o 4 S R W i d R D x S r Q B r y p m k d c M M p 6 1 Y U S w C T p v B 8 H r i N x + p 0 i y S d 2 Y U U 1 / g v m Q h I 9 h Y 6 f 7 p g X b T 8 m l l 3 C 0 U 3 Z I 7 B V o k X k a K k K H W L X x 1 e h F J B J W G c K x 1 2 3 N j 4 6 d Y G U Y 4 H e c 7 i a Y x J k P c p 2 1 L J R Z U + + n 0 4 D E 6 t k o P h Z G y J Q 2 a q r 8 n U i y 0 H o n A d g p s B n r e m 4 j / e e 3 E h J d + y m S c G C r J b F G Y c G Q i N P k e 9 Z i i x P C R J Z g o Z m 9 F Z I A V J s Z m l L c h e P M v L 5 J G u e S d l y q 3 Z 8 X q V R Z H D g 7 h C E 7 A g w u o w g 3 U o A 4 E B D z D K 7 w 5 y n l x 3 p 2 P W e u S k 8 0 c w B 8 4 n z 8 p S 4 / / &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " t Q C R + V l i 6 u 0 N Z e 8 8 L Z n G + 1 9 V R k g = " &gt; A A A B 8 H i c b V B N S w M x E M 3 W r 1 q / q h 6 9 B I v g Q c q u F f V Y 9 O K x g v 2 Q d i 3 Z d L Y N T b J L k h X L 0 l / h x Y M i X v 0 5 3 v w 3 p u 0 e t P X B w O O 9 G W b m B T F n 2 r j u t 5 N b W l 5 Z X c u v F z Y 2 t 7 Z 3 i r t 7 D R 0 l i k K d R j x S r Y B o 4 E x C 3 T D D o R U r I C L g 0 A y G 1 x O / + Q h K s 0 j e m V E M v i B 9 y U J G i b H S / d M D d N P K S W X c L Z b c s j s F X i R e R k o o Q 6 1 b / O r 0 I p o I k I Z y o n X b c 2 P j p 0 Q Z R j m M C 5 1 E Q 0 z o k P S h b a k k A r S f T g 8 e 4 y O r 9 H A Y K V v S 4 K n 6 e y I l Q u u R C G y n I G a g 5 7 2 J + J / X T k x 4 6 a d M x o k B S W e L w o R j E + H J 9 7 j H F F D D R 5 Y Q q p i 9 F d M B U Y Q a m 1 H B h u D N v 7 x I G q d l 7 7 x c u T 0 r V a + y O P L o A B 2 i Y + S h C 1 R F N 6 i G 6 o g i g Z 7 R K 3 p z l P P i v D s f s 9 a c k 8 3 s o z 9 w P n 8 A K t K Q A A = = &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " 6 u L A q 7 J / p V L + 4 7 Q 3 h N I t 9 Q d V g p U = " &gt; A A A B 8 H i c b V B N S w M x E M 3 W r 1 q / q h 6 9 B I v g Q c q u F v V Y 9 O K x g v 2 Q d i 3 Z d L Y N T b J L k h X L 0 l / h x Y M i X v 0 5 3 v w 3 p u 0 e t P X B w O O 9 G W b m B T F n 2 r j u t 5 N b W l 5 Z X c u v F z Y 2 t 7 Z 3 i r t 7 D R 0 l i k K d R j x S r Y B o 4 E x C 3 T D D o R U r I C L g 0 A y G 1 x O / + Q h K s 0 j e m V E M v i B 9 y U J G i b H S / d M D d F P v p D L u F k t u 2 Z 0 C L x I v I y W U o d Y t f n V 6 E U 0 E S E M 5 0 b r t u b H x U 6 I M o x z G h U 6 i I S Z 0 S P r Q t l Q S A d p P p w e P 8 Z F V e j i M l C 1 p 8 F T 9 P Z E S o f V I B L Z T E D P Q 8 9 5 E / M 9 r J y a 8 9 F M m 4 8 S A p L N F Y c K x i f D k e 9 x j C q j h I 0 s I V c z e i u m A K E K N z a h g Q / D m X 1 4 k j d O y d 1 4 + u 6 2 U q l d Z H H l 0 g A 7 R M f L Q B a q i G 1 R D d U S R Q M / o F b 0 5 y n l x 3 p 2 P W W v O y W b 2 0 R 8 4 n z 8 p S Y / / &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " 5 l K Q I 9 B R 8 / v S p u u Q 7 z e w Y p i i w 4 s = " &gt; A A A B 8 H i c b V B N S w M x E J 3 1 s 9 a v q k c v w S J 4 k L J b i 3 o s e v F Y w X 5 I u 5 Z s m m 1 D k + y S Z M W y 9 F d 4 8 a C I V 3 + O N / + N a b s H b X 0 w 8 H h v h p l 5 Q c y Z N q 7 7 7 S w t r 6 y u r e c 2 8 p t b 2 z u 7 h b 3 9 h o 4 S R W i d R D x S r Q B r y p m k d c M M p 6 1 Y U S w C T p v B 8 H r i N x + p 0 i y S d 2 Y U U 1 / g v m Q h I 9 h Y 6 f 7 p g X b T 8 m l l 3 C 0 U 3 Z I 7 B V o k X k a K k K H W L X x 1 e h F J B J W G c K x 1 2 3 N j 4 6 d Y G U Y 4 H e c 7 i a Y x J k P c p 2 1 L J R Z U + + n 0 4 D E 6 t k o P h Z G y J Q 2 a q r 8 n U i y 0 H o n A d g p s B n r e m 4 j / e e 3 E h J d + y m S c G C r J b F G Y c G Q i N P k e 9 Z i i x P C R J Z g o Z m 9 F Z I A V J s Z m l L c h e P M v L 5 J G u e S d l 8 5 u K 8 X q V R Z H D g 7 h C E 7 A g w u o w g 3 U o A 4 E B D z D K 7 w 5 y n l x 3 p 2 P W e u S k 8 0 c w B 8 4 n z 8 q 0 J A A &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " U o R J Y 2 i O 1 B a F V C N B u w r 3 R h A R A g c = " &gt; A A A B 8 H i c b V B N S w M x E J 3 1 s 9 a v q k c v w S J 4 k L J r i 3 o s e v F Y w X 5 I u 5 Z s m m 1 D k + y S Z M W y 9 F d 4 8 a C I V 3 + O N / + N a b s H b X 0 w 8 H h v h p l 5 Q c y Z N q 7 7 7 S w t r 6 y u r e c 2 8 p t b 2 z u 7 h b 3 9 h o 4 S R W i d R D x S r Q B r y p m k d c M M p 6 1 Y U S w C T p v B 8 H r i N x + p 0 i y S d 2 Y U U 1 / g v m Q h I 9 h Y 6 f 7 p g X b T 8 m l l 3 C 0 U 3 Z I 7 B V o k X k a K k K H W L X x 1 e h F J B J W G c K x 1 2 3 N j 4 6 d Y G U Y 4 H e c 7 i a Y x J k P c p 2 1 L J R Z U + + n 0 4 D E 6 t k o P h Z G y J Q 2 a q r 8 n U i y 0 H o n A d g p s B n r e m 4 j / e e 3 E h J d + y m S c G C r J b F G Y c G Q i N P k e 9 Z i i x P C R J Z g o Z m 9 F Z I A V J s Z m l L c h e P M v L 5 L G W c k 7 L 5 V v K 8 X q V R Z H D g 7 h C E 7 A g w u o w g 3 U o A 4 E B D z D K 7 w 5 y n l x 3 p 2 P W e u S k 8 0 c w B 8 4 n z 8 s V 5 A B &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " m t V N u 8 S k y q D a y p q r P j / N Q 4 M P R l 0 = " &gt; A A A B 8 H i c b V B N S w M x E M 3 W r 1 q / q h 6 9 B I v g Q c q u F v V Y 9 O K x g v 2 Q d i 3 Z d L Y N T b J L k h X L 0 l / h x Y M i X v 0 5 3 v w 3 p u 0 e t P X B w O O 9 G W b m B T F n 2 r j u t 5 N b W l 5 Z X c u v F z Y 2 t 7 Z 3 i r t 7 D R 0 l i k K d R j x S r Y B o 4 E x C 3 T D D o R U r I C L g 0 A y G 1 x O / + Q h K s 0 j e m V E M v i B 9 y U J G i b H S / d M D d N P K S W X c L Z b c s j s F X i R e R k o o Q 6 1 b / O r 0 I p o I k I Z y o n X b c 2 P j p 0 Q Z R j m M C 5 1 E Q 0 z o k P S h b a k k A r S f T g 8 e 4 y O r 9 H A Y K V v S 4 K n 6 e y I l Q u u R C G y n I G a g 5 7 2 J + J / X T k x 4 6 a d M x o k B S W e L w o R j E + H J 9 7 j H F F D D R 5 Y Q q p i 9 F d M B U Y Q a m 1 H B h u D N v 7 x I G q d l 7 7 x 8 d l s p V a + y O P L o A B 2 i Y + S h C 1 R F N 6 i G 6 o g i g Z 7 R K 3 p z l P P i v D s f s 9 a c k 8 3 s o z 9 w P n 8 A L d 6 Q A g = = &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " O n T I S p M g 0 L D n T x w O L 8 m w b X v 2 8 a k = " &gt; A A A B 8 H i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 S N h V U Y 9 B L x 4 j m I c k a 5 i d d J I h s 7 P L z K w Y l n y F F w + K e P V z v P k 3 T p I 9 a G J B Q 1 H V T X d X E A u u j e t + O w u L S 8 s r q 7 m 1 / P r G 5 t Z 2 Y W e 3 p q N E M a y y S E S q E V C N g k u s G m 4 E N m K F N A w E 1 o P B 9 d i v P 6 L S P J J 3 Z h i j H 9 K e 5 F 3 O q L H S / d M D t l P v 2 B u 1 C 0 W 3 5 E 5 A 5 o m X k S J k q L Q L X 6 1 O x J I Q p W G C a t 3 0 3 N j 4 K V W G M 4 G j f C v R G F M 2 o D 1 s W i p p i N p P J w e P y K F V O q Q b K V v S k I n 6 e y K l o d b D M L C d I T V 9 P e u N x f + 8 Z m K 6 l 3 7 K Z Z w Y l G y 6 q J s I Y i I y / p 5 0 u E J m x N A S y h S 3 t x L W p 4 o y Y z P K 2 x C 8 2 Z f n S e 2 k 5 J 2 X T m / P i u W r L I 4 c 7 M M B H I E H F 1 C G G 6 h A F R i E 8 A y v 8 O Y o 5 8 V 5 d z 6 m r Q t O N r M H f + B 8 / g A k u o / 8 &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " K B q 4 C E p D F 0 m N 5 n k F i u i 0 + V F w Q O U = " &gt; A A A B 8 H i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C R b B g 5 T d K u q x 6 M V j B f s h 7 V q y a b Y N T b J L k h X L 0 l / h x Y M i X v 0 5 3 v w 3 p u 0 e t P X B w O O 9 G W b m B T F n 2 r j u t 5 N b W l 5 Z X c u v F z Y 2 t 7 Z 3 i r t 7 D R 0 l i t A 6 i X i k W g H W l D N J 6 4 Y Z T l u x o l g E n D a D 4 f X E b z 5 S p V k k 7 8 w o p r 7 A f c l C R r C x 0 v 3 T A + 2 m 3 k l l 3 C 2 W 3 L I 7 B V o k X k Z K k K H W L X 5 1 e h F J B J W G c K x 1 2 3 N j 4 6 d Y G U Y 4 H R c 6 i a Y x J k P c p 2 1 L J R Z U + + n 0 4 D E 6 s k o P h Z G y J Q 2 a q r 8 n U i y 0 H o n A d g p s B n r e m 4 j / e e 3 E h J d + y m S c G C r J b F G Y c G Q i N P k e 9 Z i i x P C R J Z g o Z m 9 F Z I A V J s Z m V L A h e P M v L 5 J G p e y d l 0 9 v z 0 r V q y y O P B z A I R y D B x d Q h R u o Q R 0 I C H i G V 3 h z l P P i v D s f s 9 a c k 8 3 s w x 8 4 n z 8 m P 4 / 9 &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " f C P G w Y G 2 g 7 l H S q + S 0 H O r b v X q J B E = " &gt; A A A B 8 H i c b V B N S w M x E M 3 W r 1 q / q h 6 9 B I v g Q c p u F f V Y 9 O K x g v 2 Q d i 3 Z d L Y N T b J L k h X L 0 l / h x Y M i X v 0 5 3 v w 3 p u 0 e t P X B w O O 9 G W b m B T F n 2 r j u t 5 N b W l 5 Z X c u v F z Y 2 t 7 Z 3 i r t 7 D R 0 l i k K d R j x S r Y B o 4 E x C 3 T D D o R U r I C L g 0 A y G 1 x O / + Q h K s 0 j e m V E M v i B 9 y U J G i b H S / d M D d N P K S W X c L Z b c s j s F X i R e R k o o Q 6 1 b / O r 0 I p o I k I Z y o n X b c 2 P j p 0 Q Z R j m M C 5 1 E Q 0 z o k P S h b a k k A r S f T g 8 e 4 y O r 9 H A Y K V v S 4 K n 6 e y I l Q u u R C G y n I G a g 5 7 2 J + J / X T k x 4 6 a d M x o k B S W e L w o R j E + H J 9 7 j H F F D D R 5 Y Q q p i 9 F d M B U Y Q a m 1 H B h u D N v 7 x I G p W y d 1 4 + v T 0 r V a + y O P L o A B 2 i Y + S h C 1 R F N 6 i G 6 o g i g Z 7 R K 3 p z l P P i v D s f s 9 a c k 8 3 s o z 9 w P n 8 A J 8 a P / g = = &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " m e e 9 T K R j O N N l z r B C 7 o b t X I w x g D o = " &gt; A A A B 8 H i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C R b B g 5 R d K + q x 6 M V j B f s h 7 V q y a b Y N T b J L k h X L 0 l / h x Y M i X v 0 5 3 v w 3 p u 0 e t P X B w O O 9 G W b m B T F n 2 r j u t 5 N b W l 5 Z X c u v F z Y 2 t 7 Z 3 i r t 7 D R 0 l i t A 6 i X i k W g H W l D N J 6 4 Y Z T l u x o l g E n D a D 4 f X E b z 5 S p V k k 7 8 w o p r 7 A f c l C R r C x 0 v 3 T A + 2 m 3 k l l 3 C 2 W 3 L I 7 B V o k X k Z K k K H W L X 5 1 e h F J B J W G c K x 1 2 3 N j 4 6 d Y G U Y 4 H R c 6 i a Y x J k P c p 2 1 L J R Z U + + n 0 4 D E 6 s k o P h Z G y J Q 2 a q r 8 n U i y 0 H o n A d g p s B n r e m 4 j / e e 3 E h J d + y m S c G C r J b F G Y c G Q i N P k e 9 Z i i x P C R J Z g o Z m 9 F Z I A V J s Z m V L A h e P M v L 5 L G a d k 7 L 1 d u z 0 r V q y y O P B z A I R y D B x d Q h R u o Q R 0 I C H i G V 3 h z l P P i v D s f s 9 a c k 8 3 s w x 8 4 n z 8 n x I / + &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " E 0 X Q 3 C p a q I X Z t A 1 y A k x y c m j l f X o = " &gt; A A A B 8 H i c b V B N S w M x E J 3 1 s 9 a v q k c v w S J 4 k L L b i n o s e v F Y w X 5 I u 5 Z s m m 1 D k + y S Z M W y 9 F d 4 8 a C I V 3 + O N / + N a b s H b X 0 w 8 H h v h p l 5 Q c y Z N q 7 7 7 S w t r 6 y u r e c 2 8 p t b 2 z u 7 h b 3 9 h o 4 S R W i d R D x S r Q B r y p m k d c M M p 6 1 Y U S w C T p v B 8 H r i N x + p 0 i y S d 2 Y U U 1 / g v m Q h I 9 h Y 6 f 7 p g X b T 8 m l l 3 C 0 U 3 Z I 7 B V o k X k a K k K H W L X x 1 e h F J B J W G c K x 1 2 3 N j 4 6 d Y G U Y 4 H e c 7 i a Y x J k P c p 2 1 L J R Z U + + n 0 4 D E 6 t k o P h Z G y J Q 2 a q r 8 n U i y 0 H o n A d g p s B n r e m 4 j / e e 3 E h J d + y m S c G C r J b F G Y c G Q i N P k e 9 Z i i x P C R J Z g o Z m 9 F Z I A V J s Z m l L c h e P M v L 5 J G u e S d l y q 3 Z 8 X q V R Z H D g 7 h C E 7 A g w u o w g 3 U o A 4 E B D z D K 7 w 5 y n l x 3 p 2 P W e u S k 8 0 c w B 8 4 n z 8 p S 4 / / &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " t Q C R + V l i 6 u 0 N Z e 8 8 L Z n G + 1 9 V R k g = " &gt; A A A B 8 H i c b V B N S w M x E M 3 W r 1 q / q h 6 9 B I v g Q c q u F f V Y 9 O K x g v 2 Q d i 3 Z d L Y N T b J L k h X L 0 l / h x Y M i X v 0 5 3 v w 3 p u 0 e t P X B w O O 9 G W b m B T F n 2 r j u t 5 N b W l 5 Z X c u v F z Y 2 t 7 Z 3 i r t 7 D R 0 l i k K d R j x S r Y B o 4 E x C 3 T D D o R U r I C L g 0 A y G 1 x O / + Q h K s 0 j e m V E M v i B 9 y U J G i b H S / d M D d N P K S W X c L Z b c s j s F X i R e R k o o Q 6 1 b / O r 0 I p o I k I Z y o n X b c 2 P j p 0 Q Z R j m M C 5 1 E Q 0 z o k P S h b a k k A r S f T g 8 e 4 y O r 9 H A Y K V v S 4 K n 6 e y I l Q u u R C G y n I G a g 5 7 2 J + J / X T k x 4 6 a d M x o k B S W e L w o R j E + H J 9 7 j H F F D D R 5 Y Q q p i 9 F d M B U Y Q a m 1 H B h u D N v 7 x I G q d l 7 7 x c u T 0 r V a + y O P L o A B 2 i Y + S h C 1 R F N 6 i G 6 o g i g Z 7 R K 3 p z l P P i v D s f s 9 a c k 8 3 s o z 9 w P n 8 A K t K Q A A = = &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " 6 u L A q 7 J / p V L + 4 7 Q 3 h N I t 9 Q d V g p U = " &gt; A A A B 8 H i c b V B N S w M x E M 3 W r 1 q / q h 6 9 B I v g Q c q u F v V Y 9 O K x g v 2 Q d i 3 Z d L Y N T b J L k h X L 0 l / h x Y M i X v 0 5 3 v w 3 p u 0 e t P X B w O O 9 G W b m B T F n 2 r j u t 5 N b W l 5 Z X c u v F z Y 2 t 7 Z 3 i r t 7 D R 0 l i k K d R j x S r Y B o 4 E x C 3 T D D o R U r I C L g 0 A y G 1 x O / + Q h K s 0 j e m V E M v i B 9 y U J G i b H S / d M D d F P v p D L u F k t u 2 Z 0 C L x I v I y W U o d Y t f n V 6 E U 0 E S E M 5 0 b r t u b H x U 6 I M o x z G h U 6 i I S Z 0 S P r Q t l Q S A d p P p w e P 8 Z F V e j i M l C 1 p 8 F T 9 P Z E S o f V I B L Z T E D P Q 8 9 5 E / M 9 r J y a 8 9 F M m 4 8 S A p L N F Y c K x i f D k e 9 x j C q j h I 0 s I V c z e i u m A K E K N z a h g Q / D m X 1 4 k j d O y d 1 4 + u 6 2 U q l d Z H H l 0 g A 7 R M f L Q B a q i G 1 R D d U S R Q M / o F b 0 5 y n l x 3 p 2 P W W v O y W b 2 0 R 8 4 n z 8 p S Y / / &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " 5 l K Q I 9 B R 8 / v S p u u Q 7 z e w Y p i i w 4 s = " &gt; A A A B 8 H i c b V B N S w M x E J 3 1 s 9 a v q k c v w S J 4 k L J b i 3 o s e v F Y w X 5 I u 5 Z s m m 1 D k + y S Z M W y 9 F d 4 8 a C I V 3 + O N / + N a b s H b X 0 w 8 H h v h p l 5 Q c y Z N q 7 7 7 S w t r 6 y u r e c 2 8 p t b 2 z u 7 h b 3 9 h o 4 S R W i d R D x S r Q B r y p m k d c M M p 6 1 Y U S w C T p v B 8 H r i N x + p 0 i y S d 2 Y U U 1 / g v m Q h I 9 h Y 6 f 7 p g X b T 8 m l l 3 C 0 U 3 Z I 7 B V o k X k a K k K H W L X x 1 e h F J B J W G c K x 1 2 3 N j 4 6 d Y G U Y 4 H e c 7 i a Y x J k P c p 2 1 L J R Z U + + n 0 4 D E 6 t k o P h Z G y J Q 2 a q r 8 n U i y 0 H o n A d g p s B n r e m 4 j / e e 3 E h J d + y m S c G C r J b F G Y c G Q i N P k e 9 Z i i x P C R J Z g o Z m 9 F Z I A V J s Z m l L c h e P M v L 5 J G u e S d l 8 5 u K 8 X q V R Z H D g 7 h C E 7 A g w u o w g 3 U o A 4 E B D z D K 7 w 5 y n l x 3 p 2 P W e u S k 8 0 c w B 8 4 n z 8 q 0 J A A &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " U o R J Y 2 i O 1 B a F V C N B u w r 3 R h A R A g c = " &gt; A A A B 8 H i c b V B N S w M x E J 3 1 s 9 a v q k c v w S J 4 k L J r i 3 o s e v F Y w X 5 I u 5 Z s m m 1 D k + y S Z M W y 9 F d 4 8 a C I V 3 + O N / + N a b s H b X 0 w 8 H h v h p l 5 Q c y Z N q 7 7 7 S w t r 6 y u r e c 2 8 p t b 2 z u 7 h b 3 9 h o 4 S R W i d R D x S r Q B r y p m k d c M M p 6 1 Y U S w C T p v B 8 H r i N x + p 0 i y S d 2 Y U U 1 / g v m Q h I 9 h Y 6 f 7 p g X b T 8 m l l 3 C 0 U 3 Z I 7 B V o k X k a K k K H W L X x 1 e h F J B J W G c K x 1 2 3 N j 4 6 d Y G U Y 4 H e c 7 i a Y x J k P c p 2 1 L J R Z U + + n 0 4 D E 6 t k o P h Z G y J Q 2 a q r 8 n U i y 0 H o n A d g p s B n r e m 4 j / e e 3 E h J d + y m S c G C r J b F G Y c G Q i N P k e 9 Z i i x P C R J Z g o Z m 9 F Z I A V J s Z m l L c h e P M v L 5 L G W c k 7 L 5 V v K 8 X q V R Z H D g 7 h C E 7 A g w u o w g 3 U o A 4 E B D z D K 7 w 5 y n l x 3 p 2 P W e u S k 8 0 c w B 8 4 n z 8 s V 5 A B &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " m t V N u 8 S k y q D a y p q r P j / N Q 4 M P R l 0 = " &gt; A A A B 8 H i c b V B N S w M x E M 3 W r 1 q / q h 6 9 B I v g Q c q u F v V Y 9 O K x g v 2 Q d i 3 Z d L Y N T b J L k h X L 0 l / h x Y M i X v 0 5 3 v w 3 p u 0 e t P X B w O O 9 G W b m B T F n 2 r j u t 5 N b W l 5 Z X c u v F z Y 2 t 7 Z 3 i r t 7 D R 0 l i k K d R j x S r Y B o 4 E x C 3 T D D o R U r I C L g 0 A y G 1 x O / + Q h K s 0 j e m V E M v i B 9 y U J G i b H S / d M D d N P K S W X c L Z b c s j s F X i R e R k o o Q 6 1 b / O r 0 I p o I k I Z y o n X b c 2 P j p 0 Q Z R j m M C 5 1 E Q 0 z o k P S h b a k k A r S f T g 8 e 4 y O r 9 H A Y K V v S 4 K n 6 e y I l Q u u R C G y n I G a g 5 7 2 J + J / X T k x 4 6 a d M x o k B S W e L w o R j E + H J 9 7 j H F F D D R 5 Y Q q p i 9 F d M B U Y Q a m 1 H B h u D N v 7 x I G q d l 7 7 x 8 d l s p V a + y O P L o A B 2 i Y + S h C 1 R F N 6 i G 6 o g i g Z 7 R K 3 p z l P P i v D s f s 9 a c k 8 3 s o z 9 w P n 8 A L d 6 Q A g = = &lt; / l a t e x i t &gt; Figure 3: (a) Original HRNet and (b) DIFFNet architecture with internal feature fusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 4 :</head><label>4</label><figDesc>Visualisation of depth estimation results. The top row contains the input images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results on KITTI Benchmark using the Eigen split grouped by training methodology. M: trained on monocular videos, MS: trained on binocular videos. Se: trained with semantic labels. The best scores are bold and the second are underlined.</figDesc><table><row><cell>Method</cell><cell>Train</cell><cell>WxH</cell><cell cols="4">lower is better Abs Rel Sq Rel RMSE RMSE log</cell><cell>higher is better ? 1 ? 2 ? 3</cell></row><row><cell>SfMlearner [39]</cell><cell>M</cell><cell>640x192</cell><cell>0.183</cell><cell cols="2">1.595 6.709</cell><cell>0.270</cell><cell>0.734 0.902 0.959</cell></row><row><cell>Li [23]</cell><cell>M</cell><cell>416x128</cell><cell>0.130</cell><cell cols="2">0.950 5.138</cell><cell>0.209</cell><cell>0.843 0.948 0.978</cell></row><row><cell>Chen [2]</cell><cell cols="2">M+Se 512x256</cell><cell>0.118</cell><cell cols="2">0.905 5.096</cell><cell>0.211</cell><cell>0.839 0.945 0.977</cell></row><row><cell>Monodepth2 [11]</cell><cell>M</cell><cell>640x192</cell><cell>0.115</cell><cell cols="2">0.903 4.863</cell><cell>0.193</cell><cell>0.877 0.959 0.981</cell></row><row><cell>SGDepth [20]</cell><cell cols="2">M+Se 640x192</cell><cell>0.113</cell><cell cols="2">0.835 4.693</cell><cell>0.191</cell><cell>0.879 0.961 0.981</cell></row><row><cell>SAFENet [4]</cell><cell cols="2">M+Se 640x192</cell><cell>0.112</cell><cell cols="2">0.788 4.582</cell><cell>0.187</cell><cell>0.878 0.963 0.983</cell></row><row><cell>VC-Depth [38]</cell><cell>M</cell><cell>640x192</cell><cell>0.112</cell><cell cols="2">0.816 4.715</cell><cell>0.190</cell><cell>0.880 0.960 0.982</cell></row><row><cell>PackNet [13]</cell><cell>M</cell><cell>640x192</cell><cell>0.111</cell><cell cols="2">0.785 4.601</cell><cell>0.189</cell><cell>0.878 0.960 0.982</cell></row><row><cell>Mono-Uncertainty[28]</cell><cell>M</cell><cell>640x192</cell><cell>0.111</cell><cell cols="2">0.863 4.756</cell><cell>0.188</cell><cell>0.881 0.961 0.982</cell></row><row><cell>Fang [7]</cell><cell>M</cell><cell>640x192</cell><cell>0.111</cell><cell>-</cell><cell>4.660</cell><cell>0.186</cell><cell>0.884 0.962 0.982</cell></row><row><cell>HR-depth [25]</cell><cell>M</cell><cell>640x192</cell><cell>0.109</cell><cell cols="2">0.792 4.632</cell><cell>0.185</cell><cell>0.884 0.962 0.983</cell></row><row><cell>Johnston [17]</cell><cell>M</cell><cell>640x192</cell><cell>0.106</cell><cell cols="2">0.861 4.699</cell><cell>0.185</cell><cell>0.889 0.962 0.982</cell></row><row><cell>DIFFNet</cell><cell>M</cell><cell>640x192</cell><cell>0.102</cell><cell cols="2">0.764 4.483</cell><cell>0.180</cell><cell>0.896 0.965 0.983</cell></row><row><cell>Monodepth2 [11]</cell><cell>MS</cell><cell>640x192</cell><cell>0.106</cell><cell cols="2">0.818 4.750</cell><cell>0.196</cell><cell>0.874 0.957 0.979</cell></row><row><cell>HR-depth [25]</cell><cell>MS</cell><cell>640x192</cell><cell>0.107</cell><cell cols="2">0.785 4.612</cell><cell>0.185</cell><cell>0.887 0.962 0.982</cell></row><row><cell>Fang [7]</cell><cell>MS</cell><cell>640x192</cell><cell>0.101</cell><cell>-</cell><cell>4.512</cell><cell>0.188</cell><cell>0.881 0.961 0.981</cell></row><row><cell>DIFFNet</cell><cell>MS</cell><cell>640x192</cell><cell>0.101</cell><cell cols="2">0.749 4.445</cell><cell>0.179</cell><cell>0.898 0.965 0.983</cell></row><row><cell>Monodepth2 [11]</cell><cell>M</cell><cell>1024x320</cell><cell>0.115</cell><cell cols="2">0.882 4.701</cell><cell>0.190</cell><cell>0.879 0.961 0.982</cell></row><row><cell>Fang [7]</cell><cell>M</cell><cell>1024x320</cell><cell>0.109</cell><cell>-</cell><cell>4.581</cell><cell>0.185</cell><cell>0.890 0.964 0.983</cell></row><row><cell>PackNet [13]</cell><cell>M</cell><cell>1280x384</cell><cell>0.107</cell><cell cols="2">0.802 4.538</cell><cell>0.186</cell><cell>0.889 0.962 0.981</cell></row><row><cell>SGDepth [20]</cell><cell cols="2">M+Se 1280x384</cell><cell>0.107</cell><cell cols="2">0.768 4.468</cell><cell>0.186</cell><cell>0.891 0.963 0.982</cell></row><row><cell>SAFENet [4]</cell><cell cols="2">M+Se 1024x320</cell><cell>0.106</cell><cell cols="2">0.743 4.489</cell><cell>0.181</cell><cell>0.884 0.965 0.984</cell></row><row><cell>HR-depth [25]</cell><cell>M</cell><cell>1024x320</cell><cell>0.106</cell><cell cols="2">0.755 4.472</cell><cell>0.181</cell><cell>0.892 0.966 0.984</cell></row><row><cell>Feat-Depth [31]</cell><cell>M</cell><cell>1024x320</cell><cell>0.104</cell><cell cols="2">0.729 4.481</cell><cell>0.179</cell><cell>0.893 0.965 0.984</cell></row><row><cell>Guizilini [14]</cell><cell cols="2">M+Se 1280x384</cell><cell>0.100</cell><cell cols="2">0.761 4.270</cell><cell>0.175</cell><cell>0.902 0.965 0.982</cell></row><row><cell>DIFFNet</cell><cell>M</cell><cell>1024x320</cell><cell>0.097</cell><cell cols="2">0.722 4.345</cell><cell>0.174</cell><cell>0.907 0.967 0.984</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation Studies. MF: Multi-stage Fusion. CA: Channel-wise Attention. SA: Space-wise Attention. Red check marks identify our final system.</figDesc><table><row><cell>Method</cell><cell>Pre-train</cell><cell cols="4">Encoder Decoder MF CA SA Abs Rel Sq Rel RMSE RMSE log The lower the better</cell><cell>The higher the better ? 1 ? 2 ? 3</cell></row><row><cell>Baseline</cell><cell></cell><cell>0.124 0.108</cell><cell>0.990 0.799</cell><cell>5.158 4.609</cell><cell>0.202 0.186</cell><cell>0.858 0.952 0.974 0.888 0.963 0.982</cell></row><row><cell></cell><cell></cell><cell>0.119</cell><cell>0.937</cell><cell>4.905</cell><cell>0.198</cell><cell>0.867 0.955 0.979</cell></row><row><cell></cell><cell></cell><cell>0.105</cell><cell>0.817</cell><cell>4.593</cell><cell>0.183</cell><cell>0.893 0.964 0.982</cell></row><row><cell>DIFFNet</cell><cell></cell><cell>0.102</cell><cell>0.764</cell><cell>4.483</cell><cell>0.180</cell><cell>0.896 0.965 0.983</cell></row><row><cell></cell><cell></cell><cell>0.107</cell><cell>0.822</cell><cell>4.637</cell><cell>0.183</cell><cell>0.890 0.963 0.983</cell></row><row><cell></cell><cell></cell><cell>0.103</cell><cell>0.769</cell><cell>4.530</cell><cell>0.180</cell><cell>0.892 0.964 0.983</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Quantitative results on the challenging KITTI examples. 260, 330, 374, 377, 385, 386, 388, 394, 395, 477, 504, 518, 548, 549, 559, 683. Those from ours are bold and common hard cases are red. The corresponding images are shown in the supplementary material.</figDesc><table><row><cell>Method</cell><cell>Parameters</cell><cell>Run-time FPS</cell><cell cols="4">lower is better Abs Rel Sq Rel RMSE RMSE log</cell><cell>higher is better ? 1 ? 2 ? 3</cell></row><row><cell>Monodepth2 [11]</cell><cell>14.84M</cell><cell>99</cell><cell>0.213</cell><cell>2.197</cell><cell>6.468</cell><cell>0.295</cell><cell>0.741 0.906 0.950</cell></row><row><cell>HR-Depth [25]</cell><cell>14.62M</cell><cell>116</cell><cell>0.205</cell><cell>1.591</cell><cell>5.726</cell><cell>0.282</cell><cell>0.738 0.902 0.957</cell></row><row><cell>DIFFNet</cell><cell>10.8M</cell><cell>87</cell><cell>0.197</cell><cell>1.803</cell><cell>5.988</cell><cell>0.282</cell><cell>0.763 0.912 0.957</cell></row></table><note>1 Indices in KITTI benchmark: 58, 68, 73, 106, 164, 173, 183,</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">? 2021. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>The research presented in this paper was carried out on the High Performance Computing Cluster supported by the Research and Specialist Computing Support service at the University of East Anglia.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Denao: Monocular depth estimation network with auxiliary optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizeng</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards scene understanding: Unsupervised monocular depth estimation with semantic-aware representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Yi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">S2r-depthnet: Learning a generalizable depth-specific structural representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuejin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Safenet: Selfsupervised monocular depth estimation with semantic-aware feature extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehoon</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongki</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghwan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changick</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Miami, FL</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Towards good practice for cnn-based monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoran</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning single camera depth estimation using dual-pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neal</forename><surname>Wadhwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Ansari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Digging into self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Forget about the lidar: Self-supervised depth estimators with med probability volumes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Luis</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munchurl</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Allan Raventos, and Adrien Gaidon. 3d packing for self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitor</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rares</forename><surname>Ambrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudeep</forename><surname>Pillai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semanticallyguided representation learning for self-supervised monocular depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitor</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
	<note>Rares Ambrus, and Adrien Gaidon</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Self-supervised monocular trained depth estimation using self-attention and discrete disparity volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Selfsupervised monocular depth estimation: Solving the dynamic object problem by semantic guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Aike</forename><surname>Term?hlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Fingscheidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Syndistnet: Self-supervised monocular fisheye camera distance estimation synergized with semantic segmentation for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Varun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senthil</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Yogamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Milz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Fingscheidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Comoda: Continuous monocular depth adaptation using past experiences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yevhen</forename><surname>Kuznietsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth learning in dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanhan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Casser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning (CoRL)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hr-depth: High resolution self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyang</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Boosting monocular depth estimation models to high-resolution via content-adaptive multi-resolution merging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mahdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Miangoleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Dille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yag?z</forename><surname>Aksoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On the uncertainty of self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filippo</forename><surname>Aleotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Towards robust monocular depth estimation: Mixing datasets for zero-shot crossdataset transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Lasinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention (MICCAI)</title>
		<editor>Nassir Navab, Joachim Hornegger, William M. Wells, and Alejandro F. Frangi</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Feature-metric loss for selfsupervised learning of depth and egomotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiang</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Self-supervised joint learning framework of depth estimation via implicit cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianrong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuewei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09876</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions on Image Processing (TIP)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The temporal opportunist: Self-supervised multi-frame monocular depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Prisacariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Firman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Joon-Young Lee, and In So Kweon. Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongchan</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Inferring distributions over depth from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gengshan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiyun</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Constant velocity constraints for self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Greenwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Visual Media Production (CVMP)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The edge of depth: Explicit constraints between segmentation and depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjie</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrick</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
