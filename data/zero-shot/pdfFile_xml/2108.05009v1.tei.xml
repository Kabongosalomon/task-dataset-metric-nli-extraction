<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Deep Mul-timodal Feature Representation with Asymmetric Multi-layer Fusion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-10-12">2020. October 12-16, 2020. October 12-16, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Acm Reference Format: Yikai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anbang</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yao</surname></persName>
						</author>
						<title level="a" type="main">Learning Deep Mul-timodal Feature Representation with Asymmetric Multi-layer Fusion</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 28th ACM International Conference on Multimedia (MM &apos;20)</title>
						<meeting>the 28th ACM International Conference on Multimedia (MM &apos;20) <address><addrLine>Seattle, WA, USA MM &apos;20; Seattle, WA, USA</addrLine></address>
						</meeting>
						<imprint>
							<date type="published" when="2020-10-12">2020. October 12-16, 2020. October 12-16, 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3394171.3413621</idno>
					<note>. ACM, New York, NY, USA, 9 pages. ACM ISBN 978-1-4503-7988-5/20/10. . . $15.00</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS ? Computing methodologies ? Computer vision tasks</term>
					<term>Scene understanding</term>
					<term>Computer vision representations KEYWORDS Multimodal Learning</term>
					<term>Compact Network Design</term>
					<term>Bidirectional Fu- sion</term>
					<term>Asymmetric Operations</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a compact and effective framework to fuse multimodal features at multiple layers in a single network. The framework consists of two innovative fusion schemes. Firstly, unlike existing multimodal methods that necessitate individual encoders for different modalities, we verify that multimodal features can be learnt within a shared single network by merely maintaining modality-specific batch normalization layers in the encoder, which also enables implicit fusion via joint feature representation learning. Secondly, we propose a bidirectional multi-layer fusion scheme, where multimodal features can be exploited progressively. To take advantage of such scheme, we introduce two asymmetric fusion operations including channel shuffle and pixel shift, which learn different fused features with respect to different fusion directions. These two operations are parameter-free and strengthen the multimodal feature interactions across channels as well as enhance the spatial feature discrimination within channels. We conduct extensive experiments on semantic segmentation and image translation tasks, based on three publicly available datasets covering diverse modalities. Results indicate that our proposed framework is general, compact and is superior to state-of-the-art fusion frameworks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Understanding complex visual scenes is an essential prerequisite for robots and autonomous vehicles to operate in real-world. With the increasing availability of multimodal sensors, fusing information collected by these sensors has shown improved performance on several tasks, like scene recognition, semantic segmentation, etc. In typical multimodal settings, RGB and depth inputs have been widely used to date. Besides, other multimodal inputs such as normals, shadings, textures and edges, are also discussed to some extent. It has been largely verified that the way to effectively fuse multimodal features is essential to the final performance. In this work, we mainly focus on multimodal inputs which are aligned in pixellevel (e.g., RGB, depth and shade), and tackle two drawbacks of existing multimodal fusion works relying on deep neural networks, by proposing two innovative fusion schemes.</p><p>Firstly, existing multimodal training methods follow a common design practice that an individual encoder branch is specialized for each modality. For example, regarding to the semantic segmentation task, FuseNet <ref type="bibr" target="#b12">[13]</ref>, RDFNet <ref type="bibr" target="#b16">[17]</ref>, SSMA <ref type="bibr" target="#b27">[28]</ref> adopt two equal-sized encoders for RGB and depth inputs respectively. The underlying reason may be that for different modalities, different characteristics and feature statistics are not compatible in a single model. However, despite of the heavy parameter load, it prevents the possibility of multimodal features to implicitly fuse during joint training. To tackle this issue, we find that individual encoders are not necessary for multimodal inputs as long as Batch Normalization layers (BNs) <ref type="bibr" target="#b15">[16]</ref> are privatized. Specifically, we share all convolutional filters in both encoder and decoder, but adopt modality-specific BNs in the encoder. Modality-specific BNs estimate the channel-wise running mean and variance of activations for each modality separately, and also learn individual channel-wise scale and bias. We empirically verify the effectiveness of this scheme on various modalities, including RGB, depth, normal, shade, etc. On the one hand, this training scheme largely reduces the number of parameters needed for multimodal training. On the other hand, it allows a single network to exploit multimodal features simultaneously, which improves the generalization of convolutional neural networks and achieves better training performance in practice.</p><p>Secondly, key ingredients of multimodal fusion include how to design fusion blocks and where to implement fusion. It is verified that exploiting multi-layer fusion, i.e., fusing multimodal features at multiple stages of the network, will improve the fusion performance. In early multimodal fusion works, fusion could be simply realized by feature concatenation, addition or average. Recently, to enable more powerful feature alignment, some multi-layer fusion works adopt a pile of 3 ? 3 convolutional layers <ref type="bibr" target="#b16">[17]</ref> or attentionbased designs <ref type="bibr" target="#b27">[28]</ref> for fusion. However, as we will explain, these fusion methods tend to learn symmetric features when followed by a pointwise convolutional layer. This can be simply understood as A?B and B?A fusion leading to the same expressive ability of feature maps (regardless of the order of channels). In this work, we propose a bidirectional fusion scheme, enabling more sufficient multimodal feature fusion. We argue that although existing symmetric fusion methods are suitable for the unidirectional fusion, they are not very compatible with the bidirectional fusion. Besides, along with the emergence of powerful yet complex fusion blocks, increasing amounts of parameters are introduced when fusing multimodal features at multiple layers. To fit the bidirectional fusion scheme, we propose two brand-new asymmetric multimodal fusion operations. Being a fusion in the cross-channel direction, the channel shuffle operation strengthens the multimodal feature interactions across channels, improving the holistic feature representation ability. Being a fusion in the spatial direction within each channel, the pixel shift operation tends to enhance spatial feature discrimination, capturing fine-grained info at object edges, especially for small and thin objects. Both channel shuffle and pixel shift are plug-in and parameter-free operations.</p><p>We apply our schemes to two tasks including semantic segmentation and image translation. Our work is verified on three different datasets containing diverse application scenarios ranging from urban city driving scenes to indoor scenes, covering rich modalities including RGB, depth, shade, normal, texture, and edge. Different network architectures containing ResNet <ref type="bibr" target="#b13">[14]</ref>, Xception65 <ref type="bibr" target="#b6">[7]</ref> and U-Net <ref type="bibr" target="#b26">[27]</ref> are adopted as backbones. Experimental results prove the effectiveness and generalization of our proposed schemes.</p><p>Main contributions of this work can be summarized as follows:</p><p>? We verify that multimodal inputs can be fed into a single network with shared parameters and individual BNs for each modality in the encoder, achieving even higher performance than the common practice which uses individual networks. ? We propose two asymmetric parameter-free fusion operations, enabling bidirectional multi-layer fusion from both channel-level and pixel-level perspectives. These operations strengthen the multimodal feature interactions across channels as well as enhance the spatial feature discrimination. ? By merely introducing about 0.1% additional parameters on a given unimodal network, our fusion method is able to outperform state-of-the-art fusion methods on several datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>Multimodal Fusion. Methods to exploit multimodal information have been studied for decades <ref type="bibr">[11-13, 20, 29]</ref>, which allow better understanding of visual scenes compared to learning with unimodal inputs. Prior works usually rely on hand engineered or learned features extracted from each individual modality and combine features together with designed fusion structures. In <ref type="bibr" target="#b28">[29]</ref>, Markov random fields are explored for indoor segmentation based on RGB and depth inputs. <ref type="bibr" target="#b10">[11]</ref> improves RGB-D recognition performance by making use of the constructed geometric contour from depth data. More recently, with the success of deep convolutional neural networks, a series of multimodal fusion schemes are proposed for end-toend feature fusion. Regarding to the fusion position, these works can be categorized into single-layer fusion and multi-layer fusion methods. In schemes of single-layer fusion, multimodal features are usually merged into one branch at a particular layer. For example, <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> stack multimodal inputs by channel-wise concatenation and then feed them to the network. <ref type="bibr" target="#b29">[30]</ref> designs a transformation layer which fuses multimodal features between the encoder and decoder. <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">24]</ref> explore multimodal feature fusion at the prediction side. However, it has been verified that single-layer fusion methods can not effectively exploit multimodal features, especially for addressing high-resolution predictions <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b27">28]</ref>. Besides, in <ref type="bibr" target="#b34">[35]</ref>, it shows that single-layer fusion is sensitive to the noises in multimodal data.</p><p>Owing to these factors, multi-layer fusion methods become popular, which combine multimodal features at multiple levels, usually at every downsampling stage of a network. Existing multimodal multi-layer fusion schemes can be further classified into two kinds. The first kind is to directly send the fused features to the decoder side. RDFNet <ref type="bibr" target="#b16">[17]</ref> adopts multi-layer fusion at four downsampling stages of the ResNet (encoder), iteratively refines the fused features with the similar idea in RefineNet <ref type="bibr" target="#b21">[22]</ref>, and then sends these fused features to the decoder. SSMA <ref type="bibr" target="#b27">[28]</ref> fuses multimodal features at mid-level and high-level with an attention-based mechanism for feature calibration, and as well sends the fused features to the decoder side. However, this kind of fusion methods prevents the encoder to exploit multimodal features. The second kind is to apply fused features to one of the branches in the encoder for in-depth feature exploiting. Typical works can be traced back to FuseNet <ref type="bibr" target="#b12">[13]</ref>, which trains two individual branches to learn RGB and depth features, where multiple skip connections in the encoder from the depth branch to RGB branch are used for fusion. <ref type="bibr" target="#b34">[35]</ref> also adopts two branches for learning RGB-D features respectively, and merges depth features into RGB branch at all downsampling layers in a hierarchical manner. This kind of methods is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref> (a), which will be also discussed later. Such scheme is unidirectional and straightforward, still lacking rich feature interactions and thus may be not sufficient for fusion. To tackle this drawback, we design a bidirectional fusion scheme to improve fusion performance. Shuffle and Shift. In group convolutions <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15]</ref>, outputs that correspond to each channel are only related with a portion of input channels. To address this issue, channel shuffle is presented in ShuffleNet <ref type="bibr" target="#b35">[36]</ref> to enhance the information flow across different input channel groups, so that each channel is correlated with all groups. Inspired by ShuffleNet, a recent work <ref type="bibr" target="#b24">[25]</ref> allocates each frame feature into groups and then aggregates the grouped features via temporal shuffle operation. To date, the proposed shuffle operations are mostly adopted for strengthening correlations among different groups. In this work, we propose to use the channel shuffle operation for multimodal fusion, partially aiming to promote feature interaction among multimodal features, but also to make use of its asymmetric property, which will be analyzed in Section 3.2. There are also some research works that apply shift operations, which are related to our design. Early in <ref type="bibr" target="#b31">[32]</ref>, stacking pixel shift layers is treated as parameter-free and FLOP-free operations to improve spatial information communication. The shift design is further improved in <ref type="bibr" target="#b4">[5]</ref>, where only a few shift operations are needed, instructed by shift operation penalty and quantization-aware shift learning method. Also in <ref type="bibr" target="#b22">[23]</ref>, a temporal shift module is proposed to shift a portion of channels along the temporal dimension, aiming to boost information exchange among neighboring frames. In this work, we extend the idea of pixel shift to facilitate spatial correlations among multimodal features, and again, we point out that the shift operation can be another asymmetric fusion operation which is compatible with the proposed bidirectional fusion scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">APPROACH</head><p>In this section, we propose two multimodal fusion schemes. The first is a parameter-sharing scheme which compresses the model size and enables implicit feature fusion. The second is a bidirectional fusion scheme which enables explicit and sufficient feature fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Parameter-sharing Scheme</head><p>As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>(a), we provide a multimodal fusion scheme with two input modalities as an example. Unlike existing works which necessitate individual encoders for multiple modalities, we propose that by sharing convolutional parameters but leaving Batch Normalization layers (BNs) <ref type="bibr" target="#b15">[16]</ref> modality-specific, we are able to train a single network for multiple modalities. In modern deep neural networks, the mechanism of using BNs has become one of the most successful architectural innovations. A BN layer whitens activations over a mini-batch of features, and transforms the whitened results with channel-wise affine parameters, including scale and bias which provide the possibility of linearly transforming whitened activations to any scales. Sharing network parameters but privatizing BNs has been proved to be effective for efficient model adaption when considering multiple tasks or multiple domains <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33]</ref>. Inspired by this, we extends the idea of privatizing BNs for multimodal training, where activation statistics of different modalities are normalized separately, and channel-wise scale and bias of BNs are also learned individually for each modality. Network parameters apart from BNs are shared for all modalities. Specifically, assuming there are modalities for fusion, for the th modality, where ? {1, 2, ? ? ? , }, privatizing BN can be formulated as:</p><formula xml:id="formula_0">= ? ? ?? 2 + + ,<label>(1)</label></formula><p>where , ? R ? ? ? are the input and output feature maps of the th modality respectively; , , , denote the batch size, the number of channels, the height and width of the feature map respectively; , 2 ? R are the mean and standard deviation values of input activations over the current mini-batch of the th modality, calculated by = 1 ,?, and 2 = 1 ,?, ( ? ) 2 ; besides, , ? R are learnable scale and bias with respect to the th modality; is a small constant to avoid division by zero.</p><p>Note that the parameter-sharing indicates sharing all convolutional filters in both encoder and decoder, but privatizing BNs indicates using modality-specific BNs merely in the encoder. We find sharing BNs in the decoder part achieves better results especially for the multimodal image translation task.  In order to take full advantage of this scheme, we need to design new asymmetric fusion methods.</p><p>This training scheme largely compresses model parameters for multimodal fusion. Besides that, learning with shared parameters facilitates interior interactions of multimodal features, which even brings performance improvements. Verification results will be discussed in <ref type="table">Table 4</ref>, where nearly 50% parameters reduction is obtained with even higher evaluation scores compared with using individual encoders. Such paramter-sharing scheme is not limited to two modalities, which will be verified in <ref type="table" target="#tab_3">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Bidirectional Fusion Scheme</head><p>As described in Section 2, early works usually fuse multimodal features at one particular layer, while it has been recently verified that multi-layer fusion (fuse at multiple layers) can exploit supplementary information more adequately. Regarding typical multi-layer fusion works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b34">35]</ref>, multimodal features are usually fused into one branch and are then further exploited by the encoder. The scheme can be shown as <ref type="figure" target="#fig_1">Figure 2</ref>(a), where features learned from the second branch are merged into the first branch. This scheme allows the first branch of encoder to exploit fused features. However, as the fusion is unidirectional, the second branch remains unimodal from beginning to end and thus cannot bring informative features at later fusion layers. Besides, two branches would be highly unbalanced during training, and such unbalance may impact the fusion  performance. In our experiments, we find for the unidirectional fusion scheme, different fusion directions lead to very different performance. From this point of view, it would be worthwhile to design a new kind of multi-layer fusion that can overcome the aforementioned drawback, and improve the fusion performance. To this end, we propose a bidirectional fusion scheme for multi-layer fusion, illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>(b). In this scheme, multimodal features of different encoder branches are merged mutually, enabling rich feature interactions. However, we find commonly used fusion operations, such as concatenation and average, are not very compatible with the bidirectional fusion scheme. To make it clear, we provide a following definition using two modalities as an example.</p><formula xml:id="formula_1">! " ! # $ ! " $ ! # ! " ! # ! " ! # $ ! " = ! " + ! # $ ! # = ! # + ! "</formula><p>Definition. Let F (?, ?; ) be a fused feature of two modalities, where denotes the internal parameters of the fusion block. Let C be a single pointwise convolutional layer. We define a fusion block as symmetric if for any 1 , C 1 , there exist 2 , C 2 s.t. C 1 (F ( 1 , 2 ; 1 )) = C 2 (F ( 2 , 1 ; 2 )), holding for any two feature maps 1 , 2 . We define a fusion block as asymmetric if the definition of symmetric fusion does not hold.</p><p>The reason for introducing a convolutional layer C into the definition is because in practice, most fused features are followed by a convolutional layer which further mixes features along the channel. Regarding common fusion methods, it is obvious that addition and average operations are symmetric. The concatenation operation can be proved symmetric when exchanging the order of the same-length outputs for 1 and 2 , which can be realized by their following C 1 and C 2 . For these three fusion methods, internal fusion parameters = ?. Recently proposed fusion methods that apply internal convolutional layers, i.e., ? ?, for example attentionbased fusion blocks in SSMA <ref type="bibr" target="#b27">[28]</ref>, and MMF blocks in RDFNet <ref type="bibr" target="#b16">[17]</ref>, can also be proved symmetric when 2 is obtained by exchanging two groups of modality-specific parameters of 1 . These commonly used symmetric fusion methods are not very compatible with the bidirectional fusion scheme. See <ref type="figure" target="#fig_1">Figure 2(b)</ref>, during multimodal training, as both final outputs are usually applied with the same supervision signals, features fused by symmetric fusion methods at both branches tend to learn similar representations (potentially can be the same). This would bring redundant information at both encoder branches.</p><p>Based on the above analysis, we propose two kinds of asymmetric fusion operations to fit the bidirectional fusion scheme, called AsymFusion. These operations include channel shuffle and pixel shift, which are both parameter-free. We first introduce the designs of both operations, and show how to integrate them into popular network architectures.</p><p>Channel Shuffle. To strengthen the interaction of multimodal information flow across channels, we propose the channel shuffle operation. As illustrated in <ref type="figure" target="#fig_3">Figure 3</ref>(a), when given two feature maps 1 , 2 ? R ? ? , channel shuffle fuses two features by exchanging features corresponding to a portion of channels. Given a channel spilt point , satisfying 1 &lt; &lt; , ? Z, the channel shuffle operation can be described as:</p><formula xml:id="formula_2">F ( 1 , 2 ) = 1 [1, ? ? ? , ] || 2 [ + 1, ? ? ? , ], F ( 2 , 1 ) = 2 [1, ? ? ? , ] || 1 [ + 1, ? ? ? , ],<label>(2)</label></formula><p>where || indicates channel-wise concatenation; 1, ? ? ? , and + 1, ? ? ? , indicate channel indices, which in our experiments, make up 70%, 30% channels respectively. According to this formulation, no additional parameters are introduced, i.e., = ?. An essential property of the shuffle operation leading to its asymmetry is that, two features after shuffle have no feature overlaps. Hence, it is straightforward to find input feature maps 1 , 2 and C 1 , such that no C 2 is able to result in the same features, which means fusion by channel shuffle is asymmetric.</p><p>Pixel Shift. To improve spatial information communication of multimodal features, we introduce the second asymmetric fusion operation which uses pixel shift on feature maps, as shown in <ref type="figure" target="#fig_3">Figure 3</ref>(b). Fusion by pixel shift contains two steps. Supposing the number of channels is divisible by 4. For the first step, we divide the feature into four groups, each having /4 channels, and shift one pixel for every group with one of four spatial directions. This operation can be formulated as:</p><formula xml:id="formula_3">? 1 [ , ?, ] = O ( 1 ) [ , ? + + 1, + + 1], ? 2 [ , ?, ] = O ( 2 ) [ , ? + + 1, + + 1],<label>(3)</label></formula><p>where O (?) indicates a zero padding; , are position indicators, = [0, ?1, 0, 1] ? /4? , = [?1, 0, 1, 0] ? /4? ; and adding one pixel on position indicators is due to the zero padding.</p><p>For the second step, each fused feature is obtained by adding the original feature and the shifted feature of the other modality:</p><formula xml:id="formula_4">F ( 1 , 2 ) = 1 + ? 2 , F ( 2 , 1 ) = 2 + ? 1 ,<label>(4)</label></formula><p>again, the shift operation brings no additional parameters, i.e., = ?.</p><p>To illustrate that the shift operation is asymmetric, we simply let 1 = 0, 2 ? 0. For any C 1 , if the definition of symmetric fusion is satisfied, there should always exist C 2 such that C 1 ( ? 2 ) = C 2 ( 2 ). However, this statement cannot be true, as a shifted feature no longer keeps its original pixel alignments across channels.</p><p>Both fusion operations introduce no additional parameters, and are also FLOP-efficient. They not only have advantages on asymmetry, but also introduce channel-wise and spatial-wise feature interactions across modalities, respectively, making them more effective for multimodal multi-layer fusion. In Section 4.4, we will show that adopting shift operations in a network may improve predictions for fine-grained objects, and strengthen the representation ability to discriminate rich edge-aware information from context.</p><p>Since we have presented two asymmetric fusion operations that can be compatible with the bidirectional fusion scheme, we now consider integrating them into convolutional networks. We adopt residual blocks of ResNet <ref type="bibr" target="#b13">[14]</ref> as an example. As shown in <ref type="figure" target="#fig_3">Figure  3</ref>(c), we insert both fusion operations into the residual blocks of ResNet. As the first Conv1 ? 1 layer in each residual block performs compression on channel dimension, we speculate that adding shuffle and shift between the two Conv1 ? 1 layers where features have less channels, will lead to lower information loss during fusion. Specifically, we choose to apply two shuffle operations, for a more sufficient channel fusion, after the first Conv1 ? 1 layer and before the last Conv1 ? 1 layer respectively. Besides, we insert the shift operations after the first shuffle. We empirically find that adding the shifted features to features after the Conv3?3 layers will lead to better performance. Although the pixel shift operation constantly shifts one pixel on a feature map, the corresponding shift of the original image will vary according to the feature map size. To mix different extents of side effects caused by pixel shifts, we apply our fusion approaches at every downsampling stage of the network. Besides fusing at downsampling stages, we also try applying shuffle and shift to other layers but do not observe further improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In order to verify the generalization of our proposed schemes, we conduct experiments on two tasks, including semantic segmentation and image translation. The benchmark is performed on three datasets with diverse environments ranging from urban city scenes to indoor scenes, covering a variety of different modalities, including RGB, depth, shade, normal, texture, and edge. Besides, we apply the proposed fusion structures on different network architectures, such as ResNet, Xception65 and U-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We consider two indoor datasets NYUDv2 <ref type="bibr" target="#b28">[29]</ref>, Taskonomy <ref type="bibr" target="#b33">[34]</ref>, and an outdoor dataset CityScapes <ref type="bibr" target="#b7">[8]</ref>.</p><p>NYUDv2 is one of the most popular RGB-D datasets in the literature for indoor scene labeling, containing 1449 densely labeled pairs of RGB and depth frames. Following the standard settings, we use 795 training RGB-D pairs and 654 testing RGB-D pairs, and we evaluate our network on 40 classes with labels provided by <ref type="bibr" target="#b10">[11]</ref>.</p><p>Cityscapes is a RGB-D dataset for street scene understanding which becomes one of the standard benchmarks. The dataset contains images from 50 different cities, during varying seasons, lighting and weather conditions. The dataset provides 2875 images for training and 500 images for validation. We do not use the supplementary training data with coarse annotations.</p><p>Taskonomy is a large-scale dataset for indoor scene understanding, where each image has other corresponding modalities such as depth, normal, shade, texture, etc., with detailed annotations. We adopt its official sub-dataset containing 9000 samples. As there are no public papers or benchmarks that provide train-test splits to now, we randomly divide the data into 8000 samples for training and the rest 1000 samples for testing, and guarantee no scene overlaps in training and testing samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>For semantic segmentation tasks, in order to facilitate comparison with previous fusion approaches, we choose RefineNet <ref type="bibr" target="#b20">[21]</ref> for the dataset NYUDv2 and DeepLabv3+ <ref type="bibr" target="#b3">[4]</ref> for the dataset Cityscapes, with backbone architectures ResNet <ref type="bibr" target="#b13">[14]</ref> and Xception65 <ref type="bibr" target="#b6">[7]</ref> respectively. For ResNet architectures, we implement the fusion blocks at all downsampling stages. To make the structure clear, we provide a table showing structure details in supplementary materials. For Xception65, we apply our fusion designs to the last blocks of the entry flow, middle flow and exit flow respectively. As a standard evaluation method, We apply test-time multi-scale evaluation for all experiments and obtain final predications as average results. For the image translation task, following pix2pix, we adopt a U-Net <ref type="bibr" target="#b26">[27]</ref> generator, with an eight-layer encoder and an eight-layer decoder. We adopt multi-layer fusion and apply the fusion with shuffle and shift at the 3 rd , 5 th , 7 th layers of the encoder respectively. We use a discriminator with five convolutional layers for downsampling.</p><p>Parameter sharing strategies have been described in Section 3.1. Specifically, for both semantic segmentation and image translation tasks, we share all convolutional parameters and privatize BNs for different modalities in the encoder; we directly share all parameters including BNs in the decoder. For the semantic segmentation task, we learn an ensemble at the final predictions to further fuse multimodal prediction scores. For modalities, the ensemble is learned using a group of importance scores = [ 1 2 ? ? ? ], satisfying ? 0, =1 = 1, which can be easily implemented with a softmax function. We then adopt the technique of knowledge distillation to force predications of different modalities to mimic the learned ensemble prediction. We find this strategy brings additional improvements for multimodal fusion with negligible extra costs (only extra parameters).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB input</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Depth input</head><p>Multimodal prediction by symmetric fusion</p><p>Multimodal prediction by our asymmetric fusion Ground truth <ref type="figure">Figure 4</ref>: Illustrative results of semantic segmentation on Cityscapes dataset. In the third column, we choose the attention-based fusion method as the baseline, which achieves state-of-the-art performance as shown in <ref type="table" target="#tab_2">Table 2</ref>. We provide results predicted by our asymmetric fusion method in the fourth column. Regions with sharp predication differences are indicated with white frames. Best viewed in color at zoom 300%.</p><p>More implementation details, including learning rate and epoch settings, and the method to extend our bidirectional fusion scheme to more modalities, are provided in supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>Semantic Segmentation. We report results on both NYUDv2 and Cityscapes datasets in <ref type="table" target="#tab_0">Table 1</ref> and <ref type="table" target="#tab_2">Table 2</ref> respectively. Besides commonly used metrics for semantic segmentation, including pixel accuracy, mean accuracy, and intersection over union (IoU), we also compare total parameters of models. In <ref type="table" target="#tab_0">Table 1</ref>, we compare our AsymFusion with four state-of-the-art methods. For a quick comparison with RDFNet, which is a RGB-D fusion scheme using RefineNet as its decoder. Using the same encoder ResNet101, total parameters of RDFNet (366.7M) are much larger than RefineNet (118.1M), making it unfriendly for model deployment. In sharp contrast, when given RefineNet (ResNet101) as the unimodal architecture, our fusion model only has 118.2M total parameters, with less than 0.1M additional parameters than RefineNet, and less than 1/3 parameters compared with RDFNet. Similarly, using ResNet152 as encoder, our model merely introduces 0.11% additional parameters compared with RefineNet. Under this extremely compact setting, our fusion models still outperform other methods with a large margin (over 1% absolute gain for IoU). In <ref type="table" target="#tab_2">Table 2</ref>, we compare our fusion method to state-of-the-art methods on Cityscapes validation dataset, and our fusion method outperforms previous methods. We do not use the supplementary training set including 20,000 coarse annotations, considering the training costs. Illustrative results shown in <ref type="figure">Figure  4</ref> verify that our model captures fine-grained details.</p><p>Image Translation. We benchmark on the dataset Taskonomy for performing image translation tasks, largely due to its data variety. In this part, we consider a wide range of modalities including depth, normal, shade, texture and edge, and aim to translate these data to RGB. Since there are no published methods of multimodal image translation that has such a data variety to date, we implement two commonly used fusion methods (concatenation and average) and two recently proposed fusion methods (attention and MMF). These four methods are treated as baselines for comparison. we adopt Fr?chet Inception Distance (FID) score as the evaluation metric. FID compares the statistics of generated images against real images, by fitting a Gaussian distribution to the hidden activations of InceptionNet for compared images and compute Wasserstein-2 distance between these distributions. A lower FID score is better, indicating the generated images are more similar to real counterparts. In <ref type="table" target="#tab_3">Table 3</ref>, we provide results comparison for cases with two modalities and three modalities. For different combinations of modalities, our fusion models are consistently better than other methods. Results also indicate that our method can be extended to more modalities and maintain competitive performance. In <ref type="figure">Figure 5</ref>, we provide illustrative examples for comparing different predicted results, where our proposed method outperforms the others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Studies</head><p>Importance of Using Private BNs. A quick question is that, what is the benefit of privatizing BNs? In <ref type="table">Table 4</ref>, we compare three types of parameter sharing strategies in encoder. We adopt RefineNet (ResNet101), and conduct comparison experiments on NYUDv2 dataset. We obtain two conclusions from the results. Firstly, sharing  <ref type="figure">Figure 5</ref>: Image translation with two modalities as inputs, tested on Taskonomy dataset. We provide the result predicted by each single modality respectively, and compare the multimodal fusion performance using symmetric fusion and our asymmetric fusion. For symmetric fusion method, we adopt attention-based method, as it achieves the best performance in <ref type="table" target="#tab_3">Table 3</ref> apart from AsymFusion. The asymmetric method (AsymFusion) contains both shuffle and shift operations. Regions with sharp predication differences are indicated with red frames.  even slightly higher performance, yet largely reduces total parameters. Such finding would be instructive for multimodal learning. Components Analysis of AsymFusion. We mainly design AsymFusion using two components, channel shuffle and pixel shift. Besides, we apply knowledge distillation for additional performance improvements, mentioned in Section 4.2. In this part, we verify the importance of these parts. Results shown in <ref type="table" target="#tab_6">Table 5</ref> indicate that shuffle and shift both play importance roles for the fusion performance, which together bring over 3% IoU improvements. The knowledge distillation process further shows slight effects, with about 0.3% IoU improvements.  <ref type="figure">Figure 6</ref>: Illustrative semantic segmentation results on Cityscapes dataset, for comparing predictions without and with shift operations. Depth inputs are also used for prediction but are not shown. Red frames indicate regions for prediction, and prediction errors are highlighted in each prediction image. Results show that shift operations may benefit the prediction at small and distant objects.</p><p>Benefits of Using Shift Operations. In <ref type="figure" target="#fig_3">Figure 3</ref>, we describe how to integrate pixel shift operations into residual blocks. When integrating shift operations, there are two additional skip connections across two branches. A question arises, is the improvement actually brought by these additional skip connections instead of shift operations? To figure it out, we keep these skip connections and only remove shift operations. The numerical results are 75.5%/62.7%/49.4%, much lower than using shift operations (76.4%/63.1%/50.5%, see <ref type="table" target="#tab_6">Table 5</ref>). Illustrative results of predictions without and with shift operations are shown in <ref type="figure">Figure 6</ref>. By comparison, predictions with shift operations tend to be better at capturing details, including thin and small objects, e.g., poles, distant persons and vehicles.</p><p>Comparison of Fusion Directions. In <ref type="table">Table 6</ref>, we verify that for multi-layer fusion, the direction of fusion has impact on the fusion performance. For all reported fusion methods, we observe that fusion from depth branch to RGB branch shows better results than fusion with the opposite direction. Applying symmetric fusion methods to the bidirectional scheme shows minor drops than unidirectional counterparts sometimes. We conjecture that symmetric fusion methods tend to have similar feature representation and thus are not very compatible with bidirectional fusion, as described in Section 3.2. Our proposed asymmetric fusion method presents large performance gains when using bidirectional fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We present a compact and effective multimodal fusion framework. To start, we propose that multimodal training can be realized in one single network with modality-specific BNs, enabling implicit fusion via joint feature representation training. Regarding fusion methods, to make advantage of the bidirectional fusion scheme, we propose channel shuffle and pixel shift operations that are asymmetric with <ref type="table">Table 4</ref>: Exploration of different parameter sharing strategies for the encoder, based on NYUDv2 and Cityscapes datasets. We compare three strategies including using individual networks (widely adopted by existing multimodal works), sharing convolutional parameters and BNs, and sharing only convolutional parameters with individual BNs. We report pixel accuracy (%) and mean IoU (%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Parameter sharing strategy Pixel acc. IoU #Params.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NYUDv2</head><p>Individual   <ref type="table">Table 6</ref>: Results comparison of symmetric fusion methods and the proposed asymmetric fusion method. We compare two settings, i.e., using individual / shared convolutional parameters. Note that individual BNs are adopted for all experiments in this table. Symmetric fusion methods include concatenation, average and attention. The fusion operation addition is omitted as it performs likely to the average operation. We report pixel accuracy (%) / mean IoU (%). respect to fusion directions. These operations strengthen the interaction of multimodal information flow, and tend to improve feature representation ability to discriminate rich edge-aware information from context. Experimental results on several tasks indicate that our fusion scheme outperforms state-of-the-art counterparts, with only about 0.1% additional parameters than a given unimodal network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Params. Fusion direction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>This work is jointly supported by the National Science Foundation of China (NSFC) and the German Research Foundation (DFG) in project Cross Modal Learning, NSFC 61621136008/DFG TRR-169.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) A compact multimodal fusion scheme, with shared parameters for convolutional layers (also for fully-connected layers, if any) and individual BN parameters. (b) A comparison of total parameters for feature encoding between existing multimodal fusion schemes and ours. With the increasing of total modalities, the size of our scheme is nearly unchanged. Bidirectional fusion (with asymmetric fusion blocks)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>(a) Existing multimodal multi-layer fusion schemes mostly adopt unidirectional fusion. (b) Our proposed bidirectional fusion scheme enables each branch to exploit multimodal features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) Channel shuffle operation (b) Pixel shift operation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Proposed two asymmetric multimodal fusion operations that are compatible with bidirectional fusion scheme. In the figure, we use a hat sign to represent the fused feature, i.e.,<ref type="bibr" target="#b0">1</ref> = F ( 1 , 2 ), 2 = F ( 2 , 1 ). Best be viewed in color. (a) Channel shuffle operation exchanges a portion of features with respect to the same-indexed channels. (b) Pixel shift operation performs pixel-wise shifts on four directions, and uses the addition across multimodal features for fusion. (c) We integrate both operations into residual blocks of ResNet architectures for illustration. Blocks in color and dashed lines indicate our inserted structures. Note that the colors of blocks are consistent with the colors of arrows in the first two subfigures, for better understanding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Semantic segmentation results comparison on NYUDv2 dataset. Baseline methods mostly adopt RefineNet as the unimodal network, except for SCN. For comparison convenience, we also apply our fusion methods on RefineNet, with two backbone architectures ResNet101 and ResNet152 respectively. We report pixel accuracy (%), mean accuracy (%) and mean IoU (%).</figDesc><table><row><cell>Method</cell><cell cols="6">Data modality Backbone Pixel acc. Mean acc. IoU #Params.</cell></row><row><cell>RefineNet [21]</cell><cell>RGB</cell><cell>ResNet101</cell><cell>73.8</cell><cell>58.8</cell><cell cols="2">46.4 118.10M</cell></row><row><cell>RefineNet [21]</cell><cell>RGB</cell><cell>ResNet152</cell><cell>74.4</cell><cell>59.6</cell><cell cols="2">47.6 133.74M</cell></row><row><cell>CFN [19]</cell><cell>RGB-D</cell><cell>ResNet152</cell><cell>-</cell><cell>-</cell><cell>47.7</cell><cell>-</cell></row><row><cell>SCN [20]</cell><cell>RGB-D</cell><cell>ResNet152</cell><cell>-</cell><cell>-</cell><cell>49.6</cell><cell>-</cell></row><row><cell>RDFNet [17]</cell><cell>RGB-D</cell><cell>ResNet101</cell><cell>75.6</cell><cell>62.2</cell><cell cols="2">49.1 366.71M</cell></row><row><cell>RDFNet [17]</cell><cell>RGB-D</cell><cell>ResNet152</cell><cell>76.0</cell><cell>62.8</cell><cell cols="2">50.1 398.00M</cell></row><row><cell>RefineNet  ?</cell><cell>RGB</cell><cell>ResNet101</cell><cell>73.8</cell><cell>59.0</cell><cell cols="2">46.5 118.10M</cell></row><row><cell>RefineNet  ?</cell><cell>Depth</cell><cell>ResNet101</cell><cell>64.0</cell><cell>45.6</cell><cell cols="2">34.3 118.10M</cell></row><row><cell>AsymFusion</cell><cell>RGB-D</cell><cell>ResNet101</cell><cell>76.6</cell><cell>63.5</cell><cell cols="2">50.8 118.20M</cell></row><row><cell>AsymFusion</cell><cell>RGB-D</cell><cell>ResNet152</cell><cell>77.0</cell><cell>64.0</cell><cell cols="2">51.2 133.89M</cell></row><row><cell cols="3">? indicates our re-implemented results</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Semantic</figDesc><table><row><cell></cell><cell cols="5">segmentation results on Cityscapes dataset using</cell></row><row><cell cols="6">19 semantic labels. We apply our fusion methods to DeepLabv3+</cell></row><row><cell cols="6">(Xecption65). We include recently proposed top-performing meth-</cell></row><row><cell cols="6">ods for comparison. Extra data indicates whether use additional</cell></row><row><cell cols="6">training dataset with coarse annotations for further improving</cell></row><row><cell cols="6">performance. We report mean IoU (%) as the evaluation metric.</cell></row><row><cell>Method</cell><cell cols="2">Data modality Extra data</cell><cell>Backbone</cell><cell>IoU</cell><cell>#Params.</cell></row><row><cell>PSPNet [37]</cell><cell>RGB</cell><cell>?</cell><cell>ResNet101</cell><cell>80.9</cell><cell>56.27M</cell></row><row><cell>DeepLabv3 [3]</cell><cell>RGB</cell><cell>?</cell><cell>ResNet101</cell><cell>79.3</cell><cell>58.16M</cell></row><row><cell>Mapilary [1]</cell><cell>RGB</cell><cell>?</cell><cell>WideResNet38</cell><cell>78.3</cell><cell>135.86M</cell></row><row><cell>DeepLabv3+ [4]</cell><cell>RGB</cell><cell>?</cell><cell>Xecption65</cell><cell>78.8</cell><cell>43.48M</cell></row><row><cell>DPC [2]</cell><cell>RGB</cell><cell>?</cell><cell>Xecption65</cell><cell>80.9</cell><cell>41.82M</cell></row><row><cell>DRN [38] AdapNet++ [28] SSMA [28]</cell><cell>RGB RGB RGB-D</cell><cell>? ? ?</cell><cell>WideResNet38 ResNet50 ResNet50</cell><cell>79.7 81.2 82.2</cell><cell>129.16M 30.20M 56.44M</cell></row><row><cell>DeepLabv3+  ?</cell><cell>RGB</cell><cell>?</cell><cell>Xecption65</cell><cell>79.4</cell><cell>43.48M</cell></row><row><cell>DeepLabv3+  ?</cell><cell>Depth</cell><cell>?</cell><cell>Xecption65</cell><cell>62.3</cell><cell>43.48M</cell></row><row><cell>AsymFusion</cell><cell>RGB-D</cell><cell>?</cell><cell>Xecption65</cell><cell>82.1</cell><cell>43.52M</cell></row><row><cell cols="3">? indicates our re-implemented results</cell><cell></cell><cell></cell><cell></cell></row></table><note>BNs will lead to an obvious performance drop (4.5% absolute drop for IoU). Secondly, compared with using individual networks, shar- ing convolutional parameters and leaving BNs privatized brings</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Image translation results comparison on Taskonomy dataset, under different combinations of modalities (two or three modalities) to verify the generalization of our models. All experiments adopt the same fusion layers as described in Section 4.2. FID score is used as the evaluation metric, the lower the better.</figDesc><table><row><cell>Data modality</cell><cell>Concat</cell><cell cols="2">Average Attention</cell><cell>MMF</cell><cell>AsymFusion</cell></row><row><cell>Shade,Depth</cell><cell>96.5</cell><cell>101.3</cell><cell>87.3</cell><cell>92.0</cell><cell>82.5</cell></row><row><cell>Normal,Texture</cell><cell>88.9</cell><cell>93.0</cell><cell>83.3</cell><cell>85.9</cell><cell>77.8</cell></row><row><cell>Depth,Texture,Normal</cell><cell>86.4</cell><cell>90.2</cell><cell>81.5</cell><cell>82.1</cell><cell>75.1</cell></row><row><cell>Shade,Normal,Edge</cell><cell>92.8</cell><cell>94.4</cell><cell>85.6</cell><cell>88.6</cell><cell>79.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparison different components of our proposed fusion methods, containing channel shuffle, pixel shift, and the knowledge distillation applied at the end of the network. Experiments are conducted with RefineNet (ResNet101) on NYUDv2 dataset. We report pixel accuracy (%), mean accuracy (%) and mean IoU (%).</figDesc><table><row><cell>Shuffle</cell><cell>Shift</cell><cell cols="2">Distillation Pixel acc.</cell><cell>Mean acc.</cell><cell>IoU</cell><cell>#Params.</cell></row><row><cell>? ?</cell><cell>? ?</cell><cell>? ?</cell><cell>74.0 74.3</cell><cell>58.9 59.3</cell><cell>47.3 47.6</cell><cell>118.20M 118.20M</cell></row><row><cell>? ? ?</cell><cell>? ? ?</cell><cell>? ? ?</cell><cell>75.2 74.8 76.4</cell><cell>62.5 61.7 63.1</cell><cell>49.3 48.7 50.5</cell><cell>118.20M 118.20M 118.20M</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>76.6</cell><cell>63.5</cell><cell>50.8</cell><cell>118.20M</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">In-place activated batchnorm for memoryoptimized training of dnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Searching for efficient multi-scale architectures for dense image prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">All you need is a few shifts: Designing efficient convolutional neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Locality-sensitive deconvolution networks with gated fusion for RGB-D indoor semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Indoor semantic segmentation using depth information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Perceptual organization and recognition of indoor scenes from RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning rich features from RGB-D images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Fusenet: Incorporating depth into semantic segmentation via fusion-based CNN architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Domokos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ACCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Rdfnet: RGB-D multi-level residual feature fusion for indoor semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Revisiting batch normalization for practical domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Cascaded feature network for semantic segmentation of RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SCN: switchable context network for semantic segmentation of RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for dense prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">TSM: temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning efficient video representation with video shuffle networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.11319</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">K for the price of 1: Parameter-efficient multi-task and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Mudrakarta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>MICCAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Indoor segmentation and support inference from RGBD images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning common and specific features for RGB-D semantic segmentation with deconvolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Resolution switchable networks for runtime efficient image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Shift: A zero flop, zero parameter alternative to spatial convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Golmant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gholaminejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<title level="m">Slimmable neural networks. In: ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Taskonomy: Disentangling task transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Deep surface normal estimation with hierarchical RGB-D fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Dense relation network: Learning consistent and context-aware representation for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICIP</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
