<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sliding Line Point Regression for Shape Robust Scene Text Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixing</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">National Engineering Laboratory for Speech and Language Information Processing</orgName>
								<orgName type="institution">University of Science and Technology of China Hefei</orgName>
								<address>
									<settlement>Anhui</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Du</surname></persName>
							<email>jundu@ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">National Engineering Laboratory for Speech and Language Information Processing</orgName>
								<orgName type="institution">University of Science and Technology of China Hefei</orgName>
								<address>
									<settlement>Anhui</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sliding Line Point Regression for Shape Robust Scene Text Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Traditional text detection methods mostly focus on quadrangle text. In this study we propose a novel method named sliding line point regression (SLPR) in order to detect arbitrary-shape text in natural scene. SLPR regresses multiple points on the edge of text line and then utilizes these points to sketch the outlines of the text. The proposed SLPR can be adapted to many object detection architectures such as Faster R-CNN and R-FCN. Specifically, we first generate the smallest rectangular box including the text with region proposal network (RPN), then isometrically regress the points on the edge of text by using the vertically and horizontally sliding lines. To make full use of information and reduce redundancy, we calculate xcoordinate or y-coordinate of target point by the rectangular box position, and just regress the remaining y-coordinate or xcoordinate. Accordingly we can not only reduce the parameters of system, but also restrain the points which will generate more regular polygon. Our approach achieved competitive results on traditional ICDAR2015 Incidental Scene Text benchmark and curve text detection dataset CTW1500.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Text detection is important in our daily life as it can be applied in many areas, such as digitization of text, text translation, etc. In this study, we focus on scene text detection. Some of the previous methods <ref type="bibr" target="#b0">[1]</ref>  <ref type="bibr" target="#b1">[2]</ref> have obtained good results on many horizontal scene texts dataset based on Faster R-CNN <ref type="bibr" target="#b2">[3]</ref> or SSD <ref type="bibr" target="#b3">[4]</ref>. Some methods <ref type="bibr" target="#b4">[5]</ref> [6] <ref type="bibr" target="#b6">[7]</ref> [8] <ref type="bibr" target="#b8">[9]</ref> [10] <ref type="bibr" target="#b10">[11]</ref> also tried to solve arbitrary-oriented text detection problem. <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b10">[11]</ref> regressed first a horizontal rectangle and then a quadrilateral. <ref type="bibr" target="#b11">[12]</ref> aimed to generate an irregular polygon after regressing a rectangle. The methods mentioned above mostly treated a text line as a quadrilateral which can be completely represented by four points. However, besides the quadrilateral shape, there are many other various shapes of text line in natural scene. Therefore, recent research <ref type="bibr" target="#b12">[13]</ref>  <ref type="bibr" target="#b13">[14]</ref> have begun to explore curve text line detection. In this paper we explore both arbitrary-oriented and curve text detection. Our method named sliding line point regression (SLPR) is based on 2-step object detection methods using Faster R-CNN or R-FCN. Firstly we propose some interesting rectangular regions with region proposal network (RPN), then regress the points on the edge of text. We generate some rules to determine which points should be regressed so that there will be relevance between points. Different from <ref type="bibr" target="#b12">[13]</ref> which directly regressed both x-coordinate and y-coordinate of fixed annotated points <ref type="figure">Fig. 1</ref>: Illustration of ground truth points generated by horizontally and vertically sliding lines. and employed RNN <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> to learn their relevance, we introduce some rules to vertically and horizontally slide lines along text and then regress the intersection points of sliding lines and text lines as illustrated in <ref type="figure">Fig. 1</ref>. In this way, we can only regress x-coordinate or y-coordinate of these points, then calculate other coordinates with the position of rectangle, yielding reduction of unnecessary computation and improvement of performance.</p><p>The contributions of this paper are as follows: 1. We explore regressing multiple points on the border of text line, try to handle arbitrary-oriented and curve text detection based on Faster R-CNN and R-FCN.</p><p>2. We introduce a sliding line method to determine the ground truth points for the regression, and we make full use of the relevance of these points to generate more regular polygon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In recent years, scene text detection and recognition has drawn more and more attention. But scene text detection remains a difficult problem due to its complicated orientation and background. All the methods can be divided into three categories: character based methods, word based methods and segmentation based methods. Character based methods often need synthetic datasets because labeling characters in text lines requires additional efforts. However, the generated data is greatly deviated from the real data, which can not make the trained model to achieve the state-of-the-art results on real dataset such as the popular ICDAR2015 Incidental Scene Text benchmark. In order to solve this problem, <ref type="bibr" target="#b16">[17]</ref> used semisupervised method to finetune model on real data and obtained good results.</p><p>The segmentation based methods have also been used in text detection recently. <ref type="bibr" target="#b17">[18]</ref> trained a fully convolutional network (FCN) <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref> to predict the salient map of text regions, then traced the text line by combining the salient map and character components. <ref type="bibr" target="#b20">[21]</ref> added the border class to separate text from their neighbor. <ref type="bibr" target="#b9">[10]</ref> and <ref type="bibr" target="#b7">[8]</ref> generated text maps and regress the size and angle of the corresponding quadrilateral, or coordinates of four vertexes at the same time. Compared with the traditional segmentation methods, they made a huge breakthrough on ICDAR2015 Incidental Scene Text benchmark.</p><p>Many methods of object detection can be applied to text detection, e.g., Faster R-CNN <ref type="bibr" target="#b21">[22]</ref>, SSD <ref type="bibr" target="#b3">[4]</ref>, R-FCN <ref type="bibr" target="#b22">[23]</ref> and YOLO <ref type="bibr" target="#b23">[24]</ref>. <ref type="bibr" target="#b1">[2]</ref> used irregular 1 ? 5 convolutional filters instead of the standard 3 ? 3 convolutional filters to make the network more suitable for long text detection. <ref type="bibr" target="#b24">[25]</ref> used the attention map to remove background noise. Recently more and more researchers proposed 2-step methods based on Faster R-CNN or R-FCN. <ref type="bibr" target="#b10">[11]</ref> firstly generated axis-aligned bounding boxes and then regressed the text quadrangle. They used multiscale pool operations on the roipool layer. <ref type="bibr" target="#b8">[9]</ref> tried to segment  and detect text simultaneously. Considering the particularity of text line, <ref type="bibr" target="#b25">[26]</ref> appended different angle anchors which are suitable for arbitrary-oriented text line. More recently, <ref type="bibr" target="#b13">[14]</ref> considered the polygon case and labeled a new dataset of curve text. <ref type="bibr" target="#b12">[13]</ref> also constructed a curve text dataset named CTW1500, and they proposed a new structure named curve text detector (CTD) to solve curve text detection problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>Our model can be applied to any 2-step object detection framework such as Faster R-CNN and R-FCN. Our system simultaneously regresses the minimum rectangle including text line and the coordinates of some specific points on the boundary of text line. More specifically, take Faster R-CNN as an example, we first get some interesting regions using the RPN, then we not only regress the position of the rectangle, but also regress the coordinates of the points on the edge of the text line, finally we can get arbitrary shape text area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Which points should be regressed?</head><p>Obviously, how to determine the point set for restoring the polygon is quite important. We believe the simpler the rules, the easier the neural net learns. We do not regress the fixed points such as vertexes on the polygon because there are a large variety of shapes and angles in natural scene and it is difficult to define the order of fixed feature points for all shapes. Although for quadrilateral, we can perfectly restore it by regressing the corresponding four vertices, the determination of the order of four vertices requires a complicated rule which is difficult for the neural net to learn. Alternatively, as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, we introduce some rules to vertically and horizontally slide the lines (we use equidistant sliding in our experiment) on text line and then regress the intersection of sliding lines and text line border. On the other hand, the correlation exists among the coordinates of different intersection points due to the constraints of the sliding lines. It is not necessary to regress both x-coordinate and y-coordinate of all points simultaneously. If it is horizontal sliding, the x-coordinate of the point on the text boundary can be calculated by the coordinates of the rectangle, so we only need to regress the y-coordinate of these points. Similarly, if it is vertical sliding, we only need to regress the x-coordinate of these points. This method not only reduces the computational complexity of the network, but also adds restraints to the regressed points as the prior knowledge which can prevent generating polygons with weird shapes and further improve the accuracy. As for the number of sliding lines, we observe that this parameter is not sensitive to quadrangle text line. But in order to restore other shape text line well, after balancing the performance and network complexity, seven sliding lines are used for we decided to for vertical and horizontal directions, respectively. Accordingly a total of 14 lines with 28 intersection points are generated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The multi-task learning</head><p>To optimize the neural network parameters, as illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>, we adopt the multi-task learning to define the loss function L as:</p><formula xml:id="formula_0">L = L RPN + L SLPR (1) L RPN = L RCLS + ? R L RB (2) L SLPR = L CLS + ? B L B + ? S L SLPRB (3)</formula><p>where L RPN is the region proposal loss, L RCLS is region proposal classification loss, L RB is box regression loss. L SLPR is the loss for the second step after RPN. Similarly, the first two items L CLS and L B are respectively classification loss and box regression loss. ? R , ? B and ? S are the related weighting factors, which are all set to 1 in this study. L SLPRB is the proposed new loss item for SLPR:</p><formula xml:id="formula_1">L SLPRB = 1 4n ? ? 2n j=1 L Reg (x vj , x * vj ) + 2n i=1 L Reg (y hi , y * hi ) ? ?<label>(4)</label></formula><p>L Reg is the smooth L1 loss for the box regression task:</p><formula xml:id="formula_2">L Reg (z, z * ) = 0.5(z ? z * ) 2 if |z ? z * | &lt; 1 |z ? z * | ? 0.5 otherwise<label>(5)</label></formula><p>In Eq. <ref type="formula" target="#formula_1">(4)</ref>, n represents the number of sliding lines in one direction and we set n = 7 in our experiments. In general, each line has two intersection points with the text line border. If there are more than two intersection points, we take the smallest and the largest coordinates. x vj is x-coordinate of the intersection point v j of vertically sliding lines and text line border while y hi is y-coordinate of the intersection point h i of horizontally sliding lines and text line border. x * vj and y * hi are the corresponding estimated points from neural net outputs. For horizontally sliding lines, we only regress the ycoordinate of its intersection point. For vertically sliding lines, we only regress the x-coordinate of its intersection point. The other coordinates can be restored through the coordinates of the rectangle:</p><formula xml:id="formula_3">y vj = y min + (y max ? y min ) (j ? 1)/2 + 1 (n + 1) (6) x hi = x min + (x max ? x min ) (i ? 1)/2 + 1 (n + 1)<label>(7)</label></formula><p>x min and y min represent the minimum x-coordinate and ycoordinate of the rectangular border while x max and y max represent the maximum x-coordinate and y-coordinate of the rectangular border. ? is the floor function. In a word, in order to regress the coordinates of polygon, 32 parameters should be considered including 4 parameters for the rectangle and 28 parameters to represent x and y coordinates of intersection points on text line border.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Restoration of polygon</head><p>Through the above SLPR method, we can obtain multiple points from the output of neural nets. To restore the final quadrilateral or polygon, the following two approaches are adopted and compared:</p><p>1) Only Using Points in Long Side (PLS): The text line always extends to the long side, and the lines that slide along the long side can better reflect the shape of the text. In fact we can restore the polygon by only scanning the long side, as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. Specifically, we firstly judge whether the text line is horizontal or vertical through the regressed rectangle, and then restore the polygon through points in the corresponding direction. Taking the vertical direction as an example in <ref type="figure" target="#fig_1">Fig. 3</ref>, since we do not regress the intersection point on the rectangular border, we firstly extend the four lines near the border to find four intersection points with the rectangle, then we connect four new points and other intersection points to generate polygon.</p><p>2) Using Both of Horizontal and Vertical Points (BHVP): In fact, if we use both horizontal and vertical points to restore polygon, we can calculate a polygon or quadrangle that passes through these points roughly as shown in <ref type="figure" target="#fig_2">Fig.4</ref> by using the method in <ref type="bibr" target="#b26">[27]</ref>. In this way we can obtain dense enough points in both horizontal and vertical direction and we do not need to calculate the intersection with the rectangle as in PLS method. However, we observe BHVP is not as effective as PLS for the polygon case. So we use this method only on the quadrilateral dataset (ICDAR2015 Incidental Scene Text).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Polygonal non-maximum suppression</head><p>Non-maximum suppression (NMS) is a basic method commonly used in the object detection, and its purpose is to remove duplicate boxes. The traditional NMS method is based on rectangular boxes, which is not the best choice for other  shapes. In recent years, other NMS approaches were investigated, e.g., locality-aware NMS <ref type="bibr" target="#b9">[10]</ref>, inclined NMS <ref type="bibr" target="#b10">[11]</ref>, Mask-NMS <ref type="bibr" target="#b8">[9]</ref> and polygonal NMS (PNMS) <ref type="bibr" target="#b12">[13]</ref>. As we consider the polygon in this study, both NMS and PNMS are compared in our experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>A. Datasets 1) ICDAR2015 Incidental Scene Text.: ICDAR2015 Incidental Scene Text dataset <ref type="bibr" target="#b27">[28]</ref> is commonly used benchmark for detecting arbitrary-angle quadrangular text lines. It contains 1000 images for training, 500 images for testing. Some words which are too short, or unclear is annotated as don't cared samples.</p><p>2) CTW1500: Curve text dataset (CTW1500) is constructed by Yuliang et al. <ref type="bibr" target="#b12">[13]</ref>. Different from traditional text datasets, a text line is labelled by a polygon with 14 points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>Since our proposed SLPR can be applied to any 2-step object detection framework. We adopted Faster R-CNN in ICDAR2015 Incidental Scene Text. And because <ref type="bibr" target="#b12">[13]</ref> also proposed a 2-step framework based on R-FCN while presenting the CTW1500 dataset, to perform a fair comparison, we directly used the network in <ref type="bibr" target="#b12">[13]</ref> from https://github. com/Yuliang-Liu/Curve-Text-Detector. All experiments were implemented in Caffe <ref type="bibr" target="#b28">[29]</ref> by using the NVIDIA GTX 1080Ti GPU.</p><p>1) ICDAR2015 Incidental Scene Text.: For Faster R-CNN structure, we used an additional 64 2 anchor and replaced RoIPool with RoIAlign <ref type="bibr" target="#b29">[30]</ref> because the text line is smaller than other objects. We set anchor scales as [64 2 , 128 2 , 256 2 , 512 2 ] and set ratios as [0.5, 1, 2]. The base network is VGG16 <ref type="bibr" target="#b30">[31]</ref>, which is initialized by the pre-trained model on ImageNet database. We used stochastic gradient descent (SGD) with back-propagation and the maximum iteration was 20 ? 10 4 . Learning rates started from 10 ?3 , decays to one-tenth every 5 ? 10 4 iterations. We set weight decay as 0.0005, and momentum as 0.9. We used 1000 training incidental images in ICDAR2015 Incidental Scene Text <ref type="bibr" target="#b27">[28]</ref> and the 229 training images from ICDAR 2013 to train our network. In order to prevent over-fitting we employed data augmentation. Specifically, we randomly resized the images to <ref type="bibr">[720,</ref><ref type="bibr">850,</ref><ref type="bibr">960,</ref><ref type="bibr">1200,</ref><ref type="bibr">1400]</ref> where the numbers represent the length of the short side, and randomly rotated the images</p><formula xml:id="formula_4">among [0 ? , 15 ? , 30 ? , ..., 360 ? ].</formula><p>2) CTW1500: We used the curve text detector (CTD) network which is based R-FCN from <ref type="bibr" target="#b12">[13]</ref>. <ref type="bibr" target="#b12">[13]</ref> also added LSTM units named transverse and longitudinal offset connection (TLOC) to learn the correlation of points. But we removed it. As we only used PLS to restore polygon, Eq. (4) was modified as:</p><formula xml:id="formula_5">L SLPRB = 1 4n ? ? ? hw I( h w &gt; k) 2n j=1 L Reg (x vj , x * vj )+ I( h w &lt; 1 k ) 2n i=1</formula><p>L Reg (y hi , y * hi ) I(z) equals to 1 when z is true, otherwise 0. h and w are the height and width of the rectangle. Because most of the texts in this dataset are horizontal text line, to solve the imbalance between horizontal and vertical samples, we added ? hw = 4 to balance the losses between them. And when h is close to w , the text line may be judged as horizontal or vertical, so we set k as 0.8. The base network is ResNet-50 <ref type="bibr" target="#b31">[32]</ref>, which is initialized by the pre-trained model on ImageNet database. The max iteration was 8 ? 10 4 . The learning rate in this experiment was always 10 ?3 . We set weight decay as 0.0005, and momentum as 0.9. To conduct a fair comparison, we only used the training set in CTW1500 to train our network and did not use data augmentation. <ref type="table" target="#tab_0">Table I</ref> shows the results of SLPR system with different settings. First, for the restoration of the quadrangle for the text region, BHVP using all the points can achieve better results than PLS using only the long-side points. Second, even we aim to detect the quadrangle in this dataset, PNMS still outperforms NMS. Finally, the use of multi-scale is one way to improve detection performance   on different target sizes. We also test the multi-scale results of our system at (850, 1000), which yields about 1% absolute improvement of Hmean measure. <ref type="figure" target="#fig_3">Fig. 5</ref> lists several challenging examples of detection results on ICDAR2015 Incidental Scene Text dataset. <ref type="table" target="#tab_0">Table II</ref> gives the comparison of SLPR with state-of-the-art results on ICDAR2015 Incidental Scene Text. We can observe that our method achieved the competitive results on this dataset. 2) CTW1500: <ref type="table" target="#tab_0">Table III</ref> shows the results of our method with different NMS settings. Different from the observation in ICDAR2015 Incidental Scene Text, our method achieved the best result on NMS0.3, namely the traditional NMS method with the threshold 0.3 for calculating the IoU (Intersectionover-Union). <ref type="table" target="#tab_0">Table IV</ref> lists the results of our method compared with CTD and CTD+TLOC. We removed TLOC from <ref type="bibr" target="#b12">[13]</ref> as our base network which is the same as CTD. Clearly, the Hmean performance of our SLPR method could be increased by 5.3% over the CTD method, demonstrating the effectiveness of our simple rules to set the regression points. Even compared with the CTD+TLOC method with an additional LSTM network, SLPR still achieved 1.4% improvement of Hmean performance. <ref type="figure" target="#fig_4">Fig. 6</ref> gives several examples of the detection results of CTD, CTD+TLOC and our SLPR. We can observe that our method generated smoother regions and better detection results compared with CTD, which implied that the proposed SLPR can better handle the arbitrary-oriented case due to the novel design of the horizontally and vertically symmetrical scanning using sliding lines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) ICDAR2015 Incidental Scene Text:</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this study, we propose a novel SLPR method for the text detection in arbitrary-shape case. Compared with the curve text detection method CTD+TLOC <ref type="bibr" target="#b12">[13]</ref>, SLPR is more concise without using LSTM and obtains better performance. In the traditional quadrangle dataset (ICDAR2015 Incidental Scene Text), SLPR also achieves the state-of-the-art performance. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>The SLPR architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Restoration of the polygon by using intersection points on sliding lines along the long side.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Restoration of the polygon by using all intersection points from SLPR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>The detection results on ICDAR2015 Incidental Scene Text dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>The detection results on CTW1500 dataset. From left to right: CTD, CTD+TLOC and SLPR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>VI. ACKNOWLEDGMENT This work was supported in part by the National Key R&amp;D Program of China under contract No. 2017YFB1002202, in part by the National Natural Science Foundation of China under Grants 61671422 and U1613211, in part by the MOE-Microsoft Key Laboratory of USTC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>The performance comparison of SLPR method with different settings on ICDAR2015 Incidental Scene Text dataset.</figDesc><table><row><cell>Scales</cell><cell>NMS</cell><cell cols="4">Restoration Precision (%) Recall (%) Hmean (%)</cell></row><row><cell>(850)</cell><cell>PNMS</cell><cell>BHVP</cell><cell>86.1</cell><cell>81.6</cell><cell>83.8</cell></row><row><cell>(850)</cell><cell>NMS</cell><cell>BHVP</cell><cell>86.8</cell><cell>80.1</cell><cell>83.3</cell></row><row><cell>(850)</cell><cell>PNMS</cell><cell>PLS</cell><cell>85.6</cell><cell>81.5</cell><cell>83.5</cell></row><row><cell>(850,1000)</cell><cell>PNMS</cell><cell>BHVP</cell><cell>85.5</cell><cell>83.6</cell><cell>84.5</cell></row><row><cell>(850,1000)</cell><cell>NMS</cell><cell>BHVP</cell><cell>86.2</cell><cell>82.7</cell><cell>84.4</cell></row><row><cell>(850,1000)</cell><cell>PNMS</cell><cell>PLS</cell><cell>84.9</cell><cell>83.6</cell><cell>84.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>The comparison with the-state-of-the-art on IC-DAR2015 Incidental Scene Text dataset.</figDesc><table><row><cell>Methods</cell><cell cols="3">Precision (%) Recall (%) Hmean (%)</cell></row><row><cell>HUST [28]</cell><cell>44.0</cell><cell>37.8</cell><cell>40.7</cell></row><row><cell>Zhang et al. [18]</cell><cell>70.8</cell><cell>43.1</cell><cell>53.6</cell></row><row><cell>RRPN [26]</cell><cell>82.2</cell><cell>73.2</cell><cell>77.4</cell></row><row><cell>WordSup [17]</cell><cell>79.3</cell><cell>77.0</cell><cell>78.2</cell></row><row><cell>EAST [10]</cell><cell>83.3</cell><cell>78.3</cell><cell>80.7</cell></row><row><cell>Deep direct regression [8]</cell><cell>82.0</cell><cell>80.0</cell><cell>81.0</cell></row><row><cell>R 2 CNN [11]</cell><cell>85.6</cell><cell>79.7</cell><cell>82.5</cell></row><row><cell>FSTN [9]</cell><cell>88.6</cell><cell>80.0</cell><cell>84.1</cell></row><row><cell>SLPR (Ours)</cell><cell>85.5</cell><cell>83.6</cell><cell>84.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III :</head><label>III</label><figDesc>The performance comparison of SLPR method with different NMS settings on CTW1500 dataset.</figDesc><table><row><cell>NMS Method</cell><cell cols="3">Precision (%) Recall (%) Hmean (%)</cell></row><row><cell>NMS0.1</cell><cell>81.2</cell><cell>64.3</cell><cell>71.8</cell></row><row><cell>NMS0.2</cell><cell>81.0</cell><cell>68.7</cell><cell>74.3</cell></row><row><cell>NMS0.3</cell><cell>80.1</cell><cell>70.1</cell><cell>74.8</cell></row><row><cell>PNMS0.1</cell><cell>80.5</cell><cell>69.5</cell><cell>74.6</cell></row><row><cell>PNMS0.2</cell><cell>78.8</cell><cell>70.4</cell><cell>74.4</cell></row><row><cell>PNMS0.3</cell><cell>75.6</cell><cell>71.3</cell><cell>73.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV :</head><label>IV</label><figDesc>The comparison with CTD on CTW1500 dataset.</figDesc><table><row><cell>Method</cell><cell>Precision (%)</cell><cell cols="2">Recall (%) Hmean (%)</cell></row><row><cell>CTD+TLOC [13]</cell><cell>77.4</cell><cell>69.8</cell><cell>73.4</cell></row><row><cell>CTD [13]</cell><cell>74.3</cell><cell>65.2</cell><cell>69.5</cell></row><row><cell>SLPR (ours)</cell><cell>80.1</cell><cell>70.1</cell><cell>74.8</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Deeptext: A unified framework for text proposal generation and text detection in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07314</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Textboxes: A fast text detector with a single deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4161" to="4167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2315" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-orientation scene text detection with adaptive clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1930" to="1937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Orientation robust text line detection in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4034" to="4041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep matching prior network: Toward tighter multioriented text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01425</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep direct regression for multi-oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.08289</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Fused text segmentation networks for multi-oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.03272</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">East: An efficient and accurate scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03155</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">R2cnn: Rotational region cnn for orientation robust scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09579</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sren: Shape regression network for comic storyboard extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4937" to="4938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuliang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lianwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shuaitao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02170</idno>
		<title level="m">Detecting curve text in the wild: New dataset and new solution</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Total-text: A comprehensive dataset for scene text detection and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kheng</forename><surname>Chng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10400</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, speech and signal processing (icassp), 2013 ieee international conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A gru-based encoder-decoder approach with attention for online handwritten mathematical expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.03991</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Wordsup: Exploiting word annotations for character based text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.06720</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multioriented text detection with fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4159" to="4167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Watch, attend and parse: An end-to-end neural network based approach to handwritten mathematical expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Self-organized text detection with minimal post-processing via border learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5000" to="5009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via regionbased fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Single shot text detector with regional attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.00138</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Arbitrary-oriented scene text detection via rotation proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01086</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Pagenet: Page boundary extraction in historical handwritten documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tensmeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wigington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Barrett</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01618</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Icdar 2015 competition on robust reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Analysis and Recognition (ICDAR), 2015 13th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1156" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06870</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
