<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Action-Agnostic Human Pose Forecasting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsu-Kuang</forename><surname>Chiu</surname></persName>
							<email>hkchiu@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Adeli</surname></persName>
							<email>eadeli@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De-An</forename><surname>Huang</surname></persName>
							<email>dahuang@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
							<email>jniebles@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Action-Agnostic Human Pose Forecasting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Predicting and forecasting human dynamics is a very interesting but challenging task with several prospective applications in robotics, health-care, etc. Recently, several methods have been developed for human pose forecasting; however, they often introduce a number of limitations in their settings. For instance, previous work either focused only on short-term or long-term predictions, while sacrificing one or the other. Furthermore, they included the activity labels as part of the training process, and require them at testing time. These limitations confine the usage of pose forecasting models for real-world applications, as often there are no activity-related annotations for testing scenarios. In this paper, we propose a new action-agnostic method for shortand long-term human pose forecasting. To this end, we propose a new recurrent neural network for modeling the hierarchical and multi-scale characteristics of the human dynamics, denoted by triangular-prism RNN (TP-RNN). Our model captures the latent hierarchical structure embedded in temporal human pose sequences by encoding the temporal dependencies with different time-scales. For evaluation, we run an extensive set of experiments on Human 3.6M and Penn Action datasets and show that our method outperforms baseline and state-of-the-art methods quantitatively and qualitatively. Codes are available at https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Humans are able to predict how their surrounding environment may change and how other people move. This inclination and aptitude is crucial to make social life and interaction with others attainable <ref type="bibr" target="#b3">[4]</ref>. As such, to create machines that can interact with humans seamlessly, it is very important to convey the ability of predicting short-and long-term future of human dynamics based on the immediate present and past. Recently, computer vision researchers attempted predicting human dynamics from images <ref type="bibr" target="#b11">[11]</ref>, or through time in videos <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b37">37]</ref>. Human dynamics are mainly defined as a set of structured body joints, known Ground-truth TP-RNN <ref type="figure">Figure 1</ref>: Ground-truth pose sequences (first row) and forecasted ones by our method (second row). Solid colors indicate later time-steps and faded ones are older. The course of changes in the predicted and ground-truth poses resemble similar patterns. Furthermore, body-part movement patterns show that different parts depend on each other, but with varied temporal scales. Hence, hierarchical multi-scale modeling may encode the latent structures of human dynamics.</p><p>as poses <ref type="bibr" target="#b33">[33]</ref>. Predicting human dynamics is hence delineated by predicting the course of changes in human poses <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b43">43]</ref>. Detecting and predicting poses has long been an interesting topic in the computer vision community <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b43">43]</ref>. Recently, several methods have been introduced for forecasting human poses in the near future <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b40">40]</ref>. In a recent work, Martinez et al. <ref type="bibr" target="#b23">[23]</ref> noted that, although great advancements in pose forecasting has been achieved by prior works, they often fail to generate realistic human poses, especially in short-term predictions, and in most cases they fail to even outperform the zero-velocity predictor (i.e., repeating the very last seen pose as predictions for the future). Ghosh et al. <ref type="bibr" target="#b16">[16]</ref>, with reference to <ref type="bibr" target="#b23">[23]</ref>, attributed this finding to the side-effects of curriculum learning (such as in <ref type="bibr" target="#b11">[11]</ref>), commonly practiced for temporal forecasting. With such observations, some previous works focused on short-term forecasting of human poses <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b23">23]</ref>, and some others exclusively aimed attention at long-term predictions <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b36">36]</ref>. However, most of the previous methods achieve reasonable performance by incorporating action labels as extra data annotation in their models, i.e., they either trained pose fore-casters on each action class separately (e.g., <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b18">18]</ref>) or incorporated the action labels as an extra input to the model and concluded that including action labels improves the results (e.g., <ref type="bibr" target="#b23">[23]</ref>). Although action labels are not hard to acquire for training samples, but the use of labels for testing videos as an input of the model is unrealistic and makes the introduced models unusable for real-world applications, as action labels are not available during testing <ref type="bibr" target="#b0">[1]</ref>. Unlike these previous works, our method learns a pose forecaster regardless of their action class. We propose an action-agnostic model for pose forecasting by implicitly encoding the shortand long-term dependencies within actions.</p><p>In this paper, we propose a new recurrent neural network (RNN) model for forecasting human poses in both shortand long-term settings. To model human dynamics and to capture the latent hierarchical structure in the temporal pose sequences, we encode the temporal dependencies of different time-scales in a hierarchical interconnected sequence of RNN cells. Called Triangular-Prism Recurrent Neural Network (TP-RNN), our proposed method contains a new multi-phase hierarchical multi-scale RNN architecture that is tailored for modeling human dynamics in visual scenes. Different from the original hierarchical multi-scale RNNs (HM-RNN) for representation of natural language sequences <ref type="bibr" target="#b12">[12]</ref>, our architecture redefines hierarchies and multi-scale interconnections to accommodate human dynamics. Sequences of human poses through time involve hierarchical and multiscale structures, as movements of different body-parts (and joints) depend on each other. Besides, each of these parts (and joints) have distinct motion patterns and hence different temporal scales for particular activities. For instance, during 'walking', arms and legs move in a shorter temporal scale (i.e., more frequently) compared to the torso, which is potentially in a longer temporal scale (see <ref type="figure">Fig. 1</ref>). Learning the hierarchical multi-scale dynamics of changes in human poses enables TP-RNN to construct an implicit encoding of short-and long-term dependencies within action classes, and hence be able to predict future sequences without the demand for the supervising signal from action labels.</p><p>Our model takes pose velocities (i.e., differences between the current and the immediate previous poses, ignoring the division by the constant time duration between two consecutive frames) as inputs and outputs predictions in the same space of velocities. As opposed to the previous works <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b40">40]</ref> that focused on predicting sequences of poses (as structured objects) in the forms of either joint angles (e.g., <ref type="bibr" target="#b15">[15]</ref>) or joint locations (such as in <ref type="bibr" target="#b18">[18]</ref>), we argue that forecasting in the velocities space boosts prediction power since human poses change slightly in narrow time-steps. Different from <ref type="bibr" target="#b23">[23]</ref>, in which residual connections were applied on top of RNN (using residuals as outputs only, while inputs are poses), our method uses velocities as both inputs and outputs and shows significantly improved forecasting results.</p><p>To evaluate the proposed method, we run an extensive set of experiments on Human 3.6M <ref type="bibr" target="#b17">[17]</ref> and Penn Action <ref type="bibr" target="#b46">[46]</ref> datasets, and compare the results with several baseline and state-of-the-art algorithms on these datasets. The comparison shows that our method outperforms others in terms of the mean angle error (MAE) on Human 3.6M and the Percentage of Correct Keypoint (PCK) score on Penn Action. Our action-agnostic method leads to superior results in cases of both short-and long-term predictions (some are visualized in <ref type="figure">Fig. 1</ref>) even in comparison to the methods designed specifically for short-or long-term predictions or methods that use action labels as inputs to their models.</p><p>In summary, the contributions of this paper are three-fold: (1) we propose an action-agnostic model that trains the pose forecaster regardless of action classes; <ref type="bibr" target="#b1">(2)</ref> we propose a new model, TP-RNN, inspired by the hierarchical multi-scale RNN from NLP research, for forecasting human dynamics. TP-RNN implicitly encodes the action classes and, unlike previous methods, does not require external action labels during training; (3) we show that operating in the velocity space (i.e., using pose velocities as both inputs and outputs of the network) improves the results of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>In this Section, we review the relevant literature on human motion, activity, and pose forecasting, along with the previous works on hierarchical and multi-scale RNNs (and Long Short-Term Memory cells, i.e., LSTMs). Predicting Motion and Human Dynamics: The majority of the recent works on motion representation has mainly focused on anticipating the future at the pixel level. For instance, generative adversarial networks (GANs) were used to generate video pixels <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b37">37]</ref>, and RNNs for anticipating future video frames <ref type="bibr" target="#b22">[22]</ref>. To predict dense trajectories, Walker et al. <ref type="bibr" target="#b39">[39]</ref> used a CNN, and others have used random forests <ref type="bibr" target="#b28">[28]</ref> or variational auto-encoders <ref type="bibr" target="#b38">[38]</ref>. Other works targeted predicting the future in forms of semantic labels (e.g., <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b38">38]</ref>) or activity labels (e.g., <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b40">40]</ref>). Human dynamics, however, could be better characterized by 2D <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b30">30]</ref> or 3D <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b45">45]</ref> poses, and several works attempted to detect these poses from images or videos <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b45">45]</ref>. Modeling human motions is commonly defined in two different ways: probabilistic and state transition models (such as Bayesian and Gaussian processes <ref type="bibr" target="#b42">[42]</ref> or hidden Markov models <ref type="bibr" target="#b44">[44]</ref>), and deep learning methods, in particular RNNs and LSTMs, e.g., <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b23">23]</ref>. For instance, Jain et al. <ref type="bibr" target="#b18">[18]</ref> proposed a structural RNN to cast an arbitrary spatio-temporal graph as a RNN and use it for modeling human pose in temporal video sequences. In this work, we propose a new multi-phase hierarchical multi-scale RNN for modeling human dynamics to forecast poses. Human Pose Forecasting: Forecasting human poses in images and video sequences is relatively new compared to predicting image or video pixels. Although it can be a very useful task with great applications (e.g., in predictive surveillance, patient monitoring, etc.), just recently researchers have paid more attention to it <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b40">40]</ref>.</p><p>Specifically, Chao et al. <ref type="bibr" target="#b11">[11]</ref> proposed a 3D Pose Forecasting Network (3D-PFNet) for forecasting human dynamics from static images. Their method integrates recent advances on single-image human pose estimation and sequence prediction. In another work, <ref type="bibr" target="#b24">[24]</ref> introduced a method to predict 3D positions of the poses, given their 2D locations. Barsoum et al. <ref type="bibr" target="#b8">[8]</ref> proposed a sequence-to-sequence model for the task of probabilistic pose prediction, trained with an improved Wasserstein GAN <ref type="bibr" target="#b4">[5]</ref>. Walker et al. <ref type="bibr" target="#b40">[40]</ref> proposed a method based on variational autoencoders and GANs to predict possible future human movements (i.e., poses) and then predict future frames. Fragkiadaki et al. <ref type="bibr" target="#b15">[15]</ref> proposed two architectures for the task of pose prediction, one denoted by LSTM-3LR (3 layers of LSTM cells) and the second one as ERD (Encoder-Recurrent-Decoder). These two models are based on a sequence of LSTM units. Martinez et al. <ref type="bibr" target="#b23">[23]</ref> used a variation of RNNs to model human motion with the goal of learning time-dependent representations for human motion prediction synthesis in a short-term. Three key modifications to recent RNN models were introduced, in the architecture, loss function, and the training procedures. In another work, B?tepage et al. <ref type="bibr" target="#b9">[9]</ref> proposed an encodingdecoding network that learns to predict future 3D poses from the immediate past poses, and classify the pose sequences into action classes. These two methods incorporate a highlevel supervision in the form of action labels, which itself improves the performance. However, in many real world applications of human motion analysis there are no motion or activity labels available during inference time. Hierarchical Multi-Scale RNNs: Our proposed architecture is inspired by the hierarchical multi-scale recurrent neural networks (HM-RNN) introduced in <ref type="bibr" target="#b12">[12]</ref>. HM-RNN builds on multi-scale RNNs <ref type="bibr" target="#b19">[19]</ref> that model high-level abstraction changes slowly with temporal coherency while low-level abstraction has quickly changing features sensitive to the precise local timing <ref type="bibr" target="#b14">[14]</ref>. This architecture is able to learn the latent representation of natural language sequences in different hierarchies (e.g., words, phrases, and sentences) to build character-level language models for predicting future sequences <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b14">14]</ref>. We observe that multi-scale temporal information at different hierarchical levels can be beneficial in modeling human dynamics. However, it is difficult to adopt this approach directly because we do not have clear-cut temporal boundaries as in natural language data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Triangular-Prism RNN (TP-RNN)</head><p>As discussed earlier, sequences of human poses can be subsumed under hierarchical and multi-scale structures, since movements of different body parts hinge on movements of other parts. Also, each part often has distinct motion patterns and hence different temporal scales when performing particular activities. Therefore, in contrast to the classical single-layer LSTM or RNN architectures (such as in <ref type="bibr" target="#b18">[18]</ref>) or stacked LSTMs (e.g., in <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b41">41]</ref>), we introduce multi-phase hierarchical multi-scale upper layers of LSTM sequences to better learn the longer-term temporal relationships between different time-steps in a series of different granularities.</p><p>The inputs and outputs of the model, as mentioned earlier, are velocities. Let the pose in time t be identified by P t , then the velocity in time t can be defined as V t = P t ? P t?1 . Therefore, for any given granularity coefficient K and the number of levels M , we define a multi-phase hierarchical multi-scale RNN with scale K and M levels. On the first level, we have a regular LSTM sequence taking the velocity information at each time-step as inputs. Then, on the second level, we define K different sequences of LSTM units, with each sequence only taking the inputs from the LSTM units on the first level at time-steps that are congruent modulo K. For example, if K = 2, then we have two LSTM sequences at level 2, with the first one taking inputs from t = {1, 3, 5, . . . } and the second one from t = {2, 4, 6, . . .} (see <ref type="figure" target="#fig_0">Fig. 2</ref> for illustrations). Note that these LSTMs in the same level of the hierarchy share weights and this shifting scheme is actually used as a data augmentation strategy to learn longer-term dependencies in a more reliable way. Similarly, if we define a third level in the hierarchy, for each of the K LSTM sequences on the second level, we will have K different LSTM sequences each taking congruentmodulo-K inputs from it, resulting in a total of K 2 LSTM sequences in the third level. This process of spawning new higher-level LSTM sequences over the hierarchy continues for M ? 1 levels, which will have K M ?1 LSTM sequences in the M th level. Therefore, logically, we have a total of (K M ?1 + K M ?2 + . . . + K 3 + K 2 + K + 1) LSTM sequences in the whole architecture, while only M different ones are kept physically (since the LSTM sequences in each level share weights). For the sampling stage, we introduce a two-layer fully-connected network to generate the velocity predictions given the velocity at the current time-step and the corresponding hidden units across all hierarchies. <ref type="figure" target="#fig_0">Fig. 2</ref> illustrates an example of our architecture for K = M = 2.</p><p>Our hierarchical model is inspired by HM-RNN <ref type="bibr" target="#b12">[12]</ref>, however, as discussed earlier, we cannot directly apply HM-RNN to our task, due to the differences between human dynamics and natural language sequences. TP-RNN models short-term dependencies in lower levels of the hierarchy and long-term temporal dependencies in higher levels, and hence can capture the latent hierarchical structure in different timescales. Different from HM-RNN, since human dynamics (unlike language models) do not have natural boundaries in the sequences, TP-RNN uses the outputs of all hierarchy levels to represent the sequence and predict the next element in the sequence. Moreover, instead of having only one RNN layer in each level of the hierarchies, as we move up in the hierarchy, TP-RNN decreases the resolution by one unit but has multiple RNN layers in the higher levels, to capture the temporal dynamics from different phases of its lower hierarchy level. All RNN layers in each single hierarchy level share parameters; although this scheme does not increase model parameters, shifting phases from each lower level to create their immediate higher level RNNs helps in augmenting the data and learning better models at each level. Therefore, as we go up in the hierarchy, more parallel RNN layers are incorporated, and hence we chose the name triangular-prism RNN (see <ref type="figure" target="#fig_0">Fig. 2</ref>). This architecture design provides the following advantages over HM-RNN for modeling human dynamics: (1) the lowest layer RNN can learn the finest grained scale motion dynamics without the interference from the higher levels, and higher levels capture different characteristics of the dynamics each at a certain scale; (2) during the prediction of each time-step, we have the most up-to-date RNN outputs from different hierarchies, each of which carries temporal motion information with different scales. On the contrary, HM-RNN does not provide the most up-to-date larger scale temporal information when the current prediction time-step is not right after a boundary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our method on two challenging datasets. The results are analyzed and compared with baseline and stateof-the-art techniques, both quantitatively and qualitatively.</p><p>In our architecture, we use LSTMs with hidden size 1024 as the RNN cells (the orange and the green blocks in <ref type="figure" target="#fig_0">Fig.  2</ref>). For the final pose velocity generation networks (the red blocks in <ref type="figure" target="#fig_0">Fig. 2</ref>), we use 2 fully-connected layers with hidden sizes 256 and 128, followed by a Leaky-ReLU non-linearity layer. The training setup is similar to <ref type="bibr" target="#b23">[23]</ref>. The optimization uses mini-batch stochastic gradient descent with batch size 16, clipping the gradients up to 2 -norm value of 5. The learning rate is initialized to 0.01 and decayed along the training iterations. We train for 100,000 iterations and record the performance coverage in the end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>To test the performance of our model for human pose forecasting, we run extensive experiments using Human 3.6M <ref type="bibr" target="#b17">[17]</ref> and Penn Action <ref type="bibr" target="#b46">[46]</ref> datasets. Human 3.6M dataset: The Human 3.6M dataset <ref type="bibr" target="#b17">[17]</ref> is one of the largest publicly available datasets of human motion capture data. This dataset contains video sequences of a total of 15 different human activity categories, each performed by seven actors in two different trials. The videos were recorded at 50Hz (i.e., 20ms between each two consecutive pose frames). Following previous work <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b23">23]</ref>, in our experiments, we downsample the pose sequence by 2. In the dataset, each pose is represented as exponential map representations of 32 human joints in the 3D space, and during evaluation, we employ the measurement of the Euclidean distance between the ground-truth pose and our predicted pose in the angle space as the error metric. Consistent with the previous work, we also use Subject 5 as the test data and <ref type="bibr">Subjects 1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b11">11</ref> as training. Similar to <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b23">23]</ref>, we train our models using the past 50 frames (2000ms) as the input sequence, and forecast the future 25 frames (1000ms). The training loss is calculated by the mean angle error (MAE) from each of the predicted future frames. Penn Action dataset: The second dataset we experiment on is the Penn Action dataset <ref type="bibr" target="#b46">[46]</ref>, which contains 2326 video sequences of 15 different actions and human joint annotations for each sequence. Each human pose is represented by same data split of <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b46">46]</ref>, 1258 video sequences are used for training and the remaining 1068 video sequences are used for testing. For this dataset, the previous state-of-the-art, 3D-PFNet <ref type="bibr" target="#b11">[11]</ref>, takes the first frame image as input, and outputs the poses extracted from that frame and the future 15 frames, resulting in total of 16 frame poses. The model performance is evaluated using PCK@0.05 <ref type="bibr" target="#b11">[11]</ref>. For the experiments on this dataset, we use a single pose in a past frame (ignoring the frame image) as the input, and the outputs are the predictions of poses of the future 16 frames. Although the input format of <ref type="bibr" target="#b11">[11]</ref> is slightly different from ours, it is still a fair comparison of pose forecasting capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>Baseline Methods: We use the following recent research to compare with: ERD <ref type="bibr" target="#b15">[15]</ref>, LSTM-3LR <ref type="bibr" target="#b15">[15]</ref>, SRNN <ref type="bibr" target="#b18">[18]</ref>, Dropout-AutoEncoder <ref type="bibr" target="#b16">[16]</ref>, 3D-PFNet <ref type="bibr" target="#b11">[11]</ref>, and Residual <ref type="bibr" target="#b23">[23]</ref>. Similar to <ref type="bibr" target="#b23">[23]</ref>, we include the zero-velocity model as a na?ve baseline for comparison. We also include our implementations of different LSTM-based models as part of the comparison (i.e., conducting ablation tests).</p><p>First set of our experiments compares the single layer LSTM model with pose as the input (denoted by Single Layer (Pose)) and the same model but with velocity as the input (Single Layer (Vel.)), to demonstrate that conducting the experiments in the velocity space and feeding it into LSTM sequences can better capture the human motion dynamics. As mentioned earlier, when both inputs and outputs are all velocities with similar small numerical scales, it is easier for the model to be trained. In the second set of experiments, we build multiple 2-Layer LSTM models with different architectures using velocity as the input, including the most basic one that simply stacks 2 layers of LSTMs (Stacked 2-Layer (Vel.)), commonly called multi-layer LSTM <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b41">41]</ref>. On top of the basic model, we build further extensions with hierarchical and multi-scale structures: two independent LSTMs (Double-scale (Vel.)), which, unlike the regular multi-layer LSTM, its higher level one does not use the output from the lower level as the input. Instead, the higher level LSTM's input is the larger scale of velocity, i.e., the velocity calculated by the pose sequence only at the odd time-steps, or only at the even time-steps. The next model (Double-scale (Hier., Vel.)) is similar to HM-RNN <ref type="bibr" target="#b12">[12]</ref>, but with slight modification of setting the higher level LSTM scale to a constant number 2, due to the fact that there is no natural boundary in human motion sequences. Another model (Double-scale (Phase, Vel.)) has multiple phases in the higher level LSTMs, capturing larger scale velocity information, rather than using the lower level LSTM outputs. Finally, we implement our proposed model (TP-RNN) with double scale setting. Note, to showcase the superiority of the proposed technique we report the results for K = M = 2 in TP-RNN, which demonstrates that without the need to increase the network parameters, our network already outperforms all other methods. However, we also conduct an experiment for analyzing the effect of the number of levels in TP-RNN, and show models with more hierarchies can lead to even better results. Comparison on Human 3.6M dataset: Previous literature published their performance numbers on either short-term (up to 400ms) <ref type="bibr" target="#b23">[23]</ref> or long-term (up to 1000ms) <ref type="bibr" target="#b18">[18]</ref> predictions. Besides, some of them only report the prediction on a small set of actions (i.e., 'walking', 'eating', 'smoking', and 'discussion') <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b18">18]</ref>, while others report the results for all 15 actions <ref type="bibr" target="#b23">[23]</ref> in the Human 3.6M dataset <ref type="bibr" target="#b17">[17]</ref>. To compare with all the above different settings, for each of our architectures, we train a single action-agnostic model using sequence data from all of the 15 actions, without any supervision from the ground-truth action labels. We use the loss over each forecasted frame up to 1000ms (25 frames). We follow the settings of <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b23">23]</ref> for the length of the input seed observed pose (i.e., 2000ms, 50 frames). <ref type="table" target="#tab_1">Table 1</ref> shows the MAE for 'walking', 'eating', 'smoking', and 'discussion' for short-term predictions. Our model (TP-RNN) outperforms all the baseline results, including the current state-of-the-art, Residual model <ref type="bibr" target="#b23">[23]</ref>, in short-term forecasting. In the Residual model <ref type="bibr" target="#b23">[23]</ref>, pose information is used to predict the velocity of the next frame. Note that the numerical scale of velocity is much smaller compared to the pose. On the contrary, in our proposed model, velocity information of the past is fed into the models to predict the next velocity. Therefore, the scales of inputs and outputs are the same, which potentially puts the neural network in an easier path to train. For actions with large movements (like 'Walking' and 'Eating') our model outperforms the baselines and the state-of-the-art by a large margin. However, like other previous methods, our method has hard time to forecast 'difficult-to-predict' actions (like in 'Smoking' and 'Discussion'). Although, our results in those activities are also superior to all other methods, they are close to the zerovelocity baseline. Our proposed TP-RNN is setting a new state-of-the-art for pose forecasting on this dataset without the need of action labels at test time. Furthermore, it conducts both short-and long-term predictions simultaneously without sacrificing the accuracy of either end. <ref type="table" target="#tab_2">Table 2</ref> shows the MAE for the same set of the four actions in the long-term prediction task. The state-of-the-art model (i.e., Residual <ref type="bibr" target="#b23">[23]</ref>) does not report the long-term prediction performance results, therefore we use its opensource implementation code to collect the results for longterm predictions. Note that when changing the training loss from short-term predictions to long-term predictions, this model sacrifices the prediction accuracy in the short-term time-range (less than 400ms) in order to gain the extra longterm (400ms to 1000ms) prediction ability.</p><p>The long-term prediction of the Residual model <ref type="bibr" target="#b23">[23]</ref> still   outperforms other prior works in most of the cases. Another strong previous work in long-term forecasting is the Dropout-AutoEncoder model <ref type="bibr" target="#b16">[16]</ref>, which generates the best 1000ms prediction for the 'Discussion' action among all other models. Similar to short-term predictions, our proposed velocitybased model outperforms all the baseline and state-of-theart methods, except for the Dropout-AutoEncoder model in 1000ms prediction with respect to only the 'Discussion' action. Note that our model conducts an action-agnostic forecasting and does not sacrifice the short-term or long-term predictions. Our results in comparison with other methods that are either trained for each action separately (like <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b18">18]</ref>) or only target short-or long-term predictions (e.g., <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b23">23]</ref>) show better overall performance. As our models are trained using all 15 actions in Human 3.6M <ref type="bibr" target="#b17">[17]</ref>, without any extra supervision from the action labels, we further evaluate the proposed method by reporting the average MAE for all timepoints across all 15 action categories. In <ref type="table" target="#tab_3">Tables 3 and 4</ref>, we compare our results with the current state-of-the-art model <ref type="bibr" target="#b23">[23]</ref>, which is the only previous research experimented on all 15 action classes. <ref type="table" target="#tab_3">Table 3</ref> shows the long-term forecasting results of the remaining 11 action categories, not included in <ref type="table" target="#tab_2">Table 2</ref>. As can be seen, our proposed TP-RNN model performs better than <ref type="bibr" target="#b23">[23]</ref> in most of the action categories. <ref type="table" target="#tab_4">Table 4</ref> summarizes the short-term and long-term results from the current state-of-the-art model <ref type="bibr" target="#b23">[23]</ref> (from the paper and the code, both when including action labels as inputs to the model or not), and the results from our proposed velocity-based models by showing the average MAE for all time-points across all 15 action categories. Our models outperform the Residual model, in both short-term and longterm prediction tasks. We use the average MAE metric to also conduct the ablation analysis by evaluating the difference between each of our model extensions with hierarchical and multi-scale architectures. We can see that basic multilayer LSTM model (i.e., Stacked 2-Layer (Vel.)) does not improve the overall performance compared with single layer LSTM model (i.e., Single Layer (Vel.)). After we include the multi-scale idea in our models, we can see the improvement over the single layer model (i.e., Single Layer (Vel.)) and the basic multi-layer single-scale model (i.e., Stacked 2-Layer (Vel.)). For the long-term time-range, the model directly adapted from HM-RNN <ref type="bibr" target="#b12">[12]</ref> (i.e., Double-scale (Hier., Vel.)) does not improve the performance, because the original design of the HM-RNN model is for the natural language data with obvious boundaries in the sequence. We mitigate this limitation with our proposed TP-RNN model, which has multiple phases of LSTM on the higher levels for capturing the up-to-date larger scale temporal information. TP-RNN provides best quantitative results, as shown in <ref type="table" target="#tab_4">Table 4</ref>.</p><p>In summary, <ref type="table" target="#tab_4">Table 4</ref> shows that the previous state-ofthe-art (i.e., Residual <ref type="bibr" target="#b23">[23]</ref>) needs to compromise between short-and long-term forecasting accuracy. However, TP-RNN model performs better, especially for the long-term forecasting (1000ms in future) without sacrificing the shortterm accuracy. In terms of numbers, our long-term forecasting improvement is 1.83?1.71 1.83 ? 6.56%, which is not negligible. Besides, <ref type="table" target="#tab_2">Table 2</ref> shows that for certain action categories with hierarchical multi-scale motions (e.g., 'Walking'), long-term forecasting improvement is even more sig- </p><formula xml:id="formula_0">? 19.79%.</formula><p>To further test the significance of the improvements, we conduct a one-tailed paired t-test between our average results and those of <ref type="bibr" target="#b23">[23]</ref>. The p-value of the test equals 0.0002, which by all conventional criteria the difference between the two sets of results is considered to be extremely statistically significant. Deeper Hierarchical Structure of TP-RNN: In the previous subsections, we showed that even with the most basic architectural settings (i.e., M = 2 and K = 2), TP-RNN already outperforms the state-of-the-art. In this section, we further experiment and analyze the effect of increasing the number of levels M . <ref type="figure" target="#fig_1">Fig. 3(b)</ref> shows the average MAE of the long-term forecasting (1000ms) results from TP-RNN with different numbers of levels: M ? {2, 3, 4, 5}. In general, increasing the number of levels of TP-RNN improves the long-term forecasting accuracy, which is in accordance with our hypothesis that higher hierarchical levels are able to better capture long-term human motion dynamics and thus improve the long-term forecasting accuracy. In addition, <ref type="figure" target="#fig_1">Fig  3(a)</ref> shows similar results for short-term forecasting (400ms), which also indicates that the performance improves slightly when increasing the number of levels. However, when we increase the number of TP-RNN levels to 4 for short-term or 5 for long-term, we see a decline in the performance. The reason is that the LSTM cell(s) at the 5 th level only update once at every 2 5?1 = 16 time-step, which is too long, given that we only predict the future 25 frames (i.e., very few updates for M = 5). Besides, with deeper hierarchical structures, TP-RNN has more trainable parameters and therefore is prone to overfitting (requires more data). Qualitative Evaluations and Visualization: <ref type="figure" target="#fig_2">Fig. 4</ref> shows the visualization of pose sequences for our method, in comparisons with the Residual method <ref type="bibr" target="#b23">[23]</ref>, for the sequences for the actions 'Walking' and 'Smoking'. As it can be seen in the figure, our predictions (the middle row) are visually closer to the ground-truth (last row) compared to the state-of-the-art Residual method <ref type="bibr" target="#b23">[23]</ref>. This also supports the quantitative results shown in <ref type="table" target="#tab_2">Table 2</ref>, as our method steadily outperforms <ref type="bibr" target="#b23">[23]</ref> for the 'walking' activity: TP-RNN's MAE was 0.25 at 80ms while <ref type="bibr" target="#b23">[23]</ref> led to an MAE of 0.32, similarly, ours was 0.75 and 0.77 in 560ms and 1000ms while <ref type="bibr" target="#b23">[23]</ref> obtained 0.86  and 0.96, respectively. A similar conclusion can be made for predictions of the 'smoking' activity. Although, this activity has slight amounts of movement, still our method can capture better dynamics, if we look at the results closely. For instance, using our method, the distance of the subject's hand from the torso and face are better predicted in long-term and leg movements are more precisely forecasted, especially in shorter-term predictions. More visualizations of the forecasted poses by TP-RNN were visualized in <ref type="figure">Fig. 1</ref>, in which one can simply observe how the patterns of poses change through time, in comparison with the ground-truth.</p><p>Comparison on Penn Action Dataset: We trained TP-RNN on Penn Action dataset <ref type="bibr" target="#b46">[46]</ref> and, here, we compare its results with the previous state-of-the-art on this dataset, 3D-PFNet <ref type="bibr" target="#b11">[11]</ref>. The input to our velocity-based TP-RNN is the pose in a single frame, and we set the initial velocity to 0 (denoted by TP-RNN w/o init vel.), in order to have a fair comparison with <ref type="bibr" target="#b11">[11]</ref>. The model performance is evaluated using PCK@0.05 as in <ref type="bibr" target="#b11">[11]</ref>. PCK calculates the percentage of joint locations correctly predicted by the model. With the threshold 0.05, a joint location is counted as correctly predicted if the normalized distance between its predicted and ground-truth locations is less than 0.05. The results are shown in <ref type="table" target="#tab_5">Table 5</ref>. Our model performs significantly better than 3D-PFNet <ref type="bibr" target="#b11">[11]</ref> (a p-value of 0.0419 &lt; 0.05 significance threshold), with the results shown in <ref type="table" target="#tab_5">Table 5</ref>. For further comparison, we used the open-sourced code of <ref type="bibr" target="#b23">[23]</ref> with the necessary modifications (the same setting as our TP-RNN), and experimented on the Penn Action dataset. Additionally, we further experiment by including the initial velocity in the input, to demonstrate that the importance of the velocity information. The additional initial velocity is estimated using the difference between the current pose and the previous pose, our TP-RNN model generates even better forecasting results, as shown in the last row in <ref type="table" target="#tab_5">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, inspired by the success of hierarchical multiscale RNN (HM-RNN) frameworks in the natural language processing applications, we proposed a new model to encode different hierarchies in human dynamics at different timescales. Our model trains a set of RNNs (as LSTM sequences) at each hierarchy with different time-scales. Within each level of the hierarchy, RNNs share their learnable weights, since they are all learning a same concept in a same timescale (with different phases, i.e., different starting points of the sequence). This dramatically decreases the number of parameters to be learned in the model, while involving as much data as possible to train the higher level RNNs. As a result, the lowest layer can learn the finest grained scale motion dynamics, and higher levels capture different characteristics of the dynamics each at a certain time-scale. Furthermore, we set up a more rigorous but realistic experimental settings by conducting an action-agnostic forecasting (i.e., no usage of activity labels) and predicting both shortand long-term sequences simultaneously (unlike the previous works, which limited their settings). Despite these strict settings, our results on the Human 3.6M dataset and the Penn Action dataset show superior performance for TP-RNN to the baseline and state-of-the-art methods, in terms of both quantitative evaluations and qualitative visualizations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>PastFutureFigure 2 :</head><label>2</label><figDesc>Architecture of TP-RNN with K = 2 and M = 2. Left: 3D view of the triangular-prism RNN. Right: 2D projection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Average MAEs of short-and long-term forecasting using TP-RNN with different levels: M ? {2, 3, 4, 5}. nificant: 0.96?0.77 0.96</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of long-term pose-forecasting (up to 1000ms) for the actions 'Walking' (top) and 'Smoking' (bottom), downsampled by a factor of 2 (i.e., 13 forecasted poses out of 25 are visualized). The purple poses are groundtruth data, including past (in the left side) and future time frames (in the bottom). The blue and red poses are the predictions of [23] and our method (TP-RNN), respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>MAE for four action classes in the short-term forecasting experiment (prior-work results from<ref type="bibr" target="#b23">[23]</ref>). In each column, the best obtained results are typeset in boldface and the second best are underlined. AA: Action-Agnostic, N/A: Not Applicable. .<ref type="bibr" target="#b18">18</ref> 1.59 1.78 1.27 1.45 1.66 1.80 1.66 1.95 2.35 2.42 2.27 2.47 2.68 2.76</figDesc><table><row><cell></cell><cell>AA</cell><cell>Walking</cell><cell>Eating</cell><cell>Smoking</cell><cell>Discussion</cell></row><row><cell>milliseconds</cell><cell></cell><cell cols="4">80 160 320 400 80 160 320 400 80 160 320 400 80 160 320 400</cell></row><row><cell cols="6">ERD [15] 0.93 1LSTM-3LR [15] 0.77 1.00 1.29 1.47 0.89 1.09 1.35 1.46 1.34 1.65 2.04 2.16 1.88 2.12 2.25 2.23</cell></row><row><cell>SRNN [18]</cell><cell></cell><cell cols="4">0.81 0.94 1.16 1.30 0.97 1.14 1.35 1.46 1.45 1.68 1.94 2.08 1.22 1.49 1.83 1.93</cell></row><row><cell>Residual [23]</cell><cell></cell><cell cols="4">0.28 0.49 0.72 0.81 0.23 0.39 0.62 0.76 0.33 0.61 1.05 1.15 0.31 0.68 1.01 1.09</cell></row><row><cell>Zero-velocity</cell><cell cols="5">N/A 0.39 0.68 0.99 1.15 0.27 0.48 0.73 0.86 0.26 0.48 0.97 0.95 0.31 0.67 0.94 1.04</cell></row><row><cell>TP-RNN (Ours)</cell><cell></cell><cell cols="4">0.25 0.41 0.58 0.65 0.20 0.33 0.53 0.67 0.26 0.47 0.88 0.90 0.30 0.66 0.96 1.04</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>.39 0.68 0.99 1.35 1.32 0.27 0.48 0.73 1.04 1.38 0.26 0.48 0.97 1.02 1.69 0.31 0.67 0.94 1.41 1.96 TP-RNN (Ours) 0.25 0.41 0.58 0.74 0.77 0.20 0.33 0.53 0.84 1.14 0.26 0.48 0.88 0.98 1.66 0.30 0.66 0.98 1.39 1.74</figDesc><table><row><cell cols="6">MAE for four action classes in the long-term forecasting experiments (prior-work results from SRNN [18], Dropout-</cell></row><row><cell cols="6">AutoEncoder [16], and code from [23]). In each column, the best obtained results are typeset with boldface and the second</cell></row><row><cell cols="5">best are underlined. AA: Action-Agnostic, N/A: Not Applicable, Dropout-AE: Dropout-AutoEncoder.</cell></row><row><cell></cell><cell>AA</cell><cell>Walking</cell><cell>Eating</cell><cell>Smoking</cell><cell>Discussion</cell></row><row><cell>milliseconds</cell><cell cols="5">80 160 320 560 1000 80 160 320 560 1000 80 160 320 560 1000 80 160 320 560 1000</cell></row><row><cell>ERD [15]</cell><cell cols="5">1.30 1.56 1.84 2.00 2.38 1.66 1.93 2.88 2.36 2.41 2.34 2.74 3.73 3.68 3.82 2.67 2.97 3.23 3.47 2.92</cell></row><row><cell cols="6">LSTM-3LR [15] 1.18 1.50 1.67 1.81 2.20 1.36 1.79 2.29 2.49 2.82 2.05 2.34 3.10 3.24 3.42 2.25 2.33 2.45 2.48 2.93</cell></row><row><cell>SRNN [18]</cell><cell cols="5">1.08 1.34 1.60 1.90 2.13 1.35 1.71 2.12 2.28 2.58 1.90 2.30 2.90 3.21 3.23 1.67 2.03 2.20 2.39 2.43</cell></row><row><cell cols="6">Dropout-AE [16] 1.00 1.11 1.39 1.55 1.39 1.31 1.49 1.86 1.76 2.01 0.92 1.03 1.15 1.38 1.77 1.11 1.20 1.38 1.53 1.73</cell></row><row><cell>Residual [23]</cell><cell cols="5">0.32 0.54 0.72 0.86 0.96 0.25 0.42 0.64 0.94 1.30 0.33 0.60 1.01 1.23 1.83 0.34 0.74 1.04 1.43 1.75</cell></row><row><cell>Zero-velocity</cell><cell>N/A 0</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Long-term forecasting MAE comparison for the remaining 11 actions in Human 3.6 dataset. RNN 0.59 0.82 1.12 1.18 1.52 2.28 0.41 0.66 1.07 1.22 1.35 1.74 0.41 0.79 1.13 1.27 1.47 1.93 0.26 0.51 0.80 0.95 1.08 1.35 RNN 0.30 0.60 1.09 1.31 1.71 2.46 0.53 0.93 1.24 1.38 1.73 1.98 0.23 0.47 0.67 0.71 0.78 1.28 0.37 0.66 0.99 1.11 1.30 1.71</figDesc><table><row><cell></cell><cell>Directions</cell><cell>Greeting</cell><cell>Talking on the phone</cell><cell>Posing</cell></row><row><cell cols="5">millisec 80 160 320 400 560 1000 80 160 320 400 560 1000 80 160 320 400 560 1000 80 160 320 400 560 1000</cell></row><row><cell>[23]</cell><cell cols="4">0.44 0.69 0.83 0.94 1.03 1.49 0.53 0.88 1.29 1.45 1.72 1.89 0.61 1.12 1.57 1.74 1.59 1.92 0.47 0.87 1.49 1.76 1.96 2.35</cell></row><row><cell cols="5">TP-RNN 0.38 0.59 0.75 0.83 0.95 1.38 0.51 0.86 1.27 1.44 1.72 1.81 0.57 1.08 1.44 1.59 1.47 1.68 0.42 0.76 1.29 1.54 1.75 2.47</cell></row><row><cell></cell><cell>Purchases</cell><cell>Sitting</cell><cell>Sitting down</cell><cell>Taking photo</cell></row><row><cell cols="5">millisec 80 160 320 400 560 1000 80 160 320 400 560 1000 80 160 320 400 560 1000 80 160 320 400 560 1000</cell></row><row><cell>[23]</cell><cell cols="4">0.60 0.86 1.24 1.30 1.58 2.26 0.44 0.74 1.19 1.40 1.57 2.03 0.51 0.93 1.44 1.65 1.94 2.55 0.33 0.65 0.97 1.09 1.19 1.47</cell></row><row><cell cols="2">TP-Waiting</cell><cell>Walking dog</cell><cell>Walking together</cell><cell>Average of all 15</cell></row><row><cell cols="5">millisec 80 160 320 400 560 1000 80 160 320 400 560 1000 80 160 320 400 560 1000 80 160 320 400 560 1000</cell></row><row><cell>[23]</cell><cell cols="4">0.34 0.65 1.09 1.28 1.61 2.27 0.56 0.95 1.28 1.39 1.68 1.92 0.31 0.61 0.84 0.89 1.00 1.43 0.43 0.75 1.11 1.24 1.42 1.83</cell></row><row><cell>TP-</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison of average MAE across all 15 actions of Human 3.6 dataset with prior and baseline models (ablation study). In each column, the best results are typeset in bold and the second best are underlined. For 'Residual<ref type="bibr" target="#b23">[23]</ref>', both short-term (from paper) and long-term (from code) results are reported. AA: Action-Agnostic.</figDesc><table><row><cell></cell><cell cols="2">AA 80 160 320 400 560 1000</cell></row><row><cell>Residual [23] (short)</cell><cell>0.36 0.67 1.02 1.15 -</cell><cell>-</cell></row><row><cell>Residual [23] (short)</cell><cell>0.39 0.72 1.08 1.22 -</cell><cell>-</cell></row><row><cell>Residual [23] (long)</cell><cell cols="2">0.43 0.75 1.11 1.24 1.42 1.83</cell></row><row><cell>Residual [23] (long)</cell><cell cols="2">0.42 0.73 1.09 1.23 1.42 1.84</cell></row><row><cell>Zero-velocity</cell><cell cols="2">-0.40 0.71 1.07 1.21 1.42 1.85</cell></row><row><cell>Single Layer (Pose)</cell><cell cols="2">0.49 0.83 1.20 1.34 1.53 1.92</cell></row><row><cell>Single Layer (Vel.)</cell><cell cols="2">0.39 0.67 1.00 1.13 1.32 1.73</cell></row><row><cell cols="3">Stacked 2-Layer (Vel.) 0.38 0.66 1.01 1.13 1.32 1.74</cell></row><row><cell>Double-scale (Vel.)</cell><cell cols="2">0.37 0.66 0.99 1.11 1.30 1.73</cell></row><row><cell cols="3">Double-scale (Hier., Vel.) 0.37 0.66 1.00 1.12 1.32 1.76</cell></row><row><cell cols="3">Double-scale (Phase, Vel.) 0.37 0.66 1.00 1.12 1.31 1.72</cell></row><row><cell>TP-RNN (Ours)</cell><cell cols="2">0.37 0.66 0.99 1.11 1.30 1.71</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparison with prior works on the Penn Action dataset in terms of PCK@0.05. Best results are typset in bold, and the second best are underlined. TP-RNN is tested with or without incorporating an initial pose velocity (details in the text).68.3 58.5 50.9 44.7 40.0 36.4 33.4 31.3 29.5 28.3 27.3 26.4 25.7 25.0 24.5 3D-PFNet [11] 79.2 60.0 49.0 43.9 41.5 40.3 39.8 39.7 40.1 40.5 41.1 41.6 42.3 42.9 43.2 43.3 TP-RNN w/o init vel. 82.3 68.9 61.5 56.9 53.9 51.7 50.0 48.5 47.3 46.2 45.6 45.0 44.6 44.3 44.1 43.9 TP-RNN w/ init vel. 84.5 72.0 64.8 60.3 57.2 55.0 53.4 52.1 50.9 50.0 49.3 48.7 48.3 47.9 47.6 47.3</figDesc><table><row><cell>Future frame</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>12</cell><cell>13</cell><cell>14</cell><cell>15</cell><cell>16</cell></row><row><cell cols="2">Residual [23] 82.4 Past</cell><cell></cell><cell>Future</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Past</cell><cell></cell><cell></cell><cell>Future</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements We thank Panasonic for their support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-label discriminative weakly-supervised human activity recognition and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adeli-Mosabbeb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fathy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="241" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Encouraging lstms to anticipate actions very early</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Aliakbarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andersson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="280" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Monocular 3d pose estimation and tracking by detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="623" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Non-verbal communication in human social interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Argyle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1972" />
			<publisher>Cambridge U. Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Wasserstein gan. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Structured prediction with short/longrange dependencies for human activity recognition from depth skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Arzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Aghajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Azirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Raahemifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Intelligent Robots and Systems</title>
		<imprint>
			<biblScope unit="page">2017</biblScope>
			<publisher>IROS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<title level="m">IEEE/RSJ International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="560" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">End-to-end attention-based large vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4945" to="4949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Hp-gan: Probabilistic 3d human motion prediction via gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Barsoum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09561</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep representation learning for human motion prediction and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>B?tepage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kragic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kjellstr?m</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Realtime multiperson 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Forecasting human dynamics from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hierarchical multiscale recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural networks for long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">El</forename><surname>Hihi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="493" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recurrent network models for human dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2015 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4346" to="4354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning human motion models for long-term predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Aksan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02827</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hu-man3.6M: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Structuralrnn: Deep learning on spatio-temporal graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5308" to="5317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A clockwork rnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Koutnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1863" to="1871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A hierarchical representation for future action prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="689" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Predicting deeper into the future of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">of: ICCV 2017-International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Geometry-based next frame prediction from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1700" to="1707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On human motion prediction using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Vnect: Realtime 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">44</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">3d human pose estimation using 2d-data and an alternative phase space representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Granum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Procedure Humans</title>
		<meeting>edure Humans</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Pintea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="172" to="187" />
		</imprint>
	</monogr>
	<note>D?ja vu</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Randomized trees for human pose detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rihan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramalingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Orrite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>CVPR 2008. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Lcr-net: Localization-classification-regression for human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2017-IEEE Conference on Computer Vision &amp; Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Anticipation in human-robot cooperation: A recurrent neural network approach for multiple action sequences prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schydlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rakovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jamone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Santos-Victor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.10503</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Real-time human pose recognition in parts from single depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1297" to="1304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="362" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Online localization and prediction of actions and interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Lifting from the deep: Convolutional 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings</title>
		<imprint>
			<biblScope unit="page" from="2500" to="2509" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Human pose forecasting via deep markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Toyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09240</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Generating videos with scene dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="613" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An uncertain future: Forecasting from static images using variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="835" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dense optical flow prediction from a static image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2015 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2443" to="2451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The pose knows: Video forecasting by generating pose futures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3352" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A long short-term memory model for answer sentence selection in question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nyberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="707" to="712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Gaussian process dynamical models for human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="283" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4724" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Leveraging hierarchical parametric networks for skeletal joints based action segmentation and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="724" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Accurate 3d pose estimation from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2011 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="731" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">From actemes to action: A strongly-supervised representation for detailed action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
