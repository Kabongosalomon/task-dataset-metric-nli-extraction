<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Classification of Point Cloud Scenes with Multiscale Voxel Deep Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Roynard</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Robotics</orgName>
								<orgName type="institution" key="instit1">Mines ParisTech</orgName>
								<orgName type="institution" key="instit2">PSL Research University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Deschaud</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Robotics</orgName>
								<orgName type="institution" key="instit1">Mines ParisTech</orgName>
								<orgName type="institution" key="instit2">PSL Research University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Goulette</surname></persName>
							<email>francois.goulette@mines-paristech.fr</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Robotics</orgName>
								<orgName type="institution" key="instit1">Mines ParisTech</orgName>
								<orgName type="institution" key="instit2">PSL Research University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Classification of Point Cloud Scenes with Multiscale Voxel Deep Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>natural ter- rain</term>
					<term>green: high vegetation</term>
					<term>light green: low vegetation</term>
					<term>chartreuse green: buildings</term>
					<term>yellow: hard scape</term>
					<term>orange: scanning artefacts</term>
					<term>red: cars)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this article we describe a new convolutional neural network (CNN) to classify 3D point clouds of urban or indoor scenes. Solutions are given to the problems encountered working on scene point clouds, and a network is described that allows for point classification using only the position of points in a multi-scale neighborhood.</p><p>On the reduced-8 Semantic3D benchmark <ref type="bibr" target="#b7">[8]</ref>, this network, ranked second, beats the state of the art of point classification methods (those not using a regularization step). Figure 1: Example of classified point cloud on Semantic3D test set (blue: man-made terrain, cerulean blue: natural terrain, green: high vegetation, light green: low vegetation, chartreuse green: buildings, yellow: hard scape, orange: scanning artefacts, red: cars).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Autonomous systems need 3D maps of the world for perception and navigation tasks. These maps can be represented as 3D point clouds with all semantic information needed like lane borders, traffic signs...</p><p>Mobile Laser Scanning (MLS) systems can now scan large areas, like cities or even countries. The produced 3D point clouds can be used as maps for autonomous systems. To do so, the automatic classification of the data is necessary and is still challenging, regards to the number of objects present in an urban scene. For the object classification task, deep-learning methods work very well on 2D images. The easiest way to transfer these methods to 3D is to use 3D grids. It works well when the data is just one single object <ref type="bibr" target="#b27">[28]</ref>.</p><p>But it is much more complicated for the task of point classification of a complete scene (e. g. an urban cloud) made up of many objects of very different sizes and potentially interwoven with each other (e. g. a lamppost passing through vegetation). Moreover, in this kind of scene, there are classes more represented (floor and buildings) than others (pedestrians, traffic signs).</p><p>This article proposes both a training method that balances the number of points per class during each epoch, and a 3D CNN capable of effectively learning how to classify scenes containing objects at multiple scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">STATE OF THE ART 2.1 Shallow and Multi-Scale Learning for 3D point cloud classification</head><p>There is a great variety of work for classifying 3D point cloud scenes by shallow learning methods or without learning. Methods can generally be classified into one of the two approaches: classify each point, then group them into objects, or conversely, divide the cloud into objects and classify each object.</p><p>The first approach is followed by <ref type="bibr" target="#b25">[26]</ref> which classifies each point by calculating simple descriptors such as dimensionality attributes on an optimal neighborhood (minimizing some entropy depending on dimensionality attributes). <ref type="bibr" target="#b8">[9]</ref> introduced multi-scale features, computing the same kind of features at different scales to capture both context and local shape around the point. After classifying each point, the points can be grouped into objects by CRF <ref type="bibr" target="#b29">[30]</ref> or by regularization methods <ref type="bibr" target="#b12">[13]</ref>.</p><p>The segmentation step of the second approach is usually heuristic-based and contains no learning. <ref type="bibr" target="#b0">[1]</ref> segments the cloud using super-voxels, <ref type="bibr" target="#b20">[21]</ref> uses mathematical morphology operators and <ref type="bibr" target="#b18">[19]</ref> makes a region growth to extract the soil, then groups the points by related component. After segmentation, objects are classified by computing global descriptors that can be simple geometrical descriptors <ref type="bibr" target="#b20">[21]</ref>, shape functions <ref type="bibr" target="#b26">[27]</ref> or histograms of distribution of normals <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep-Learning for 3D point cloud classification</head><p>Over the past three years, there has been a growing body of work that attempts to adapt deep learning methods or introduces new "deep" approaches to classifying 3D point clouds. This is well illustrated by the ShapeNet Core55 challenge <ref type="bibr" target="#b28">[29]</ref>, which involved 10 research teams and resulted in the design of new network architectures on both voxel grids and point cloud. The best architectures have beaten the state of the art on the two proposed tasks: part-level segmentation of 3D shapes and 3D reconstruction from single view image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">on 2D Views of the cloud</head><p>The most direct approach is to apply 2D networks to images obtained from the point cloud. Among other things, we can think of the following projections:</p><p>? RGB image rendered from a virtual camera, ? depth-map, from a virtual camera,</p><p>? range image, directly from the sensor,</p><p>? panorama image <ref type="bibr" target="#b21">[22]</ref>,</p><p>? elevation-map.</p><p>These methods can be improved by taking multiple views of the same object or scene, and then voting or fusing the results <ref type="bibr" target="#b3">[4]</ref> (ranked 5th on reduced-8 Semantic benchmark). In addition, these methods greatly benefit from existing 2D expertise and pre-trained networks on image datasets <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15]</ref> that contain much more data than point cloud datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">on Voxel Grid</head><p>The first deep networks used to classify 3D point clouds date from 2015 with VoxNet <ref type="bibr" target="#b15">[16]</ref>, this network transforms an object instance by filling in an occupancy or density grid and then applies a Convolutional Neural Network (CNN). Later <ref type="bibr" target="#b10">[11]</ref> applied the same type of network to classify clouds of urban points, the network then predicts the class of a point from the occupancy grid of its neighborhood. However, we cannot compare with this architecture because the experimental data has not been published. Best results on ModelNet benchmarks are obtained using deeper CNNs <ref type="bibr" target="#b4">[5]</ref> based on the architecture of Inception-resnet <ref type="bibr" target="#b23">[24]</ref> and voting on multiple 3D view of objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">on Graph</head><p>Another approach is to use graphs, indeed the raw point cloud having no structure, it is very difficult to derive general information from it. Whereas a graph gives relations of neighborhoods and distances between points and allows for example to make convolutions as in SPGraph <ref type="bibr" target="#b13">[14]</ref> or to apply graph-cut methods on CRF as in SEGCloud <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4">on Point Cloud</head><p>For the time being, there are still quite a few methods that take the point cloud directly as input. These methods have the advantage of working as close as possible to the raw data, so we can imagine that they will be the most efficient in the future. The first method of this type is PointNet <ref type="bibr" target="#b16">[17]</ref> which gets fairly good results on ModelNet for object instance classification. PointNet is based on the observation that a point cloud is a set and therefore verifies some symmetries (point switching, point addition already in the set...) and is therefore based on the use of operators respecting these symmetries like the global Pooling, but these architectures lose the hierarchical aspect of the calculations that make the strength of the CNN. This gap has been filled with PointNet++ <ref type="bibr" target="#b17">[18]</ref> which extracts neighborhoods in the cloud, applies PoinNet and groups the points hierarchically to gradually aggregate the information as in a CNN. Two other approaches are proposed by <ref type="bibr" target="#b6">[7]</ref> to further account for the context. The first uses PointNet on multiscale neighborhoods, the second uses PointNet on clouds extracted from a 2D grid and uses recurrent networks to share information between grid boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">APPROACH</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Learning on fully annotated registered point clouds</head><p>Training on scenes point cloud leads to some difficulties not faced when the point cloud is a single object. For the point classification task, each point is a sample, so the number of samples per class is very unbalanced (from thousands of points for the class "pedestrian" to tens of millions for the class "ground"). Also with the training method of deep-learning, an Epoch would be to pass through all points of the cloud, which would take a lot of time. Indeed, two very close points have the same neighborhood, and will therefore be classified in the same way. Moreover, for each point one needs to retrieve a neighborhood of the point at a certain scale (or several scales). Even with structures optimized for this task (such as k-d tree or octree) this step can take a lot of time on very large clouds.</p><p>We propose a training method that solves these two problems. We randomly select N (for example 1000) points in each class, then we train on these points mixed randomly between classes, and we renew this mechanism at the beginning of each Epoch.</p><p>Once a point p to classify is chosen, we compute a grid of voxels given to the convolutional network by building an occupancy grid centered on p whose empty voxels contain 0 and occupied voxels contain 1. We only use n ? n ? n cubic grids where n is pair, and we only use isotropic space discretization steps ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data Augmentation and Training</head><p>Some classic data augmentation steps are performed before projecting the 3D point clouds into the voxels grid:</p><p>? Flip x and y axis, with probability 0.5</p><p>? Random rotation around z-axis ? Random scale, between 95% and 105%</p><p>? Random occlusions (randomly removing points), up to 5%</p><p>? Random artefacts (randomly inserting points), up to 5%</p><p>? Random noise in position of points, the noise follows a normal distribution centered in 0 with standard deviation 0.01m</p><p>The cost function used is cross-entropy, and the optimizer used is ADAM <ref type="bibr" target="#b11">[12]</ref> with a learning rate of 0.001 and ? = 10 ?8 , which are the default settings in most deep-learning libraries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Test</head><p>To label a complete point cloud scene, the naive method is to go through all the points of the cloud, and for each point:</p><p>? look for all the neighboring points that fit into the occupation grid,</p><p>? create this grid,</p><p>? infer the class of the point via the pre-trained network.</p><p>However, two points very close to each other will have the same neighborhood occupancy grid and therefore the network will predict the same class. A faster test method is therefore to sub-sample the cloud to be tested. This has two beneficial effects: reduce the number of inferences and neighborhood searches, and each neighborhood search takes less time. To infer the point class of the initial cloud, we give each point the class of the nearest point in the subsampled cloud, which can be done efficiently if the subsampling method used retains the correct information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">NETWORK ARCHITECTURES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">3D essential layers</head><p>We denote:</p><p>? Conv(n, k, s, p) a convolutional layer that transforms feature maps from previous layer into n new feature maps, with a kernel of size k ? k ? k and stride s and pads p on each side of the grid.</p><p>? DeConv(n, k, s, p) a transposed convolutional layer that transforms feature maps from previous layer into n new feature maps, with a kernel of size k ? k ? k and stride s and pads p on each side of the grid.</p><p>? FC(n) a fully-connected layer that transforms the feature maps from previous layer into n feature maps.</p><p>? MaxPool(k) a layer that aggregates on each feature map every group of 8 neigboring voxels.</p><p>? MaxUnPool(k) a layer that computes an inverse of MaxPool(k).</p><p>? ReLU, LeakyReLU and PReLU common non-linearities used after linear layers as Conv and FC. ReLU(x) returns the positive part of x, and to avoid null gradient if x is negative, we can add a slight slope which is fixed (LeakyReLU) or can be learned (PReLU).</p><p>? So f tMax a non-linearity layer that rescales a tensor in the range [0, 1] with sum 1.</p><p>? BatchNorm a layer that normalizes samples over a batch.</p><p>? DropOut(p) a layer that randomly zeroes some of the elements of the input tensor with probability p.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Classification Network Architecture</head><p>The choosen network architecture is inspired from <ref type="bibr" target="#b22">[23]</ref> that works well in 2D. Our network follows the architecture:</p><formula xml:id="formula_0">Conv(32, 3, 1, 0) ? Conv(32, 3, 1, 0) ? MaxPool(2) ? Conv(64, 3, 1, 0) ? Conv(64, 3, 1, 0) ? MaxPool(2) ? FC(1024) ? FC(N c )</formula><p>where N c is the number of classes, and each Conv and FC layer is followed by BatchNorm ? PReLU and a Squeezeand-Excitation block <ref type="bibr" target="#b9">[10]</ref> except the last FC layer that is followed by a So f tMax layer. This network takes as input a 3D occupancy grid of size 32 ? 32 ? 32, where each voxel of the grid contains 0 (empty) or 1 (occupied) and has a size of 10cm ? 10cm ? 10cm. This type of method is very dependent on the space discretization step ? selected. Indeed, a small ? allows to understand the object finely around the point and its texture (for example to differentiate the natural ground from the ground made by man) but a large ? allows to understand the context of the object (for example if it is locally flat and horizontal around the point there can be ambiguity between the ground and the ceiling, but there is no more ambiguity if we add context).</p><p>Since a 3D scene contains objects at several scales, this type of network can have difficulty classifying certain objects. So we also propose a multiscale version of our network called MSK_DeepVoxScene for the K-scales version (or abbreviated in MSK_DVS).</p><p>We take several versions of the previous network without the fully-connected layer. The input of each version is given a grid of the same size 32 ? 32 ? 32, but with different sizes of voxels (for example 5cm, 10cm and 15cm). We then retrieve a vector of 1024 characteristics from each version, which we concatenate before giving to a fully-connected classifier layer. See <ref type="figure" target="#fig_0">figure 2</ref> for a graphical representation of MS3_DeepVoxScene. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Name</head><p>LiDAR type Covered Area</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of points (subsampled) Number of classes</head><p>Paris-Lille-3D <ref type="bibr" target="#b19">[20]</ref> multi-fiber MLS 55000m 2 143.1M (44.0M) 9 Semantic3D <ref type="bibr" target="#b7">[8]</ref> static LiDAR 110000m 2 1660M (79.5M) 8 S3DIS <ref type="bibr" target="#b2">[3]</ref> MatterPort 6020m 2 695.9M (36.9M) 13 <ref type="table">Table 1</ref>: Comparison of 3D point cloud scenes datasets. Paris-Lille-3D contains 50 classes but for our experimentations we keep only 9 coarser classes. In brackets is indicated the number of points after subsampling at 2 cm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>To carry out our experiments we have chosen the 3 datasets of 3D scenes which seem to us the most relevant to train methods of deep-learning, Paris-Lille-3D <ref type="bibr" target="#b19">[20]</ref>, S3DIS <ref type="bibr" target="#b2">[3]</ref> and Semantic3D <ref type="bibr" target="#b7">[8]</ref>. Among the 3D point cloud scenes datasets, these are those with the most area covered and the most variability (see <ref type="table">table 1</ref>). The covered area is obtained by projecting each cloud on an horizontal plane in pixels of size 10cm ? 10cm, then summing the area of all occupied pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Paris-Lille-3D</head><p>The Paris-Lille-3D dataset consists of 2 km of 3D point clouds acquired by Mobile Laser Scanning using with a Velodyne HDL-32e mounted on a van. Clouds are georeferenced using IMU and GPS-RTK only, no registration or SLAM methods are used, resulting in a slight noise. Because the scene is scanned at approximately constant speed, the point density is roughly uniform. The dataset consists of 3 files, one acquired in Paris and two acquired in Lille including Lille1.ply much larger than Lille2.ply. To validate our architectures by K-fold method, we cut Lille1.ply into two folds containing the same number of points. In addition, this dataset contains 50 classes, some of which only appear in some folds and with very few points. We therefore decide to delete and group together some classes to keep only 9 coarser classes: ground buildings poles bollards trash cans barriers pedestrians cars natural Some qualitative results on Paris-Lille-3D dataset are shown in <ref type="figure" target="#fig_1">figure 3</ref>. We can observe that some trunks of trees are classified as poles. It may means that the context is not sufficiently taken into account (even so the 15 cm grid is 4.8 m large) In addition, the ground around objects (except cars) is classified as belonging to the object. One can imagine that cars are not affected by this phenomenon because this class is very present in the dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Semantic3D</head><p>The Semantic3D dataset was acquired by static laser scanners, it is therefore more dense than a dataset acquired by MLS as Paris-Lille-3D, but the density of points varies considerably depending on the distance to the sensor. And there are occlusions due to the fact that sensors do not turn around the objects. Even by registering several clouds acquired from different viewpoints, there are still a lot of occlusions. To minimize the problem of very variable density, we subsample the training clouds at 2 cm. This results in a more uniform density at least close to the sensor and avoids redundant points. After subsampling, the dataset contains 79.5M points. Some qualitative results on Semantic3D dataset are shown in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">S3DIS</head><p>The S3DIS dataset is made up of 6 RGB 3D point cloud scenes taken from 3 different buildings and containing 13 classes. On this dataset we sub-sample the clouds to 2 cm in the same way as for the Semantic3D dataset. After sub-sampling, the entire dataset contains 36, 9M points. As done in <ref type="bibr" target="#b24">[25]</ref> we compare our results only on fold 5 since this cloud was acquired in another building than the other 5 clouds. Some qualitative results on S3DIS dataset are shown in <ref type="figure" target="#fig_2">figure 4</ref>. We observe above all that there is a big confusion between the clutter class and the other classes, this can be explained because this class contains objects of all shapes and sizes that can sometimes resemble objects of an existing class. In addition, as on Paris-Lille-3D dataset, the floor around objects such as chairs is classified as belonging to the object instead of the floor. One can guess that during training the network saw only few points on the border between the floor and a chair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Metrics</head><p>To confirm the interest of multi-scale CNNs, we compare the performance of our two architectures on these three datasets. And on Semantic3D and S3DIS we compare our results with those of the literature. The metrics used to evaluate performance </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison with the state of the art</head><p>For a comparison with the state-of-the-art methods on reduced-8 Semantic3D benchmark see table 2. For MS1_DeepVoxScene several resolutions have been tested, and by cross-validation on the Semantic3D training set the 10 cm resolution is the one that maximizes validation accuracy. DeepVoxScene's choice of MS3_DeepVoxScene resolution results from this observation, we keep a resolution that obtains good performance in general, and we add a finer resolution of 5 cm to better capture the local surface near the point, and a coarser resolution of 15 cm to better understand the context of the object to which the point belongs. Our method achieves better results than all methods that classify cloud by points (i. e. without regularization). Better results could probably be achieved by adding for exemple a CRF after classification.</p><p>For a comparison with the state-of-the-art methods on S3DIS 5th fold see <ref type="table" target="#tab_2">table 3</ref>. We observe a confusion between the classes wall and board (and more slightly with beam, column, window and door), this is explained mainly because these classes are very similar geometrically and we do not use color. To improve these results, we should not sub-sample the clouds to keep the geometric information thin (such as the table slightly protruding from the wall) and add a 2 cm scale in input to the network, but looking for neighbourhoods would then take an unacceptable amount of time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Study of the different architectures</head><p>To evaluate our architecture choices, we tested this classification task by one of the first 3D convolutional networks: VoxNet <ref type="bibr" target="#b15">[16]</ref>. This allows us both to validate the choices made for the generic architecture of the MS1_DeepVoxScene network and to validate the interest of the multi-scale network. We reimplemented VoxNet using the deep-learning library Pytorch. See     See <ref type="table" target="#tab_4">table 5</ref> for a comparison per class between MS1_DeepVoxScene and MS3_DeepVoxScene on Paris-Lille-3D dataset. This shows that the use of multi-scale networks improves the results on some classes, in particular the buildings, barriers and pedestrians classes are greatly improved (especially in Recall), while the car class loses a lot of Precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>We have proposed both a training method that balances the number of points per class seen during each epoch, as well as a multi-scale CNN that is capable of learning to classify point cloud scenes. This is achieved by both focusing on the local shape of the object around a point and by taking into account the context of the object.</p><p>We validated the use of our multi-scale network for 3D scene classification by ranking second on Semantic3D benchmark and by ranking better than state-of-the-art point classification methods (those without regularization).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Our Multi-Scale Voxel Network architecture: MS3_DeepVoxScene (all tensors are represented as 2D tensors instead of 3D for simplicity).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Example of classified point cloud on Paris-Lille-3D dataset. Left: classified with MS3_DVS, right: ground truth (blue: ground, cerulean blue: buildings, dark green: poles, green: bollards, light green: trash cans, yellow: barriers, dark yellow: pedestrians, orange: cars, red: natural).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Example of classified point cloud on S3DIS dataset. Left: classified with MS3_DVS, right: ground truth (blue: clutter, cerulean blue: floor, dark green: wall, green: column, dark yellow: chair, light yellow: table, dark orange: bookcase, light orange: sofa).are the following:P c = T P c T P c + FP c R c = T P c T P c + FN c F1 c = 2T P c 2T P c + FP c + FN c = 2 P c R c P c + R cAcc c = T P c T P c + FN c IoU c = T P c T P c + FP c + FN c Where P c , R c , F1 c , Acc c and IoU c represent respectively Precision, Recall, F1-score, Accuracy and Intersection-over-Union score of class c. And T P c , T N c , FP c and FN c are respectively the number of True-Positives, True-Negatives, False-Positives and False-Negatives in class c.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>table 4</head><label>4</label><figDesc>for a comparison between VoxNet<ref type="bibr" target="#b15">[16]</ref>, MS1_DeepVoxScene and MS3_DeepVoxScene on the 3 datasets.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Per class IoU</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Rank</cell><cell>Method</cell><cell>Averaged IoU</cell><cell>Overall Accuracy</cell><cell>man-made</cell><cell>terrain</cell><cell>natural</cell><cell>terrain</cell><cell>high</cell><cell>vegetation</cell><cell>low</cell><cell>vegetation</cell><cell>buildings</cell><cell>hard scape</cell><cell>scanning</cell><cell>artefacts</cell><cell>cars</cell></row><row><cell>1</cell><cell>SPGraph[14]</cell><cell>73.2%</cell><cell>94.0%</cell><cell cols="13">97.4% 92.6% 87.9% 44.0% 93.2% 31.0% 63.5% 76.2%</cell></row><row><cell>2</cell><cell>MS3_DVS(Ours)</cell><cell>65.3%</cell><cell>88.4%</cell><cell cols="13">83.0% 67.2% 83.8% 36.7% 92.4% 31.3% 50.0% 78.2%</cell></row><row><cell>3</cell><cell>RF_MSSF</cell><cell>62.7%</cell><cell>90.3%</cell><cell cols="13">87.6% 80.3% 81.8% 36.4% 92.2% 24.1% 42.6% 56.6%</cell></row><row><cell>4</cell><cell>SegCloud[25]</cell><cell>61.3%</cell><cell>88.1%</cell><cell cols="13">83.9% 66.0% 86.0% 40.5% 91.1% 30.9% 27.5% 64.3%</cell></row><row><cell>5</cell><cell>SnapNet_[4]</cell><cell>59.1%</cell><cell>88.6%</cell><cell cols="13">82.0% 77.3% 79.7% 22.9% 91.1% 18.4% 37.3% 64.4%</cell></row><row><cell>9</cell><cell>MS1_DVS(Ours)</cell><cell>57.1%</cell><cell>84.8%</cell><cell cols="13">82.7% 53.1% 83.8% 28.7% 89.9% 23.6% 29.8% 65.0%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Top-5 Results on Semantic3D reduced-8 testing set. MS3_DVS is our MS3_DeepVoxScene with voxel sizes of 5 cm, 10 cm and 15 cm and MS1_DVS is our MS1_DeepVoxScene with voxel size of 10 cm (added for comparison with non multi-scale deep network).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Per class IoU (in %)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Mean IoU</cell><cell>Mean Accuracy</cell><cell>ceiling</cell><cell>floor</cell><cell>wall</cell><cell>beam</cell><cell>column</cell><cell>window</cell><cell>door</cell><cell>chair</cell><cell>table</cell><cell>bookcase</cell><cell>sofa</cell><cell>board</cell><cell>clutter</cell></row><row><cell>PointNet [17]</cell><cell>41.09%</cell><cell>48.98%</cell><cell>88.80</cell><cell>97.33</cell><cell>69.80</cell><cell>0.05</cell><cell>3.92</cell><cell>46.26</cell><cell>10.76</cell><cell>52.61</cell><cell>58.93</cell><cell>40.28</cell><cell>5.85</cell><cell>26.38</cell><cell>33.22</cell></row><row><cell>MS3_DVS(Ours)</cell><cell>46.32%</cell><cell>57.93%</cell><cell>79.03</cell><cell>88.07</cell><cell>53.55</cell><cell>0.00</cell><cell>20.47</cell><cell>29.01</cell><cell>37.29</cell><cell>68.84</cell><cell>63.72</cell><cell>47.44</cell><cell>61.62</cell><cell>16.50</cell><cell>36.64</cell></row><row><cell>SEGCloud [25]</cell><cell>48.92%</cell><cell>57.35%</cell><cell>90.06</cell><cell>96.05</cell><cell>69.86</cell><cell>0.00</cell><cell>18.37</cell><cell>38.35</cell><cell>23.12</cell><cell>75.89</cell><cell>70.40</cell><cell>58.42</cell><cell>40.88</cell><cell>12.96</cell><cell>41.60</cell></row><row><cell>SPG [14]</cell><cell>54.67%</cell><cell>61.75%</cell><cell>91.49</cell><cell>97.89</cell><cell>75.89</cell><cell>0.00</cell><cell>14.25</cell><cell>51.34</cell><cell>52.29</cell><cell>86.35</cell><cell>77.40</cell><cell>65.49</cell><cell>40.38</cell><cell>7.23</cell><cell>50.67</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results on S3DIS 5th fold.</figDesc><table><row><cell>Class</cell><cell cols="3">MS3_DVS MS1_DVS VoxNet [16]</cell></row><row><cell>Paris-Lille-3D</cell><cell>89.29%</cell><cell>88.23%</cell><cell>86.59%</cell></row><row><cell>Semantic3D</cell><cell>79.36%</cell><cell>74.05%</cell><cell>71.66%</cell></row><row><cell>S3DIS</cell><cell>73.08%</cell><cell>69.36%</cell><cell>66.28%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison of mean F1 scores of MS3_DVS, MS1_DVS and VoxNet<ref type="bibr" target="#b15">[16]</ref>. For each dataset, the F1 score is average on all folds.</figDesc><table><row><cell></cell><cell cols="2">Precision</cell><cell cols="2">Recall</cell></row><row><cell>Class</cell><cell cols="4">MS3_DVS MS1_DVS MS3_DVS MS1_DVS</cell></row><row><cell>ground</cell><cell>97.74%</cell><cell>97.08%</cell><cell>98.70%</cell><cell>98.28%</cell></row><row><cell>buildings</cell><cell>85.50%</cell><cell>84.28%</cell><cell>95.27%</cell><cell>90.65%</cell></row><row><cell>poles</cell><cell>93.30%</cell><cell>92.27%</cell><cell>92.69%</cell><cell>94.16%</cell></row><row><cell>bollards</cell><cell>98.60%</cell><cell>98.61%</cell><cell>93.93%</cell><cell>94.16%</cell></row><row><cell>trash cans</cell><cell>95.31%</cell><cell>93.52%</cell><cell>79.60%</cell><cell>80.91%</cell></row><row><cell>barriers</cell><cell>85.70%</cell><cell>81.56%</cell><cell>77.08%</cell><cell>73.85%</cell></row><row><cell>pedestrians</cell><cell>98.53%</cell><cell>93.62%</cell><cell>95.42%</cell><cell>92.89%</cell></row><row><cell>cars</cell><cell>93.51%</cell><cell>96.41%</cell><cell>98.38%</cell><cell>97.71%</cell></row><row><cell>natural</cell><cell>89.51%</cell><cell>88.23%</cell><cell>92.52%</cell><cell>91.53%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Per class Precision and Recall averaged on the 4 folds of Paris-Lille-3D dataset.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Segmentation based classification of 3d urban point clouds: A supervoxel based approach with evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Aijazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Checchin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Trassoudaine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1624" to="1650" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cad-model recognition and 6dof pose estimation using 3d cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aldoma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vincze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gossow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gedikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Workshops (ICCV Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="585" to="592" />
		</imprint>
	</monogr>
	<note>2011 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">3d semantic parsing of large-scale indoor spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Brilakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unstructured point cloud semantic labeling using deep segmentation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Audebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics Workshop on 3D Object Retrieval</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.04236</idno>
		<title level="m">Generative and discriminative voxel modeling with convolutional neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploring spatial context for 3d semantic segmentation of point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kontogianni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="716" to="724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">net: A new large-scale point cloud cassification benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Semantic3d</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, volume IV-1-W1</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="91" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Fast semantic segmentation of 3d point clouds with strongly varying density. ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="177" to="184" />
			<pubPlace>Prague, Czech Republic</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Squeeze-and-Excitation Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-09" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Point cloud labeling using 3d convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition (ICPR), 2016 23rd International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2670" to="2675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A structured regularization framework for spatially smoothing semantic labelings of 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Raguet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vallet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mallet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weinmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="page" from="102" to="118" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Large-scale point cloud semantic segmentation with superpoint graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09869</idno>
		<imprint>
			<date type="published" when="2017-11" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00593</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5105" to="5114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Fast and robust segmentation and classification for change detection in urban point clouds. ISPRS -International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Roynard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
		<idno>XLI-B3:693-699</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Paris-Lille-3D: a large and high-quality ground truth urban point cloud dataset for automatic segmentation and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Roynard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-11" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Detection, segmentation and classification of 3d urban objects using mathematical morphology and supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Serna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="243" to="255" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ensemble of panorama-based convolutional neural networks for 3d model classification and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sfikas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Pratikakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Theoharis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-09" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Segcloud</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.07563</idno>
		<title level="m">Semantic segmentation of 3d point clouds</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semantic point cloud interpretation based on optimal neighborhoods, relevant features and efficient classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weinmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jutzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mallet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="286" to="304" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ensemble of shape functions for 3d object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wohlkinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vincze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Biomimetics (ROBIO), 2011 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2987" to="2992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Largescale 3d shape reconstruction and segmentation from shapenet core55</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.06104</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sensor fusion for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Candra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vetter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zakhor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation (ICRA), 2015 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1850" to="1857" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
