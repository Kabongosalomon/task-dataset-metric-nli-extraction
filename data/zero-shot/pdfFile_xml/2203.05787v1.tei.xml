<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Democracy Does Matter: Comprehensive Feature Mining for Co-Salient Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyue</forename><surname>Yu</surname></persName>
							<email>siyue.yu@xjtlu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">XJTLU</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Liverpool</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimin</forename><surname>Xiao</surname></persName>
							<email>jimin.xiao@xjtlu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">XJTLU</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingfeng</forename><surname>Zhang</surname></persName>
							<email>bingfeng.zhang@xjtlu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">XJTLU</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Liverpool</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eng</forename><forename type="middle">Gee</forename><surname>Lim</surname></persName>
							<email>enggee.lim@xjtlu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">XJTLU</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Democracy Does Matter: Comprehensive Feature Mining for Co-Salient Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Co-salient object detection, with the target of detecting co-existed salient objects among a group of images, is gaining popularity. Recent works use the attention mechanism or extra information to aggregate common co-salient features, leading to incomplete even incorrect responses for target objects. In this paper, we aim to mine comprehensive co-salient features with democracy and reduce background interference without introducing any extra information. To achieve this, we design a democratic prototype generation module to generate democratic response maps, covering sufficient co-salient regions and thereby involving more shared attributes of co-salient objects. Then a comprehensive prototype based on the response maps can be generated as a guide for final prediction. To suppress the noisy background information in the prototype, we propose a self-contrastive learning module, where both positive and negative pairs are formed without relying on additional classification information. Besides, we also design a democratic feature enhancement module to further strengthen the co-salient features by readjusting attention values. Extensive experiments show that our model obtains better performance than previous state-of-the-art methods, especially on challenging real-world cases (e.g., for CoCA, we obtain a gain of 2.0% for MAE, 5.4% for maximum Fmeasure, 2.3% for maximum E-measure, and 3.7% for Smeasure) under the same settings. Source code is available at https://github.com/siyueyu/DCFM .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Co-salient object detection (CoSOD) aims to detect the common salient objects among a group of input images. Unlike salient object detection (SOD), which is to detect the most attractive objects by mimicking human eyes <ref type="bibr">[3,21,</ref> * corresponding author <ref type="bibr" target="#b0">1</ref> The work was supported by National Natural Science Foundation of China under 61972323. Response maps generated by the previous approach <ref type="bibr" target="#b10">[11]</ref>; (c) Ours. It can be seen that ours can cover more co-salient objects. <ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b44">45]</ref>, CoSOD focuses on detecting salient yet co-existed objects among all the input images. In this case, CoSOD faces two main challenges: 1) reduce the interference of noisy background in complex scenes; 2) mine integral co-salient objects with large appearance variations. Some works introduce extra SOD dataset to provide saliency guidance <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53]</ref> or predict saliency maps <ref type="bibr" target="#b18">[19]</ref> in order to mask out the co-salient objects. However, these approaches highly depend on the extra dataset, leading to supererogatory human effort to provide annotations.</p><p>Recent approaches <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b51">52]</ref> try to use attention mechanism <ref type="bibr" target="#b38">[39]</ref> to strengthen co-salient features or build feature consistency to formulate the shared attributes of co-salient objects for integral predictions. However, there are two main drawbacks when directly applying attention mechanism for this task. On the one hand, the response maps reflecting the shared attributes, obtained in the attention mechanism, can only cover limited pixels belonging to co-salient objects, as shown in <ref type="figure" target="#fig_0">Fig. 1.(b)</ref>. In this case, it is difficult for the model to learn comprehensive shared attributes of co-salient objects. On the other hand, for complex scenes, the attention mechanism tends to focus on the wrong object regions, as shown in the second picture of- <ref type="figure" target="#fig_0">Fig. 1.(b)</ref>. Some methods such as GCoNet <ref type="bibr" target="#b10">[11]</ref> propose a kind of group collaborative learning by collecting artificial negative group pairs. However, their pairs are grouped based on the auxiliary classification information, which requires major effort to group dissimilar negative category pairs as there is no clear definition of natural discrete object categories in real world <ref type="bibr" target="#b36">[37]</ref>.</p><p>To solve aforementioned issues, we design a novel Democratic</p><p>Co-salient-Feature-Mining framework (DCFM). Our DCFM can directly mine more comprehensive features and suppress the noisy background effectively without using extra SOD dataset or classification information. Specifically, in order to mine sufficient cosalient information, we first design a democratic prototype generation module (DPG), where democratic response maps are generated to capture more shared attributes. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.(c), our response maps cover more regions of co-salient objects. Then, a prototype with comprehensive co-salient information can be generated according to the democratic response maps, which can further guide the model to predict the co-salient objects.</p><p>Next, in order to suppress noisy background information in our prototype and avoid introducing extra classification information, we propose a simple self-contrastive learning module (SCL) to form positive and negative pairs to filter noise. We argue that the prototype generated from original images should be consistent with that generated when the image background regions are erased, and should be different from that generated when the co-salient objects are erased. Thus, a self-contrastive loss among these prototypes is designed to suppress the influence of noisy background and help the model learn more discriminative features of co-salient objects.</p><p>Finally, to further strengthen the detected co-salient features from the above modules, we design a democratic feature enhancement module (DFE) based on the attention mechanism <ref type="bibr" target="#b38">[39]</ref>. As mentioned before, the attention mechanism tends to focus on a limited number of correlated features, which fails to provide comprehensive information. Therefore, we readjust the attention values to generate a democratic attention map aggregating more correlated pixels for feature enhancement.</p><p>Generally, our main contributions can be summarized as:</p><p>? A democratic prototype generation module (DPG) is designed to build response maps covering sufficient co-salient regions, so as to generate a prototype containing comprehensive shared attributes as guidance for co-saliency prediction.</p><p>? A self-contrastive learning module (SCL) is proposed to help our model reduce the influence of noisy background without relying on additional classification in-formation, where both positive and negative samples are generated from the image itself.</p><p>? A democratic feature enhancement module (DFE) is designed to further strengthen the co-salient features by adjusting attention values to involve more related pixels.</p><p>? Extensive experiments show that our method performs better than state-of-the-art methods, especially on challenging real-world cases, such as the CoCA dataset, we obtain a gain of 2.0% for MAE, 5.4% for maximum F-measure, 2.3% for maximum E-measure, and 3.7% for S-measure under the same settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Co-Salient Object Detection</head><p>CoSOD is gaining increasing popularity. Some works establish graphs to model the relationship among pixels from a group of images <ref type="bibr">[15-18, 43, 51]</ref>, then the co-salient objects can be mined with consistent features. Some works adopt extra salient object detection to mine salient objects first and then conduct CoSOD <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53]</ref>. Besides, SAEF <ref type="bibr" target="#b37">[38]</ref> proposes to use saliency proposals generated by unsupervised deep learning based models first and then conduct CoSOD according to those proposals. Other works <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b53">54]</ref> try to formulate shared attributes among input images to reflect the co-salient pixels and use classification information as a supplement of semantic information. In CoEGNet <ref type="bibr" target="#b8">[9]</ref>, edge detection is used for better structure prediction. More information on CoSOD can be found in surveys <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b46">47]</ref>. Although these methods have obtained outstanding performances, they rely on extra information to learn discriminative co-salient features. Therefore, we consider thoroughly exploring the intrinsic characteristics of co-salient objects and background to realize CoSOD without using the SOD dataset or extra classification information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Contrastive Learning</head><p>Contrastive learning is prevalent in self-supervised classification. SimCLR <ref type="bibr" target="#b1">[2]</ref> studies the importance of grouping negative pairs for contrastive learning. MoCo <ref type="bibr" target="#b13">[14]</ref> adopts a memory bank to reduce the influence of insufficient negative pairs. Additionally, contrastive learning has also be applied in many other tasks, such as video grounding <ref type="bibr" target="#b27">[28]</ref>, long-tailed recognition <ref type="bibr" target="#b5">[6]</ref>, action recognition <ref type="bibr" target="#b33">[34]</ref> and visual localization <ref type="bibr" target="#b36">[37]</ref>. However, as argued in <ref type="bibr" target="#b36">[37]</ref>, it is difficult to define artificial negative pairs for contrastive learning, and they use soft assignments of images instead. In this paper, we also study how to group positive/negative pairs without artificial defined categories in CoSOD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Attention Mechanism</head><p>Attention mechanism has been applied in different tasks. It has been applied in machine translation to draw global relationships between input and output <ref type="bibr" target="#b38">[39]</ref>. It is also utilized in non-local networks to deal with detection and segmentation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref>. Besides, A 2 GN N <ref type="bibr" target="#b45">[46]</ref> utilize it for affinity maps in weakly semantic segmentation. Additionally, it is used in video object segmentation to segment and track the target object <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b43">44]</ref>. Moreover, it is deployed in referring expression grounding <ref type="bibr" target="#b34">[35]</ref> and language-person search <ref type="bibr" target="#b22">[23]</ref>. Recently, attention mechanism has been applied in SOD or CoSOD. MSANet <ref type="bibr" target="#b54">[55]</ref> applies attention mechanism to mine salient features and suppress background information. Besides, it is also used in CA-FCN <ref type="bibr" target="#b12">[13]</ref> and RCAU <ref type="bibr" target="#b21">[22]</ref> as co-attention to link co-salient objects for CoSOD. However, we find that the attention mechanism tends to focus on a limited number of pixels. Therefore, in this paper, we try to introduce democracy into the attention mechanism to involve more related pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>The CoSOD dataset includes groups of images with labels. Each group is represented as</p><formula xml:id="formula_0">G = {I, Y }, where I = {x n } N n=1 , Y = {y n } N n=1 ,</formula><p>x n is the input image, y n is the corresponding label, N is the total number of images in group G, and all images contain related objects. The labels are unavailable during inference. The model needs to detect the co-existed salient objects in each image of the same group. In this work, we aim to design a model that can detect the co-salient objects by thoroughly exploring the shared attributes to mine comprehensive cosalient features, and suppress noisy background through self-contrastive learning without using classification information or extra SOD dataset.</p><p>The framework of our method and the learning procedure are demonstrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. There are five main modules in our network, including a feature extractor, a democratic prototype generation module (DPG), a selfcontrastive learning module (SCL), a democratic feature enhancement module (DFE), and a decoder. Note that the SCL is only applied for training and will be removed during inference. The overall process can be summarized as: 1) Firstly, the feature extractor encodes a group of relative images (N images) as initial features, which are then proceeded by the DPG to generate a comprehensive cosalient prototype.</p><p>2) Meanwhile, to avoid mining noisy information from the background in the prototype, our SCL is deployed for auxiliary training.</p><p>3) Then, the prototype is fused into the visual features, and the fused features are transmitted into the DFE to strengthen the features further. 4) Finally, the strengthened features are input into the decoder to predict the corresponding co-saliency maps.</p><p>In the following sections, the details about the democratic prototype generation module, the self-contrastive learning module, and the democratic feature enhancement module will be discussed, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Democratic Prototype Generation Module</head><p>Our democratic prototype generation module (DPG) mainly contains three parts in series, which are the residual block, the seed selection block (SSB), and the democratic response block (DRB).</p><p>After passing the feature extractor, we obtain the initial features F ext ? R N ?C?H?W (C, H, W are the channel number, height, and width), which are processed by the residual block first to generate strengthened residual features F res :</p><formula xml:id="formula_1">F res = F ext + conv 1?1 (F ext ),<label>(1)</label></formula><p>where conv 1?1 represents for the 1 ? 1 convolution layer and F res ? R N ?C?H?W . Then, the generated features F res are passed into the SSB to select the most discriminative seeds for the cosalient objects in each input image. Next, the selected seeds are correlated with the residual feature maps to produce the response maps by the DRB. Finally, the response maps are multiplied with the residual features and averaged to generate the prototype, containing comprehensive co-salient feature information and guiding following prediction.</p><p>Seed Selection Block (SSB). The SSB is demonstrated in <ref type="figure" target="#fig_2">Fig. 3</ref>. This block is deployed to detect each image's most representative pixel as seed for response map generation. First, the residual features F res are input to our SSB. Then, the attention mechanism is employed, in which two 1 ? 1 convolution layers are deployed to obtain two feature maps, namely K ? R N ?C?H?W and Q ? R N ?C?H?W . After reshaping both K and Q to shape R N HW ?C , the feature similarity map (S) of each pixel is computed as</p><formula xml:id="formula_2">S = KQ ? ,<label>(2)</label></formula><p>where S ? R N HW ?N HW , ? means transpose, and each row of S represents similarities between one pixel and all pixels of the N inputs. Then, we first reshape S into S ? R N HW ?N ?HW and choose its maximum similarity value in each image, to get N maximum similarity values for each pixel. This process is calculated by where S N -max ? R N HW ?N . Afterwards, the average of the N maximum similarity values is treated as the co-salient probability of each pixel,</p><formula xml:id="formula_3">S N -max = max i=1???HW S[:, :, i],<label>(3)</label></formula><formula xml:id="formula_4">? N Inputs ? #$% 1 ? 1 Conv ? &amp;#* Residual Block Democratic Prototype Generation Module (DPG) ? ? Democratic Prototype Generation Module (DPG) Self-Contrastive Loss Self-Contrastive Learning Module (SCL) IoU Loss Decoder ? Democratic Feature Enhancement Module (DFE) ? ? ? Feature Extractor N Outputs ? % ? ? ? ? ? ? ? &amp;#* Shared SSB DRB ' .+/'- ()*+* ( ()*+* 0 ()*+* ? Element-wise multiplication , 1'$ ?Element-wise addition Average -*./ 2 ! &amp; Mean</formula><formula xml:id="formula_5">P = 1 N N n=1 S N -max [:, n],<label>(4)</label></formula><p>where P ? R N HW . Then, the probability map is reshaped back to P ? R N ?H?W . We can locate the pixel with the highest probability of being the co-salient object in each image by</p><formula xml:id="formula_6">P max = max h=1,???H w=1,???W P [:, h, w],<label>(5)</label></formula><formula xml:id="formula_7">index = ind(P max ),<label>(6)</label></formula><p>where ind(?) means taking out the index of P max . Finally, we take out the feature vectors from the F res according to the index in Eq. (6) as the final seeds by</p><formula xml:id="formula_8">D = F res (index ).<label>(7)</label></formula><p>Note that each image will provide one seed vector, and there are totally N seeds. These seeds can represent the essential characteristics of the co-salient objects in each input image and be used for localization. Democratic Response Block (DRB). The DRB is demonstrated in <ref type="figure" target="#fig_2">Fig. 3</ref>. If we directly use the seeds D as the prototype, it fails to aggregate comprehensive characteristics of the co-salient objects. This is because it is difficult for limited seeds to express the integral co-existed objects, especially when there are large appearance variations among the group. Thus, we try to involve more pixels of the co-salient objects to generate a comprehensive prototype by considering the correlation between each pixel and the seeds D from SSB.</p><p>Specifically, we first use L2 normalization in channel dimension to obtain the normalized residual features ?F res ? 2 and the normalized seeds ?D? 2 . Then, ?D? 2 are treated as the kernel to conduct convolution on the ?F res ? 2 :</p><formula xml:id="formula_9">M = conv ?D? 2 (?F res ? 2 ),<label>(8)</label></formula><p>where M means response maps, conv ?D? 2 is the convolution with ?D? 2 as kernel. Since D has N seed vectors, the size of response maps become R N ?N ?H?W after Eq. <ref type="formula" target="#formula_9">(8)</ref>, with channel dimension being the number of response maps for each input. The final democratic response map of each image is computed as the mean value of the N response maps:</p><formula xml:id="formula_10">M final = 1 N N n=1 M [:, n, :, :],<label>(9)</label></formula><p>where M f inal ? R N ?H?W . In this way, more pixels have chance to contribute to the response maps. Finally, the prototype (proto ? R 1?C ) is generated by</p><formula xml:id="formula_11">proto = avg(M final ? F res ),<label>(10)</label></formula><formula xml:id="formula_12">1 ? 1 Conv 1 ? 1 Conv ? ? D: N ? C + * : NHW ? NHW + * +,-(. : NHW ? N , -(. ? "#$ : N ? C ? H ? W K: N ? C ? H ? W Q: N ? C ? H ? W ? ? "#$ ? ? ? ? -: N ? N ? H ? W Average 12"3": 1 ? C Column Mean Seed Selection Block (SSB) Democratic Response Block (DRB) ? ? -*%/() : N ? H ? W ? ? ? Index</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Max probability Max value</head><p>Take out Shared where the M final is broadcast to the same size as F res , ? denotes element-wise multiplication, and avg(?) means averaging feature vector of all the pixels from all inputs.</p><formula xml:id="formula_13">? Element-wise multiplication ? ? ? ? Reshape Max ,: NHW ,: N ? H ? W !"#$ ! ! ? 0 ? 1 ? 2 ? 0 ? 0 ? 0 ? 0 ? 1 ? 1 ? 1 ? 1 ? 2 ? 2 ? 2 ? 2 ? 2 ? 1 ? 0 Mean</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Self-Contrastive Learning Module</head><p>To further help the DPG to suppress the noise of background, and learn discriminative features without depending on classification information, a self-contrastive learning module (SCL) is designed as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. Our motivation is that the prototype generated by the original inputs (proto) should be consistent with co-salient prototype generated by inputs where background is erased (proto c ), but different from the background prototype generated by inputs where the co-salient objects are erased (proto b ). Note that the inputs here are the initial extracted features F ext from the feature extractor. The co-salient prototype and background prototype can be generated as</p><formula xml:id="formula_14">proto c = ? DPG (F ext ? Y ? ),<label>(11)</label></formula><formula xml:id="formula_15">proto b = ? DPG (F ext ? (1 ? Y ? )),<label>(12)</label></formula><p>where ? DPG is short for the process of DPG, '?' means downscaling the groundtruth Y to the same size as F ext then broadcasting to the same channel number. Then, proto and proto c are treated as a positive pair, while proto and proto b are treated as a negative pair. A self-contrastive loss is designed to pull together the positive pair and push away the negative pair. First, we define the cosine-style similarity between the prototypes by</p><formula xml:id="formula_16">cos(p 1 , p 2 ) = (1 + p 1 ? p 2 |p 1 | |p 2 | ) ? 0.5.<label>(13)</label></formula><p>After that, the self-contrastive loss is defined as</p><formula xml:id="formula_17">cos c = cos(proto, proto c ),<label>(14)</label></formula><formula xml:id="formula_18">cos b = cos(proto, proto b ),<label>(15)</label></formula><formula xml:id="formula_19">L sc = ?log(cos c + ?) ? log(1 ? cos b + ?),<label>(16)</label></formula><p>where ? is a small constant value ensuring non-zero values for log(?) and set as 1 ? 10 ?5 . Our SCL is only applied during training as an auxiliary loss to help the DPG learn more discriminative co-salient features. This part is not used during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Democratic Feature Enhancement Module</head><p>We design a democratic feature enhancement module (DFE) to further strengthen the fused co-salient features from DPG for final prediction. Our DFE is based on the attention mechanism <ref type="bibr" target="#b38">[39]</ref>. We observe that conventional attention <ref type="bibr" target="#b38">[39]</ref> tends to focus on a limited number of related pixels. Thus, we argue that democracy also matters in this case, and more pixels should be involved in enhancing the fused features. Thus, we try to amplify small positive attention values to involve more pixels for feature enhancement. Negative attention values are not considered here as they usually represent irrelevance. First, we generate the fused features using the guidance of both response maps and the prototype derived from Eq. (9) and Eq. (10) in the DPG as</p><formula xml:id="formula_20">F fused = F res ? M final + F res ? proto,<label>(17)</label></formula><p>where both the M final and proto are broadcast into the same size as F res . Therefore, the fused features by Eq. (17) contain both specific attributes and shared attributes. The fused features of each input image are enhanced with our DFE individually and independently. As shown in <ref type="figure" target="#fig_4">Fig. 4</ref>, the corresponding F fused is input to a 1 ? 1 convolution layer followed by a ReLU activation to obtain F conv ? R C?H?W first. After that, key, query and  value convolutions are applied and then reshaped to generate F k ? R HW ?C , F q ? R HW ?C and F v ? R HW ?C . Then, the initial attention map (A) can be computed by</p><formula xml:id="formula_21">A = F k F ? q ,<label>(18)</label></formula><p>where A ? R HW ?HW and ? means transpose. Next, a softmax is applied to A to obtain the normalized attention map (A norm ). Moreover, the initial attention map A is sorted in a descending order to generate the sorting index matrix (Z). As we adopt the descending order, the small attention values are assigned with large sorting index. Then, we apply the following formula to amplify the small positive attention values,</p><formula xml:id="formula_22">A re i,j = (Z i,j + 1) ? , if A i,j &gt; 0 1, else ,<label>(19)</label></formula><p>where A re denotes the weights for readjusting the attention, ? is a coefficient for determining the degree of amplification, i and j are the spatial index. Then, the final attention map is computed by</p><formula xml:id="formula_23">A final = A norm ? A re ,<label>(20)</label></formula><p>and the final enhanced features can be computed by</p><formula xml:id="formula_24">F enh = F conv + A final F v ,<label>(21)</label></formula><p>where the result of A final F v is first reshaped back into the same size as F conv . Finally, the augmented features F enh are transmitted into the decoder to predict the corresponding co-saliency maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Objective Function</head><p>The objective function for training is a combination of IoU loss <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b52">53]</ref> and our self-contrastive loss in Eq. <ref type="bibr" target="#b15">(16)</ref>. The IoU loss can be illustrated as</p><formula xml:id="formula_25">L iou = 1 ? 1 N ? ? ? Y ? Y ,<label>(22)</label></formula><p>where? denotes for predictions and Y denotes for the groundtruth. Then, the final objective function is</p><formula xml:id="formula_26">L tot = L iou + ?L sc ,<label>(23)</label></formula><p>where ? is to balance IoU loss and self-contrastive loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>We use Feature Pyramid Network (FPN) <ref type="bibr" target="#b24">[25]</ref> with VGG-16 <ref type="bibr" target="#b32">[33]</ref> as our backbone. The hyper-parameter ? in Eq. <ref type="formula" target="#formula_1">(19)</ref> is 3 and ? in Eq. (23) is 0.1. Additionally, we use Adam <ref type="bibr" target="#b19">[20]</ref> as our optimizer to train our model for 200 epochs. The learning rate is set as 1 ? 10 ?5 for feature extractor and </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Dataset and Evaluation Metrics</head><p>Dataset. We use COCO-SEG <ref type="bibr" target="#b39">[40]</ref>, a subset of COCO dataset <ref type="bibr" target="#b25">[26]</ref>, which contains 9,213 images from 65 groups for training. We evaluate our method on three popular CoSOD benchmarks: CoCA <ref type="bibr" target="#b53">[54]</ref>, Cosal2015 <ref type="bibr" target="#b47">[48]</ref> and CoSOD3k <ref type="bibr" target="#b9">[10]</ref>. CoCA and CoSOD3k are proposed for challenging real-world co-saliency evaluation, containing multiple co-salient objects in some images, large appearance and scale variations, and complex background clutters. Cosal2015 is a widely used large dataset for the evaluation.</p><p>Evaluation Metrics. The evaluation metrics include mean absolute error (MAE ) <ref type="bibr" target="#b3">[4]</ref>, maximum F-measure (F max ? ) <ref type="bibr" target="#b0">[1]</ref>, maximum E-measure (E max ? ) <ref type="bibr" target="#b7">[8]</ref> and Smeasure (S ? ) <ref type="bibr" target="#b6">[7]</ref>. Specifically, the value of MAE is the smaller, the better. While others are the larger, the better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with State-of-The-Art</head><p>Compared Methods. We mainly compare with previous state-of-the-art methods trained on common single CoSOD training dataset for fair comparison, including CSMG <ref type="bibr" target="#b49">[50]</ref>, GCAGC <ref type="bibr" target="#b50">[51]</ref>, CoEGNet <ref type="bibr" target="#b8">[9]</ref>, GICD <ref type="bibr" target="#b53">[54]</ref>, GCoNet <ref type="bibr" target="#b10">[11]</ref>, and DeepACG <ref type="bibr" target="#b48">[49]</ref>. We also list several methods trained on both CoSOD dataset and SOD dataset, such as CADC <ref type="bibr" target="#b51">[52]</ref>, ICNet <ref type="bibr" target="#b18">[19]</ref> and CoADNet <ref type="bibr" target="#b52">[53]</ref>.</p><p>Quantitative Comparison. In Tab. 1, we list the performance comparisons between ours and previous state-ofthe-art methods. It can be seen that our method reaches a <ref type="table">Table 1</ref>. Comparisons with other state-of-the-art approaches on 3 benchmarks. ? means that larger is better and ? denotes that smaller is better. 'SOD' denotes training with extra SOD dataset.  <ref type="figure">Figure 5</ref>. The qualitative comparisons with other state-of-the-art methods. It is evident that our method can predict smoother co-saliency maps with less noise of background, compared with other state-of-the-art methods. More can be found in our supplementary material.</p><formula xml:id="formula_27">CoCA CoSOD3k Cosal2015 Methods SOD MAE ? F max ? ? E max ? ? S ? ? MAE ? F max ? ? E max ? ? S ? ? MAE ? F max ? ? E max ? ? S ? ?</formula><p>new state-of-the-art performance compared with other approaches under the same settings. Specifically, for the two challenging real-world datasets CoCA and CoSOD3k, e.g., for CoCA, we obtain a gain of 2.0% for MAE, 5.4% for maximum F-measure, 2.3% for maximum E-measure, and 3.7% for S-measure compared with GCoNet <ref type="bibr" target="#b10">[11]</ref>. Moreover, our method can even outperform those trained with extra SOD dataset on these two datasets, such as ICNet <ref type="bibr" target="#b18">[19]</ref> and CADC <ref type="bibr" target="#b51">[52]</ref>. For Cosal2015, our method obtains comparable results with DeepACG <ref type="bibr" target="#b48">[49]</ref> and GCoNet <ref type="bibr" target="#b10">[11]</ref>. This phenomenon may be caused by the fact that both Deep-ACG <ref type="bibr" target="#b48">[49]</ref> and GCoNet <ref type="bibr" target="#b10">[11]</ref> use extra classification information to provide structure information, while our method does not rely on any extra information.</p><p>Qualitative Comparison. We also report some qualitative comparisons with state-of-the-art methods in <ref type="figure">Fig. 5</ref>. The groups are from CoCA dataset. It can be found that our model can predict more integral and less noisy co-saliency maps compared with others. Specifically, when there are multiple co-salient objects in one image, like the group 'Strawberry', our model can detect all the target objects, compared with CoEGNet <ref type="bibr" target="#b8">[9]</ref> and CSMG <ref type="bibr" target="#b49">[50]</ref>. In the group 'Soap bubble', ours are sensitive to appearance variations compared with others. When the background noise level is high, such as the group 'Pocket watch', our predictions contain less noise, compared with GCoNet <ref type="bibr" target="#b10">[11]</ref> and GICD <ref type="bibr" target="#b53">[54]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>We conduct the ablation study of our method on the CoCA dataset by adding one module each time and treating the network with all our modules removed as the baseline. The results are shown in Tab. 2. It can be found that each proposed module contributes a lot. With our DPG, the performance can increase 3.2% for MAE, 5.4% for maximum F-measure, 2.8% for maximum E-measure, and 4.1% for Smeasure. Our SCL enables further improvement by 1.0% Compared with the baseline (row 1), each part of DPG devotes to the final results. Specifically, if we only use RB and SSB, where we take the mean of seeds as the prototype, the results are even lower than the case without SSB, comparing row 2 and 3. On the other hand, with DRB, comparing row 3 and 4, the results will be increased by 2.9% for MAE, 4.8% for maximum F-measure, 2.4% for maximum E-measure, and 3.9% for S-measure. This phenomenon can verify that democracy does matter. More co-salient pixels should be enrolled for the comprehensive prototype. Impact of Self-Contrastive Learning Module. We also evaluate two main parts in our self-contrastive loss as listed in Tab. 4. We conduct this experiment by removing one part each time. It can be seen that with only positive pair cos c , by comparing row 1 and 2, we can get comparable results. With only negative pair cos b , the performance is clearly improved, by comparing row 1 and 3. This phenomenon proves that the negative pair is important for removing background noise. Nevertheless, by comparing row 3 and 4, the contrastive learning with both positive and negative pairs promotes balanced training for higher results. More analysis can be found in our supplementary material. Impact of Democratic Feature Enhancement Module. We also experiment on the readjustment of attention values in Tab. 5. When the readjustment is removed but using conventional attention in our DFE, the performance is even worse than the case without our DFE, as shown in row 1 and 2. Thus, democracy does matter in this module as well. Conventional attention mechanism focusing on limited pixels cannot provide sufficient information for the decoder while more related pixels should be involved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have proposed a new method for CoSOD without using the SOD dataset and classification information. We design a democratic prototype generation module (DPG) to build democratic response maps first so as to generate a comprehensive prototype as guidance for further prediction. Moreover, to help suppress noisy background information in the prototype, we design a selfcontrastive learning module (SCL), where both positive and negative pairs are generated from the image itself without relying on classification information. Besides, we also design a democratic feature enhancement module (DFE) to strengthen co-salient features from DPG for final prediction. Both our DPG and DFE show that democracy does matter. More related pixels should be involved for mining comprehensive features for CoSOD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Supplementary Material</head><p>In this supplementary material, we will analyze the selfcontrastive learning module (SCL) with some visualizations. Besides, we will provide more qualitative comparisons between our model and other state-of-the-art approaches. Additionally, we will compare the complexity of our method with others. We will also analyze the influence of ? in Eq. <ref type="bibr" target="#b18">(19)</ref> in our paper. Moreover, we will discuss about some failure cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Self-Contrastive Learning Module Analysis</head><p>We display some response maps in different cases on the CoCA dataset <ref type="bibr" target="#b53">[54]</ref> in <ref type="figure">Fig. 6</ref>. Note that this dataset is used for evaluation. are sensitive to the surroundings of the co-salient objects. In this case, our assumption of SCL, where proto and proto c are pulled together while proto and proto b are pushed away, can be verified. With our SCL, the model can learn to differentiate co-salient features and background features. Thus, the noise information can be suppressed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Qualitative Comparison</head><p>We list more qualitative comparisons with previous sateof-the-art methods in <ref type="figure">Fig. 7</ref>. We use the CoCA dataset <ref type="bibr" target="#b53">[54]</ref> for demonstration, as it is a challenging real-world dataset, containing more challenging cases. The compared methods include CSMG <ref type="bibr" target="#b49">[50]</ref>, GCAGC <ref type="bibr" target="#b50">[51]</ref>, CoEGNet <ref type="bibr" target="#b8">[9]</ref>, GICD <ref type="bibr" target="#b53">[54]</ref>, GCoNet <ref type="bibr" target="#b10">[11]</ref>, and DeepACG <ref type="bibr" target="#b48">[49]</ref>. It is evident that our predictions are closer to the ground truth. Specifically, when the background contains misleading objects, such as the humans in the group 'Binoculars', our model can suppress the noisy information and focus on the targets, compared with GCoNet <ref type="bibr" target="#b10">[11]</ref> and GICD <ref type="bibr" target="#b53">[54]</ref>. Additionally, when there are complex background clutters, like images in the groups 'Pillow' and 'Tablet', compared with all other methods, ours are robust to this challenging setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Complexity Analysis with State-of-the-art Methods</head><p>The computational complexity of Eq.(2) and Eq.(18) in our paper is O((N HW ) 2 ) and O((HW ) 2 ) respectively. The increment of FLOPs is small since the input size is small. We list the complexity comparisons in Tab. 6, ' ?' means without DFE. Ours can achieve an impressive performance with fewer FLOPs and parameters compared with CADC <ref type="bibr" target="#b51">[52]</ref> and GICD <ref type="bibr" target="#b53">[54]</ref>. Besides, ours can obtain a better performance with limited increment of FLOPs and parameters compared with GCoNet <ref type="bibr" target="#b10">[11]</ref>, especially for DCFM ?. Overall, our method has an impressive performance with comparable runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Influence of Alpha in Eq.(19) in Our Paper</head><p>We add the ablation study of alpha in Tab. 7. The performance smoothly increases with larger alpha. However, performance decreases when alpha is too big (?=4). When ?&gt;4, the model even fails to be trained. This is because in this case, the weight of small positive attention values will be much bigger. Thus, the attention mechanism will be confused and tend to focus on those small values but neglect original high values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. Limitation Discussion</head><p>We also report some failure cases in <ref type="figure">Fig. 8</ref>. As shown in the figure, it is difficult for our model to predict small objects precisely. This may be caused by the fact that the inputs are resized into the size of 224 ? 224. Then, with the feature extractor, the size of the output features is 14 ? 14.</p><p>In this case, it may cause information lost for small objects. Thus, it is difficult for our model to capture the corresponding features. Therefore, how to enhance model robustness for small objects is a direction for our future work. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Visualization of response maps. (a) Inputs; (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The framework of our network and the learning procedure. Specifically, the network contains five main parts, including a feature extractor, a democratic prototype generation module (DPG), a self-contrastive learning module (SCL), a democratic feature enhancement module (DFE), and a decoder. Note that the SCL is only used during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The framework of the seed selection block (SSB) and democratic response block (DRB). The inputs are the residual features. Then, the co-salient seeds are selected first from the residual features by SSB. After that, the response maps are produced using the selected seeds and the residual features through DRB. The final response maps and the input residual features are fused to generate the prototype.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Flow chat of democratic feature enhancement module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1?10 ? 4</head><label>4</label><figDesc>for other parts. The weight decay is set as 1?10 ?4 . In each training episode, we randomly choose one group (16 samples) of relative images. For inference, all samples in each group are input at one time. The inputs are resized into 224 ? 224 for both training and inference. The computational complexity of Eq. (2) is O((N HW ) 2 ) and Eq. (18) is O((HW ) 2 ). The increment of FLOPs is small since the input size is small. The total training time is around 3 hours and the inference time is around 84.4 fps. All experiments are run on one NVIDIA GeForce RTX 2080 Ti.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>M final denotes the normal response maps generated by original inputs, M final c denotes the co-salient response maps generated by inputs where the background regions are erased, and M final b denotes the background response maps generated by inputs where the co-salient objects are erased. Then, proto, proto c and proto b can be derived based on the corresponding response maps. As shown in Fig. 6, it can be found that the M final can focus on most regions of the target co-salient objects. Moreover, comparing M final c and M final b , the M final c can highlight all the related co-salient objects. In contrast, the M final b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .Figure 7 .Figure 8 .</head><label>678</label><figDesc>Visualization of the response maps in different cases. The visualizations can verify our assumption of the self-contrastive learning module as M final is consistent with M final c but different from M final b. More visualizations of our predictions and comparisons with previous state-of-the-art approaches. It can be found that our model can better differentiate the co-salient objects and background in complex scenes. Visualizations of some failed cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .Table 3 .Table 4 .</head><label>234</label><figDesc>Ablation study for our proposed modules. 'Base.' denotes baseline. Our overall method obtains the best results.Base. DPG SCL DFE MAE ? F max ? Ablation study for different parts in DPG. 'RB' means the residual block. The overall process obtains the best performance. Ablation study for different parts in Eq. (16) of SCL. 'cosc' denotes the case with only positive pair for the loss and 'cos b ' denotes that with only negative pair. DFE is not used. The evaluation of each block of our DPG is listed in Tab. 3. The experiment is conducted by adding one block at a time.</figDesc><table><row><cell>? E max ?</cell><cell>? S ? ?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 .</head><label>5</label><figDesc>Ablation study for readjustment in DFE. 'w/o DFE' denotes not using DFE, 'w/o RA' denotes using DFE without readjustment and 'w/ RA' denotes using DFE with readjustment.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">MAE ? F max ?</cell><cell cols="2">? E max ?</cell><cell>? S ? ?</cell></row><row><cell cols="2">1 w/o DFE</cell><cell>0.087</cell><cell cols="2">0.592</cell><cell cols="2">0.775</cell><cell>0.701</cell></row><row><cell cols="2">2 w/o RA</cell><cell>0.100</cell><cell cols="2">0.567</cell><cell cols="2">0.769</cell><cell>0.691</cell></row><row><cell>3</cell><cell>w/ RA</cell><cell>0.085</cell><cell cols="2">0.598</cell><cell cols="2">0.783</cell><cell>0.710</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 .Table 7 .</head><label>67</label><figDesc>Complexity comparisons. 'param.' denotes the number of parameters. We set 5 inputs to compute FLOPs. method FLOPs (G) param. (M) runtime (fps) F max ? Influence of alpha in Eq.(19) in our paper.</figDesc><table><row><cell>?</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radhakrishna</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheila</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int. Conf. Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Global context-aware progressive aggregation network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuyao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianqian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runmin</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficient salient region detection with soft image abstraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Warrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Yan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Crook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Review of visual saliency detection with comprehensive information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runmin</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianjun</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weisi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuit Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2941" to="2959" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Parametric contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiequan</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisheng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis., 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Structure-measure: a new way to evaluate foreground maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Enhanced-alignment measure for binary foreground map evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Re-thinking co-salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengpeng</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge-Peng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Taking a deeper look at co-salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge-Peng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog., 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Group collaborative learning for cosalient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhu</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Co-saliency detection with co-attention fully convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangshuai</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenting</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingjie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuit Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="877" to="889" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-scale graph fusion for co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongyao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyun</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Co-saliency detection via a general optimization model and adaptive graph learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyue</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="3193" to="3202" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multiple graph convolutional networks for cosaliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyue</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A unified multiple graph learning and convolutional network model for co-saliency estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyue</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Int. Conf. Multimedia</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Icnet: intra-saliency correlation network for cosaliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wen-Da Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adam: a method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int. Conf. Learn. Represent</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Uncertainty-aware joint salient object and camouflaged object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunqiu</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<idno>2021. 1</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Detecting robust co-saliency with recurrent co-attention neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lv</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlong</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCAI</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Transformer based language-person search with multiple region slicing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Eng Gee Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuit Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Salient object detection with purificatory mechanism and structural similarity loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinming</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqun</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingcan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="6855" to="6868" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Microsoft coco: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Samnet: stereoscopically attentive multi-scale network for lightweight salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin-Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Wang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3804" to="3814" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Interventional video grounding with dual contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoshun</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sicong</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<idno>2021. 2</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Video object segmentation using space-time memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-scale interactive network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youwei</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<idno>2020. 1</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mfnet: multi-filter directive network for weakly supervised salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongri</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<idno>2021. 1</idno>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Basnet: boundaryaware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuebin</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masood</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jagersand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int. Conf. Learn. Represent</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semi-supervised action recognition with temporal contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omprakash</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abir</forename><surname>Das</surname></persName>
		</author>
		<idno>2021. 2</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Discriminative triad matching and reconstruction for weakly referring expression grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Eng Gee Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goulermas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4189" to="4195" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Disentangled high quality salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lv</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijie</forename><surname>Zhong</surname></persName>
		</author>
		<idno>2021. 1</idno>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint/>
	</monogr>
	<note>Shouhong Ding, and Mofei Song</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Soft contrastive learning for visual localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janine</forename><surname>Thoma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc V</forename><surname>Danda Pani Paudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep co-saliency detection via stacked autoencoder-enabled fusion and self-trained cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Chi</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Jui</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoning</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Yu</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1016" to="1031" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Robust deep co-saliency detection with group semantic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Few-shot semantic segmentation with democratic attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haochen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianbin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiantong</forename><surname>Zhen</surname></persName>
		</author>
		<idno>2020. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep group-wise fully convolutional network for co-saliency detection with graph propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><forename type="middle">El</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farouk</forename><surname>Bourahla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="5052" to="5063" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Fast pixel-matching for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyue</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Eng Gee Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sign. Process. Image Communication</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">116373</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Structure-consistent weakly supervised salient object detection with local saliency coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyue</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eng Gee</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Affinity attention graph neural network for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A review of co-saliency detection algorithms: fundamentals, applications, and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans.Intell. Syst. and Tech</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Detection of co-salient objects by looking deep and wide</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="215" to="232" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deepacg: co-saliency detection via semantic-aware contrast gromov-wasserstein distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingliang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Tong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingshan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Cosaliency detection via mask-guided fully convolutional networks with multi-scale label smoothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingshan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Adaptive graph convolutional network with attention graph clustering for co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingshan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Summarize and search: learning consensus-aware dynamic convolution for co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Coadnet: collaborative aggregation-anddistribution networks for co-salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runmin</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Gradient-induced co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenda</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Multi-type self-attention guided degraded saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meijun</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
