<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NDDR-CNN: Layerwise Feature Fusing in Multi-Task CNNs by Neural Discriminative Dimensionality Reduction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayi</forename><surname>Ma</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Wuhan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingbo</forename><surname>Zhao</surname></persName>
							<email>mbzhao4@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="institution">City University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
							<email>alan.yuille@jhu.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">NDDR-CNN: Layerwise Feature Fusing in Multi-Task CNNs by Neural Discriminative Dimensionality Reduction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a novel Convolutional Neural Network (CNN) structure for general-purpose multi-task learning (MTL), which enables automatic feature fusing at every layer from different tasks. This is in contrast with the most widely used MTL CNN structures which empirically or heuristically share features on some specific layers (e.g., share all the features except the last convolutional layer). The proposed layerwise feature fusing scheme is formulated by combining existing CNN components in a novel way, with clear mathematical interpretability as discriminative dimensionality reduction, which is referred to as Neural Discriminative Dimensionality Reduction (NDDR). Specifically, we first concatenate features with the same spatial resolution from different tasks according to their channel dimension. Then, we show that the discriminative dimensionality reduction can be fulfilled by 1 ? 1 Convolution, Batch Normalization, and Weight Decay in one CNN. The use of existing CNN components ensures the end-to-end training and the extensibility of the proposed NDDR layer to various state-of-the-art CNN architectures in a "plug-andplay" manner. The detailed ablation analysis shows that the proposed NDDR layer is easy to train and also robust to different hyperparameters. Experiments on different task sets with various base network architectures demonstrate the promising performance and desirable generalizability of our proposed method. The code of our paper is available at https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep convolutional neural networks (CNNs) have greatly pushed the previous limits of various computer vision tasks since the seminal work <ref type="bibr" target="#b19">[20]</ref> in 2012. CNN models can naturally integrate hierarchical features and classifiers, which can be trained in an end-to-end man-* indicates corresponding authors. ner. Benefiting from that, significant improvements have been witnessed in fundamental computer vision tasks, such as image classification <ref type="bibr">[14-16, 20, 45]</ref>, object detection <ref type="bibr">[8, 9, 13, 25, 35, 36, 42-44, 47, 48, 50]</ref>, semantic segmentation <ref type="bibr">[1-5, 27, 33, 37, 51]</ref>, etc.</p><p>One of the main factors that can further boost the CNN performance is multi-task learning (MTL), which is engaged in learning multiple related tasks simultaneously. This is because related tasks can benefit from each other by jointly learning certain shared, or more precisely, mutually related representations <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b18">19]</ref>. The multiple supervision signals originating from different tasks in MTL can be viewed as implicit data augmentation (on labels) or additional regularization (among different tasks) <ref type="bibr" target="#b38">[39]</ref>. This enables to learn mutually related representations that work well for multiple tasks, thus avoiding overfitting and leading to better generalizability.</p><p>Most commonly, the CNN structure for MTL is heuristically determined by sharing all convolutional layers, and splitting at fully-connected layers for task-specific losses. However, as different layers learn low-, mid-, and high-level features <ref type="bibr" target="#b56">[57]</ref>, a natural question arises: Why would we assume that the low-and mid-level features for different tasks in MTL should be identical, especially when the tasks are loosely related? If not, is it optimal to share the features until the last convolutional layer?</p><p>The study in Misra et al. <ref type="bibr" target="#b30">[31]</ref> reveals that sharing/splitting at different layers gives different performances. Especially, improper features sharing at some layers may degrade the performance of some, or even all, tasks. In addition, the deep nature of CNNs makes it infeasible to exhaustively test all the possible structures to find the optimal sharing/splitting scheme. In order to tackle this issue, Misra et al. used trainable scalars to weighted-sum the features from different tasks at multiple CNN levels and achieved state-of-the-art performance <ref type="bibr" target="#b30">[31]</ref>.</p><p>We consider this problem in another way, i.e., by leveraging all the hierarchical features from different tasks. This is because that the CNN layers trained by different tasks can be treated as different feature descriptors, therefore the fea-tures learned from them can be treated as different representations/views of input data. We hypothesize that these features, obtained from multiple feature descriptors (i.e., different CNN levels from multiple tasks), contain additional discriminative information of input data, which should be exploited in MTL towards better performance.</p><p>Specifically, starting with K single-task networks (from K tasks), a direct attempt to take advantage of hierarchical features from all the tasks is that: we may concatenate all the task-specific features with the same spatial resolution from different tasks according to the feature channel dimension. After that, we expect the CNN to learn a discriminative feature embedding for each task, by receiving these concatenated features as inputs. However, most existing CNNs have carefully designed structures, which only receive features (tensors) with a fixed number of feature channels. By concatenating features, we substantially enlarge the number of channels as K times if we have K tasks. This makes it impossible to feed these concatenated features to the following layers of the CNN.</p><p>This property of the CNN motivates us to conduct discriminative dimensionality reduction on the concatenated features. Its purpose is to learn a discriminative feature embedding, and to reduce the feature dimension such that it can satisfy the input channel requirement of the following layers. Feature transformation is one of the most important approaches to tackle the discriminative dimension reduction problem. It aims to learn a projection matrix that projects the original high-dimensional features into a low-dimensional representation, while keeping as much discriminative information as possible.</p><p>In this paper, we show that, from the perspective of feature transformation, discriminative dimensionality reduction is closely related to some common operations of modern CNNs. Specifically, the transformation in discriminative dimensionality reduction is in fact equivalent to the 1 ? 1 convolution. In addition, the constraints on the norm of the transformation weights (i.e., the weights of the 1 ? 1 convolutional layer) and input feature vectors can be represented by weight decay and batch normalization <ref type="bibr" target="#b16">[17]</ref>, respectively. We refer to the combination of these operations as Neural Discriminative Dimensionality Reduction (NDDR). Therefore, we are able to link the original singletask networks from different tasks by the NDDR layers. Desirably, the proposed network structure can be trained endto-end in the CNN without any extraordinary operations.</p><p>It is worth noting that this paper focuses on a general structure for general-purpose MTL. The proposed NDDR layer combines existing CNN components in a novel way, which possesses clear mathematical interpretability as discriminative dimensionality reduction. Moreover, the use of the existing CNN components is desirable to guarantee the extensibility of our method to various state-of-the-art CNN architectures, where the proposed NDDR layer can be used in a "plug-and-play" manner. The rest of this paper is organized as follows. First, we describe the NDDR layer and propose a novel NDDR-CNN as well as its variant NDDR-CNN-Shortcut for MTL in Sect. 3. After that, we discuss the related works in Sect. 2, where we show that our method can generalize several state-of-the-art methods, which can be treated as our special cases. In Sect. 4, the ablation analysis is performed, where the hyperparameters used in our network are suggested. Following that, the experiments are performed on different network structures and different task sets in Sect. 5, demonstrating the promising performance and desirable generalizability of our proposed method. We make concluding remarks in Sect. 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Various computer vision tasks benefit from MTL <ref type="bibr" target="#b40">[41]</ref>, such as detection <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref><ref type="bibr" target="#b49">50]</ref>, human pose and semantic segmentation <ref type="bibr" target="#b51">[52]</ref>, surface normal prediction, depth prediction, semantic segmentation <ref type="bibr" target="#b5">[6]</ref>, action recognition <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b53">54]</ref>, etc. Several human face related tasks, including face landmark detection, attributes detection (such as smile and glasses), gender classification, and face orientation, were studied in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b48">49]</ref>. Yim et al. used face alignment and reconstruction as auxiliary tasks for face recognition <ref type="bibr" target="#b55">[56]</ref>. MTL on sequential data was also studied in <ref type="bibr" target="#b23">[24]</ref>. Recently, Kokkinos proposed a UberNet which enables a great number of low-, mid-, and high-level vision tasks to be handled simultaneously <ref type="bibr" target="#b18">[19]</ref>.</p><p>CNN based MTL theory has also been greatly developed in recent years. Long and Wang proposed a deep relationship network to enable the feature sharing at the fully-connected layers <ref type="bibr" target="#b27">[28]</ref>. Starting with a thin network, a top-down layerwise widening method was proposed to automatically determine which layer to split <ref type="bibr" target="#b28">[29]</ref>. Yang and Hospedales used tensor decomposition at initialization to share the MTL weights <ref type="bibr" target="#b54">[55]</ref>. The weights to combine the task-specific losses were also studied, and a Bayesian approach was proposed to predict these weights <ref type="bibr" target="#b17">[18]</ref>. The cross-stitch network used trainable scalars to fuse (i.e., weighted sum) the features at layers in the same level from different tasks <ref type="bibr" target="#b30">[31]</ref>. Most recently, the sluice network predefines several subspaces on the features from each task and learns the weights to fuse the features across different subspaces <ref type="bibr" target="#b39">[40]</ref>.</p><p>Our method is also related to discriminative dimensionality reduction. The goal of the discriminative dimensionality reduction techniques is to reduce the computational and storage costs, by learning a low-dimensional embedding that retains most of the discriminative information. Linear discriminant analysis (LDA) is one of the most popular conventional discriminative dimensionality reduction methods, which aims to seek the optimal projection matrix by maxi-mizing the between-class variance and meanwhile minimizing the within-class variance <ref type="bibr" target="#b29">[30]</ref>. In addition, low-rank metric learning <ref type="bibr" target="#b25">[26]</ref> can also be viewed as a discriminative dimensionality reduction technique.</p><p>Introduced by network in network <ref type="bibr" target="#b21">[22]</ref>, 1 ? 1 convolution has been widely used in many modern CNN architectures <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b45">46]</ref>. For example, it was used in ResNet to reduce the number of weights to train, by producing a "bottleneck unit" <ref type="bibr" target="#b13">[14]</ref>. 1 ? 1 convolution is also implemented in the feature pyramid network to fuse hierarchical features (in different CNN levels) on a single task <ref type="bibr" target="#b22">[23]</ref>. Note that we do NOT claim the 1 ? 1 convolution as our novelty. Instead, we use 1 ? 1 convolution together with batch normalization and weight decay in a novel way, which yields an NDDR layer. In other words, we formulate the multi-task feature fusing paradigm as a discriminative dimensionality reduction problem, and use the NDDR layer, which is composed of 1 ? 1 convolution, batch normalization, and weight decay, to learn the feature embeddings from multiple tasks. The use of the existing CNN components ensures the extensibility of our method to various state-of-the-art CNN architectures in a "plug-and-play" manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section, we propose a novel method to automatically learn the optimal structure for layerwise feature fusing in a multi-task CNN. Instead of the "split-style" multi-task CNN (e.g., split at the last convolutional layer for different task-specific losses), we consider the "fuse-style" network combining multiple single-task networks via discriminative dimensionality reduction.</p><p>We first relate the discriminative dimensionality reduction to 1 ? 1 convolution and propose the NDDR layer. Then, a novel multi-task network is proposed, namely NDDR-CNN, where the NDDR layer is leveraged to connect the original single-task networks. Moreover, a variant of NDDR-CNN is introduced, namely NDDR-CNN-Shortcut, which enables to directly route the gradients to the lower NDDR layers by shortcut connections. Finally, we give the implementation details of the proposed network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">NDDR Layer</head><p>As discussed in previous sections, we aim to utilize the hierarchical features learned from different tasks. It is unlike the most widely used method which heuristically shares all the low-(and mid-) level features and splits the network at the last convolutional layer.</p><p>In order to do that, we first concatenate the task-specific features from different tasks according to the channel dimension. Then, we use a discriminative dimensionality reduction technique to reduce the feature channels such that the output features satisfy the channel dimension requirement of the next CNN layers. We refer to the new CNN layer with such operations as the Neural Discriminative Dimensionality Reduction (NDDR) layer.</p><p>Specifically, let F i l ? R N ?H?W ?C be the output features (arranged in a tensor) at an intermediate layer l of task i. Regarding K tasks, concatenating the features from them according to the channel dimension gives:</p><formula xml:id="formula_0">F l = [F 1 l , ..., F K l ] ? R N ?H?W ?KC .<label>(1)</label></formula><p>Discriminative dimensionality reduction learns a transformation W to reduce the dimensionality of the input features, while keeping most discriminative information:</p><formula xml:id="formula_1">F i * l = F l W i ,<label>(2)</label></formula><p>where W i ? R KC?M and M &lt; KC is the projection matrix to be learned for each task i. In our case, M is equal to C (i.e., F i * l ? R N ?H?W ?C ) in order to satisfy the channel size requirement of the following CNN layers.</p><p>Conventional discriminative dimensionality reduction methods learn the transformation W with specific assumptions/objectives which make the features more separable. For example, Linear Discriminative Analysis (LDA) learns W by minimizing the projected within-class variation and meanwhile maximizing the projected between-class variation <ref type="bibr" target="#b29">[30]</ref>. Intuitively, the objective function of the discriminative dimensionality reduction is related to the CNN loss, i.e., the features projected by discriminative dimensionality reduction are more separable, therefore giving a smaller CNN loss.</p><p>Motivated by this, we aim to learn the transformation W in the CNN implicitly by back-propagation. The transformation W ? R KC?C can be represented precisely by a convolution operation with stride 1 and size (C ? 1 ? 1 ? KC), where these size dimensions represent filters, kernel height, kernel width, and channels, respectively. It is worth noting that the convolution with 1 ? 1 kernel size and 1 stride enables to perform the computations only according to channels, rather than fusing the features at different spatial locations or changing the spatial sizes of the features.</p><p>In addition, discriminative dimensionality reduction methods also have constraints on the norms of the transformation W (to avoid a trivial solution) and the input features F l (otherwise, the learned projections may project the features to some noise directions). We borrow this idea to our NDDR layer for stable learning, which can be achieved by imposing batch normalization on the input features and 2 weight decay on the 1 ? 1 convolutional weights W , respectively.</p><p>In summary, a novel NDDR layer is proposed in this section. The NDDR layer can be constructed by: 1) concatenating the task-specific features with the same spatial resolution from different tasks according to the channel dimension, and 2) using 1?1 convolution to learn a discriminative <ref type="figure">Figure 1</ref>. The network structure of NDDR-CNN. In the NDDR layer, we concatenate the outputs of original single-task networks from multiple tasks (two tasks shown here), and use 1 ? 1 convolution to perform discriminative dimensionality reduction. Therefore, the output of the NDDR layer retains the discriminative information from both the input features, and can be fed to the following layers of the singletask networks. The proposed NDDR layer can be leveraged to connect the original single-task networks of multiple levels for layerwise feature fusing (best view in color). feature embedding for each task. We also use batch normalization on the input features of the NDDR layer for stable learning. We train the NDDR layer by back-propagating the task-specific losses and the 2 weight decay loss on the 1?1 convolutional weights W . Without any extraordinary operations, the network with our NDDR layer can be trained in an end-to-end fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">NDDR-CNN Network</head><p>We insert the NDDR layers in multiple levels of the original single-task networks, to enable layerwise feature fusing/embedding for different tasks. We refer to the proposed network for MTL as the NDDR-CNN network. <ref type="figure">Figure 1</ref> shows the NDDR-CNN network structure for two tasks. It can easily be extended to K-task problems. Let the number of channels for the single-task features be D. Then NDDR-CNN for K tasks can be constructed by: 1) concatenating the features from K tasks according to the channel dimension, and 2) using 1 ? 1 convolution with (filters ?1 ? 1? channels) = (C ? 1 ? 1 ? KC) to conduct dimensionality reduction, where C is the channel dimension size of the output features from each task.</p><p>Note that the elements of the NDDR layer are common CNN operations, which ensures that the proposed NDDR layer can be extended to various state-of-the-art CNN architectures in a "plug-and-play" manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">NDDR-CNN Network with Shortcuts</head><p>In order to avoid gradient vanishing at lower NDDR layers, we propose a new network that enables to pass gradients directly from the last convolutional layer to the lower ones via shortcut connections, namely NDDR-CNN-Shortcut. <ref type="figure">Figure 2</ref>. The NDDR-CNN-Shortcut network. In NDDR-CNN-Shortcut, we use shortcut connections to enable gradients to directly route to the lower NDDR layers. This is done by resizing the lower NDDR output to the spatial size of the last NDDR output, then concatenating the resized features of the same task according to the channel dimension, and finally using 1 ? 1 convolution to do dimensionality reduction (best view in color).</p><p>Specifically, the output of each NDDR layer is resized to the spatial sizes of the last convolutional output. Then we concatenate all the resized feature maps of the same task from different layers together according to the channel dimension. Finally, in order to fit the input size of the following fully-convolutional/connected layers, we further use 1 ? 1 convolution to learn more compact feature tensors (e.g., in the VGG network, we reduce the channel dimension of concatenated features to 512). An illustration of the NDDR-CNN-Shortcut network is shown in <ref type="figure">Fig. 2.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Relationship to State-of-the-art Methods</head><p>Our method is closely related to the cross-stitch network <ref type="bibr" target="#b30">[31]</ref>. In order to seek the optimal network struc-ture for MTL, the cross-stitch network <ref type="bibr" target="#b30">[31]</ref> uses trainable scalars to scale the features at layers in the same level from different tasks, and then adds them together as new features. Our work is related to the cross-stitch network but has three major differences: 1) We have different motivations, i.e., our work is motivated by learning discriminative low-dimensional embeddings on the concatenated features from multiple tasks. 2) Our method can generalize the cross-stitch network by fixing the off-diagonal elements of the projection matrix to 0, and only updating the diagonal elements with the same value (i.e., update the projection matrix by ? and ? in Eq. <ref type="formula" target="#formula_3">(3)</ref>). 3) We further propose an NDDR-CNN-Shortcut model, which further uses hierarchical features from different CNN levels for better training and convergence. Similarly, our network also takes the sluice network <ref type="bibr" target="#b39">[40]</ref> as a special case: the sluice network predefines a fixed number of subspaces to fuse the features from different tasks between different subspaces (each contains multiple feature channels), while our model can automatically fuse the features according to each single channel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Implementation Details</head><p>Note that the state-of-the-art convolutional network architectures such as VGG <ref type="bibr" target="#b44">[45]</ref>, ResNet <ref type="bibr" target="#b13">[14]</ref>, and DenseNet <ref type="bibr" target="#b15">[16]</ref> typically group similar operations into stages/blocks, where each stage contains {convolution-activation} n (possibly with pooling). In order to make the least modification to the baseline network architecture to investigate the performance of the proposed NDDR layer, we only connect the two networks by applying the NDDR layer at the end of each stage/block. For example, we apply the NDDR layers at the outputs of pool1, pool2, pool3, pool4, and pool5 for the VGG network. Similarly, as much deeper as ResNet is, we still apply only 5 NDDR layers in it, e.g., at the outputs of conv1n3, conv2 3n3, conv3 4n3, conv4 6n3, and conv5 3n3 for ResNet-101. Also, it is worth noting that the additional parameters introduced by the NDDR layers are also very few with respect to those for the whole networks. For example, when applying NDDR layers at pool1, pool2, pool3, pool4, and pool5 of the VGG-16 network, the additional parameters for the NDDR layers are only 1.2M, being 0.8% compared to the original 138M parameters of the entire VGG-16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Ablation Analysis</head><p>In this section, several ablations have been done to analyze NDDR-CNN. Two factors about the NDDR layers are analyzed, i.e., different 1 ? 1 convolutional weight initializations, and the scales of the "base" learning rate (i.e., the learning rate for the remaining network) to train the NDDR layers. We also analyze which pretrained weights should be used as initialization, i.e., the weights trained on ImageNet or different single tasks. We use a two-task problem here and in the following sections. For the ablation analysis, we use semantic segmentation and surface normal prediction. Dataset. The NYU v2 dataset <ref type="bibr" target="#b31">[32]</ref> is used for semantic segmentation and surface normal prediction. We use the official train/val splits which include 795 images for training and 654 images for validation. For semantic segmentation, the NYU v2 dataset contains 40 classes such as beds, cabinets, clothes, books, etc. <ref type="bibr" target="#b10">[11]</ref>. The NYU v2 dataset also has the pixel-level surface normal ground-truths precomputed by the depth labeling <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b31">32]</ref>. Network Architecture. We use the state-of-the-art architecture for pixel-level tasks, i.e., Deeplab <ref type="bibr" target="#b3">[4]</ref>, for both semantic segmentation and surface normal prediction. Deeplab is essentially a VGG or ResNet network backbone with atrous convolution and atrous spatial pyramid pooling. We do not implement Fully Connected CRFs or multi-scale inputs as they are not related to the NDDR layer we proposed. We are careful to stick closely to the proposed NDDR layer by using the same atrous convolution and atrous spatial pyramid pooling for all the methods, so as to clearly see the effects of simply incorporating the NDDR layer. We use the Deeplab-VGG-16 architecture in all the ablation analysis. Losses. We use the softmax cross-entropy loss for semantic segmentation. For surface normal prediction, we use the 2 regression loss after normalizing the normal vector of each pixel to have unit 2 norm (i.e., this represents a direction for a certain angle). Therefore, our loss for surface normal prediction is also equivalent to the cosine loss. Evaluation Metrics. The performance of semantic segmentation is evaluated by mean Intersection over Union (mIoU) and Pixel Accuracy (PAcc). For surface normal estimation, we use Mean and Median angle distances of all the pixels for evaluation (the lower the better). In addition, we also use the metrics introduced by <ref type="bibr" target="#b5">[6]</ref>, which are the percentage of pixels that are within the angles of 11 ? , 22.5 ? , 30 ? to the ground-truth (the higher the better).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Initializations for NDDR Layers</head><p>In order to have a mild initialization which resembles single-task networks, we keep the diagonal weights of the NDDR layer as non-zeros. Recall that the NDDR layer for a two-task problem is</p><formula xml:id="formula_2">F out = F in 1 , F in 2 W 1 , W 2 .</formula><p>In order to initialize the NDDR weights W 1 and W 2 , we let:</p><formula xml:id="formula_3">F out 1 = F in 1 , F in 2 ? ? ? ? ? ? 0 ... 0 ? 0 ... 0 0 ? ... 0 0 ? ... 0 . . . . . . . . . . . . . . . . . . 0 0 ... ? 0 0 ... ? ? ? ? ? ? ,<label>(3)</label></formula><p>where F in 1 , F in 2 are the inputs to the NDDR layer and F out 1 is the output which will be fed to Task 1 <ref type="bibr" target="#b0">1</ref>   ? = 1 and ? = 0, the whole network will start with the single-task networks, i.e., F out 1 = F in 1 . We refer to this as diagonal initialization.</p><p>In the experiments, we have 5 different diagonal initializations with (?, ?) ranging from (1, 0) to (0, 1), i.e., from the mildest initialization from the same tasks to the most severe initialization from the opposite tasks. In addition, we also discuss the random initialization of the whole weight matrices [W 1 , W 2 ] with Xavier initialization <ref type="bibr" target="#b9">[10]</ref>. <ref type="table" target="#tab_0">Table 1</ref> shows the performance with different initializations of the NDDR layer. The results show that the diagonal initialization is better than Xavier initialization 2 , and that the initialization of (?, ?) has a little effect on results. In the following experiments, we use diagonal initialization with (?, ?) = (0.9, 0.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Learning Rates for NDDR Layer</head><p>In this section, we discuss the learning rate for the NDDR layer. There are two main reasons to set a larger learning rate specifically for the NDDR layer. First, as analyzed in Sect. 4.1, the NDDR-CNN becomes single-task networks if we set a very large weight (e.g., ? = 1, much larger than the weights of perception convolutional layers) at the diagonal of W 1 . Thus, we hypothesize that the magnitude of the NDDR layer weights should be larger, therefore requiring a larger learning rate. Second, a larger learning rate for NDDR layers is also necessary if we fine-tune  the NDDR-CNN from the pretrained single-task networks. Therefore, we analyze the proper learning rate for the NDDR layer as how many times it should be with respect to the base learning rate (for the remaining network excluding the NDDR layers). <ref type="table" target="#tab_1">Table 2</ref> shows the performance of using different learning rates for the NDDR layer. It verifies that larger learning rates should be applied for NDDR layers. In the following experiments, we use 100 times of the base learning rate for the NDDR layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Pretrained Weights for Network Initialization</head><p>Two network initialization strategies can be applied. We may use the network weights pretrained on a general task (e.g. pretrained Deeplab-VGG-16 <ref type="bibr" target="#b3">[4]</ref> for semantic segmentation on Pascal VOC 2012 <ref type="bibr" target="#b6">[7]</ref>) or finetuned on corresponding target single tasks. The results with different pretrained models are summarized in <ref type="table" target="#tab_3">Table 3</ref>, which show that initializing the finetuned weights from target single tasks performs better. The results indicate that by simply adding several NDDR layers, we have enlarged the capability of the (converged) original networks, which further enables to skip the previously existing saddle points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we perform various experiments on both different network structures and different task sets to demonstrate the promising performance and desirable generalizability of the proposed NDDR-CNN.</p><p>Specifically, VGG-16 <ref type="bibr" target="#b44">[45]</ref> and ResNet-101 <ref type="bibr" target="#b13">[14]</ref> have been used in our experiments, we put the results on AlexNet <ref type="bibr" target="#b19">[20]</ref> in <ref type="table" target="#tab_0">Table A1</ref>. In addition, we also test our proposed NDDR-CNN-Shortcut with the VGG structure, where the gradients can be passed to the lower NDDR layers by the shortcut connections. This can further demonstrate the performance of the proposed NDDR layer. We refer to this network as VGG-16-Shortcut.</p><p>For evaluation, we train each task separately using the common single-task network architecture without NDDR layers as our single task baseline. The results from the most widely used heuristic multi-task network structure are performed as our multi-task baseline, where all the convolutional layers are shared and the split takes place after  <ref type="table">Table 4</ref>. Experimental results on semantic segmentation and surface normal prediction using VGG-16. Sing., Mul., C.-S., and Sluice represent the single-task baseline, the multiple-task baseline, the cross-stitch network, and the sluice network, respectively.</p><p>the last convolutional layer. We also investigate the performances of the cross-stitch network <ref type="bibr" target="#b30">[31]</ref> and the state-ofthe-art sluice network <ref type="bibr" target="#b39">[40]</ref> for comparison, in which we apply the same number of cross-stitch/sluice layers at the same locations as our NDDR layers. We use the number of subspaces as 2 for sluice network as suggested in <ref type="bibr" target="#b39">[40]</ref>. For the fair comparison, we use the best hyperparameters in <ref type="bibr" target="#b30">[31]</ref> and <ref type="bibr" target="#b39">[40]</ref> to train the corresponding networks 3 .</p><p>As we aim to a general purpose MTL method, very diverse task sets are chosen to evaluate our performance. These include pixel-level labeling tasks on scene images, i.e., semantic segmentation and surface normal prediction, and image-level classification tasks on human faces, i.e., age and gender classification. In the following subsections, we perform the semantic segmentation and surface normal prediction on NYU v2 dataset <ref type="bibr" target="#b31">[32]</ref>, and the age and gender classification on the IMDB-WIKI dataset <ref type="bibr" target="#b37">[38]</ref>. We detail the task configurations in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Semantic Segmentation and Surface Normal Prediction</head><p>In this section, we test our network on VGG-16, ResNet-101, and VGG-16-Shortcut to verify the desirable performance of the proposed network. In addition, by doing this, we further demonstrate the desirable generalizability of the proposed NDDR layers on different network architectures.</p><p>The configurations of the semantic segmentation, surface normal prediction, and the best hyperparameters to train the network can be found in Sect. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Experiments on VGG-16 Network</head><p>In this section, we combine two VGG-16 networks by applying the NDDR layer at the outputs of pool1, pool2, pool3, pool4 and pool5. <ref type="table">Table 4</ref> shows the results on semantic segmentation and surface normal prediction using the VGG-16 network.  <ref type="table">Table 5</ref>. Experimental results on semantic segmentation and surface normal prediction using ResNet-101. Sing., Mul., C.-S., and Sluice represent the single-task baseline, the multiple-task baseline, the cross-stitch network, and the sluice network, respectively.  <ref type="table">Table 6</ref>. Experimental results on semantic segmentation and surface normal prediction using VGG-16-Shortcut. Sing., Mul., C.-S., and Sluice represent the single-task baseline, the multiple-task baseline, the cross-stitch network, and the sluice network, respectively.</p><p>Though as simple as our method is, it significantly outperforms the state-of-the-art methods. For example, our method outperforms the sluice network by around 3.8% in "within 11.25 ? " metric in surface normal prediction, and 1.1%-1.2% for both metrics in semantic segmentation. These results demonstrate the promising performance of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Experiments on ResNet-101 Network</head><p>We perform the NDDR layers in the ResNet-101 network, where the NDDR layers are only applied at the output of conv1n3, conv2 3n3, conv3 4n3, conv4 6n3 and conv5 3n3.</p><p>The results are shown in <ref type="table">Table 5</ref>. It indicates that our method consistently outperforms the baseline and state-ofthe-art results. Noted that comparing with the as deep as 101-layer network, we only slightly modified the ResNet-101 by adding five NDDR layers. These results further demonstrate the efficacy of the proposed NDDR layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Experiments on VGG-16 Network with Shortcut Connections</head><p>We test the proposed NDDR-CNN-Shortcut with the VGG-16 structure, i.e., the VGG-16-Shortcut network to further validate our performance. Compared with ResNet, the VGG-16-Shortcut network resembles more to DenseNet <ref type="bibr" target="#b15">[16]</ref>. In VGG-16-Shortcut, <ref type="figure">Figure 3</ref>. Some example illustrations and statistics of ages and genders for the IMDB-WIKI dataset. The statistics show that we have sufficient samples to train both genders, and the ages of most samples are between <ref type="bibr">20 -50.</ref> the gradients can be passed to the lower NDDR layers by the direct and shortest shortcut connections, rather than by multiple shortcuts in ResNet-like networks where the gradients may still decay <ref type="bibr" target="#b3">4</ref> .</p><p>The results for VGG-16-Shortcut are shown in <ref type="table">Table 6</ref>. Compared with the performance on the "vanilla" VGG-16 network (i.e., <ref type="table">Table 4</ref>), the results of all the methods are improved in VGG-16-Shortcut. Especially, the improvements in our method are higher than those in our counterpart. <ref type="table">Table 6</ref> shows that our method consistently outperforms the state-of-the-art methods. Especially, our method outperforms the sluice network by 3.1% for "within 11.25 ? " metric in surface normal prediction, and 1.0%-1.5% for the two metrics in semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Age Estimation and Gender Classification</head><p>Dataset. We use the IMDB-WIKI dataset <ref type="bibr" target="#b37">[38]</ref> for this task set, which contains 460723 images collected from 20284 subjects. After filtering out the images with more than one faces and the images without age or gender labels, the remaining 187103 images from 12325 subjects are used to perform our experiments. These contain images for both genders with ages from 0 to 99. We randomly choose 24090 images from 2000 subjects for evaluation, and the remaining 163013 images from 10325 subjects are used for training. In the training set, we have sufficient samples for both male and female, but the training data for ages is imbalanced. Some image examples, with the gender and age statistics, are shown in <ref type="figure">Fig. 3</ref>. Network Architecture. The VGG-16 network is used as the base network in this experiment, with the NDDR layers applied after pool1, pool2, pool3, pool4 and pool5. Losses. Motivated by <ref type="bibr" target="#b37">[38]</ref>, we treat both age and gender estimations as classification problems, i.e., 2-class and 100class classifications. We use softmax cross-entropy loss in both tasks. Evaluation Metrics. Classification accuracy (Acc) is used to evaluate the gender classification. For age estimation, we follow the metric from <ref type="bibr" target="#b37">[38]</ref>. That is, for each image i, we treat the output p i ? R 100 from softmax as the probabilities for different ages (i.e., 0-99). Therefore the final age esti- <ref type="bibr" target="#b3">4</ref> Note that we did not implement the ResNet-like shortcuts, such as DenseNet. This is because that the ResNet-like shortcuts in DenseNet is less related to the NDDR layer we proposed. Therefore, we carefully stitch to the factors that influence the NDDR layer to analysis it more clearly.  <ref type="table">Table 7</ref>. It shows that our method on age estimation significantly outperforms the state-of-the-art methods, i.e., (8.5 ? 8.0)/8.5 ? 5.9% for Mean AE and (7.0 ? 6.2)/7.0 ? 11.4% for Median AE. While for the gender classification, our method just performs comparably with the cross-stitch network. This is because that gender classification is a two-class classification problem with sufficient labeled samples for each gender. Therefore, it benefits less from the other task (with another set of labels) when learning the representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we proposed a novel CNN structure for general-purpose MTL. Firstly, the task-specific features with the same spatial resolution from different tasks were concatenated. Then, we performed Neural Discriminative Dimensionality Reduction (NDDR) over them to learn a discriminative feature embedding for each task, which also satisfies input sizes of the following layers.</p><p>The NDDR layer is simple and effective, which is constructed by combining existing CNN components in a novel way. The proposed NDDR networks can be trained in an end-to-end fashion without any extraordinary operations of a modern CNN. This desirable property guarantees that the proposed NDDR layer can easily be extended to various state-of-the-art CNN architectures in a "plug-and-play" manner. In addition, our proposed NDDR-CNN generalizes several state-of-the-art CNN based MTL models, such as the cross-stitch network <ref type="bibr" target="#b30">[31]</ref> and the sluice network <ref type="bibr" target="#b39">[40]</ref>.</p><p>We performed detailed ablation analysis, showing that the proposed NDDR layer is easy to train and also robust to different hyperparameters. The experiments on various CNN structures and different task sets demonstrate the promising performance and desirable generalizability of our proposed method. An interesting future research direction can be studying explicitly imposing various dimensionality reduction assumptions on the NDDR layer.</p><p>? Additional ablation analysis for the cross-stitch network on VGG-16 backbone <ref type="bibr" target="#b44">[45]</ref> to verify the hyperparameters for the cross-stitch network used in our main text are optimal.</p><p>Surface Normal Prediction Semantic Seg.   <ref type="table" target="#tab_1">Table A2</ref>. Ablation analysis for the cross-stitch network on VGG-16. This is to ensure that the hyperparameters for the cross-stitch network, i.e., (?, ?) = (0.9, 0.1) and 1000x learning rate for fuse layers, used in our main text are the best ones for the cross-stitch network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1. Semantic Segmentation and Surface Normal Prediction on AlexNet</head><p>We conduct Semantic Segmentation and Surface Normal Prediction on AlexNet <ref type="bibr" target="#b19">[20]</ref> with FCN32s <ref type="bibr" target="#b26">[27]</ref>, as those in the cross-stitch network paper <ref type="bibr" target="#b30">[31]</ref>. We also use the same hyperparameters as the those in <ref type="bibr" target="#b30">[31]</ref>. The results in <ref type="table" target="#tab_0">Table  A1</ref> show that our method outperforms the cross-stitch network and the sluice network on AlexNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2. Ablation Analysis for the Cross-Stitch Network on VGG-16</head><p>In this section, we verify that, in our main text, we have fair comparisons with the state-of-the-art cross-stitch network, especially regarding the hyperparameters on different network backbones. In other words, we show that the hyperparameters for the cross-stitch network, originally obtained from <ref type="bibr" target="#b30">[31]</ref> on AlexNet, are still the best for other network backbones. This can be investigated by doing ablation analysis of the cross-stitch network on other network backbones. The ablation analysis of the cross-stitch network on VGG-16 <ref type="bibr" target="#b44">[45]</ref> is shown in <ref type="table" target="#tab_1">Table A2</ref>, which demonstrates that the best hyperparameters of the cross-stitch network have been used in our main text for fair comparativeevaluation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The results with different initializations for the NDDR layers.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Surface Normal Prediction</cell><cell></cell><cell cols="2">Semantic Seg.</cell></row><row><cell></cell><cell cols="2">Angle Distance</cell><cell cols="3">Within t ? (%)</cell><cell>(%)</cell><cell></cell></row><row><cell></cell><cell cols="2">(Lower Better)</cell><cell cols="3">(Higher Better)</cell><cell cols="2">(Higher Better)</cell></row><row><cell>(?, ?)</cell><cell>Mean</cell><cell>Med.</cell><cell>11.25</cell><cell>22.5</cell><cell>30</cell><cell>mIoU</cell><cell>PAcc</cell></row><row><cell>(1, 0)</cell><cell>14.0</cell><cell>10.3</cell><cell>53.2</cell><cell>79.1</cell><cell>88.6</cell><cell>36.2</cell><cell>66.5</cell></row><row><cell>(0.9, 0.1)</cell><cell>13.9</cell><cell>10.2</cell><cell>53.5</cell><cell>79.5</cell><cell>88.8</cell><cell>36.2</cell><cell>66.4</cell></row><row><cell>(0.5, 0.5)</cell><cell>13.9</cell><cell>10.2</cell><cell>53.5</cell><cell>79.3</cell><cell>88.6</cell><cell>36.0</cell><cell>66.4</cell></row><row><cell>(0.1, 0.9)</cell><cell>14.3</cell><cell>10.6</cell><cell>52.4</cell><cell>78.5</cell><cell>88.0</cell><cell>35.7</cell><cell>66.1</cell></row><row><cell>(0, 1)</cell><cell>14.2</cell><cell>10.6</cell><cell>52.5</cell><cell>78.2</cell><cell>87.8</cell><cell>35.7</cell><cell>65.9</cell></row><row><cell>Random</cell><cell>15.0</cell><cell>11.6</cell><cell>49.0</cell><cell>76.7</cell><cell>87.0</cell><cell>33.4</cell><cell>64.4</cell></row><row><cell></cell><cell cols="4">Surface Normal Prediction</cell><cell></cell><cell cols="2">Semantic Seg.</cell></row><row><cell></cell><cell cols="2">Errors</cell><cell cols="3">Within t ? (%)</cell><cell cols="2">(%)</cell></row><row><cell></cell><cell cols="2">(Lower Better)</cell><cell cols="3">(Higher Better)</cell><cell cols="2">(Higher Better)</cell></row><row><cell>Scale</cell><cell>Mean</cell><cell>Med.</cell><cell>11.25</cell><cell>22.5</cell><cell>30</cell><cell>mIoU</cell><cell>PAcc</cell></row><row><cell>1</cell><cell>14.7</cell><cell>11.2</cell><cell>50.1</cell><cell>77.3</cell><cell>87.4</cell><cell>35.9</cell><cell>65.9</cell></row><row><cell>10</cell><cell>14.4</cell><cell>10.7</cell><cell>51.9</cell><cell>78.1</cell><cell>87.9</cell><cell>36.0</cell><cell>66.1</cell></row><row><cell>10 2</cell><cell>13.9</cell><cell>10.2</cell><cell>53.5</cell><cell>79.5</cell><cell>88.8</cell><cell>36.2</cell><cell>66.4</cell></row><row><cell>10 3</cell><cell>13.9</cell><cell>10.6</cell><cell>52.4</cell><cell>79.6</cell><cell>89.2</cell><cell>35.7</cell><cell>66.4</cell></row></table><note>. By writing the weight of NDDR in this way, it shows that if we initialize</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table /><note>The results with different learning rates for the NDDR layers (i.e., the scale with respect to the base learning rate for other layers). The learning rates are represented as different scales with respect to those for other perception convolutional layers.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>The results with different pretrained models. Pret. means the pretrained Deeplab-VGG-16 weights for semantic segmentation on Pascal VOC 2012, and Sing. represents the finetuned weights from the corresponding target single tasks (through singletask networks).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>{0, 1, ..., 99} ? R 100 is the age dictionary. We use Mean Absolute Error (Mean AE) and Median Absolute Error (Median AE) for evaluating the age estimation.The experimental results are show in</figDesc><table><row><cell></cell><cell>Age</cell><cell></cell><cell>Gender</cell></row><row><cell></cell><cell cols="2">(Lower Better)</cell><cell>(Higher Better)</cell></row><row><cell></cell><cell cols="2">Mean AE Median AE</cell><cell>Acc. (%)</cell></row><row><cell>Single-Task</cell><cell>9.1</cell><cell>7.4</cell><cell>83.5</cell></row><row><cell>Multi-Task</cell><cell>9.0</cell><cell>7.4</cell><cell>82.3</cell></row><row><cell>Cross-Stitch</cell><cell>8.6</cell><cell>7.0</cell><cell>84.0</cell></row><row><cell>Sluice</cell><cell>8.5</cell><cell>7.0</cell><cell>83.9</cell></row><row><cell>Ours</cell><cell>8.0</cell><cell>6.2</cell><cell>84.0</cell></row><row><cell cols="4">Table 7. Experimental results on age and gender classification.</cell></row><row><cell cols="2">mation is calculated by age  *  i =</cell><cell cols="2">99 k=0 p i (k)dict(k), where</cell></row><row><cell>dict =</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table A1 .</head><label>A1</label><figDesc>The results for Semantic Segmentation and Surface Normal Prediction on AlexNet.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Surface Normal Prediction</cell><cell></cell><cell cols="2">Semantic Seg.</cell></row><row><cell></cell><cell cols="2">Angle Dist.</cell><cell cols="2">Within t ? (%)</cell><cell></cell><cell>(%)</cell><cell></cell></row><row><cell>(?, ?)</cell><cell>Mean</cell><cell>Med.</cell><cell>11.25</cell><cell>22.5</cell><cell>30</cell><cell>mIoU</cell><cell>PAcc</cell></row><row><cell>(0.9, 0.1)</cell><cell>15.2</cell><cell>11.7</cell><cell>48.6</cell><cell>76.0</cell><cell>86.5</cell><cell>34.8</cell><cell>65.0</cell></row><row><cell>(0.7, 0.3)</cell><cell>15.5</cell><cell>11.6</cell><cell>48.7</cell><cell>75.1</cell><cell>85.5</cell><cell>34.4</cell><cell>64.6</cell></row><row><cell>(0.5, 0.5)</cell><cell>15.9</cell><cell>12.0</cell><cell>47.5</cell><cell>73.7</cell><cell>84.4</cell><cell>33.9</cell><cell>64.0</cell></row><row><cell>Scale</cell><cell>Mean</cell><cell>Med.</cell><cell>11.25</cell><cell>22.5</cell><cell>30</cell><cell>mIoU</cell><cell>PAcc</cell></row><row><cell>1</cell><cell>15.3</cell><cell>11.9</cell><cell>47.9</cell><cell>75.8</cell><cell>86.3</cell><cell>34.5</cell><cell>64.6</cell></row><row><cell>10</cell><cell>15.5</cell><cell>12.0</cell><cell>47.3</cell><cell>75.1</cell><cell>86.0</cell><cell>35.0</cell><cell>65.0</cell></row><row><cell>10 2</cell><cell>15.3</cell><cell>11.8</cell><cell>48.1</cell><cell>75.6</cell><cell>86.2</cell><cell>35.1</cell><cell>65.2</cell></row><row><cell>10 3</cell><cell>15.2</cell><cell>11.7</cell><cell>48.6</cell><cell>76.0</cell><cell>86.5</cell><cell>34.9</cell><cell>65.0</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We take an NDDR layer from one task as an example, and the initialization of the NDDR layer for the other task is identical.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that the results from Xavier initialization (inTable 1) are still comparable with the previous state-of-the-art method (i.e. the cross-stitch network and the sluice network inTable 4) in surface normal prediction.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We show that the hyperparameters, originally from AlexNet in<ref type="bibr" target="#b30">[31]</ref>, are still the best for other network backbones. Please seeTable A2in Appendix.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is partially supported by NSFC 61773295, NSFC 61601112, ONR N00014-12-1-0883.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>We conduct additional experiments in this section including:</p><p>? Semantic Segmentation and Surface Normal Prediction using AlexNet <ref type="bibr" target="#b19">[20]</ref> backbone.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoderdecoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with task-specific edge detection using cnns and a discriminatively trained domain transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4545" to="4554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Attention to scale: Scale-aware semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3640" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Perceptual organization and recognition of indoor scenes from RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Heterogeneous face attribute estimation: A deep multi-task learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Squeeze-andexcitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multitask learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ubernet: Training auniversal convolutional neural network for low-, mid-, and highlevel vision using diverse datasets and limited memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Discriminatively trained dense surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubor</forename><surname>Ladick?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Zeisl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep multi-task learning with shared memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Low-rank similarity metric learning in high dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cun</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning multiple tasks with deep relationship networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fully-adaptive feature sharing in multi-task networks with applications in person attribute classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangfei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><surname>Javidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">PCA versus LDA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="228" to="233" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cross-stitch networks for multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Pushmeet Kohli Nathan Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Weakly-and semi-supervised learning of a deep convolutional network for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1742" to="1750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hyperface: A deep multi-task learning framework for face detection, landmark localization, pose estimation, and gender recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">YOLO9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Deep expectation of real and apparent age from a single image without facial landmarks. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Rasmus Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-07" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05098</idno>
		<title level="m">An overview of multi-task learning in deep neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Latent multi-task architecture learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Bingel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Augenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>S?gaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Contextual Priming and Feedback for Faster R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Training Region-based Object Detectors with Online Hard Example Mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Beyond Skip Connections: Top-Down Modulation for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.06851</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Pcl: Proposal cluster learning for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">Loddon</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Multiple instance detection network with online instance classifier refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2843" to="2851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Multi-task learning by deep collaboration and application in facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Trottier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Gigu?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brahim</forename><surname>Chaib-Draa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00111</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A-Fast-RCNN: Hard Positive Generation via Adversary for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Zoom better to see clearer: Human part segmentation with auto zoom net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangting</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Joint multi-person pose estimation and semantic part segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangting</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Discriminative multiinstance multitask learning for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhua</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangqian</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinbo</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="519" to="529" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Latent max-margin multitask learning with skelets for 3-d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhua</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinbo</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="439" to="448" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deep multitask representation learning: A tensor factorisation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Rotating your face using multi-task deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junho</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heechul</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byungin</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changkyu</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dusik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="676" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
