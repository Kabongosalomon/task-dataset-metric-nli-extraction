<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HPRNet: Hierarchical Point Regression for Whole-Body Human Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nermin</forename><surname>Samet</surname></persName>
							<email>nermin@ceng.metu.edu.tr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Engineering METU</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Akbas</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Engineering METU</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">HPRNet: Hierarchical Point Regression for Whole-Body Human Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present a new bottom-up one-stage method for whole-body pose estimation, which we call "hierarchical point regression," or HPRNet for short. In standard body pose estimation, the locations of ? 17 major joints on the human body are estimated. Differently, in whole-body pose estimation, the locations of fine-grained keypoints (68 on face, 21 on each hand and 3 on each foot) are estimated as well, which creates a scale variance problem that needs to be addressed. To handle the scale variance among different body parts, we build a hierarchical point representation of body parts and jointly regress them. The relative locations of fine-grained keypoints in each part (e.g. face) are regressed in reference to the center of that part, whose location itself is estimated relative to the person center. In addition, unlike the existing twostage methods, our method predicts whole-body pose in a constant time independent of the number of people in an image. On the COCO WholeBody dataset, HPRNet significantly outperforms all previous bottom-up methods on the keypoint detection of all whole-body parts (i.e. body, foot, face and hand); it also achieves state-of-the-art results on face (75.4 AP) and hand (50.4 AP) keypoint detection. Code and models are available at https://github.com/ nerminsamet/HPRNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>As a challenging computer vision task, human pose estimation aims to localize human body keypoints in images and videos. Human pose estimation has an important role in several vision tasks and applications such as action recognition <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b54">56,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b33">35]</ref>, human mesh recovery <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b24">26]</ref>, augmented/virtual reality <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr" target="#b53">55]</ref>, animation and gaming <ref type="bibr" target="#b0">[2,</ref><ref type="bibr">1,</ref><ref type="bibr" target="#b1">3,</ref><ref type="bibr" target="#b27">29]</ref>. Unlike the standard human pose estimation task, whole-body pose estimation aims to detect face, hand and foot keypoints in addition to the standard human body keypoints. The challenge in this problem is the extreme scale variance or imbalance among different whole-body parts. For example, the relatively small scale of face and hand keypoints make accurate localization of face and hand keypoints more difficult compared to the standard body keypoints such as elbow, knee and hip. Direct application of existing human pose estimation methods do not yield satisfactory results due to this scale variance problem.</p><p>Even though human pose estimation has been well studied for the past few decades, the whole-body pose estimation task has not been sufficiently explored, mainly due to the lack of large-scale fully annotated whole-body keypoint datasets. The previous few methods <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b15">17]</ref>, trained several deep networks separately on different face, hand and body datasets, and ensembled them during inference. These methods suffer from issues arising from datasets' biases, variations of illumination, pose and scales, and complex training and inference pipelines.</p><p>Recently, in order to address the missing benchmark issue, Jin et al. <ref type="bibr" target="#b23">[25]</ref> introduced a novel dataset for whole-body pose estimation, called COCO WholeBody. COCO Whole-Body extends COCO keypoints dataset <ref type="bibr" target="#b32">[34]</ref> by further annotating face, hand and foot keypoints. In addition to the standard, 17 human body keypoints from the COCO keypoints dataset; 68 facial landmarks, 42 hand keypoints and 6 foot keypoints are annotated ( <ref type="figure" target="#fig_0">Figure 1</ref>). Along with these 133 whole-body keypoint annotations, the dataset also has face and hand bounding box annotations that were automatically computed from the extreme keypoints of the corresponding part. They also proposed a strong baseline, called ZoomNet, which has set the state of the art. ZoomNet is a top-down, two-stage method based on the human pose estimation model HRNet <ref type="bibr" target="#b48">[50]</ref>. Given an image, ZoomNet first detects person instances using the FasterRCNN <ref type="bibr" target="#b43">[45]</ref> person detector, then it predicts 17 body and 6 foot keypoints using a CNN model. Later, to overcome the scale variance between whole-body parts, ZoomNet crops the hand and face areas that it detected and transforms them to higher resolu-tions using seperate CNNs to further perform face and hand keypoint estimation.</p><p>There are two main approaches for human pose and whole-body pose estimation; bottom-up <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b2">4,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b21">23,</ref><ref type="bibr" target="#b22">24]</ref> and top-down <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b40">42,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b13">15,</ref><ref type="bibr" target="#b48">50,</ref><ref type="bibr" target="#b52">54]</ref>. Bottom-up methods directly detect human body keypoints and later group them to obtain final poses for each person in a given image. On the other hand, top-down methods (e.g. ZoomNet) first detect and extract person instances, then apply pose estimation on each instance separately. The grouping stage of bottom-up methods is more efficient than repeating pose estimation for each person instance. As a result, top-down methods slow down with the increasing number of people ( <ref type="figure">Figure 6</ref>). However, compared to bottom-up methods, better accuracies are obtained by top-down approaches.</p><p>In this paper, we propose a new bottom-up method, HPRNet, that explicitly handles the hierarchical nature of whole-body pose estimation by regressing keypoints hierarchically. To this end, in addition to the estimation of standard body keypoints, we define the bounding box centers of relatively small body parts such as face and hands with offsets to the person instance center ( <ref type="figure" target="#fig_3">Figure 3</ref>). Concurrently, we build another level of regression where we define each hand and face keypoints with an offset to their corresponding hand and face bounding box centers. We jointly train each level of regression hierarchy and regress all wholebody keypoints with respect to their defined center points. This hierarchical bottom-up approach brings two benefits. First, the scale variance among different body parts are handled naturally as the relative distances within each part are in a similar range and each part-type is processed by a separate sub-network. Second, being a bottom-up method, HPRNet's inference speed is minimally affected by the number of persons in the input image. This is in contrast to the top-down methods such as ZoomNet, which significantly slows down with more person instances (65.7 ms for an image containing 1 person versus 668.2 ms for an image with 10 persons). Our method is based on the center-point based bottom-up object detection methods <ref type="bibr" target="#b58">[60,</ref><ref type="bibr" target="#b44">46,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b29">31]</ref>. These methods can easily be extended to the keypoint estimation task <ref type="bibr" target="#b58">[60,</ref><ref type="bibr" target="#b45">47]</ref>.</p><p>We validated the effectiveness of our method through ablation experiments and comparisons with the state of the art (SOTA) on the COCO WholeBody dataset. Our method significantly outperforms all bottom-up methods. It also outperforms the SOTA top-down method ZoomNet in the detection of face and hand keypoints, while being significantly faster than ZoomNet.</p><p>Our major contribution in this paper is the proposal of a one-stage, bottom-up method to close the performance gap between the bottom-up and top-down methods. In contrast to top-down methods, our method runs almost in constant time, independent from the number of persons in the input image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Human Body Pose Estimation</head><p>We can categorize the current approaches for multiperson pose estimation into two: bottom-up and top-down. In the bottom-up methods <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b2">4,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b21">23,</ref><ref type="bibr" target="#b22">24]</ref>, given an image, body keypoints detected first, without knowing the number or location of person instances or to which person instances these keypoints belong. Later, detected keypoints are grouped and assigned to person instances. Recently, center-based object detection methods <ref type="bibr" target="#b58">[60]</ref> have been extended to perform human pose estimation <ref type="bibr" target="#b58">[60,</ref><ref type="bibr" target="#b45">47]</ref>. These methods represent keypoints with an offset value to the center of the person box and directly regresses them during training. In order to improve localization of keypoints, they also estimate the heatmap of each keypoint as in other bottom-up methods <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b26">28]</ref>. At inference, using center offsets, they group and assign keypoints to person instances. Since bottom-up methods detect all people keypoints at once, they are fast.</p><p>Top-down methods <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b40">42,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b13">15,</ref><ref type="bibr" target="#b48">50,</ref><ref type="bibr" target="#b52">54]</ref> first detect person instances in the input image. Commonly, they use an off-the-shelf object detector (e.g. FasterRCNN <ref type="bibr" target="#b43">[45]</ref>) to obtain person boxes. Next, top-down methods estimate a single person pose for each cropped person box. By cropping and resizing each person box, top-down methods have the advantage to zoom into the details of each person. Therefore, top-down approaches are more capable of handling scale variance issues. As a result, state-of-the-art results are obtained by top-down methods and there is an accuracy gap between top-down and bottom-up approaches. However, since a pose estimation model is run for each person instance, top-down methods tend to be slow on average, that is, they get significantly slower with increasing number of persons in an image ( <ref type="figure">Figure 6</ref>).</p><p>One may think that using human body pose estimation methods on a whole-body pose estimation dataset (i.e. COCO WholeBody) could be a solution for whole-body pose estimation. However, as it is stated in the COCO WholeBody dataset paper <ref type="bibr" target="#b23">[25]</ref>, due to the large scale variance between whole body parts, applying these methods directly results in suboptimal accuracies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Whole-body Pose Estimation</head><p>Whole-body pose estimation requires accurate localization of keypoints on body, face, hand and feet. Detection of keypoints is well studied for each of these body parts independently, under face alignment <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b51">53,</ref><ref type="bibr" target="#b50">52,</ref><ref type="bibr" target="#b57">59]</ref>, facial landmark detection <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b56">58]</ref>, hand pose estimation <ref type="bibr" target="#b38">[40,</ref><ref type="bibr" target="#b37">39]</ref>, hand tracking <ref type="bibr" target="#b46">[48,</ref><ref type="bibr" target="#b47">49]</ref> and feet keypoint detection <ref type="bibr" target="#b4">[6]</ref> top- However, there are not many works on the whole-body pose estimation mostly due to lack of a large-scale annotated dataset. Prior to the release of the COCO WholeBody dataset <ref type="bibr" target="#b23">[25]</ref>, OpenPose <ref type="bibr" target="#b4">[6]</ref> attempted to detect the wholebody keypoints. For this purpose, OpenPose ensembles 5 separately trained models namely human body pose estimation, hand detection, face detection, hand pose estimation and face pose estimation. Due to these multiple models, training and inference of OpenPose are complex and costly. Our end-to-end trainable single network eliminates these drawbacks.</p><p>Hidalgo et al. presented a bottom-up method called SN <ref type="bibr" target="#b15">[17]</ref>. Their model extends PAF <ref type="bibr" target="#b5">[7]</ref> for whole-body pose estimation. Similar to PAF <ref type="bibr" target="#b5">[7]</ref>, they predict heatmaps for each keypoint and use part affinity maps for grouping. SN model is trained on a dataset that is sampled from different datasets. Both SN and our proposed model HPRNet are bottom-up methods. However, SN falls short of handling scale variations between whole-body parts whereas hierarchical point representation of HPRNet overcomes this issue.</p><p>The first step towards having a whole-body pose estimation benchmark is the release of the COCO WholeBody dataset <ref type="bibr" target="#b23">[25]</ref>. Jin et al. extended the existing COCO key-points <ref type="bibr" target="#b32">[34]</ref> dataset by further annotating face, hands and feet keypoints ( <ref type="figure" target="#fig_0">Figure 1</ref>). They also proposed a strong, two-stage, top-down model to perform whole-body pose estimation on the COCO WholeBody dataset. Similar to topdown human pose estimation methods, Jin et al. <ref type="bibr" target="#b23">[25]</ref> first obtain candidate person boxes in an image using Faster-RCNN <ref type="bibr" target="#b43">[45]</ref>. Next, using a single network called Zoom-Net, detection of whole-body keypoints is performed on the person boxes. ZoomNet is composed of 4 sub CNN networks. First, FeatureNet processes input person boxes and extracts shared features at two scales. Next, using features from FeatureNet, BodyNet detects body and foot keypoints. BodyNet is also responsible for the prediction of the face and hand bounding box corner points to roughly extract face and hand areas. Later, cropped face and hand bounding boxes are fed to the FaceHead and HandHead networks to detect the keypoints on face and hands. They use HRNet-W32 <ref type="bibr" target="#b48">[50]</ref>   keypoint with an offset value to a carefully selected location can handle the scale variance. Based on this, we extend the center-based human pose estimation method <ref type="bibr" target="#b58">[60]</ref> to perform whole-body pose estimation by introducing hierarchical regression of keypoints. We also show that hierarchical regression of keypoints for small scale whole-body parts (i.e. face and hand) is more effective than cropping and zooming into them.</p><formula xml:id="formula_0">L FL L FL L cor L size L size L offset L offset L offset</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Model</head><p>HPRNet is a one-stage end-to-end trainable network that learns regressing the whole-body keypoints. In HPRNet, the input image first passes through a backbone network and output of the backbone is fed to <ref type="bibr" target="#b6">8</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Hierarchical Regression of Whole-Body Keypoints</head><p>In HPRNet, we build a hierarchical regression mechanism, where we define each of the whole-body keypoints with a relative location (i.e. offset) to a specific point on the person box.</p><p>We represent each of the (standard) 17 keypoints on the body with an offset to the center of the person bounding box. Unlike the body; face, hand and foot are small parts. Based on this, we define each of this parts with a relative location to their part center as follows; (i) each of 68 face keypoints is defined with an offset to the center of face bounding box, (ii) each of 21 left hand keypoints is defined with an offset to left hand bounding box center, (iii) each of 21 right hand keypoints is defined with an offset to right hand bounding box center, (iv) each of 3 left foot keypoints is defined with an offset to left foot bounding box center, (v) each of 3 right foot keypoints is defined with an offset to right foot bounding box center. Face, hand and foot bounding boxes are automatically extracted from the groundtruth keypoint annotations.</p><p>We treat the bounding box center of the face, left hand, right hand, left foot and right foot as a body part keypoint and define each of them with an offset value to the person box center <ref type="figure" target="#fig_3">(Figure 3</ref>). We illustrate the hierarchical regression of whole-body keypoints in <ref type="figure" target="#fig_4">Figure 4b</ref>.</p><p>At inference, after detecting all the keypoints in the input image, we group and assign them to person instances. To achieve this, we get predicted person centers from the output of Person Center Heatmap branch as in CenterNet <ref type="bibr" target="#b58">[60]</ref>. Next, we obtain the offset values on the predicted person center locations of Body Keypoint Offsets branch output. After that, we add these offsets to person centers to obtain the regressed body keypoint locations. At the same time, we extract the detected body keypoints from the outputted heatmap of the Body Keypoint Heatmaps branch. At the last step, we match the detected and regressed keypoints based on L2 distance and only take the keypoints inside the predicted person bounding box.</p><p>Next, we group face and hand keypoints (and foot keypoints as well, if we are using the Hierarchical Model-I ( <ref type="figure" target="#fig_4">Figure 4b)</ref>. We obtain predicted part centers from the output of Body Keypoint Heatmaps branch. Then, we collect the offset values on the corresponding predicted part center locations of Hand Keypoint Offsets and Face Keypoint Offsets branch output. Finally, we add these offsets to the part centers to obtain the face and hand keypoints.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Regression of Foot Keypoints</head><p>Ideally, each labeled foot part in the COCO Whole-Body dataset should have 3 keypoint annotations. However, more than 20% of annotated feet have missing annotations (i.e. they have one or two keypoints annotations, instead of three). These missing annotations present a challenge to HPRNet, since we automatically extract foot centers from the annotated extreme points. In the case of the missing foot keypoints, the obtained foot center point is not reliable. To deal with this issue, we treat the foot keypoints as body keypoints as shown in <ref type="figure" target="#fig_4">Figure 4c</ref>, and represent them by their offsets to the center of the person bounding box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Network Architecture</head><p>Given an input image of size 4H ? 4W ? 3, the backbone network outputs a feature map of size H ? W ? D. The backbone's output is fed to the following subsequent branches. Each branch has one convolutional layer with 3 ? 3 filters followed by a ReLU layer and another convolutional layer with 1 ? 1 filters.</p><p>? Person Center Heatmap branch outputs H ? W sized tensor for person center point predictions.</p><p>? Person Center Correction branch predicts H ? W ? 2 sized tensor for the local offsets of center locations across the spatial axes. These offsets help to recover the lost precision of the center points due to downsampling operations through the network.</p><p>? </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Objective Functions</head><p>For the optimization of the Person Center Heatmap (PCH) and Body Keypoint Heatmap (BKH) branches, we use the modified focal loss <ref type="bibr" target="#b31">[33]</ref> as done in previous work <ref type="bibr" target="#b29">[31,</ref><ref type="bibr" target="#b59">61,</ref><ref type="bibr" target="#b58">60,</ref><ref type="bibr" target="#b44">46]</ref>. Modified focal loss (FL) is presented in Equation 1. I ? R 4W ?4H?3 is our input image. In HPR-Net,due to downsampling operations, the spatial output size of each branch is 4 times smaller resulting in W ?H. Therefore, Y ? [0, 1] W ?H?C is the ground truth heatmap for person centers and keypoints. C corresponds to class number and keypoint types. For instance, in the Person Center Heatmap branch, we have only person class, thus C = 1. Y ? [0, 1] W ?H?C is the predicted heatmap output by the branches where? x,y,c = 1 indicates presence of a person center or keypoint at location (x, y) for class c. In the following all equations, N is the total number of ground truth person centers or keypoints in image I. ? and ? are focal loss parameters and set as ? = 2 and ? = 4 as in Corner-   Net <ref type="bibr" target="#b29">[31]</ref>.</p><formula xml:id="formula_1">L FL = ?1 N xyc ? ? ? ? ? ? ? ? ? 1 ?? xyc ? log ? xyc if Y xyc = 1 (1 ? Y xyc ) ? ? xyc ? log 1 ?? xyc otherwise</formula><p>(1) To compensate for the discretization error of the person center points due to down-sampling operations through the network, we optimize the Person Center Correction according to the following L1 loss similar to the bottom-up object detectors <ref type="bibr" target="#b29">[31,</ref><ref type="bibr" target="#b59">61,</ref><ref type="bibr" target="#b58">60,</ref><ref type="bibr" target="#b44">46]</ref>.T ? R W ?H?2 is the predicted local offset by the network to recover the lost precision of person center points. p ? R 2 is a ground truth keypoint and p = p 4 is the corresponding ground keypoint location at low-resolution.</p><formula xml:id="formula_2">L cor = 1 N p Tp ? p 4 ?p<label>(2)</label></formula><p>We optimize the Body Keypoint Offset, Hand Keypoint Offset and Face Keypoint Offset branches using the L1 loss. The generic formulation of keypoint regression is presented in Equation 3. In the equation,? ? R H?W ?k?2 is the regression output of keypoints k for a specific whole-body part (i.e. body, face, hand), and B part is the ground truth center of that part's bounding box.</p><formula xml:id="formula_3">L offset = k ? [k] ? B part<label>(3)</label></formula><p>Finally, for the Person Box H &amp; W and Face Box H &amp; W branches, we use L1 loss and scale it by 0.1 as in  CenterNet <ref type="bibr" target="#b58">[60]</ref>. In the Equation 4, s n = (w, h) is the width and height values of the each object (or face) n and S ? R W ?H?2 is the predicted width and height values.</p><formula xml:id="formula_4">L size = 1 N N n=1 ? pn ? s n<label>(4)</label></formula><p>We obtain the overall loss by summing the losses from all branches as follows:</p><formula xml:id="formula_5">L overall =L P CH FL + L BKH FL + L cor + L body offset + L f ace offset + L hand offset + 0.1L person size + 0.1L f ace size<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>This section describes the experiments we conducted to show the effectiveness of our proposed method. First, we present ablation experiments to compare hierarchical models I and II shown in <ref type="figure" target="#fig_4">Figure 4</ref>. Next, we compare our method with our baseline CenterNet <ref type="bibr" target="#b58">[60]</ref>  <ref type="figure" target="#fig_4">(Figure 4a</ref>). Finally, we provide performance comparison with the state of the art and a run-time analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>We use Deep Layer Aggregation (DLA) <ref type="bibr" target="#b55">[57]</ref> backbone for ablation and baseline com-parison experiments, and Hourglass-104 <ref type="bibr" target="#b29">[31]</ref> as our backbone network for state of the art comparison. For all experiments, during training we resize the images to 512 ? 512 pixels. At inference we use images with their original sizes without applying any scaling. We train all the models with a batch size of 32 for 140 epochs using the Adam optimizer <ref type="bibr" target="#b25">[27]</ref>. We set the initial learning rate to 1.25 ? 10 ?4 and divided it by 10 at epochs 90 and 120. We trained all of the models on 4 Tesla V100 GPUs, and tested using a single GTX 1080 TI GPU. We used PyTorch <ref type="bibr" target="#b41">[43]</ref> to implement our models. All of our experiments are conducted on the COCO WholeBody Dataset <ref type="bibr" target="#b23">[25]</ref> and results are presented in keypoint AP (AP kp ) and keypoint recall AR (AR kp ) metrics without any test time augmentation. All results are obtained on the COCO WholeBody validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Hierarchical Model-I vs Hierarchical Model-II</head><p>In <ref type="table">Table 1</ref>, we compare Hierarchical Model-I and Hierarchical Model-II (see <ref type="figure" target="#fig_4">Figure 4</ref>). As it can be seen from the table, regressing foot keypoints as a part of the body keypoints, improves the foot AP kp significantly by 15.6 points (33.5 vs. 49.1). Moreover, hand and whole-body AP kp s also improved about 3 points in this setup. Based on these results, for the rest of the experiments we use the Hierarchical Model-II. <ref type="figure">Figure 5</ref>: Sample whole-body keypoint detection results of HPRNet. We show correctly detected people, and their wholebody poses. Detection box is marked with a purple bounding box, and body pose estimation is shown with blue color for the left parts, red color for the right parts. For clarity, we mark the detected keypoints on face, hand and foot with magenta, green and cyan colors, respectively. Detected faces are marked with an orange bounding box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with Baseline</head><p>To obtain the baseline results, we regress all the 133 keypoints to the person instance box center during training as in CenterNet <ref type="bibr" target="#b58">[60]</ref> (see <ref type="figure" target="#fig_4">Figure 4b</ref>). In <ref type="table" target="#tab_5">Table 2</ref>, we compare HPRNet with the baseline model in terms of accuracy and recall. Our proposed HPRNet significantly outperforms the baseline results for all AP kp and AR kp metrics except the whole-body AP kp . <ref type="table" target="#tab_6">Table 3</ref> presents the performance of our models and several established keypoint estimation models on the COCO WholeBody validation set. We also present average run times if available. HPRNet performs best among the bottom-up methods. Other bottom-up methods especially fail to accurately localize foot keypoints. The performance gap between the second best performing bottom-up method and our method is 40.9 AP kp points on the foot keypoint detection. Similarly, our method outperforms other bottom-up methods for the body, face, hand and whole-body keypoint detection by a large margin. Among the top-down methods, ZoomNet outperforms the well known OpenPose <ref type="bibr" target="#b4">[6]</ref> and HRNet <ref type="bibr" target="#b48">[50]</ref>. Here, ZoomNet is a two-stage framework where at the first stage person candidates are extracted with FasterRCNN and at the second stage ZoomNet is run on these candidate boxes. HRNet can be seen as a one-stage counterpart of ZoomNet and finally OpenPose is a multimodel which requires separate training for each whole-body part. HPRNet obtains state-of-the-art results on face and hand keypoint detection. Our model with Hourglass-104 backbone outperforms ZoomNet on the detection of face  <ref type="figure">Figure 6</ref>: Runtime analysis of ZoomNet and our models with respect to number of people in an image. As the number of people in an image increases, the runtime of Zoom-Net linearly increases. Whereas, our models almost have constant run time. keypoints by 13.1 AP kp points and hand keypoints by 10.3 AP kp points. These successful results on the face and hand keypoint detection, further shows the effectiveness of our proposed bottom-up hierarchical approach over the Zoom-Net's zoom-in mechanism. However, for the detection of the body and whole-body keypoints ZoomNet performs best among all methods. Among all methods, our HPRNet with the DLA backbone is the fastest one (37 ms) with constant run time. In <ref type="figure">Figure 5</ref>, we show sample qualitative results for our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with the State-of-the-art</head><p>Runtime Analysis Average run time of ZoomNet (including Faster RCNN for person detector) on a single image is 174.7 ms. Similarly, the average run time of HPRNet with DLA and Hourglass-104 backbones is 37 ms (26 ms for feedforward and 11 ms for keypoint grouping and assignment) and 101 ms (90 ms for feedforward and 11 ms for keypoint grouping and assignment). HPRNet is significantly faster than ZoomNet. Moreover, as a top-down method, run time of ZoomNet increases as the number of people on an image increases. We compare the run time of our models and ZoomNet in <ref type="figure">Figure 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Face Detection from Keypoints</head><p>In this section, we studied the face detection task and compared HPRNet and ZoomNet. We first extracted face boxes using extreme face keypoints and calculated AP scores as in object detection. Our model outperformed ZoomNet in face detection (46.2 AP vs 37.7 AP). Later, using an additional branch for face detection we train another model (see <ref type="figure" target="#fig_1">Figure 2</ref>). Our model with an extra face detection branch further improved the performance of HPR-Net for face detection achieving 55.8 AP and 56.4 AP with DLA and Hourglass-104 backbones respectively. Results are presented in <ref type="table" target="#tab_7">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we introduced HPRNet as a bottom-up, one-stage method for whole-body keypoint detection. HPR-Net handles scale variance among whole-body parts by hierarchically regressing whole-body keypoints. We evaluated the effectiveness of our method through baseline comparison and ablation experiments on hierarchical structure of whole-body keypoints. Our method achieves state-of-theart results in the detection of face and hand keypoints on the COCO WholeBody dataset; it also outperforms all other bottom-up methods in the detection of all whole-body parts. We conducted a run time analysis between HPRNet and ZoomNet and showed that in contrast to ZoomNet, HPRNet runs in constant time, independent of the number of persons in an image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head><p>The numerical calculations reported in this paper were fully performed at TUBITAK ULAKBIM, High Performance and Grid Computing Center (TRUBA resources).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Whole-body keypoints as defined in the COCO WholeBody dataset. There is a total of 133 keypoints. In addition to standard 17 human body keypoints (top-left) from the COCO keypoints dataset, there are 68 face (top-right), 42 hand (21 keypoints for each) (bottom-right) and 6 foot (3 for each) (bottom-left) keypoints are annotated. Image source: https://github.com/jin-s13/COCO-WholeBody ics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Network architecture of the proposed HPRNet for whole-body keypoint detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>All regressed keypoints in HPRNet. Blue keypoints are body keypoints as defined in COCO keypoints and COCO WholeBody datasets. Colored square points correspond to the face (18), left hand (19), right hand(20), left foot (21) and right foot (22) box centers. Blue keypoints (1-17) and colored square points are defined with an offset to the center of the person instance. For simplicity, face and hand keypoints are sparsely illustrated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Hierarchical representations of whole-body keypoints. (a) Each of 133 whole-body keypoints is defined with an offset to the person box center. (b) Body keypoints and other part centers (i.e. foot, face and hand) are defined with offsets to the person box center. Foot, face and hand keypoints are defined with offsets according to their corresponding part centers. (c) Considering the sparsity of foot keypoint annotations, we define them with their offset values to the person box center. In both Hierarchical Model-I and Hierarchical Model-II, body keypoints are defined with offset values to the person box center. Each face and hand keypoint is defined with an offset to face and hand bounding box centers, respectively. Table 1: Comparison of Hierarchical Model-I (HM-I) and Hierarchical Model-II (HM-II) as in Figure 4. Training foot keypoints with offset values to the person box center outperforms the model when trained with offset values to the foot part centers. Both models are trained with DLA backbone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>network for the BodyNet and HRNetV2p-W18 [51] network for the FaceHead and HandHead networks.Even though bottom-up approaches are fast, they are not robust enough to handle the scale variance across the wholebody parts. However, we hypothesize that representing each</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Optional branch</cell></row><row><cell>HxWx1</cell><cell>HxWx2</cell><cell>HxWx2</cell><cell>HxWx26</cell><cell>HxWx52</cell><cell>HxWx84</cell><cell>HxWx136</cell><cell>HxWx2</cell></row><row><cell>Person Center Heatmaps</cell><cell>Person Center Correction</cell><cell>Person Box W&amp;H</cell><cell>Body Keypoint Heatmaps</cell><cell>Body Keypoint Offsets</cell><cell>Hand Keypoint Offsets</cell><cell>Face Keypoint Offsets</cell><cell>Face Box W&amp;H</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>HxWxD</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Backbone</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Input Image</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>separate branches, namely; Person Center Heatmap, Person Center Correction, Person W &amp; H, Body Keypoint Offsets, Body Keypoint Heatmaps, Hand Keypoint Offsets, Face Keypoint Offsets and Face Box W &amp; H. We show the network architecture of HPRNet inFigure 2.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>AP kp AR kp AP kp AR kp AP kp AR kp AP kp AR kp AP kp AR kp AP kp AR kp</figDesc><table><row><cell>Method</cell><cell cols="2">body</cell><cell cols="2">foot</cell><cell cols="2">face</cell><cell cols="2">hand</cell><cell cols="2">whole-body</cell><cell cols="2">all-mean</cell></row><row><cell>HM-I</cell><cell>55.5</cell><cell>63.4</cell><cell>33.5</cell><cell>55.3</cell><cell>74.6</cell><cell>83.5</cell><cell>44.1</cell><cell>57.8</cell><cell>28.0</cell><cell>40.5</cell><cell>47.1</cell><cell>60.1</cell></row><row><cell>HM-II</cell><cell>55.2</cell><cell>63.1</cell><cell>49.1</cell><cell>60.9</cell><cell>74.6</cell><cell>83.7</cell><cell>47.0</cell><cell>60.8</cell><cell>31.5</cell><cell>44.6</cell><cell>51.5</cell><cell>62.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Comparing HPRNet with the baseline model. To obtain the baseline results, we regress all the 133 keypoints to the person instance box center during training as inFigure 4b. Both models are trained with DLA backbone.AP kp AR kp AP kp AR kp AP kp AR kp AP kp AR kp AP kp AR kp AP kp AR kp</figDesc><table><row><cell>Method</cell><cell cols="2">body</cell><cell cols="2">foot</cell><cell cols="2">face</cell><cell cols="2">hand</cell><cell cols="2">whole-body</cell><cell cols="2">all-mean</cell></row><row><cell cols="2">Baseline 46.7</cell><cell>55.5</cell><cell>33.6</cell><cell>48.9</cell><cell>52.0</cell><cell>60.2</cell><cell>26.4</cell><cell>39.1</cell><cell>33.3</cell><cell>43.4</cell><cell>38.4</cell><cell>49.4</cell></row><row><cell cols="2">HPRNet 55.2</cell><cell>63.1</cell><cell>49.1</cell><cell>60.9</cell><cell>74.6</cell><cell>83.7</cell><cell>47.0</cell><cell>60.8</cell><cell>31.5</cell><cell>44.6</cell><cell>51.5</cell><cell>62.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Comparison with the state-of-the-art on COCO WholeBody validation set. The methods are divided into two groups: top-down and bottom-up. The best results and run times are boldfaced separately for each group. HPRNet performs best among the bottom-up methods. HPRNet also obtains state-of-the-art results on face and hand keypoint detection outperforming ZoomNet. Among all methods, HPRNet with DLA backbone is the fastest one. * indicates that run time linearly increases as the number of people in an image increases. HG is Hourglass-104. R. time is Running time.</figDesc><table><row><cell>Method</cell><cell cols="2">body AP kp AR kp</cell><cell>AP kp</cell><cell cols="2">foot AR kp</cell><cell>AP kp</cell><cell cols="2">face AR kp</cell><cell cols="2">hand AP kp AR kp</cell><cell cols="2">whole-body AP kp AR kp</cell><cell cols="2">all-mean AP kp AR kp</cell><cell>R. time (ms)</cell></row><row><cell>Top-down methods:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>OpenPose [6]</cell><cell>56.3</cell><cell>61.2</cell><cell cols="2">53.2</cell><cell>64.5</cell><cell cols="2">48.2</cell><cell>62.6</cell><cell>19.8</cell><cell>34.2</cell><cell>33.8</cell><cell>44.9</cell><cell>42.3</cell><cell>53.5</cell><cell>45</cell></row><row><cell>HRNet  *  [50]</cell><cell>65.9</cell><cell>70.9</cell><cell cols="2">31.4</cell><cell>42.4</cell><cell cols="2">52.3</cell><cell>58.2</cell><cell>30.0</cell><cell>36.3</cell><cell>43.2</cell><cell>52.0</cell><cell>44.6</cell><cell>52.0</cell><cell>-</cell></row><row><cell>ZoomNet  *  [25]</cell><cell>74.3</cell><cell>80.2</cell><cell cols="2">79.8</cell><cell>86.9</cell><cell cols="2">62.3</cell><cell>70.1</cell><cell>40.1</cell><cell>49.8</cell><cell>54.1</cell><cell>65.8</cell><cell>62.1</cell><cell>70.6</cell><cell>175</cell></row><row><cell>Bottom-up methods:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PAF [7]</cell><cell>26.6</cell><cell>32.8</cell><cell cols="2">10.0</cell><cell>25.7</cell><cell cols="2">30.9</cell><cell>36.2</cell><cell>13.3</cell><cell>32.1</cell><cell>14.1</cell><cell>18.5</cell><cell>19.0</cell><cell>29.1</cell><cell>100</cell></row><row><cell>SN [17]</cell><cell>28.0</cell><cell>33.6</cell><cell cols="2">12.1</cell><cell>27.7</cell><cell cols="2">38.2</cell><cell>44.0</cell><cell>13.8</cell><cell>33.6</cell><cell>16.1</cell><cell>20.9</cell><cell>21.6</cell><cell>32.0</cell><cell>216</cell></row><row><cell>AE [36]</cell><cell>40.5</cell><cell>46.4</cell><cell cols="2">7.7</cell><cell>16.0</cell><cell cols="2">47.7</cell><cell>58.0</cell><cell>34.1</cell><cell>43.5</cell><cell>27.4</cell><cell>35.0</cell><cell>31.5</cell><cell>39.8</cell><cell>-</cell></row><row><cell>Ours (HPRNet-DLA)</cell><cell>55.2</cell><cell>63.1</cell><cell cols="2">49.1</cell><cell>60.9</cell><cell cols="2">74.6</cell><cell>83.7</cell><cell>47.0</cell><cell>60.8</cell><cell>31.5</cell><cell>44.6</cell><cell>51.5</cell><cell>62.6</cell><cell>37</cell></row><row><cell>Ours (HPRNet-HG)</cell><cell>59.4</cell><cell>68.3</cell><cell cols="2">53.0</cell><cell>65.4</cell><cell cols="2">75.4</cell><cell>86.8</cell><cell>50.4</cell><cell>64.2</cell><cell>34.8</cell><cell>49.2</cell><cell>54.6</cell><cell>66.8</cell><cell>101</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Face detection results. The first group of results are obtained from extreme face keypoints for both ZoomNet and HPRNet. The HPRNet results in the second group are obtained with an extra face detection branch. HG is Hourglass-104.MethodAP AP 50 AP 75 AP M AP L When face boxes are extracted from extreme face keypoints:</figDesc><table><row><cell>ZoomNet</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Azure kinect body tracking joints</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">How huawei ml kit&apos;s face detection and hand keypoint detection capabilities helped with creating the game crazy rockets</title>
		<ptr target="https://medium.com/huawei-developers/how-huawei-ml-kits-face-detection-and-hand-keypoint-detection-capabilities-helped-with-creating-6a22fdb7f967" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Human pose estimation via convolutional part heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Face alignment by explicit shape regression. International Journal of Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Openpose: realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gines</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pose2mesh: Graph convolutional network for 3d human pose and mesh recovery from a 2d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsuk</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ar poser: Automatically augmenting mobile pictures with digital avatars imitating poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokcen</forename><surname>Cimen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Maurhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Sumner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Guay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th International Conference on Computer Graphics, Visualization, Computer Vision and Image Processing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Retinaface: Single-shot multi-level face localisation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Ververas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fully automatic multiperson human motion capture for vr applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Elhayek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Onorina</forename><surname>Kovalenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pramod</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jameel</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didier</forename><surname>Stricker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Virtual Reality and Augmented Reality</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rmpe: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqin</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Singlenetwork whole-body pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gines</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaadhav</forename><surname>Raaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haroon</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Part-aligned pose-guided recurrent network for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Arttrack: Articulated multi-person tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Levinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deepercut: A deeper, stronger, and faster multi-person pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Posetrack: Joint multi-person pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Kama: 3d keypoint aware body mesh articulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunrong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.13502</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-person articulated tracking with spatial and temporal embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Towards multiperson pose tracking: Bottom-up and top-down methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xujie</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV PoseTrack Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Whole-body human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lumin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multiposenet: Fast multi-person pose estimation using pose residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salih</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Akbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Animepose: Multi-person 3d pose estimation and animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laxman</forename><surname>Kumarapu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prerana</forename><surname>Mukherjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Appearance consensus driven self-supervised human mesh recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jogendra</forename><surname>Nath Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mugalodi</forename><surname>Rakesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><forename type="middle">Mysore</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R Venkatesh</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Actional-structural graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">2d/3d pose estimation and action recognition using multitask deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Diogo C Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tabia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Associative embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05424</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Knowledgeguided deep fractal neural networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanghan</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiquan</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deepprior++: Improving fast and accurate 3d hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Hands deep in deep learning for hand pose estimation. 20th Computer Vision Winter Workshop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Personlab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Chris Bregler, and Kevin Murphy. Towards accurate multi-person pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nori</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Houghnet: Integrating near and long-range evidence for bottom-up object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nermin</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samet</forename><surname>Hicsonmez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Akbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Houghnet: Integrating near and long-range evidence for visual detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nermin</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samet</forename><surname>Hicsonmez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Akbas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Accurate, robust, and flexible real-time hand tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cem</forename><surname>Keskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duncan</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Leichter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Vinnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd annual ACM conference on human factors in computing systems</title>
		<meeting>the 33rd annual ACM conference on human factors in computing systems</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Fast and robust hand tracking using detection-guided optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Oulasvirta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04514</idno>
		<title level="m">Wenyu Liu, and Jingdong Wang. High-resolution representations for labeling pixels and regions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Mnemonic descent method: A recurrent process applied for end-to-end face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Snape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mihalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Epameinondas</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Project-out cascaded regression with an application to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Mo2cap2: Real-time mobile 3d motion capture with a cap-mounted fisheye camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avishek</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhoefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Pa3d: Poseaction 3d machine for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deep layer aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning deep representation for face alignment with auxiliary attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Bottom-up object detection by grouping extreme and center points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
