<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TricorNet: A Hybrid Temporal Convolutional and Recurrent Network for Video Action Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Ding</surname></persName>
							<email>liding@rochester.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Rochester Rochester</orgName>
								<address>
									<postCode>14627</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
							<email>chenliang.xu@rochester.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Rochester Rochester</orgName>
								<address>
									<postCode>14627</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TricorNet: A Hybrid Temporal Convolutional and Recurrent Network for Video Action Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Action segmentation as a milestone towards building automatic systems to understand untrimmed videos has received considerable attention in the recent years. It is typically being modeled as a sequence labeling problem but contains intrinsic and sufficient differences than text parsing or speech processing. In this paper, we introduce a novel hybrid temporal convolutional and recurrent network (TricorNet), which has an encoder-decoder architecture: the encoder consists of a hierarchy of temporal convolutional kernels that capture the local motion changes of different actions; the decoder is a hierarchy of recurrent neural networks that are able to learn and memorize long-term action dependencies after the encoding stage. Our model is simple but extremely effective in terms of video sequence labeling. The experimental results on three public action segmentation datasets have shown that the proposed model achieves superior performance over the state of the art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Action segmentation is a challenging problem in high-level video understanding. In its simplest form, action segmentation aims to segment a temporally untrimmed video by time and label each segmented part with one of k pre-defined action labels. For example, given a video of Making Hotdog (see <ref type="figure" target="#fig_0">Fig. 1</ref>), we label the first 10 seconds as take bread, and the next 20 seconds as take sausage, and the remaining video as pour ketchup following the procedure dependencies of making a hotdog. The results of action segmentation can be further used as input to various applications, such as video-to-text <ref type="bibr" target="#b2">[3]</ref> and action localization <ref type="bibr" target="#b14">[15]</ref>.</p><p>Most current approaches for action segmentation <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b6">7]</ref> use features extracted by convolutional neural networks, e.g., two-stream CNNs <ref type="bibr" target="#b18">[19]</ref> or local 3D ConvNets <ref type="bibr" target="#b23">[24]</ref>, at every frame after a downsampling as the input, and apply a one-dimensional sequence prediction model, such as recurrent neural networks, to label actions on frames. Despite the simplicity in handling video data, action segmentation is treated similar to text parsing <ref type="bibr" target="#b1">[2]</ref>, which results the local motion changes in various actions being under-explored. For example, the action pour ketchup may consist of a series of sub-actions, e.g., pick up the ketchup, squeeze and pour, and put down the ketchup. Furthermore, the time duration of performing the same action pour ketchup may vary according to different people and contexts.</p><p>Indeed, the recent work by Lea et al. <ref type="bibr" target="#b9">[10]</ref> starts to explore the local motion changes in action segmentation. They propose an encoder-decoder framework, similar to the deconvolution networks in image semantic segmentation <ref type="bibr" target="#b15">[16]</ref>, for video sequence labeling. By using a hierarchy of 1D temporal convolutional and deconvolutional kernels in the encoder and decoder networks, respectively, their model is effective in terms of capturing the local motions and achieves state-of-the-art performance in various action segmentation datasets. However, one obvious drawback is that it fails to capture the long-term dependencies of different actions in a video due to its fixed-size, local receptive fields. For example, pour ketchup usually happens after both take bread and take sausage for a typically video of Making Hotdog. In addition, a dilated temporal convolutional network, similar to the WavNet for speech processing <ref type="bibr" target="#b24">[25]</ref>, is also tested in <ref type="bibr" target="#b9">[10]</ref>, but has worse performance, which further suggests the existence of differences between video and speech data, despite they are both being represented as sequential features.</p><p>To overcome the above limitations, we propose a novel hybrid TempoRal COnvolutional and Recurrent Network (TricorNet), that attends to both local motion changes and long-term action dependencies for modeling video action segmentation. TricorNet uses frame-level features as the input to an encoder-decoder architecture. The encoder is a temporal convolutional network that consists of a hierarchy of one-dimensional convolutional kernels, observing that the convolutional kernels are good at encoding the local motion changes; the decoder is a hierarchy of recurrent neural networks, in our case Bi-directional Long Short-Term Memory networks (Bi-LSTMs) <ref type="bibr" target="#b5">[6]</ref>, that are able to learn and memorize long-term action dependencies after the encoding process. Our network is simple but extremely effective in terms of dealing with different time durations of actions and modeling the dependencies among different actions.</p><p>We conduct extensive experiments on three public action segmentation datasets, where we compare our proposed models with a set of recent action segmentation networks using three different evaluation metrics. The quantitative experimental results show that our proposed TricorNet achieves superior or competitive performance to state of the art on all three datasets. A further qualitative exploration on action dependencies shows that our model is good at capturing long-term action dependencies and produce smoother labeling.</p><p>For the rest of the paper, we first survey related work in the domain of action segmentation and action detection in Sec. 2. We introduce our hybrid temporal convolutional and recurrent network with some implementation variants in Sec. 3. We present both quantitative and qualitative experimental results in Sec. 4, and conclude the paper in Sec. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>For action segmentation, many existing works use frame-level features as the input and then build temporal models on the whole video sequence. Yeung et al. <ref type="bibr" target="#b26">[27]</ref> propose an attention LSTM network to model the dependencies of the input frame features in a fixed-length window. Huang et al. <ref type="bibr" target="#b6">[7]</ref> consider the weakly-supervised action labeling problem. Singh et al. <ref type="bibr" target="#b19">[20]</ref> present a multi-stream bi-directional recurrent neural network for fine-grained action detection task. Lea et al. <ref type="bibr" target="#b9">[10]</ref> propose two temporal convolutional networks for action segmentation and detection. The design of our model is inspired by <ref type="bibr" target="#b19">[20]</ref> and <ref type="bibr" target="#b9">[10]</ref> and we compare with them in the experiments. Lea et al. <ref type="bibr" target="#b10">[11]</ref> introduce a spatial CNN and a spatiotemporal CNN; the latter is an end-to-end approach to model the whole sequence from frames. Here, we use the output features of their spatial CNN as the input to our TricorNet, and compare with their results obtained by the spatiotemporal CNN. Richard et al. <ref type="bibr" target="#b17">[18]</ref> use a statistical language model to focus on localizing and classifying segments in videos of varying lengths. Kuehne et al. <ref type="bibr" target="#b8">[9]</ref> propose an end-to-end generative approach for action segmentation, using Hidden Markov Models on dense trajectory features. We also compare with their results on the 50 Salads dataset.</p><p>Another related area is action detection. Peng and Schmid <ref type="bibr" target="#b16">[17]</ref> propose a two-stream R-CNN to detect actions. Yeung et al. <ref type="bibr" target="#b27">[28]</ref> introduce an action detection framework based on reinforcement learning. Li et al. <ref type="bibr" target="#b13">[14]</ref> present joint classification-regression recurrent neural networks for human action detection from 3D skeleton data. Those methods primarily work for single-action, short videos. The recent work by <ref type="bibr" target="#b28">[29]</ref> considers action detection and dependencies for untrimmed and unconstrained Youtube videos. It is likely that the action segmentation and detection works if fused can be beneficial to each other, but the topic is out of the scope of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>In this section, we introduce our proposed model along with some implementation details. The input to our TricorNet is a set of frame-level video features, e.g., output from a CNN, for each frame of a given video. Let X t be the input feature vector at time step t for 1 ? t ? T , where T is the total number of time steps in a video sequence that may vary among videos in a dataset. The action label for each frame is defined by a sparse vector Y t ? {0, 1} c , where c is the number of classes, such that the true class is 1 and all others are 0. In case there are frames that do not have a pre-defined action label, we use one additional background label for those frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Temporal Convolutional and Recurrent Network</head><p>A general framework of our TricorNet is depicted in <ref type="figure" target="#fig_1">Fig. 2</ref>. The TricorNet has an encoder-decoder structure. Both encoder and decoder networks consist of K layers. We define the encoding layer as L (i) E , and the decoding layer as L (i) D , for i = 1, 2, . . . , K. There is a middle layer L mid between the encoder and the decoder. Here, K is a hyper-parameter that can be turned based on the size and appearance of the video data in a dataset. A large K means the network is deep and, typically, requires more data to train. Empirically, we set K = 2 for all of our experiments.</p><p>In the encoder network, each layer L (i) E is a combination of temporal (1D) convolutions, a non-linear activation function E = f (?), and max pooling across time. Using F i to specify the number of convolutional filters in an encoding layer L (i) E , we define the collection of convolutional filters as W</p><formula xml:id="formula_0">(i) E = {W (j) } Fi j=1 with a corresponding bias vector b i E ? R Fi .</formula><p>Given the output from the previous encoding layer after pooling E (i?1) , we compute activations of the current layer L (i) E :</p><formula xml:id="formula_1">E (i) = f (W (i) E * E (i?1) + b (i) E ) ,<label>(1)</label></formula><p>where * denotes the 1D convolution operator. Note that E (0) = (X 1 , . . . , X T ) is the collection of input frame-level feature vectors. The length of the convolutional kernel is another hyper-parameter. A larger length means a larger receptive field, but will also reduce the discrimination between adjacent time steps. We report the best practices in Sec. 4.</p><p>Here, the middle level layer L mid is the output of the last encoding layer E (K) after the pooling, and it is used as the input to the decoder network. The structure of the decoding part is a reserved hierarchy compared to the encoding part, and it also consists of K layers. We use Bi-directional Long Short-Term Memory (Bi-LSTM) <ref type="bibr" target="#b5">[6]</ref> units to model the long-range action dependencies, and up-sampling to decode the frame-level labels. Hence, each layer L (i) D in the decoder network is a combination of up-sampling and Bi-LSTM.</p><p>Generally, recurrent neural networks use a hidden state representation h = (h 1 , h 2 , . . . , h t ) to map the input vector x = (x 1 , x 2 , . . . , x t ) to the output sequence y = (y 1 , y 2 , . . . , y t ). In terms of the LSTM unit, it updates its hidden state by the following equations:</p><formula xml:id="formula_2">i t = ?(W xi x t + W hi h t?1 + b i ) , f t = ?(W xf x t + W hf h t?1 + b f ) , o t = ?(W xo x t + W ho h t?1 + b o ) , g t = tanh(W xc x t + W hc h t?1 + b c ) , c t = f t c t?1 + i t g t , h t = o t tanh(c t ) ,<label>(2)</label></formula><p>where ?(?) is a sigmoid activation function, tanh(?) is the hyperbolic tangent activation function, i t , f t , o t , and c t are the input gate, forget gate, output gate, and memory cell activation vectors, respectively. Here, W s and bs are the weight matrices and bias terms. A Bi-LSTM layer contains two LSTMs: one goes forward through time and one goes backward. The output is a concatenation of the results from the two directions.</p><p>In TricorNet, we use the updated sequences of hidden states H (i) as the output of each decoding layer L (i) D . We use H i to specify the number of hidden states in a single LSTM layer. Hence, for layer L (i) D , the output dimension at each time step will be 2H i as a concatenation of a forward and a backward LSTM. The output of the whole decoding part will be a matrix D = H (K) ? R T ?2H K , which means at each time step t = 1, 2, . . . , T , we have a 2H K -dimension vector D t that is the output of the last decoding layer L (K) D . Finally, we have a softmax layer across time to compute the probability that the label of frame at each time step t takes one of the c action classes, which is given by:</p><formula xml:id="formula_3">Y t = sof tmax(W d D t + b d ) ,<label>(3)</label></formula><p>where? t ? [0, 1] c is the output probability vector of c classes at time step t, D t is the output from our decoder for time step t, W d is a weight matrix and b d is a bias term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Variation</head><p>In order to find the best structure of combining temporal convolutional layers and Bi-LSTM layers, we test three different model designs (shown in <ref type="figure" target="#fig_2">Fig. 3</ref>). We detail their architectures as follows, and the experiments of different models are presented in Sec. 4.  <ref type="figure" target="#fig_1">Fig. 2</ref>. The intuition of this design is to use different temporal convolutional layers to encode local motion changes, and apply different Bi-LSTM layers to decode the sequence and learn different levels of long-term action dependencies.</p><p>TricorNet (high). TricorNet (high) deploys the Bi-LSTM units only at the middle-level layer L mid , which is the layer between encoder and decoder. It uses temporal convolutional kernels for both encoding layers L </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Implementation Details</head><p>In this work, some of the hyper-parameters of all three TricorNets are fixed and used throughout all the experiments. In the encoding part, we use max pooling with width = 2 across time. Each temporal convolutional layer L (i) E has 32 + 32i filters. In the decoding part, up-sampling is performed by simply repeating each entry twice. The latent state of each LSTM layer L (i) D is given by 2H i . We use Normalized Rectified Linear Units <ref type="bibr" target="#b9">[10]</ref> as the activation function for all the temporal convolutional layer, which is defined as:</p><formula xml:id="formula_4">N orm.ReLU (?) = ReLU (?) max ReLU (?) + ,<label>(4)</label></formula><p>where max ReLU (?) is the maximal activation value in the layer and = 10 ?5 .</p><p>In our experiments, the models are trained from scratch using only the training set of the target dataset. Weights and parameters are learned using the categorical cross entropy loss with Stochastic Gradient Descent and ADAM <ref type="bibr" target="#b7">[8]</ref> step updates. We also add spatial dropouts between convolutional layers and dropouts between Bi-LSTM layers. The models are implemented with Keras <ref type="bibr" target="#b0">[1]</ref> and TensorFlow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>We conduct quantitative experiments on three challenging action segmentation datasets and use three different metrics to evaluate the performance of TricorNet variants. For each dataset, we compare our results with some baseline methods as well as state of the art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>University of Dundee 50 Salads <ref type="bibr" target="#b21">[22]</ref> dataset captures 25 people preparing mixed salad two times each, and has annotated accelerometer and RGB-D video data. In our experiment, we only use the features extracted from video data. The duration of videos varies from 5 to 10 minutes. We use the fine-granularity action labels (mid-level), where each video contains 17 classes of actions performed when making salads, such as cut cheese, peel cucumber. We use the spatial CNN <ref type="bibr" target="#b10">[11]</ref> features as our input with cross validation on five splits, which are provided by <ref type="bibr" target="#b9">[10]</ref>.</p><p>Georgia Tech Egocentric Activities (GTEA) <ref type="bibr" target="#b3">[4]</ref> dataset contains seven types of daily activities such as making sandwich, tea, or coffee. Each activity is performed by four different people, thus totally 28 videos. For each video, there are about 20 fine-grained action instances such as take bread, pour ketchup, in approximately one minute. To make the results comparable, we use the features provided by <ref type="bibr" target="#b9">[10]</ref>, which are outputs from a spatial CNN, also with cross-validation splits.</p><p>JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS) <ref type="bibr" target="#b4">[5]</ref> consists of 39 videos capturing eight surgeons performing elementary surgical tasks on a bench-top. Each surgeon has about five videos with around 20 action instances. There are totally 10 different action classes. Each video is around two minutes long. In this work, we use the videos of suturing tasks. Features and cross-validation splits are provided by <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Metrics</head><p>One of the most common metrics used in action segmentation problems is frame-wise accuracy, which is straight-forward and can be computed easily. However, a drawback is that models achieving similar accuracy may have large qualitative differences. Furthermore, it fails to handle the situation that models may produce large over-segmentation errors but still achieve high frame-wise accuracy. As a result, some work <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> tends to use a segmental edit score, as a complementary metric, which penalizes over-segmentation errors. Recently, Lea et al. <ref type="bibr" target="#b9">[10]</ref> use a segmental overlap F1 score, which is similar to mean Average Precision (mAP) with an Intersection-Over-Union (IoU) overlap criterion. In this paper, we use frame-wise accuracy for all three datasets. According to reported scores by other papers, we also use segmental edit score for JIGSAWS, overlap F1 score with threshold k = 10, 25, 50 for GTEA, and for 50 Salads we use both.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>In experiments, we have tried different convolution lengths and number of layers on each dataset; we report the best practices in discussions below. A background label is introduced for frames that do not have an action label. Since we are using the same features as <ref type="bibr" target="#b9">[10]</ref> for 50 Salads and GTEA, and features from <ref type="bibr" target="#b12">[13]</ref> for JIGSAWS, we also obtain the results of baselines from their work along with state-of-the-art methods.</p><p>50 Salads. In <ref type="table" target="#tab_0">Table 1</ref> , we show that the proposed TricorNet significantly outperforms the state of the art upon all three metrics. We found the best number of layers is two and the best convolution length is 30. We use H = 64 hidden states for each direction of the Bi-LSTM layers. We also observed that using the same number of hidden states for all the Bi-LSTM layers usually achieves better results, however, the number of hidden states has less influence on the results. To test the stability of performance, we use the same parameter setting to conduct multiple experiments. The results usually vary approximately within a 1% range. The parameters in each layer are randomly initialized; the same below.   GTEA. <ref type="table" target="#tab_1">Table 2</ref> shows that our proposed TricorNet achieves superior overlap F1 score, with a competitive frame-wise accuracy on GTEA dataset. The best accuracy is achieved by the ensemble approach from Singh et al. <ref type="bibr" target="#b20">[21]</ref>, which combines EgoNet features with TDD <ref type="bibr" target="#b25">[26]</ref>. Since our approach uses simpler spatial features as <ref type="bibr" target="#b9">[10]</ref>, it is very likely to improve our result by using the state-of-the-art features. Both TricorNet and TricorNet (low) have a good performance on all metrics and outperform ED-TCN. We use convolution length equals to 20 and H = 64 hidden states for LSTMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>JIGSAWS.</head><p>In <ref type="table" target="#tab_2">Table 3</ref>, we show our state-of-the-art result on JIGSAWS dataset. Similar to the results on other datasets, TricorNet tends to achieve better segmental scores as well as keeping a superior or competitive frame-wise accuracy. Besides, TricorNet (low) also has a good performance, which may due to the relatively small size of the dataset. We set 15 for convolution length and 64 hidden states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Action Dependencies</head><p>TricorNet is proposed to learn long-range dependencies of actions taking the advantage of hierarchical LSTM units, which is not a function of ED-TCN using only temporal convolution. <ref type="figure" target="#fig_4">Figure 4</ref> shows the prediction results of a sample testing video from 50 Salads dataset. We find two typical mistakes made by other methods which are very likely due to visual similarity between different actions. ED-TCN predicts action at t 1 to be cut lettuce, of which the ground truth is peel cucumber. A possible reason is that the almost-peeled cucumber looks like lettuce in color. Therefore, when reaching some decision boundary before t 1 , ED-TCN changes its prediction from peel cucumber to cut lettuce. However, TricorNet manages to preserve the prediction peel cucumber after t 1 , possibly because of the overall rarity that cut lettuce comes directly after peel cucumber. In this case, TricorNet uses some long-range action dependencies help improve the performance of action segmentation.</p><p>At t 2 , ED-TCN predicts action to be place tomato into bowl, which is very unreasonable to come after peel cucumber. A possible explanation is that when people placing objects, hands usually cover and make the objects hard to recognize. Thus, it is hard to visually distinguish between actions such as place cucumber into bowl and place tomato into bowl. However, TricorNet succeeds in predicting t 2 as place cucumber into bowl even it is hard to recognize from the video frames. It is possible that the dependency between peel cucumber and place cucumber into bowl is learned by TricorNet. This suggests that action dependency helps improve the accuracy of labeling a segmented part. We also find similar examples in GTEA and JIGSAWS datasets. <ref type="figure" target="#fig_5">Figure 5</ref> shows that in some cases (at t 3 in GTEA and t 4 in JIGSAWS), TricorNet can achieve a better accuracy labeling segmented parts.</p><p>Thus, TricorNet can better learn the action ordering and dependencies, which will improve the performance of action segmentation when sometimes it is visually unrecognizable through video frames. This is very likely to happen in real world, especially for video understanding problems that the video data may be noisy or simply due to occlusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose TricorNet, a novel hybrid temporal convolutional and recurrent network for video action segmentation problems. Taking frame-level features as the input to an encoder-decoder architecture, TricorNet uses temporal convolutional kernels to model local motion changes and uses bi-directional LSTM units to learn long-term action dependencies. We provide three model variants to comprehensively evaluate our model design. Experimental results on three public action segmentation datasets with different metrics show that our proposed model achieves superior performance over the state of the art. A further qualitative exploration on action dependencies shows that our model is good at capturing long-term action dependencies, which help to produce segmentation in a smoother and preciser manner.</p><p>Limitations. In experiments we find that all the best results of TricorNet are achieved with number of layers K = 2. It will either over-fit or stuck in local optimum when adding more layers. Considering all three datasets are relatively small with limited training data (despite they are standard in evaluating action segmentation), using more data is likely going to further improve the performance.</p><p>Future Work. We consider two directions for the future work. Firstly, the proposed TricorNet is good to be evaluated on other action segmentation datasets to further explore its strengths and limitations. Secondly, TricorNet can be extended to solve other video understanding problems, taking advantage of its flexible structural design and superior capability of capturing video information.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An overview of video action segmentation problems with our proposed methodology, with example video and action labels from GTEA<ref type="bibr" target="#b3">[4]</ref> dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The overall framework of our proposed TricorNet. The encoder network consists of a hierarchy of temporal convolutional kernels that are effective at capturing local motion changes. The decoder network consists of a hierarchy of Bi-LSTMs that model the long-term action dependencies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Model variants of TricorNet. TricorNet. TricorNet uses temporal convolutional kernels for encoding layers L (i) E and uses Bi-LSTMs for decoding layers L (i) D , as proposed in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>E</head><label></label><figDesc>and decoding layers L (i) D . The intuition is to use Bi-LSTM to model the sequence dependencies at an abstract level, where the information is highly compressed while keeping both encoding and decoding locally. This is expected to perform well when action labels are coarse.TricorNet (low). TricorNet (low) deploys the Bi-LSTM units only at the layer L (K) D , which is the last layer of the decoder. It uses temporal convolutional kernels for the encoding layers L (i) E and the rest of the layers L (i) D , where i &lt; k in the decoder. The intuition of this design is to use Bi-LSTM to decode details only. For the cases where action labels are fine-grained, it is better to focus on learning dependencies at a low-level, where information is less compressed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Top: Example images in a sample testing video from 50 Salads dataset. Middle: Ground truth and predictions from different models. Bottom: Two typical mistakes caused by visual similarity (left -peeled cucumber is similar to lettuce in color; right -hands will cover object when placing); TricorNet avoids the mistakes by learning long-range dependencies of different actions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Left: Example images in a sample testing video from GTEA dataset, with ground truth and predictions from different models. Right: Example images in a sample testing video from JIGSAWS dataset, with ground truth and predictions from different models. Our proposed TricorNet avoids some classification mistakes when it can still perform a precise segmentation between actions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Experimental results on University of Dundee 50 Salads dataset. We use frame-wise accuracy (Acc.), segmental edit score (Edit) and overlap F1 score with thresholds (F1@10, 25, 50) for evaluation. The top-two results for each metric are in boldface; the same applies for other tables.</figDesc><table><row><cell cols="3">50 Salads (Mid) Acc. Edit F1@{10, 25, 50}</cell></row><row><cell cols="2">Spatial CNN [11] 54.9 24.8</cell><cell>32.3, 27.1, 18.9</cell></row><row><cell>IDT+LM [18]</cell><cell>48.7 45.8</cell><cell>44.4, 38.9, 27.8</cell></row><row><cell>ST-CNN [11]</cell><cell>59.4 45.9</cell><cell>55.9, 49.6, 37.1</cell></row><row><cell>Bi-LSTM</cell><cell>55.7 55.6</cell><cell>62.6, 58.3, 47.0</cell></row><row><cell cols="2">Dilated TCN [10] 59.3 43.1</cell><cell>52.2, 47.6, 37.4</cell></row><row><cell>ED-TCN [10]</cell><cell>64.7 59.8</cell><cell>68.0, 63.9, 52.6</cell></row><row><cell>TricorNet (high)</cell><cell>65.5 59.3</cell><cell>66.4, 62.7, 53.3</cell></row><row><cell>TricorNet (low)</cell><cell>65.4 59.0</cell><cell>67.0, 63.2, 52.1</cell></row><row><cell>TricorNet</cell><cell>67.5 62.8</cell><cell>70.1, 67.2, 56.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Experimental results on Georgia Tech Egocentric Activities dataset. We use frame-wise accuracy (Acc.) and overlap F1 score with thresholds (F1@10,25,50) for evaluation.</figDesc><table><row><cell>GTEA</cell><cell cols="2">Acc. F1@{10, 25, 50}</cell></row><row><cell>Spatial CNN [11]</cell><cell>54.1</cell><cell>41.8, 36.0, 25.1</cell></row><row><cell>ST-CNN [11]</cell><cell>60.6</cell><cell>58.7, 54.4, 41.9</cell></row><row><cell>Bi-LSTM</cell><cell>55.5</cell><cell>66.5, 59.0, 43.6</cell></row><row><cell cols="2">EgoNet+TDD [21] 68.5</cell><cell>-</cell></row><row><cell cols="2">Dilated TCN [10] 58.3</cell><cell>58.8, 52.2, 42.2</cell></row><row><cell>ED-TCN [10]</cell><cell>64.0</cell><cell>72.2, 69.3, 56.0</cell></row><row><cell>TricorNet (high)</cell><cell>62.4</cell><cell>75.2, 71.3, 58.0</cell></row><row><cell>TricorNet (low)</cell><cell>64.7</cell><cell>77.3, 73.4, 62.9</cell></row><row><cell>TricorNet</cell><cell>64.8</cell><cell>76.0, 71.1, 59.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Experimental results on JHU-ISI Gesture and Skill Assessment Working Set dataset. We use frame-wise accuracy (Acc.) and segmental edit score (Edit) for evaluation.</figDesc><table><row><cell>JIGSAWS</cell><cell cols="2">Acc. Edit</cell></row><row><cell>MSM-CRF [23]</cell><cell>71.7</cell><cell>-</cell></row><row><cell cols="3">Spatial CNN [11] 74.0 37.7</cell></row><row><cell>ST-CNN [11]</cell><cell cols="2">77.7 68.0</cell></row><row><cell>TCN [13]</cell><cell cols="2">81.4 83.1</cell></row><row><cell cols="3">TricorNet (high) 79.4 83.3</cell></row><row><cell>TricorNet (low)</cell><cell cols="2">82.2 84.9</cell></row><row><cell>TricorNet</cell><cell cols="2">82.9 86.8</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank Colin Lea for sharing source code and frame-level features used in his previous work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Incremental parsing with minimal features using bi-directional lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A thousand frames in just a few words: lingual description of videos through latent topics and sparse object stitching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Doell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to recognize objects in egocentric activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference On</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3281" to="3288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Jhu-isi gesture and skill assessment working set (jigsaws): A surgical activity dataset for human motion modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Vedula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Reiley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahmidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zappella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>B?jar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Yuh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI Workshop: M2CAI</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bidirectional lstm networks for improved phoneme classification and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks: Formal Models and Their Applications-ICANN 2005</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="753" to="753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Connectionist temporal modeling for weakly supervised action labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An end-to-end generative framework for video segmentation and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Computer Vision (WACV), 2016 IEEE Winter Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks for action segmentation and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Segmental spatiotemporal cnns for fine-grained action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="36" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning convolutional action primitives for fine-grained action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation (ICRA), 2016 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1642" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks: A unified approach to action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2016 Workshops</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="47" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Online human action detection using joint classification-regression recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spot on: Action localization from pointly-supervised proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-region two-stream r-cnn for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Temporal action detection using a statistical language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3131" to="3140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A multi-stream bi-directional recurrent neural network for fine-grained action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">First person action recognition using deep learned descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2620" to="2628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Combining embedded accelerometers with computer vision for recognizing food preparation activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Mckenna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 ACM international joint conference on Pervasive and ubiquitous computing</title>
		<meeting>the 2013 ACM international joint conference on Pervasive and ubiquitous computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="729" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Surgical gesture segmentation and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zappella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="339" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Wavenet: A generative model for raw audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Action recognition with trajectory-pooled deep-convolutional descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4305" to="4314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Every moment counts: Dense detailed labeling of actions in complex videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.05738</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">End-to-end learning of action detection from frame glimpses in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Procnets: Learning to segment procedures in untrimmed and unconstrained videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09788</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
