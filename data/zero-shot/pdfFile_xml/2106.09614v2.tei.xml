<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">To Fit or Not to Fit: Model-based Face Reconstruction and Occlusion Segmentation from Weak Supervision</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunlu</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Vetter</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Egger</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kortylewski</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of Automation</orgName>
								<orgName type="department" key="dep2">Dept. of Mathematics and Informatics</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Basel</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Dept. of Mathematics and Informatics</orgName>
								<orgName type="institution" key="instit1">ANDREAS MOREL-FORSTER</orgName>
								<orgName type="institution" key="instit2">University of Basel</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Dept. of Mathematics and Informatics</orgName>
								<orgName type="institution">University of Basel</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">Chair of Visual Computing</orgName>
								<orgName type="department" key="dep2">Dept. of Brain and Cognitive Sciences</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
								<address>
									<addrLine>Friedrich-Alexander-Universit?t Erlangen-N?rnberg</addrLine>
									<country>Germany, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department" key="dep1">Max Planck Institute for Informatics</orgName>
								<orgName type="department" key="dep2">Dept. of Computer Science</orgName>
								<orgName type="institution">Johns Hopkins</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">To Fit or Not to Fit: Model-based Face Reconstruction and Occlusion Segmentation from Weak Supervision</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS Concepts: ? Computing methodologies ? Reconstruction Additional Key Words and Phrases: Wireless sensor networks</term>
					<term>media access control</term>
					<term>multi-channel</term>
					<term>radio interference</term>
					<term>time synchronization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>University, USA</head><p>3D face reconstruction under occlusions is highly challenging due to the large variability of occluders. Currently, the most successful methods fit a 3D face model through inverse rendering and assume a given segmentation of the occluder to avoid fitting the occluder. However, training an occlusion segmentation model requires large amounts of annotated data. In this work, we introduce a model-based approach for 3D face reconstruction that is highly robust to occlusions but does not require any occlusion annotations for training. In our approach, we exploit the fact that generative face models can only synthesize human faces, but not the occluders. We use this property to guide the decision-making process of an occlusion segmentation network and resulting in unsupervised training. The main challenge is that the model fitting and the occlusion segmentation are mutually dependent on each other, and need to be inferred jointly. We resolve this chicken-and-egg problem with an EM-type training strategy. This leads to a synergistic effect, in which the segmentation network prevents the face encoder from fitting to the occlusion, enhancing the reconstruction quality. The improved 3D face reconstruction, in turn, enables the segmentation network to better predict the occlusion. Qualitative and quantitative experiments on the CelebA-HQ, the AR databases, and the NoW challenge demonstrate that the proposed pipeline achieves the state-of-the-art 3D face reconstruction under occlusion. Moreover, the segmentation network localizes occlusions accurately despite being trained without any occlusion annotation. The code is available at https://FakeLinkforDoubleBlind.com.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Monocular 3D face reconstruction aims at estimating the pose, shape, and albedo of a face, as well as the illumination conditions and camera parameters of the scene. Solving for all these factors from a single image is an ill-posed problem. Model-based face autoencoders [ <ref type="bibr" target="#b30">Tewari et al. 2017</ref>] overcome this problem by performing 3D reconstruction through fitting a 3D Morphable Model (3DMM) <ref type="bibr" target="#b0">[Blanz and Vetter 2003;</ref><ref type="bibr" target="#b8">Egger et al. 2020</ref>] to a target image. The 3DMM provides prior knowledge about the face albedo and geometry such that 3D face reconstruction from a single image becomes feasible, enabling face autoencoders to set the current state-of-the-art in 3D face reconstruction <ref type="bibr" target="#b4">[Deng et al. 2019b</ref>]. The network architectures in the face autoencoders are devised to enable end-to-end reconstruction and to enhance reconstruction speed compared to optimization-based alternatives <ref type="bibr" target="#b18">[Kortylewski et al. 2018b;</ref><ref type="bibr" target="#b40">Zhu et al. 2015]</ref>, and sophisticated losses are designed to stabilize the training and to get better performance <ref type="bibr" target="#b4">[Deng et al. 2019b]</ref>.</p><p>A major remaining challenge for face autoencoders is that their performance in in-the-wild environments is still limited by nuisance factors such as occlusion, extreme illumination, and poses. Among those nuisances, occlusions are ubiquitous and inherently difficult to handle because of their wide variety in shape, appearance, and locations. A core problem caused by occlusions is that the face model adapts to occluded face regions and as a result, the reconstructed face will be distorted (as seen in our experiments). Therefore an important open question for occlusion-robust 3D face reconstruction is to decide which pixels to fit and which not to fit in a target image.</p><p>Existing solutions to face reconstruction under occlusions often follow a bottom-up approach. For example, a multi-view shape consistency loss is used as prior to regularize the shape variation of the same face in different images <ref type="bibr" target="#b4">[Deng et al. 2019b;</ref><ref type="bibr" target="#b9">Feng et al. 2020;</ref><ref type="bibr"></ref> arXiv:2106.09614v2 [cs.CV] 11 Mar 2022 <ref type="bibr" target="#b32">Tiwari et al. 2022]</ref>, or the face symmetry is used to detect occluders <ref type="bibr" target="#b34">[Tran et al. 2018</ref>]. Most existing methods apply segmentation methods to locate the face region <ref type="bibr" target="#b26">[Saito et al. 2016]</ref>, or to detect skin <ref type="bibr" target="#b4">[Deng et al. 2019b</ref>] and subsequently exclude the occluded image regions during reconstruction. These segmentation methods operate in a supervised manner, which is infeasible in practice due to the high cost and efforts for acquiring a great variety of occlusion annotations from in-the-wild images.</p><p>In this work, we introduce an approach for model-based face reconstruction that is highly occlusion-robust, without requiring any human occlusion annotation. In particular, we propose to train a face autoencoder and a segmentation network in a cooperative manner. The segmentation network answers the question of whether the face model should 'fit or not to fit' certain pixels so that the face reconstruction is not affected by the occlusion. To train the segmentation network in an unsupervised manner, we exploit the fact that generative face models can only synthesize human faces, but not the occluders. We use this property to guide the decisionmaking process of an occlusion segmentation network and resulting in unsupervised training. We find that the discrepancy between the target image and the rendered face image ( <ref type="figure" target="#fig_0">Fig.1 1st and 2nd</ref> rows) can serve as a supervision signal to guide the training of the segmentation network. The face reconstruction network, in turn, becomes robust to occlusions by using the prediction from the segmentation network to mask out the occluded pixels during fitting. This leads to a synergistic effect, in which the occlusion segmentation first guides the face autoencoder to fit image regions that are easy to classify as face regions. The improved face fitting, in turn, enables the segmentation model to refine its prediction.</p><p>The training process follows the core idea of the Expectation-Maximization (EM) algorithm, by alternating between training the face autoencoder given the current estimate of the segmentation mask, and subsequently training the segmentation network based on the current 3D face reconstruction. The EM-like training strategy resolves the problem that the estimated occlusion segmentation depends on the estimated face model parameters and vice-versa. Importantly, the unsupervised training of the segmentation network is reached by regularizing and preserving the similarities among the target image and the reconstructed image under the estimated occlusion mask, and we introduce several losses to achieve this.</p><p>We demonstrate the effectiveness of our method by conducting experiments on the CelebA-HQ dataset <ref type="bibr" target="#b19">[Liu et al. 2015]</ref>, the AR database <ref type="bibr" target="#b21">[Martinez and Benavente 1998</ref>] and the NoW challenge <ref type="bibr" target="#b28">[Sanyal et al. 2019b</ref>], where we achieve state-of-the-art performance in 3D face reconstruction. Remarkably, our method is able to predict accurate occlusion masks without requiring any supervision during training.</p><p>In summary, we make the following contributions in this paper:</p><p>(1) We introduce an approach for model-based 3D face reconstruction that is highly robust occlusion, without requiring any human occlusion annotation.</p><p>(2) Our model achieves state-of-the-art performance at 3D face reconstruction under occlusions and provides accurate estimates of the facial occlusion masks on in-the-wild images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Model-based face autoencoders <ref type="bibr" target="#b30">[Tewari et al. 2017</ref>] solve the 3D face reconstruction task by fitting a face model to the target image with an encoder and a renderer, as well as the 3DMM, as the decoder. Typically, the encoder first estimates parameters from a target image, including the shape, texture, and pose of the target, and the illumination and camera settings from the scene. Then the renderer synthesizes a 2D image using the estimated parameters with an illumination model and a projection function. The face is reconstructed by retrieving the parameters which result in a synthesized image most similar to the target image. The 3DMM <ref type="bibr" target="#b0">[Blanz and Vetter 2003</ref>] plays a paramount role in the face autoencoders, because it parameterizes the latent distribution space of faces, and therefore can connect the encoder with the renderer and enable end-to-end training. The model-based face autoencoders have been proven effective in improving the reconstruction. They simplify the optimization step and enhance the reconstruction speed <ref type="bibr" target="#b31">[Tewari et al. 2018]</ref>, improve the details of shape and texture <ref type="bibr" target="#b9">[Feng et al. 2020;</ref><ref type="bibr" target="#b11">Gecer et al. 2019;</ref><ref type="bibr" target="#b23">Richardson et al. 2017;</ref><ref type="bibr" target="#b34">Tran et al. 2018</ref><ref type="bibr" target="#b35">Tran et al. , 2019</ref>, and can also reconstruct more discriminative features <ref type="bibr" target="#b4">[Deng et al. 2019b;</ref><ref type="bibr" target="#b12">Genova et al. 2018]</ref>.</p><p>Despite the advantages of the face autoencoders, their performance under occlusions is still limited. To solve this issue, some early methods <ref type="bibr" target="#b24">[Romdhani and Vetter 2003]</ref> resort to robust fitting losses, but they are not robust to illumination variations and appearance variations in eye and mouth regions. In recent years, shape consistency losses have been used as prior to constrain the face shape across images of the same subject <ref type="bibr" target="#b4">[Deng et al. 2019b;</ref><ref type="bibr" target="#b9">Feng et al. 2020;</ref><ref type="bibr" target="#b28">Sanyal et al. 2019b;</ref><ref type="bibr" target="#b32">Tiwari et al. 2022</ref>]. The variation of identity features of the 3D shape is restricted so that the shape reconstruction remains robust even in unconstrained environments. However, such methods usually need identity labels and do not promise robust texture reconstruction. Besides, many methods conduct face segmentation before reconstruction to lead the model to better fit the unoccluded face region. For example, a random forest detector for hair is proposed <ref type="bibr" target="#b22">[Morel-Forster 2016]</ref> so that the face model does not fit the hair region, and a semantic segmentation network is trained to better locate the face region <ref type="bibr" target="#b26">[Saito et al. 2016]</ref>. A skin detector is employed to impose different weights on the pixels during reconstruction to guide the network to put more attention on the skin-colored regions and prevent it from fitting the occlusions <ref type="bibr" target="#b4">[Deng et al. 2019b</ref>]. However, the skin-colored occlusion, such as hair, hands, and so on, can not be distinguished correctly and the skin detector is sensitive to illumination. Yildirim et al. propose to explicitly model the 3D shape of certain types of occlusions and the shadow cast by them, in order to decompose the target into face regions and occlusion, and therefore the occlusions can be excluded during training <ref type="bibr" target="#b37">[Yildirim et al. 2017</ref>]. However, the types of occlusions are limited. Generally, these off-the-shelf segmentation models require labeled data for training. Although using synthesized images can be used for training, there is a domain gap between the real images and the synthesized ones <ref type="bibr" target="#b17">[Kortylewski et al. 2018a</ref>]. Unlike these methods, we merge the segmentation procedure into a model-based face autoencoder, which exploits the face model prior, and consequently does not require additional supervision. The solid single lines show the forward path: given a target image , the reconstruction network, , estimates the latent parameters and subsequently renders an image , containing only the face. Then, and are stacked and fed into the segmentation network, , which predicts the segmentation mask . The dashed lines show that is used to mask out the estimated occlusions in and to get assembly occlusion-free images, namely ? and ? . The double-lined arrows indicate the compared image pairs in the losses for (orange) and (blue). For optimizing , two groups of losses (shown in the orange rectangle on the right) are proposed: 1) losses expanding the mask so that the parts indicating face are preserved and 2) losses aiming at shrinking the mask so that the non-facial regions are excluded. aims to minimize and reach the balance between the two groups of losses. The losses for (seen in the blue box) compare ? and ? in the pixel level so that the reconstruction is not affected by the occlusion, and, since the perceptual features are sensitive to noises in the mask, and in the perceptual level. By training alternatively the two networks and exploiting the synergy between the segmentation and the reconstruction tasks, the proposed pipeline is capable of both reconstructing faces under occlusions robustly and conducting face segmentation.</p><p>The most relevant method to ours is proposed by Egger et al. . They jointly adapt a face model to a target image and segment the target image into face, beard, and occlusion, and the segmentation models are trained with an EM-like algorithm, where different models for beard, foreground, and background are optimized in alternating steps. Compared to our method, their method does independent per-image optimization so the whole fitting-segmentation process is repeated given a new image, while ours solves both reconstruction and segmentation together as learning problems on a larger set of training data and only one forward run is enough for an input image after the model is trained. Therefore our method is much faster, more robust, and more effective. Besides, their method requires specific statistical occlusion models for beard, foreground, and background, while in our proposed method, the segmentation network is guided by several qualitycontrolling losses comparing a target image with the reconstructed face, which are intuitive and much easier to implement. <ref type="bibr">De Smet et al. also</ref> propose to conduct face model fitting and occlusion segmentation jointly <ref type="bibr" target="#b2">[De Smet et al. 2006</ref>], but they estimate the occlusions based on an appearance distribution model, which is sensitive to illumination variation and many other subtle changes in appearance. <ref type="bibr">Maninchedda et al.</ref> propose to solve face reconstruction and segmentation in a joint manner <ref type="bibr" target="#b20">[Maninchedda et al. 2016</ref>], but depth maps are required to provide supervision. In comparison, our pipeline learns from only weak supervision and does not need specific models for different types of occlusion. The face autoencoder also enables us to adapt the face model more efficiently. In addition, we integrate perceptual losses which enable the segmentation network to reason over semantic features instead of only over independent pixels, which increases the robustness to illumination and other factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">APPROACH</head><p>We introduce a neural network-based pipeline that conducts 3D face reconstruction and occlusion segmentation jointly. In the following, we first discuss our proposed pipeline architecture (3.1) and then discuss how the model can be trained in an EM-type manner without any supervision regarding the occlusions (3.2). Finally, we discuss how the EM training is initialized in an unsupervised manner (3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Network Architecture</head><p>Our goal is to robustly reconstruct the 3D face from a single target image , even under severe occlusion. To solve this challenging problem, we integrate a model-based face autoencoder, , with a segmentation network, , and create synergy between them, as demonstrated in <ref type="figure">Fig. 2</ref>. For face reconstruction, the segmentation mask cuts the estimated occlusions out during model fitting, making the reconstruction network robust to occlusion. For segmentation, the reconstructed result provides reference, enhancing the segmentation accuracy. In this section, we explain how the two networks are connected together and how they benefit each other.</p><p>The model-based face autoencoder, , is expected to reconstruct the complete face appearance from the visible face regions in the target image, . It consists of an encoder and a computer graphics renderer as its decoder. The encoder estimates the latent parameters = [ , , , ] ? R 257 , i.e. the 3D shape ? R 144 and texture ? R 80 of a 3DMM, as well as the illumination ? R 27 and camera parameters ? R 6 of the scene. Given the latent parameters, the decoder renders a face image = R( ) of the target face.</p><p>Standard face autoencoders <ref type="bibr" target="#b30">[Tewari et al. 2017</ref>] fit the face model parameters, regardless of whether the underlying pixels depict a face or occlusion. Consequently, the face model is distorted by the occluded face regions, as shown in the second row in <ref type="figure" target="#fig_2">Figure 3</ref>, it is obvious that the illumination, appearance, and shape are estimated incorrectly. To resolve this fundamental problem of face autoencoders, we introduce an unsupervised segmentation network, whose output can be used to mask the occlusions out during model fitting and therefore make the autoencoder robust to occlusion.</p><p>The segmentation network, , takes the stacked target image and the synthesized image as input and predicts a binary mask, = ( , ), to describe whether a pixel depicts the face (1) or not (0). Since contains the estimated intact face, it provides the segmentation network with prior knowledge and helps the estimation.</p><p>The face autoencoder and the segmentation network are coupled together during training to induce a synergistic effect which makes the segmentation more accurate and reconstruction more robust under occlusion, as shown in the last row in <ref type="figure" target="#fig_2">Figure 3</ref>. In 3.2, we describe how the pipeline can be trained end-to-end, despite the entanglement between the two networks, and how the high-level losses work that relieve our pipeline of any annotation for occlusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">EM-type Training</head><p>Due to the mutual dependencies between the face autoencoder and the segmentation network, we conduct an Expectation-Maximization (EM) like strategy, where we train the two networks in an alternating manner. This enables a stable convergence of the model training process. Similar to other EM-type training strategies, our training process starts from a rough initialization of the model parameters which is obtained in an unsupervised manner (as described at the end of this section). We then optimize the two networks in an alternating manner, as described in the following.</p><p>Training the segmentation network. When training the segmentation network, the parameters of the face autoencoder are fixed and only the segmentation network is optimized. Instead of hunting for labeled data, we propose four losses enforcing intrinsic similarities among the images. Each loss works to either include pixels indicating face or the opposite. The losses work either on the perceptual level or the pixel level, to fully exploit the visual clues. The perceptual-level losses compare the intermediate features of two images extracted by a pretrained face recognition model (we use Arcface <ref type="bibr" target="#b3">[Deng et al. 2019a]</ref>). We use the cosine distance, ( , ) = 1 ? ? ? ? ? ? , to compute the distance between the features. Perceptual losses are common for training face autoencoders, which encourage encoding facial details that are important for face recognition <ref type="bibr" target="#b9">[Feng et al. 2020]</ref>. We found that computing the perceptual losses is also very helpful to segmentation (see 4.4).</p><p>Since the proposed losses have overlapped or opposite functions to each other, only by reaching a balance among these losses can the network yield in good segmentation result. The proposed losses are as follows:</p><formula xml:id="formula_0">? = ?? ?? min ? ? ?N ( ) ( ) ? ( ? ) 2 2 (1) = ( ( ? ), ( ? )) (2) = ? / (3) = ( ( ? ), ( ))<label>(4)</label></formula><p>The pixel-level neighbour loss 1, ? , compares a pixel, ( ), at location on the target image, with the pixels on the rendered image in the neighbouring region, N ( ) of this pixel, so that this loss is stable even if there are small misalignments. Note that it only accounts within the face region, ?, predicted by the segmentation network, to encourage the mask to avoid those pixels with higher pixel-level reconstruction error.</p><p>Similarly, in Equation 2 we introduce a perceptual-level loss to encourage the mask to discard the parts with higher perceptual differences. These two losses, 1 and 2, aim at shrinking the mask where the pixel-level and perceptual differences are large. Without any other constraints, the segmentation network would output an all-zero mask to make them both 0. On the contrary, once there is a force to encourage the network to preserve some image parts, the segmentation network tends to preserve the parts with smaller losses, which in fact are the ones well-explained by the face model and therefore is much more likely to depict face. Therefore, Equation 3 and 4 are proposed to counterwork 1 and 2. Equation 3 is an area loss, that enlarges the ratio between the number of estimated facial pixels, , and the number of pixels in the rendered face region, . It prevents the segmentation network from discarding too many pixels.</p><p>(Eq. 4), ensures that the perceptual face features remain similar when the occluders in the target image are masked out and encourages the model to preserve as much of the visible face region as possible. Likewise, the network would keep the most-likely face region to decrease Equation 3 and 4 in the presence of 1 and 2.</p><p>We use an additional regularization term, = ? ( ( ) ? 0.5) 2 , to encourage the face mask to be binary. The total loss for training the segmentation network is:</p><formula xml:id="formula_1">= 1 ? + 2 + 3 + 4 + 5</formula><p>, with 1 = 15, 2 = 3, 3 = 0.5, and 4 = 2.5, and 5 = 10. Analysis of the influence of the hyperparameters is provided in the supplementary material.</p><p>During training, the segmentation network is guided seeking a balance between discarding pixels that cannot be explained well by the face autoencoder, while preserving pixels that are important to retain the perceptual representations of the target and rendered face images. Therefore no supervision for occlusions is required.</p><p>Training the face autoencoder. In the second step, we continue to optimize the parameters of the face autoencoder, while keeping the segmentation network fixed. The losses for training the face autoencoder include:</p><formula xml:id="formula_2">= ( ? ) ? 2 2 (5) = ( ( ), ( )) (6) = ? 2 2<label>(7)</label></formula><p>Above are two reconstruction losses:</p><p>(5) at the image level and (6) at the perceptual level, and a landmark loss <ref type="formula" target="#formula_2">(7)</ref> used to estimate the pose, where and stand for the 2D landmark coordinates on and , respectively <ref type="bibr" target="#b4">[Deng et al. 2019b</ref>]. We set the weights for the landmarks on the nose ridge and inner lip as 20, and the rest as 1. A regularization term is also required for the 3DMM: . To sum up, the loss for training the face autoencoder can be represented as:</p><p>= 1 + 2 + 3 + 4 , where 1 = 0.5, 2 = 0.25, 3 = 5 ? 4, and 4 = 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Unsupervised Initialization</head><p>As every other EM-type training strategy, our training needs to be roughly initialized. To achieve this, we generate preliminary masks using an occlusion robust loss ] so that the initialization is unsupervised:</p><formula xml:id="formula_3">( ( )) = ? 1 2 2 ( ( ) ? ( )) 2 + (8) ( ) = 1 if ( ( ) ? ( )) 2 &lt; 0 otherwise<label>(9)</label></formula><p>We assume that the reconstruction error at pixel in the face regions follows a zero-mean Gaussian distribution. Therefore, we can express the log-likelihood that a pixel belongs to the face regions as ( ) (Eq. 8), where and are constant. We also assume that the values of the non-face pixels follow a uniform distribution, i.e., ( ? ) is a constant. Finally, a pixel at position is classified as face or non-face by comparing the log-likelihoods. This reduces to thresholding of the reconstruction error with a constant parameter <ref type="formula" target="#formula_3">(Equation 9</ref>). When increases, the initialized masks allow the pixels on the target image to have a larger difference to the reconstructed pixels and encourage the reconstruction network to fit to these pixels. Empirically, we found that = 0.17 leads to a good enough initialization.</p><p>To initialize the face autoencoder, the preliminary mask, ( ), is obtained in the forward pass using Eq. 8 and Equation 9, after the reconstructed face is rendered. Then ( ) is directly used to mask out the roughly-estimated occluded regions as in Eq. 5, preventing the face autoencoder from fitting to any possible occlusions. Subsequently, the segmentation network is pre-trained by using these preliminary masks as ground-truth labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, results of systematic experiments show that our weakly-supervised method reaches the state-of-the-art face shape reconstruction accuracy and competitive occlusion segmentation results compared to the state-of-the-art and methods that use full supervision in terms of occlusion labels. Our ablation study shows the effectiveness of the segmentation network and our proposed losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment setting</head><p>Our face encoder shares the structure of the ResNet 50 <ref type="bibr" target="#b15">[He et al. 2016</ref>] and uses the Basel Face Model (BFM) 2017  as the 3D face model, with the differentiable renderer proposed in <ref type="bibr" target="#b16">[Koizumi and Smith 2020]</ref>. The segmentation network follows the UNet architecture <ref type="bibr" target="#b25">[Ronneberger et al. 2015]</ref>. The proposed pipeline is trained on the CelebA-HQ trainset <ref type="bibr" target="#b19">[Liu et al. 2015]</ref>, following their protocol. Facial landmarks are detected using the method of <ref type="bibr" target="#b1">[Bulat and Tzimiropoulos 2017]</ref>, and images are pre-processed in the same way as <ref type="bibr" target="#b4">[Deng et al. 2019b]</ref>. The perceptual features are extracted by the pre-trained ArcFace <ref type="bibr" target="#b3">[Deng et al. 2019a</ref>]. The Adadelta optimizer is used, with an initial learning rate of 0.1, and a decay rate of 0.99 at every 5k iterations. The learning rate for the segmentation network is 0.06 times the one for the reconstruction network. In every 30k iterations, 25k iters are for the face autoencoder training, and the rest are for training the segmentation network. For initialization, the face autoencoder is trained for 300k iterations. Afterwards, the face autoencoder and segmentation network are trained jointly for 200k iterations. The speed is evaluated on an RTX 2080 Ti, with batch size 12. It takes about 120 hours for the initialization of the face autoencoder, and about 80 hours to train the complete pipeline. After the training, it takes 49 ms for reconstruction and 70 ?s for segmentation on average for one image. The reconstruction and the  <ref type="bibr" target="#b30">[Tewari et al. 2017</ref>] (the 4th row), and the proposed method (the last two rows) on occluded faces from the CelebA-HQ testset (the first 8 columns) and the AR database (the last 2 columns). Note that all the masks are binarized. Baselines. We compare our method with two state-of-the-art model-based face autoencoders, i.e. the MoFA <ref type="bibr" target="#b30">[Tewari et al. 2017]</ref> and the Deep3D <ref type="bibr" target="#b4">[Deng et al. 2019b</ref>]. Additionally, to achieve fair comparison between our proposed weakly-supervised method and supervised methods, we train our reconstruction network in supervised settings, in which the supervised pipelines use the ground truth (GT) masks provided by the CelebA-HQ database to exclude occlusions during training. The GT masks of the CelebA-HQ database stand for the merge of their manually labeled masks for skin, hair, accessories, and so on. Two data augmentation methods for occlusion handling, i.e. the cutmix <ref type="bibr" target="#b39">[Yun et al. 2019</ref>] and cutout <ref type="bibr" target="#b5">[DeVries and Taylor 2017]</ref>, are also implemented to enhance the performance of the supervised pipelines. We refer to the three baselines as Supervised MoFA, MoFA cutmix, and MoFA cutout, respectively.</p><p>Databases The CelebA-HQ testset <ref type="bibr" target="#b19">[Liu et al. 2015]</ref> and the AR database <ref type="bibr" target="#b21">[Martinez and Benavente 1998</ref>] are used for evaluating the effectiveness of fitting and face segmentation. For the AR database, 120 manually-segmented results in ] are used as GT masks. We also evaluate the shape reconstruction accuracy on the subsets of the NoW database <ref type="bibr" target="#b27">[Sanyal et al. 2019a</ref>]. The standard deviation is provided after ? ? ? . The CelebA-HQ database and the AR database are publicly available and the occlusion labels for the AR dataset are publicly shared by the authors of ]. <ref type="figure" target="#fig_4">Fig. 4</ref> shows some results of the Deep3D network, the MoFA network, and our proposed method for qualitative comparison. Note that all the masks are binarized by rounding the pixels. The segmentation masks provided by the Deep3D result from a skin detector which assumes that skin color follows the simple multivariate Gaussian distribution. It shows that in our segmentation results, some skin-colored occlusions are better detected, and some small occlusions are also located well. Furthermore, our segmentation is more robust to illumination variations. It can also be observed from the reconstructed images that the illumination and texture of the faces are better estimated. Visually speaking, our method reaches competitive fitting results and improved segmentation masks. Please refer to the supplementary materials for more quantitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Reconstruction Quality</head><p>Image fitting accuracy shows how much the fitting results get misled by occlusions. We evaluate the Root Mean Square Error (RMSE) between the input image and the reconstructed image inside visible face regions, with provided GT segmentation masks. We compare different methods on the AR database, CelebA-HQ testset (referred to as 'CelebA-Overall'), and two randomly-selected occluded (750 random images) and unoccluded subsets (558 random images), referred to as 'CelebA-Occluded' and 'CelebA-Unoccluded',  respectively. As shown in Tab. 1, our fitting accuracy is competitive even to the fully supervised MoFA with data augmentation (i.e. the cutout and cutmix). Shape reconstruction accuracy is evaluated on the NoW Dataset. The cumulative errors on the testset shown in Tab. 2 indicate that our results reach the state-of-the-art even with a considerably smaller amount of training data and without constraints on identity consistency. To further evaluate the occlusion robustness, 62 pairs of images in the evaluation set are selected with comparable poses with or without occlusions in its publicly-available evaluation set. Tab. 3 shows that the shape reconstruction accuracy of our pipeline is barely affected by occlusions, and reaches a similar level as the fully-supervised pipelines. Please refer to the supplementary for a more detailed analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Occlusion Segmentation</head><p>The accuracy of occlusion segmentation is indicated by four indices: accuracy (ACC), precision (Positive Predictive Value, PPV), recall rate (True Positive Rate, TPR), and F1 score (F1). These indices are only calculated inside the rendered regions for all pipelines. Tab. 4 shows the results on the AR database, where the results of  are provided for comparison. We separate the AR dataset into three subsets, which include faces without occlusions (neutral), faces with glasses (glasses), and faces with scarves (scarf). According to Tab. 4, the masks predicted by our method show a higher accuracy, recall rate, and F1 score, and competitive precision compared to the skin detector used in <ref type="bibr" target="#b4">[Deng et al. 2019b</ref>] and the segmentation method proposed in ].   <ref type="figure" target="#fig_8">Fig. 5</ref>. Qualitative comparison for ablation study. From left to right: target images, masks estimated by the 'Pretrained', 'Baseline', 'Neighbour', and 'Perceptual' pipelines, and the reconstruction results and predicted masks of the proposed pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>In this section, we verify the usefulness of the segmentation network and the proposed neighbour loss, ? (Eq. 1), and the coupled perceptual losses, (Eq. 2) and (Eq. 4), for its training. We compare the segmentation performances of ablated pipelines on the AR testset, since the samples are with heavier occlusion, and test the shape reconstruction quality on the NoW evaluation subset. The pre-trained model using the occlusion robust function is referred to as 'Pretrained'. We refer to the segmentation network trained without the neighbour loss or perceptual losses as 'Baseline' and in order to compensate for the lack of such losses we use the pixel-wise reconstruction loss . The 'Neighbour' pipeline refers to the proposed segmentation network trained only with ? and without the two perceptual losses, and the 'Perceptual' pipeline stands for our proposed network trained only with two perceptual losses, and and without ? . The results in Tab. 5 indicate that with the segmentation network, the segmentation results excel the pretrained model in almost all the indices, which verifies the usefulness of the segmentation network. Comparison among the segmentation results of the 'Baseline', 'Neighbour' and the 'Perceptual' pipelines shows that both losses contribute significantly to the segmentation accuracy. <ref type="figure" target="#fig_8">Fig. 5</ref> provides a visual comparison among the ablated pipelines. It highlights that the occlusion robust function is not robust to illumination variations, and the segmentation network brings great benefit to the robustness to illumination. The neighbour loss encourages the network to produce smoother results, and the perceptual losses help to locate the occlusions more accurately. Generally, the reconstruction performance of our proposed method are the best one and the segmentation accuracies are also competitive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Limitations and Societal Impact</head><p>Despite the accurate reconstruction and segmentation proven in the experiments, there are several limitations.</p><p>The main issue is that the segmentation performance relies largely on the generative ability of the face model. If the model is too expressive, and can even fit the occlusion, then the segmentation network will not expel the occlusions properly. Likewise, if the model under-fits the target images, the segmentation network tends to regard more pixels as occlusions. For example, in the eye region, the eye gaze usually cannot be properly reconstructed, so some pixels are regarded as occlusions. Although it is solved partially by the neighbour loss, which enhances the robustness to small misalignments, there is still space for improvement. We assume that this problem can be alleviated by a enhanced face appearance model or shape model, such as the detail model proposed by <ref type="bibr" target="#b9">[Feng et al. 2020]</ref>. Additionally, we only predict occlusions inside the rendered face region. We assume that a full-face model or a head model can solve this problem.</p><p>As for the societal impact, in general, our proposed pipeline has the potential to bring face reconstruction to the real world and to save costs of occlusion labeling, which is generally required in many existing deep-learning-based methods. The model-based reconstruction methods improved by our method could contribute to many applications, including Augmented Reality (AR), Virtual Reality (VR), surveillance, 3D design, and so on. Each of these applications may bring societal and economic benefits, and risks at the same time: The application of AR or VR could bring profits to the entertainment industry and also may result in unethical practices such as demonizing the image of others, identity fraud, and so on. The application of surveillance could help arrest criminals, yet might also invade the privacy and safety of others. The application in 3D design enables the quick capture of the 3D shape of an existing face but might also cause problems in portrait rights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we have shown how to solve face reconstruction and occlusion segmentation jointly in a weakly-supervised way, so as to enhance the robustness to occlusions for model-based face autoencoders in unconstrained environments. Comprehensive experiments have shown that our method reaches state-of-the-art reconstruction accuracy on the NoW Challenge and provides better segmentation masks as well.</p><p>Theoretically, our proposed method can be integrated with the existing face autoencoders. More importantly, we believe that the fundamental concepts of our approach can go beyond the context of face reconstruction and will inspire future work. More specifically, we expect that our pipeline will be extended to many other implementations, such as human body reconstruction, or object reconstruction, with a reliable generative model. We also expect that the masks will be useful for other tasks, e.g. image completion, recognition, or more.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A OVERVIEW</head><p>In this section we reveal more implementation details and provide more analytical and visual comparisons on the shape reconstruction and segmentation performances. (1)</p><p>As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, for every pixel ( , ) in the target image, we search in a 3 ? 3 neighborhood ( , ) in the reconstructed image <ref type="bibr">( , )</ref> for the pixel that is most similar to ( , ) in intensity. This neighbour loss accounts for small misalignments of the face model during segmentation.</p><p>C QUANTITATIVE ANALYSIS C.1 Reconstruction Performance on the NoW Challenge. <ref type="figure">Fig. 2</ref> shows the cumulative error curves of the proposed method and the state-of-the-arts regarding the NoW Challenge testset. With a higher percentage of sampling points with lower errors, our proposed method performs the best on the NoW Challenge.</p><p>We further compare analytically the distributions of reconstruction errors of DECA <ref type="bibr" target="#b9">[Feng et al. 2020</ref>] and our proposed method on the NoW validation set, as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. To further disentangle the influence of occlusions from other factors, we categorize the samples according to the yaw angles (the angles are rounded off to the nearest 10), and use the error bars under different poses to reflect the distribution of the reconstruction errors. It is obvious from the plots that our proposed method exceeds DECA in mean errors and also yields in much lower variations, even without identity supervision (which emphasize the shape consistency of a same identity) and with significantly less training data. Besides, the lower C.2 Segmentation Accuracy on the Celeb A HQ testset.</p><p>Tab. 1 indicates that the masks predicted by our method show a competitive accuracy, precision, and F1 score, compared to the skin detector used in <ref type="bibr" target="#b4">[Deng et al. 2019b</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Hyper-parameter Analysis</head><p>In this section we systematically evaluate the influence of the hyperparameters, 1 to 5 , used for segmentation. Recall that the total  loss for training the segmentation network is:</p><formula xml:id="formula_4">= 1 ? + 2 + 3 + 4 + 5<label>(2)</label></formula><p>, with 1 = 15, 2 = 3, 3 = 0.5, and 4 = 2.5, and 5 = 10. We call this set of parameters as 'standard parameters'. We use the control variates method, namely changing one of the 5 parameters while fixing the others, to evaluate the influence of each hyper-parameters. The accuracy (ACC), precision (Positive Predictive Value, PPV), recall rate (True Positive Rate, TPR), and F1 score (F1) are taken to indicate the segmentation performance. We use the AR dataset <ref type="bibr" target="#b21">[Martinez and Benavente 1998</ref>] because the segmentation labels are more accurate. As shown in <ref type="figure" target="#fig_8">Fig. 5</ref>, it is clear that with the increase of the neighbour loss ? and the perceptual-level loss , the segmentation network tends to regard more pixels as non-facial. On the contrary, with the increase of the area loss and the pixel-wise preserve loss , the segmentation network takes more pixels as face. This observation is consistent with our theory in section 3.2. <ref type="figure" target="#fig_8">Fig. 5</ref> also indicates that the indices are positively related to the area loss and preserving loss , and are negatively related to the neighbour loss ? and the perceptual-level loss . The binary loss, , barely affects the segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D QUALITATIVE COMPARISON AND ABLATION STUDY D.1 Qualitative Comparison on Face Reconstruction and Segmentation</head><p>In this section, we provide more visual results of out method on the Celeb A HQ testset <ref type="bibr" target="#b19">[Liu et al. 2015]</ref>, the AR dataset <ref type="bibr" target="#b21">[Martinez and Benavente 1998]</ref>, and the NoW Challenge <ref type="bibr" target="#b27">[Sanyal et al. 2019a</ref>]. <ref type="figure" target="#fig_4">Fig. 4</ref> shows the results on faces with general occlusions. <ref type="figure" target="#fig_10">Fig. 6</ref> shows the performance under extreme lighting. <ref type="figure" target="#fig_9">Fig. 7</ref> shows the performance of segmenting skin-colored occlusions. <ref type="figure" target="#fig_13">Fig. 8</ref> shows the robustness of our method to large poses. <ref type="figure" target="#fig_11">Fig. 9</ref> and 10 shows more samples for ablation study.      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Our proposed method conducts face reconstruction and occlusion segmentation jointly, and is operated under weak supervision. From top to bottom: target images, our reconstruction images, and the estimated occlusion masks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Fig. 2. Porposed pipeline. The solid single lines show the forward path: given a target image , the reconstruction network, , estimates the latent parameters and subsequently renders an image , containing only the face. Then, and are stacked and fed into the segmentation network, , which predicts the segmentation mask . The dashed lines show that is used to mask out the estimated occlusions in and to get assembly occlusion-free images, namely ? and ? . The double-lined arrows indicate the compared image pairs in the losses for (orange) and (blue). For optimizing , two groups of losses (shown in the orange rectangle on the right) are proposed: 1) losses expanding the mask so that the parts indicating face are preserved and 2) losses aiming at shrinking the mask so that the non-facial regions are excluded. aims to minimize and reach the balance between the two groups of losses. The losses for (seen in the blue box) compare ? and ? in the pixel level so that the reconstruction is not affected by the occlusion, and, since the perceptual features are sensitive to noises in the mask, and in the perceptual level. By training alternatively the two networks and exploiting the synergy between the segmentation and the reconstruction tasks, the proposed pipeline is capable of both reconstructing faces under occlusions robustly and conducting face segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>In the presence of occlusion, the proposed method can reconstruct faces more faithfully than previous model-based face autoencoders. The images from top to bottom are: target images, results of the MoFA network<ref type="bibr" target="#b30">[Tewari et al. 2017]</ref>, and the results of ours.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Qualitative comparison on the reconstruction and segmentation results of the Deep3D [Deng et al. 2019b] network (the 2nd and 3rd rows), the MoFA network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>neighbor lossFig. 1. Visual explanation of the neighbor loss (Eq. 1). For an pixel at ( , ) on the target image (left), we search for the most similar pixel in intensity in its neighboring region, , , on the reconstructed image (right).In this paper, we introduce a new image-level neighbor loss, ? , that compares one pixel in the target image to a small region in the reconstructed image:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 3 .</head><label>3</label><figDesc>The distribution of the reconstruction errors under on the neutral and occluded subsets of the NoW validation set. The results of DECA<ref type="bibr" target="#b9">[Feng et al. 2020</ref>] are on the left, and ours on the right. The x axis indicates the approximated poses of the samples, and the y axis denotes the reconstruction error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 4 .</head><label>4</label><figDesc>Comparison on random samples in the Celeb A HQ [Liu et al. 2015] testset. (a) Target image. (b) and (c) Reconstruction and segmentation results of the Deep3D network [Deng et al. 2019b]. d) Reconstructed result of the MoFA network [Tewari et al. 2017]. (e) and (f) Reconstruction and segmentation results of ours.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 5 .</head><label>5</label><figDesc>Analysis of hyper-parameters. The subplots show the change of for indices, namely accuracy, precision, F1 score, and recall rate, with the change of the hyper-parameters. The corresponding segmentation results are shown below each subplot. In each subplot, to evaluate the effect of each hyper parameter , the other hyper-parameters ( ? ) are fixed. The red dots denote the 'standard' positions where the set of parameters in the paper is used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7 .</head><label>7</label><figDesc>Comparison on samples with occlusions that the skin detector in<ref type="bibr" target="#b4">[Deng et al. 2019b]</ref> fails to locate in the Celeb A HQ testset<ref type="bibr" target="#b19">[Liu et al. 2015]</ref>.(a) Target image. (b) and (c) Reconstruction and segmentation results of the Deep3D network [Deng et al. 2019b]. d) Reconstructed result of the MoFA network [Tewari et al. 2017]. (e) and (f) Reconstruction and segmentation results of ours.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 6 .</head><label>6</label><figDesc>Comparison on samples with extreme illumination conditions in the Celeb A HQ [Liu et al. 2015] and the AR [Martinez and Benavente 1998] testsets. (a) Target image. (b) and (c) Reconstruction and segmentation results of the Deep3D network [Deng et al. 2019b]. d) Reconstructed result of the MoFA network [Tewari et al. 2017]. (e) and (f) Reconstruction and segmentation results of ours.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 9 .</head><label>9</label><figDesc>Qualitative comparison for ablation study on the Celeb A HQ testset<ref type="bibr" target="#b19">[Liu et al. 2015]</ref>. From left to right are (a) target images, masks estimated by the (b) 'Pretrained', (c) 'Baseline', (d) 'Neighbour', and (e) 'Perceptual' pipelines, and (f) the reconstruction results and (g) predicted masks of the proposed pipelines, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 10 .</head><label>10</label><figDesc>Qualitative comparison for ablation study on the AR testset [Martinez and Benavente 1998]. From left to right are (a) target images, masks estimated by the (b) 'Pretrained', (c) 'Baseline', (d) 'Neighbour', and (e) 'Perceptual' pipelines, and (f) the reconstruction results and (g) predicted masks of the proposed pipelines, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 8 .</head><label>8</label><figDesc>Comparison on samples with occlusions and large poses in the NoW Database [Sanyal et al. 2019a] shows that our method can effectively handle occlusions even when there are large poses. (a) Target image. (b) and (c) Reconstruction and segmentation results of the Deep3D network [Deng et al. 2019b]. d) Reconstructed result of the MoFA network [Tewari et al. 2017]. (e) and (f) Reconstruction and segmentation results of ours.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>RMSE on the CelebA-HQ testsets and the AR testset.</figDesc><table><row><cell>Testset</cell><cell cols="6">MoFA [Tewari et al. 2017]Supervised MoFASupervised MoFA-cutmixSupervised MoFA-cutoutDeep3D [Deng et al. 2019b] Proposed</cell></row><row><cell>CelebA-Unoccluded</cell><cell>8.77 ? 0.40</cell><cell>8.71 ? 0.38</cell><cell>8.75 ? 0.39</cell><cell>8.72 ? 0.40</cell><cell>8.49 ? 0.39</cell><cell>8.38 ? 0.42</cell></row><row><cell>CelebA-Occluded</cell><cell>9.20 ? 0.45</cell><cell>9.01 ? 0.45</cell><cell>9.04 ? 0.44</cell><cell>8.99 ? 0.45</cell><cell>8.79 ? 0.45</cell><cell>8.71 ? 0.48</cell></row><row><cell>CelebA-Overall</cell><cell>8.99 ? 0.47</cell><cell>8.86 ? 0.44</cell><cell>8.90 ? 0.44</cell><cell>8.85 ? 0.45</cell><cell>8.64 ? 0.44</cell><cell>8.55 ? 0.48</cell></row><row><cell>AR-Overall</cell><cell>9.53 ? 0.33</cell><cell>9.34 ? 0.33</cell><cell>9.33 ? 0.32</cell><cell>9.28 ? 0.31</cell><cell>9.11 ? 0.37</cell><cell>8.93 ? 0.35</cell></row><row><cell cols="4">segmentation networks have 25.6M and 34.5M parameters, respec-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">tively. There is no fine-tuning with 3D data on any of the testsets in</cell><cell></cell><cell></cell><cell></cell></row><row><cell>our experiments.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Reconstruction error (mm) on the NoW Challenge<ref type="bibr" target="#b27">[Sanyal et al. 2019a</ref>].</figDesc><table><row><cell>Method</cell><cell cols="2">median mean std</cell></row><row><cell>Deep3D [Deng et al. 2019b]</cell><cell>1.23</cell><cell>1.54 1.29</cell></row><row><cell>DECA [Feng et al. 2020]</cell><cell>1.09</cell><cell>1.38 1.18</cell></row><row><cell>PRNet [Feng et al. 2018]</cell><cell>1.50</cell><cell>1.98 1.88</cell></row><row><cell>RingNet [Sanyal et al. 2019a]</cell><cell>1.21</cell><cell>1.53 1.31</cell></row><row><cell>3DMM-CNN [Tuan Tran et al. 2017]</cell><cell>1.84</cell><cell>2.33 2.05</cell></row><row><cell>Proposed</cell><cell>1.04</cell><cell>1.30 1.10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Reconstruction error (mm) on the non-occluded and occluded data in the NoW validation subset.</figDesc><table><row><cell></cell><cell cols="2">Unoccluded Subset</cell><cell cols="2">Occluded Subset</cell></row><row><cell>Method</cell><cell cols="4">median mean std median mean std</cell></row><row><cell>Deep3D [Deng et al. 2019b]</cell><cell>1.33</cell><cell>1.67 1.41</cell><cell>1.40</cell><cell>1.73 1.41</cell></row><row><cell>DECA [Feng et al. 2020]</cell><cell>1.18</cell><cell>1.47 1.24</cell><cell>1.29</cell><cell>1.56 1.29</cell></row><row><cell>MoFA [Tewari et al. 2017]</cell><cell>1.35</cell><cell>1.69 1.42</cell><cell>1.36</cell><cell>1.69 1.41</cell></row><row><cell>Supervised MoFA</cell><cell>1.02</cell><cell>1.25 1.04</cell><cell>1.05</cell><cell>1.29 1.09</cell></row><row><cell>Supervised MoFA-cutmix</cell><cell>1.05</cell><cell>1.28 1.04</cell><cell>1.08</cell><cell>1.33 1.11</cell></row><row><cell>Supervised MoFA-cutOUT</cell><cell>1.03</cell><cell>1.28 1.06</cell><cell>1.09</cell><cell>1.34 1.10</cell></row><row><cell>Proposed</cell><cell>1.03</cell><cell>1.25 1.03</cell><cell>1.07</cell><cell>1.34 1.19</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Evaluation of occlusions segmentation accuracy on the AR testsets.<ref type="bibr" target="#b4">Deng et al. 2019b</ref>] 0.88 0.93 0.94 0.93 ? 0.04 0.88 0.92 0.92 0.92 ? 0.04 0.80 0.80 0.93 0.86 ? 0.05 Egger et al.[Egger et al.  </figDesc><table><row><cell></cell><cell></cell><cell cols="2">Unoccluded</cell><cell></cell><cell></cell><cell></cell><cell>Glasses</cell><cell></cell><cell></cell><cell></cell><cell>Scarf</cell></row><row><cell>Method</cell><cell cols="3">ACC PPV TPR</cell><cell>F1</cell><cell cols="3">ACC PPV TPR</cell><cell>F1</cell><cell cols="3">ACC PPV TPR</cell><cell>F1</cell></row><row><cell cols="2">Deep3D [2018] -</cell><cell>-</cell><cell>-</cell><cell>0.90</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.87</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.86</cell></row><row><cell>Proposed</cell><cell cols="12">0.88 0.96 0.91 0.93 ? 0.03 0.88 0.98 0.85 0.91 ? 0.04 0.86 0.97 0.81 0.88 ? 0.05</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Ablation study on the AR testsets and the NoW evaluation subset.</figDesc><table><row><cell></cell><cell>AR-unoccluded</cell><cell></cell><cell>AR-glasses</cell><cell></cell><cell>AR-scarf</cell><cell></cell><cell cols="2">NoW Evaluation Set</cell></row><row><cell>Method</cell><cell>ACC PPV TPR</cell><cell>F1</cell><cell>ACC PPV TPR</cell><cell>F1</cell><cell>ACC PPV TPR</cell><cell>F1</cell><cell cols="2">median mean std</cell></row><row><cell cols="7">Pretrained 0.75 0.95 0.77 0.85 ? 0.05 0.78 0.97 0.72 0.82 ? 0.05 0.70 0.89 0.62 0.73 ? 0.07</cell><cell>1.06</cell><cell>1.32 1.14</cell></row><row><cell>Baseline</cell><cell cols="6">0.81 0.96 0.83 0.89 ? 0.04 0.81 0.97 0.76 0.85 ? 0.05 0.79 0.96 0.71 0.82 ? 0.07</cell><cell>1.06</cell><cell>1.32 1.15</cell></row><row><cell cols="7">Neighbour 0.85 0.95 0.88 0.91 ? 0.04 0.84 0.95 0.81 0.87 ? 0.04 0.83 0.94 0.79 0.85 ? 0.06</cell><cell>1.06</cell><cell>1.32 1.15</cell></row><row><cell cols="7">Perceptual 0.89 0.96 0.92 0.94 ? 0.03 0.89 0.98 0.87 0.92 ? 0.04 0.87 0.97 0.84 0.90 ? 0.05</cell><cell>1.06</cell><cell>1.32 1.14</cell></row><row><cell cols="7">Proposed 0.88 0.96 0.91 0.93 ? 0.03 0.88 0.98 0.85 0.91 ? 0.04 0.86 0.97 0.81 0.88 ? 0.05</cell><cell>1.05</cell><cell>1.31 1.14</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">? Chunlu Li, Andreas Morel-Forster, Thomas Vetter, Bernhard Egger*, and Adam Kortylewski*</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">? Chunlu Li, Andreas Morel-Forster, Thomas Vetter, Bernhard Egger*, and Adam Kortylewski*</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">? Chunlu Li, Andreas Morel-Forster, Thomas Vetter, Bernhard Egger*, and Adam Kortylewski*</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">? Chunlu Li, Andreas Morel-Forster, Thomas Vetter, Bernhard Egger*, and Adam Kortylewski*</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">? Chunlu Li, Andreas Morel-Forster, Thomas Vetter, Bernhard Egger*, and Adam Kortylewski*</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14">? Chunlu Li, Andreas Morel-Forster, Thomas Vetter, Bernhard Egger*, and Adam Kortylewski*(a) (b) (c) (d) (e) (f) (g)</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Face recognition based on fitting a 3d morphable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1063" to="1074" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">How far are we from solving the 2D &amp; 3D Face Alignment problem? (and a dataset of 230,000 3D facial landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Generalized EM Approach for 3D Model Based Face Recognition under Occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">De</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fransens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2006.26</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2006.26" />
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;06)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1423" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niannan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4690" to="4699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Accurate 3d face reconstruction with weakly-supervised learning: From single image to image set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sicheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunde</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<title level="m">Improved regularization of convolutional neural networks with cutout</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Towards High Fidelity Monocular Face Reconstruction with Rich Reflectance using Self-supervised Learning and Ray Tracing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdallah</forename><surname>Dib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cedric</forename><surname>Thebault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junghyun</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe-Henri</forename><surname>Gosselin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis</forename><surname>Chevallier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Occlusion-aware 3d morphable models and an illumination prior for face image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Egger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandro</forename><surname>Sch?nborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kortylewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Morel-Forster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Blumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="page" from="1269" to="1287" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">2020. 3d morphable face models-past, present, and future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Egger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wuhrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thabo</forename><surname>Zollhoefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Beeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Kortylewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Romdhani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiwen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bolkart</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.04012</idno>
		<title level="m">Learning an Animatable Detailed 3D Face Model from In-The-Wild Images</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Joint 3D Face Reconstruction and Dense Alignment with Position Map Regression Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohu</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">GANFIT: Generative Adversarial Network Fitting for High Fidelity 3D Face Reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baris</forename><surname>Gecer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stylianos</forename><surname>Ploumpis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised training for 3d morphable model regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Genova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrester</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Vlasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8377" to="8386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Morphable face models-an open framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Gerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Morel-Forster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Blumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Egger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Luthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandro</forename><surname>Sch?nborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th IEEE International Conference on Automatic Face &amp; Gesture Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="75" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Towards Fast, Accurate and Stable 3D Dense Face Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="152" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Look Ma, no landmarks!&quot;-Unsupervised, model-based dense face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuro</forename><surname>Koizumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="690" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Training deep face recognition systems with synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kortylewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Gerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Egger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Morel-Forster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Vetter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05891</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kortylewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Wieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Morel-Forster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Wieczorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonali</forename><surname>Parbhoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Vetter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.07969</idno>
		<title level="m">Informed MCMC with Bayesian neural networks for facial image analysis</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep Learning Face Attributes in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semantic 3d reconstruction of heads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Maninchedda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>H?ne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastien</forename><surname>Jacquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="667" to="683" />
		</imprint>
	</monogr>
	<note>Ama?l Delaunoy, and Marc Pollefeys</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The AR face database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Benavente</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998-01" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep. 24 CVC Technical Report</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Generative shape and image analysis by combining Gaussian processes and MCMC sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Morel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Forster</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Ph. D. Dissertation. University_of_Basel</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning Detailed Face Reconstruction From a Single Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Or-El</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient, Robust and Accurate Fitting of a 3D Morphable Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="59" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Real-time facial segmentation and performance capture from rgb input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunsuke</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianye</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="244" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to Regress 3D Face Shape and Expression from an Image without 3D Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soubhik</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiwen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to regress 3D face shape and expression from an image without 3D supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soubhik</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiwen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7763" to="7772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Self-Supervised Monocular 3D Face Reconstruction by Occlusion-Aware Multi-view Geometry Consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxiang</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingmin</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">12360</biblScope>
			<biblScope unit="page" from="53" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mofa: Model-based deep convolutional face autoencoder for unsupervised monocular reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeongwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1274" to="1283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Self-Supervised Multi-Level Face Model Learning for Monocular Reconstruction at Over 250 Hz</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeongwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Occlusion Resistant Network for 3D Face Reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hitika</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vinod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kurmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Sheng</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="813" to="822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Regressing Robust and Discriminative 3D Morphable Models With a Very Deep Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Tuan Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacopo</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1599" to="1608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Extreme 3D Face Reconstruction: Seeing Through Occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Tuan Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacopo</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Nirkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?rard</forename><surname>Medioni</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00414</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2018.00414" />
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3935" to="3944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Towards high-fidelity nonlinear 3D face morphable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Regressing robust and discriminative 3D morphable models with a very deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Tuan Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacopo</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?rard</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5163" to="5172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Causal and compositional generative models in online perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilker</forename><surname>Yildirim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Janner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Belledonne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wallraven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winrich</forename><surname>Freiwald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CogSci</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">To Fit or Not to Fit: Model-based Face Reconstruction and Occlusion Segmentation from Weak Supervision ? 9</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Discriminative 3D morphable model fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
