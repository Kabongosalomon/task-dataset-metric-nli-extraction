<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">STRUCTURED TIME SERIES PREDICTION WITHOUT STRUCTURAL PRIOR</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-09">September 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darko</forename><surname>Drakulic</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NAVER LABS Europe</orgName>
								<address>
									<settlement>Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Marc</forename><surname>Andreoli</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NAVER LABS Europe</orgName>
								<address>
									<settlement>Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">STRUCTURED TIME SERIES PREDICTION WITHOUT STRUCTURAL PRIOR</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-09">September 2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Time series prediction is a widespread and well studied problem with applications in many domains (medical, geoscience, network analysis, finance, econometry etc.). In the case of multivariate time series, the key to good performances is to properly capture the dependencies between the variates. Often, these variates are structured, i.e. they are localised in an abstract space, usually representing an aspect of the physical world, and prediction amounts to a form of diffusion of the information across that space over time. Several neural network models of diffusion have been proposed in the literature. However, most of the existing proposals rely on some a priori knowledge on the structure of the space, usually in the form of a graph weighing the pairwise diffusion capacity of its points. We argue that this piece of information can often be dispensed with, since data already contains the diffusion capacity information, and in a more reliable form than that obtained from the usually largely hand-crafted graphs. We propose instead a fully data-driven model which does not rely on such a graph, nor any other prior structural information. We conduct a first set of experiments to measure the impact on performance of a structural prior, as used in baseline models, and show that, except at very low data levels, it remains negligible, and beyond a threshold, it may even become detrimental. We then investigate, through a second set of experiments, the capacity of our model in two respects: treatment of missing data and domain adaptation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We are interested in the problem of prediction in multi-variate time series where nothing is known about the process generating the data. We consider time series which are structured, i.e. their measurements consist of homogeneous vectors taken at different instants in time and different points of an abstract space, called locations. Instant-location pairs are called events. This configuration is quite common in applications, such as the prediction of service demand <ref type="bibr" target="#b17">[18]</ref>, of stock prices <ref type="bibr" target="#b3">[4]</ref>, of geophysical phenomena like rainfall <ref type="bibr" target="#b18">[19]</ref>, or of road traffic <ref type="bibr" target="#b11">[12]</ref>.</p><p>In its most general formulation, the problem we address is that of predicting the measurements at a given set of (target) events, given measurements at a related set of (source) events. How source and target are related is often left implicit, but in any case, it is useful to represent each problem instance as an event grid, the Cartesian product of a set of instants and a set of locations, where the source events in the grid have known measurements while those of the target events are to be estimated. In other words, we view prediction as a form of matrix completion.</p><p>However, classical matrix completion methods work on one large incomplete matrix which they propose to complete by exploiting patterns discovered on its known parts. Instead, in this work, we focus on so-called "short term" prediction, where a large training set of small complete instances, from which patterns can be learnt, is used in the completion of a set of test instances, assumed sampled from the same distribution. That problem setting could be characterised as a form of "localised" matrix completion, where the large global matrix is only accessible through selected small local patches (the instances).</p><p>In this setting, we address the question of how to deal with prior knowledge. Prior information attached to individual events is easily dealt with, assuming it is homogeneous: it can be appended to the measurement at each event, and used consists of multihead attention blocks MHA (the masked version is indicated by a masked corner), with one input (self-attention) or two inputs (cross-attention, indicated by a thicker outline). The model alternates attention in the temporal dimension (MHA (T) ), and in the spatial dimension (MHA <ref type="bibr">(N)</ref> ). This is achieved by simple manipulation of indices, alternatively blending into the batch dimension B the spatial dimension N (SPLIT <ref type="bibr">(N)</ref> ) and the temporal dimension T (SPLIT (T) ), then restoring it (MERGE <ref type="bibr">(N)</ref> and MERGE (T) , respectively). D is the embedding dimension. Skip connections are shown, but normalisation, dropout and feed-forward blocks have been omitted for clarity. All the other blocks in the figure are detailed in the text.</p><p>as context. But relational information involving event pairs (or tuples), which we call structural prior, poses difficulties. It is tempting to explicitly incorporate it in the information diffusion mechanism of the model, but that should be done in such a way that the effect of the prior wanes when more data becomes available for training, so that eventually, the model is entirely data-driven (as in Bayesian learning). Unfortunately, this is not the case of many published models, in which the information diffusion mechanism of the model is organically impacted by the structural prior, for example through a GCN, whatever the amount of data it sees. Furthermore, it is often the case that the type or intensity of the relations captured by the structural prior differ from one domain to another, making domain adaptation more difficult if the model depends on it.</p><p>For these reasons, we propose the simplest possible model, called ADN for Attention Diffusion Network, which does not rely on any structural prior whatsoever. We choose an attention based encoder-decoder architecture, where attention is adapted to the bi-dimensionality of events as location-instant pairs. It allows information diffusion in both the temporal and spatial dimension (but not simultaneously), controlled by attention parameters entirely learnt from the data. We experimentally measure the impact of this design choice on the performance, and show that, at least in our use case, very little data is sufficient to be on a par with state of the art models which do rely on a structural prior. We then proceed to explore the advantages of our light-weight approach when dealing with missing data or domain adaptation. Missing data is simulated by randomly removing locations in the training instances, and the question is how that impacts the performance. For domain adaptation, the question is how to transfer a model learnt on one domain to a new domain with completely new locations (but the same patterns of instants), using only a small amount of data from the new domain.</p><p>Use case To fix ideas, we illustrate our proposal with the problem of data driven short term road traffic prediction, for which numerous datasets are publicly available, as well as an abundant literature using them. The event locations in that context are a set of sensor stations spread across a road network, equipped with any variety of sensors fused to produce, at regular intervals, a fixed set of features (such as vehicle speed or density) describing the traffic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>One of the first models explicitly dealing with short term prediction as spatio-temporal diffusion is DCRNN <ref type="bibr" target="#b11">[12]</ref>, developed in the context of road traffic prediction. It is based on a RNN architecture for temporal diffusion, coupled with a GCN <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8]</ref> for spatial diffusion. They use the GRU variant of RNN, but a similar approach based on LSTM was proposed in <ref type="bibr" target="#b18">[19]</ref>. Some approaches <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b10">11]</ref> do not rely on RNN for temporal diffusion and instead model all diffusion as a Graph Neural Network. Several papers, while sticking to the RNN framework underlying DCRNN, expand the capacity of its GCN in various ways <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b5">6]</ref>. Some papers introduce forms of attention within the RNN framework <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13]</ref>. More recent papers entirely replace the RNN framework by Transformer attention <ref type="bibr" target="#b22">[23]</ref>. In particular, <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b1">2]</ref> embed temporal and spatial attention mechanisms into ad-hoc processing pipelines.</p><p>Unfortunately, all these proposals assume the existence of a graph which captures prior knowledge on the diffusion capacity between locations, or try to infer one as in the "self-adaptive adjacency matrices" of <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b8">9]</ref>. On the contrary, our model is free from such assumption, and still achieves performance on a par with most single-graph based models.</p><p>In effect, our model relies on the many latent graphs created by the attention heads, which are specialised for each instance and adapted to each phase of the prediction. This greatly simplifies the design of the model and increases its versatility, facilitating in particular domain adaptation (e.g. in the traffic prediction task, transferring a model learnt on the road network of one city to another).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The ADN Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data model</head><p>A problem instance involves a number N of locations and T o of instants. Measurement vectors, given at some of these location-instant pairs (events), are homogeneous of fixed dimension P . While P is constant over all the instances, N and T o can be instance dependent. We assume that instances are assembled in such a way that the locations in an instance are all somehow related, and for each location, measurement vectors are given at all the instants in a time interval containing a distinguished reference instant (the same for all the locations in the instance). This reflects the task, which is to predict, at each location, the measurement vectors at instants after the reference instant, given those at the reference instant and before in the interval.</p><p>Using padding and masking of instants on the right and on the left of the intervals where measurements are available (one per location), they can be made identical for all the locations in the instance, of common length T o =T +T with T instants up to (and including) the reference one, and T instants after it. Thus, a problem instance can be represented as a (masked) tensor of shape N, T +T , P . Using more padding and masking, B such instances can be batched together in such a way they all share the same N, T, T . The reference instants of the instances in the batch are aligned at position T but do not need to be the same. As for locations, they need neither be aligned nor shared at all. Thus, a batch is represented as a (masked) tensor of size B, N, T +T , P where each index b is associated with an instance, each index pair b, n with a location, each index pair b, t with an instant (the reference instant of instance b being at t=T ) and each index p with a feature of the measurement vectors.</p><p>The data contains two auxiliary pieces of information in addition to the batches: descriptors for locations and instants. They capture the prior information, assumed homogeneous in each case, available on locations and instants, and even possibly events (ignored here for simplicity). The auxiliary information about locations contains at least their identity, which carries strictly no implicit relational information on the locations themselves but allows to relate occurrences of the same location across instances. As for instants, their identity alone is a poor signal, since test instances typically involve instants never seen in training instances (usually, the train-test data partition is based on chronology, where training instances are chosen before a given date and test ones afterwards). Instead, we add information capturing the inherent periodicity of the data, assumed known or discovered by Fourier analysis. For example, in the case of road traffic prediction, the 7-day and 24-hr periodicity of traffic is captured by attaching to each instant the identity of its day of the week among the 7 days of a week and of the 5 min slot containing it among the 288 such slots in a day. Note that this does not carry any implicit relational information on instants: the system treats Monday or Tuesday as meaningless identifiers, and does not know a priori that one is just before the other, nor, similarly, that the 9:05am-9:10am slot is just before the 9:10am-9:15am slot. <ref type="figure" target="#fig_0">Figure 1</ref> gives an overview of our prediction model, based on a traditional Encoder-Decoder architecture. The encoder and decoder pipelines each consists of an initialisation layer followed by a sequence of transformation layers. A final layer produces the prediction for each measurement feature of each target event (PRED block). In general, it is a distribution from which the actual value is sampled (SAMPLE block), but this includes the limit case of a distribution concentrated on a single value, where sampling is trivial. During training, no sampling is done and the output of PRED is directly compared to the original input batch through some LOSS function block, typically Cross-Entropy, which encompasses, in the limit case, the Mean Squared Error (MSE) and Mean Absolute Error (MAE) <ref type="bibr" target="#b0">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Prediction model</head><p>In the initialisation layer ENC-INI, for each of the B instances, each of the N locations and each of the T source instants (up to the reference one) involved in the batch, the associated event descriptors, for the instant and for the location, are retrieved and projected into the embedding dimension D then summed and augmented with the positional encoding in the (chronologically ordered) sequence of instants. The result is an embedding of all the events in the batch as a tensor of shape B, N, T, D , which is then added to a projection in the embedding space of the data batch at the same source instants (up to T ), yielding the full batch embedding of shape B, N, T, D (in blue in <ref type="figure" target="#fig_0">Figure 1</ref>). Similarly, block DEC-INI yields batch embeddings of shape B, N, T , D (green in the figure), collected from the reference instant T up to the penultimate one T +T ?1, i.e. the target instants, shifted to the right, as in the standard Transformer model of <ref type="bibr" target="#b22">[23]</ref>. Now, in the standard Transformer model, the transformation layers ENC-LAYER and DEC-LAYER manipulate 3-way tensors of shape B, S, D or B, S , D , where B is the batch size, S, S the source and target sequence lengths and D the embedding dimension of the model. The associated attention matrix in each attention head is then of size S 2 , S 2 or SS , depending on whether it is a self-or cross-attention, in the encoder or the decoder.</p><p>In our model, on the other hand, the transformation layers have to deal with 4-way tensors. For example, the encoder self-attention has to deal with tensors of shape B, N, T, D . They could of course be reshaped into 3-way tensors of shape B, N T, D , which would amount to treating events atomically, ignoring their bi-dimensionality as locationinstant pairs. They would then be amenable to the standard Transformer treatment of sets of cardinality N T , with selfattention matrices of size (N T ) 2 , which is usually unfeasible. We therefore adopt a simple dimensionality reduction technique, known for a long time in the world of convolution networks under the name "spatial separability" <ref type="bibr" target="#b20">[21]</ref>.</p><p>Since attention is a generalised form of convolution <ref type="bibr" target="#b0">[1]</ref>, separability works all the same with attention, where it is also known as axial attention <ref type="bibr" target="#b6">[7]</ref>.</p><p>Separable attention processes event-indexed objects along each dimension of the events (temporal and spatial) separately and alternately, just as spatially separable convolutions alternate processing images along their width and height dimensions. While the "atomic event" approach with attention matrices of size (N T ) 2 would allow each event to directly influence all the other events in the matrices, with separable attention, the influence is indirect: an event n, t can influence an event n , t first through the influence of t on t at location n in the temporal attention, then the influence of n on n at instant t in the spatial attention. The presence of multiple transformation layers in the pipeline ensures that the two types of influences are thoroughly interleaved, and the order in which the temporal and spatial attention are performed within each layer ends up being unimportant. For the sake of design simplicity, we depart here from models which try to make that order instance dependent and learnable by adding to each layer a gating mechanism arbitrating between spatial and temporal attention <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>The obvious advantage of separable attention is that the attention matrices are now only of size N 2 and T 2 , respectively, assumed to be feasible, or at least better amenable to the many proposals in the literature aiming at breaking the quadratic space complexity of attention <ref type="bibr" target="#b21">[22]</ref>. In practice, separability is straightforward to implement, as it amounts to alternately reshaping each batch of shape B, N, T, D as shown in <ref type="figure" target="#fig_0">Figure 1</ref>, into 3-way tensors of shape BN, T, D (dashed line) then BT, N, D (dotted line), each processed by standard Transformer attention blocks. Of course, separability as described above for the encoder self-attention is similarly applied to the other attention blocks in the model (decoder self-and cross-attention).</p><p>We use teacher forcing <ref type="bibr" target="#b23">[24]</ref>, as in Transformer. At train time, the model is used as an auto-encoder: when presented with a batch, the model is applied to it just once, and the loss measures the difference between output and input. At test time, the model is used as an auto-regressor: when presented with a batch, the model is iteratively applied, with the output of an iteration passed as input to the next. Experiments are conducted on three public traffic datasets PEMS-BAY, METR-LA and PEMS07 released by <ref type="bibr" target="#b11">[12]</ref> and <ref type="bibr" target="#b19">[20]</ref>. The first two are the most commonly used for measuring model performances, and consist of 207 and 325 locations, respectively. We also tested our model on PEMS07 (883 locations), to check its scalability to larger road traffic networks. All three datasets provide data as speed averages or sum of traffic flow on contiguous 5 minute intervals. They also include distance matrices, holding inter-location distances measured on the road network. These are used only in the baselines, not in our model (ADN) since they constitute precisely the kind of structural prior which we seek to avoid. Instances in each of the three datasets are obtained by splitting the data into 2 hour moving windows, with one-hour overlap. Each instance thus consists of 24 instants (one every 5 minutes) and the reference instant is always taken to be the middle one (at position 12). We follow the same procedure for dispatching instances into training, validation, and test subsets as in the literature (details provided in Appendix). Although some authors do data cleaning (like excluding zeros), we do not perform any preprocessing of the data. All models are implemented using the PyTorch machine learning framework and trained on a machine with 4 NVIDIA V100 GPUs with 32GB memory per GPU. For all datasets, we set the batch size to 64 and limit the training to 100 epochs. Models are trained using the MAE loss (Mean Absolute Error, or L1) with the Adam optimizer with parameters ? 1 =0.9, ? 2 =0.98 and =1e ?9 . The initial learning rate is set to 0.002 and halved at the 15th, 30th and 45th epoch. Training gradients are clipped at 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Base performance</head><p>Here, we compare our model on the PEMS-BAY and METR-LA benchmarks with two types of baselines: traditional methods like Historical Average (HA), and Auto-Regressive Integrated Moving Average (ARIMA) with Kalman filter; and neural methods: simple Feed-Forward Neural Network (FNN, two hidden layers and L2 regularization), Spatio-Temporal Graph Convolutional Networks (STGCN) <ref type="bibr" target="#b26">[27]</ref>, Spatio-Temporal Graph Structure Learning for Traffic Forecasting (SLCNN) <ref type="bibr" target="#b28">[29]</ref>, Diffusion Convolutional Recurrent Neural Network (DCRNN) <ref type="bibr" target="#b11">[12]</ref>, Graph WaveNet (GWN) <ref type="bibr" target="#b24">[25]</ref> and Spatial-Temporal Graph Attention Networks for Traffic Flow Forecasting (STGAT) <ref type="bibr" target="#b8">[9]</ref>. For the evaluation, we report Mean Absolute Errors (MAE), Root Mean Squared Errors (RMSE) and Mean Absolute Percentage Error (MAPE) and performances are compared on predictions at 15, 30 and 60 minute horizons. Missing values are excluded in calculating these metrics. <ref type="table" target="#tab_0">Table 1</ref> shows only the MAE metric, while the full results are provided in Appendix. Our model is on a par or outperforms the baselines (except, surprisingly, SLCNN on just the METR-LA dataset, but recall our hyper-parameters were optimised on PEMS-BAY only).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Impact of data scarcity</head><p>As might be expected, the fact that our model does not rely on a structural prior may affect performance when data is scarce, since, in that case, limited training is not compensated by the prior. We put that assumption to the test, by  <ref type="table">Table 2</ref>: MAE at 1 hour on PEMS07. All the baseline results are taken from <ref type="bibr" target="#b10">[11]</ref>.</p><p>artificially reducing the training dataset of PEMS-BAY and observing the degradation of performances, on our model and on one of the baselines which do use a structural prior, namely GWN <ref type="bibr" target="#b24">[25]</ref>. We do not reduce the number of training locations, so that the effect of the structural prior is unchanged, but we reduce the number of training instants: we remove from the training set the instances whose reference instant is outside the x percent earliest, while keeping the same validation and test sets (which are themselves chosen as having the latest reference instants). The results for various values of x, in <ref type="figure" target="#fig_2">Figure 2</ref>, show that the performance advantage brought by the structural prior remains limited overall, even at scarce data regimes, at least above 5% 2 . Furthermore, not only does this advantage tend to disappear with more data, but, beyond a certain threshold (ca. 30%), the effect becomes counter-productive and the prior-free model (ours) starts to outperform the model with prior. That confirms our intuition that the prior does not vanish with more data (GWN is not a Bayesian model), but keeps affecting the performance even when the contribution of data alone would be superior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Impact of large instances</head><p>One limitation of our model, and more generally of all attention-based models, is the quadratic space requirement of attention matrices. While temporal attention poses no problem since the temporal dimensions of instances remain very small (T =T =12), spatial attention may become a problem as the possible number N of locations in an instance increases, for example when tackling the PEMS07 dataset, which has the largest number of locations (more than double that of the other datasets).</p><p>As expected, with PEMS07, spatial attention matrices do not fit into memory on our standard hardware. But, taking advantage of the flexibility of our model, it is possible to work around this problem and even outperform other models. Indeed, while it may seem desirable to deal with instances involving all the locations in the network, this is in no way compulsory, and we can slice the instances along the spatial dimension as much as needed to reduce the size of the slices. We don't have to worry about slicing the corresponding structural prior since we don't make use of one.</p><p>If for example we partition the location set into k subsets, one full instance of spatial dimension N becomes k partial instances of dimension approximately N k (the dimension of the slices need not be strictly equal, nor even close). Again, as we preclude the use of a structural prior, we do not seek to optimise the way partitioning is done, e.g. along geographical lines as in <ref type="bibr" target="#b14">[15]</ref>, and instead draw fully random partitions. Actually, partitions are redrawn at each training epoch. Our experiments show that the best results are reached by partitioning locations into k=16 subsets during the training, which allows the attention matrices to all fit in memory.</p><p>Performance of this approach is compared to Spatial-Temporal Fusion Graph Neural Networks (STFGNN) <ref type="bibr" target="#b10">[11]</ref>, for which results on the PEMS07 dataset are available, as well as all the baselines presented in that paper: Long Short-Term Memory -Fully Connected Neural Network (FC-LSTM), DCRNN, STGCN, GWN, Spatial-Temporal Syn-  chronous Graph Convolutional Networks (STSGCN) <ref type="bibr" target="#b19">[20]</ref>. Following <ref type="bibr" target="#b10">[11]</ref>, we run experiments 10 times and report the mean and standard deviation in <ref type="table">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Impact of missing data</head><p>Often, datasets are not of uniform quality, and some instances may contain "holes", typically if the sensors at some locations fail and report no measurement during the time span of the instance. In our model, such locations are simply removed from the instance, and again, we don't have to worry about applying the same removal in the structural prior.</p><p>We conducted an experiment on the PEMS-BAY dataset, simulating missing data by removing at random a proportion x of locations in each instance. The results for various values of x, in <ref type="figure" target="#fig_3">Figure 3</ref>, show the remarkable robustness of our model to missing locations in the training set. The impact on performance is negligible at levels up to 60% missing locations, and remains competitive at up to 95% missing locations. Of course, it is important to observe that random location removal is performed independently in each instance: the same results could not be achieved if removals were inter-dependent (e.g., obviously, in the extreme case where the random subset of removed locations were constrained to be the same in all instances).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis of the results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Spatial analysis</head><p>To further investigate the effect of using a structural prior (in the baselines) vs not using it (in our model), we sought to compare the latent relationship between locations, as built by our model, and the explicit relationship provided by the prior, based on the inter-location distances on the road network.</p><p>In fact, our model builds many relationships between locations, essentially one for each instance and in each head of each attention block (examples are shown in Appendix). But they all indirectly derive from the embeddings of the location descriptors learnt in the initial layer of the encoder (ENC-INI in <ref type="figure" target="#fig_0">Figure 1</ref>). <ref type="figure" target="#fig_4">Figure 4</ref> gives a 2D representation of these embeddings (of dimension D) obtained by a standard statistical dimensionality reduction method, t-SNE <ref type="bibr" target="#b13">[14]</ref>. The figure shows distinct clusters which, after verification, correspond roughly to distinct segments of the road net- work. In other words, the relational information constructed by our model is close to that provided by the structural prior.</p><p>However, we noticed some discrepancies. Take location labelled R (in red) on the figure. We coloured all the other locations by their distance to R according to the structural prior (the darker the colour, the longer the distance). We would expect that locations which are close in the prior are also close in the model, but we observe that a cluster of locations close to R in the prior (i.e. with light colour), encircled in red in the figure, appear far from R in the model. Taking a closer look at these exceptions, we realised they correspond to locations which are indeed close to R on the road network, but only through a path which is very unlikely, e.g. involving a U-turn between the lanes in opposite directions of a motorway, through a bridge over it.</p><p>In all the datasets we use, the prior is obtained as the inter-location network distances as computed by some mapping tool such as OpenStreetMap, i.e. the length of the shortest path between locations on the road network, but that does not take into account the probability of vehicles following that path. This is the kind of defects of the prior which can have a lasting detrimental effect on models which rely on a prior but do not ensure that it gives way to data when data becomes abundant. We suspect that is what happens in most baselines, which use a prior but not in a Bayesian way. Our model, which does not involve a prior at all, is safe in that respect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Temporal analysis</head><p>We conducted a similar visual analysis of the embeddings on instants, rather than locations. A t-SNE representation of the different time slots in the different days of the week is given in <ref type="figure" target="#fig_5">Figure 5</ref>, showing only Friday (Monday to Thursday are similar) and Saturday (Sunday is similar). It shows that our model does learn quite precisely the ordering of time slots in a day, as well as their cyclic nature, although this information is not contained in the instant descriptors given to the system, limited to undistinguishability, not ordering information. While it might be argued that positional encoding, applied in the ENC-INI and DEC-INI blocks of the model, may carry implicit ordering, this is not what underlies the results of <ref type="figure" target="#fig_5">Figure 5</ref>, since all the experiments reported in this paper have been conducted with no positional encoding! In fact, the only place in the model where explicit temporal ordering information is indeed exploited is in the mask of the decoder self-attention, which prevents instant embeddings in one layer from influencing the embeddings of the explicitly designated previous instants in the next layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Domain adaptation</head><p>Finally, we consider the problem of domain adaptation, where a model learnt with data from a source domain is applied to a target domain. To simplify, we focus on the case where the source and target domains differ only in their locations, and not in their periodicity patterns (which fully characterize the instants), although these could also be in the scope of the adaptation. More precisely, we assume that source and target domains have no location in common, as in the case of PEMS-BAY and METR-LA already considered in <ref type="bibr" target="#b8">[9]</ref>. Using different datasets, <ref type="bibr" target="#b15">[16]</ref> consider a similar problem under the name "transfer learning" (it is unclear how that differs from what is discussed here).</p><p>Models which rely on a structural prior seem to have an advantage when it comes to domain adaptation: they can leverage the prior to do zero-shot adaptation, i.e. without seeing any data from the new domain, just the prior. However, it can be expected that the quality of the prediction in that case will be rather poor and probably of no practical use. Furthermore, while in the traffic application the structural prior is cheap compared to data acquisition, it may not be the case in other contexts. Instead, we propose a "few shot" adaptation approach, where data over a very short period (a few days vs several months for the full dataset) is accessible from the target domain for fine-tuning the model learnt on the source domain.</p><p>In fact, only a small part of the model needs fine-tuning. Indeed, the parameters of the ADN model consist of two sets: ? the parameters of the attention blocks in the encoder and decoder: we may assume these are generic, and characterize the road traffic prediction task per se, with no reference to a particular domain, so they do not need fine-tuning; ? the initial embeddings of the events (locations and instants): we assumed similar periodicity patterns between source and target, so the instant embeddings can be reused as such, while nothing can be reused from the source location embeddings since each location is entirely characterised by its identity, and no identity is shared between source and target domains.</p><p>Hence, in the fine-tuning step, we only need to learn, from scratch, the embeddings of the target domain locations, while keeping all the other parameters fixed.</p><p>Results for different amounts of fine-tuning data, expressed as the number of days its instances span, are presented in <ref type="figure" target="#fig_6">Figure 6</ref>. We compare performance with the STGAT model <ref type="bibr" target="#b8">[9]</ref>. To be fair, instead of comparing with just the number reported in that paper, which corresponds to zero-shot learning by substitution of the prior (no fine-tuning), we do both a substitution of the prior and a fine-tuning of (all) the parameters of the STGAT model on the same data. In both models, we run 100 epochs of fine-tuning using the hyper-parameters learnt on the source domain PEMS-BAY.</p><p>The experimental results show that our model needs only a small amount of fine-tuning data to reach good performance: at 1hr horizon, two days are enough to cancel the performance advantage the structural prior gives to STGAT. Beyond that amount, our model keeps improving, at a faster rate than STGAT. More details are available in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we have considered the problem of structured time-series prediction, and proposed a light-weight model, not relying on any prior knowledge on the relations between events in the time series. We show that such structural prior, if used in a non-Bayesian way as in most models from the literature, can have a detrimental effect on performance. We also show that our model, being more versatile, can better deal with the problems of missing data and domain transfer.  . The East-West and West-East lanes of the motorway, with blue and red markers, respectively, clearly correspond to two distinct clusters in the embedding space. In the structural prior based on network distances, these two sets of locations are much closer together, since there always exist short paths allowing U-turn from one lane to the other of the motorway. Such paths are very seldom used and should have no impact on the diffusion of traffic information. Models using the structural prior will diffuse information through these unlikely paths, while our model is clearly not affected with this.  <ref type="table">Table 4</ref>: Complete performance results of our model on the two moderate-sized benchmark datasets, PEMS-BAY and METR-LA. Baselines are the same as in the main text, described in the section of the paper on "Base Performance". Results for the baseline models are taken from the corresponding papers.  <ref type="table">Table 5</ref>: Complete performance results concerning the larger benchmark dataset PEMS07. Baselines are the same as in the main text, described in the section of the paper on "Impact of large instances". Results of the baseline models are taken from the corresponding paper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FC-LSTM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MAE RMSE MAPE%</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The overall architecture of our ADN model (top) is a traditional Encoder-Decoder network. Each encoder and decoder layer (bottom)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>4. 1 . 2</head><label>12</label><figDesc>Hyper-parameters and implementation The model hyperparameters are tuned on the PEMS-BAY dataset and reused on all the other datasets. The best results are achieved with a model composed of 3 layers, both in the encoder and the decoder, each built with 2-head spatial attentions and 4-head temporal attentions. Model embedding size D: 32; dimension of the feed-forward layer: 256; dropout value: 0.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>PEMS-BAY with (simulated) scarce data. The structural prior gives a performance advantage to the GWN baseline at very scarce data regime, but becomes a hindrance when data is abundant. 98 ? 0.42 25.30 ? 0.52 25.38 ? 0.49 26.85 ? 0.05 24.26 ? 0.14 22.07 ? 0.11 21.62 ? 0.10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>PEMS-BAY with (simulated) missing data. The dashed lines correspond to baseline models learnt with the full dataset, shown here for reference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Visualisation of the PEMS-BAY location embeddings. The colours indicate the distance, according to the prior, to the distinguished location labelled R.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Visualisation of the embeddings of the 288 time slots of each day, on Friday (left) and Saturday (right), in the same referential given by the bounding box.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Domain adaptation from PEMS-BAY to METR-LA. The dashed lines correspond to baseline models learnt on the full target domain, shown here for reference. Our model learnt on the full target domain is also reported.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Left: adjacency matrix for METR-LA, used as structural prior in DCRNN model; right: decoder spatial attention matrix for a randomly chosen instance in our model. Although we display just one (out of six) attention matrix, some similarities can be detected.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Correspondence between the actual position on the road network of a set of locations (right figure) and the t-SNE representation of their embeddings in the model (left)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>MAE at various horizons on PEMS-BAY and METR-LA datasets. All the baseline results are taken from the corresponding papers. In all cases, our model performs either best (bold) or second best (italics).</figDesc><table><row><cell></cell><cell></cell><cell cols="9">HA ARIMA FNN STGCN SLCNN DCRNN GWN STGAT ADN Ours</cell></row><row><cell>PEMS-</cell><cell>BAY</cell><cell>15 min 2.88 30 min 2.88 1 hour 2.88</cell><cell>1.62 2.33 3.38</cell><cell>2.20 2.30 2.46</cell><cell>1.36 1.81 2.49</cell><cell>1.44 1.72 2.03</cell><cell>1.38 1.74 2.07</cell><cell>1.30 1.63 1.95</cell><cell>1.32 1.61 1.91</cell><cell>1.30 1.62 1.90</cell></row><row><cell>METR-</cell><cell>LA</cell><cell>15 min 4.16 30 min 4.16 1 hour 4.16</cell><cell>3.99 5.15 6.90</cell><cell>2.20 4.23 4.49</cell><cell>2.88 3.47 4.59</cell><cell>2.53 2.88 3.30</cell><cell>2.77 3.15 3.60</cell><cell>2.69 3.07 3.53</cell><cell>2.66 3.01 3.46</cell><cell>2.61 2.98 3.42</cell></row><row><cell cols="3">4.1 Experimental setting</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">4.1.1 Datasets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Details of the size and nature of the 3 datasets used in the experimental part of the paper. .59 6.8 2.88 5.59 6.8 2.88 5.59 6.8 4.16 7.80 13.0 4.16 7.80 13.0 4.16 7.80 13.0 ARIMA 1.62 3.30 3.5 2.33 4.76 5.4 3.38 6.50 8.3 3.99 8.21 9.6 5.15 10.45 12.7 6.90 13.23 17.4 FNN 2.20 4.42 5.2 2.30 4.63 5.4 2.46 4.98 5.9 3.99 7.94 9.9 4.23 8.17 12.9 4.49 8.69 14.0 STGCN 1.36 2.96 2.9 1.81 4.27 4.2 2.49 5.69 5.8 2.88 5.74 7.6 3.47 7.24 9.6 4.59 9.40 12.7 SLCNN 1.44 2.90 3.0 1.72 3.81 3.9 2.03 4.53 4.8 2.53 5.18 6.7 2.88 6.15 8.0 3.30 7.20 9.7 DCRNN 1.38 2.95 2.9 1.74 3.97 3.9 2.07 4.74 4.9 2.77 5.38 7.3 3.15 6.45 8.8 3.60 7.59 10.5 GWN 1.30 2.74 2.7 1.63 3.70 3.7 1.95 4.52 4.6 2.69 5.15 6.9 3.07 6.22 8.4 3.53 7.37 10.0 STGAT 1.32 2.76 2.8 1.61 3.68 3.7 1.91 4.43 4.6 2.66 5.12 6.9 3.01 6.12 8.1 3.46 7.19 9.8 ADN Ours 1.30 2.86 2.7 1.62 3.81 3.8 1.90 4.56 4.5 2.61 5.14 6.7 2.98 6.25 8.2 3.42 7.44 10.0</figDesc><table><row><cell></cell><cell>Dataset</cell><cell># of nodes</cell><cell></cell><cell>Data range</cell><cell cols="3">Data type Training Validation Testing</cell></row><row><cell></cell><cell>METR-LA</cell><cell>207</cell><cell cols="2">3/1/2012 -1/30/2012</cell><cell>speed</cell><cell>70%</cell><cell>10%</cell><cell>20%</cell></row><row><cell></cell><cell>PEMS-BAY</cell><cell>325</cell><cell cols="2">1/1/2017 -5/3/2017</cell><cell>speed</cell><cell>70%</cell><cell>10%</cell><cell>20%</cell></row><row><cell></cell><cell>PEMS07</cell><cell>883</cell><cell cols="2">5/1/2017 -8/31/2017</cell><cell>flow</cell><cell>60%</cell><cell>20%</cell><cell>20%</cell></row><row><cell></cell><cell cols="4">PEMS-BAY (MAE, RMSE, MAPE%)</cell><cell cols="3">METR-LA (MAE, RMSE, MAPE%)</cell></row><row><cell>Model</cell><cell>15 min</cell><cell cols="2">30 min</cell><cell>60 min</cell><cell>15 min</cell><cell></cell><cell>30 min</cell><cell>60 min</cell></row><row><cell>HA</cell><cell>2.88 5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>? 0.42 25.30 ? 0.52 25.38 ? 0.49 26.85 ? 0.05 24.26 ? 0.14 22.07 ? 0.11 21.62 ? 0.10 MRSE 45.94 ? 0.57 38.58 ? 0.70 38.78 ? 0.58 42.78 ? 0.07 39.03 ? 0.27 35.80 ? 0.18 37.10 ? 0.04 MAPE% 13.20 ? 0.53 11.66 ? 0.33 11.08 ? 0.18 12.12 ? 0.41 10.21 ? 1.65 9.21 ? 0.07 8.93 ? 0.06</figDesc><table><row><cell></cell><cell>DCRNN</cell><cell>STGCN</cell><cell>GWN</cell><cell>STSGCN</cell><cell>STFGNN</cell><cell>ADN Ours</cell></row><row><cell>MAE</cell><cell>29.98</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>.32 9.98 5.14 10.40 12.51 6.68 12.39 18.71 1 day 3.35 6.30 9.56 4.27 8.20 12.34 4.89 9.72 15.23 2 days 3.32 6.26 9.12 4.21 7.80 12.05 4.82 9.96 14.74 3 days 3.31 6.23 9.00 4.15 8.68 11.55 4.79 9.62 14.13 7 days 3.23 6.18 9.13 3.82 7.54 11.49 4.57 8.99 14.48 14 days 3.17 5.90 8.93 3.71 7.19 10.84 4.49 8.80 13.58 .28 10.84 4.72 10.13 13.61 5.68 11.92 16.54 2 days 3.59 7.57 9.91 4.23 8.98 12.09 4.81 9.45 13.74 3 days 3.41 7.37 9.45 3.96 7.62 11.38 4.75 9.37 13.66 7 days 3.18 6.11 8.73 3.70 7.36 10.84 4.32 8.75 13.26 14 days 3.04 5.83 8.13 3.53 7.04 10.07 4.14 8.42 12.38 Table 6: The full results of our experiments of domain adaptation from PEMS-BAY to METR-LA.</figDesc><table><row><cell>Model</cell><cell>k-shots</cell><cell cols="2">15 mins</cell><cell>30 mins</cell><cell>1 hour</cell></row><row><cell cols="3">STGAT 3.96 8ADN Ours --1 day 3.86 8</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">ExperimentsBeyond reporting the performance of our model on benchmarks, we conduct a number of experiments measuring the impact of: (i) data scarcity; (ii) missing data; (iii) large instances. We further analyse the results of our experiments to try and explain some of the performances of our model. Finally, we consider the problem of domain adaptation. 1 MSE (resp. MAE) is the limit of a cross-entropy loss with a Normal (resp. Laplace) distribution whose variance tends to 0.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Observe that below 1 week of data, some days of the week are not even represented in the training data, but still present in test, deeply impacting the performance.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Convolution, attention and structure embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Marc</forename><surname>Andreoli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.01289</idno>
		<idno>arXiv: 1905.01289</idno>
		<imprint>
			<date type="published" when="2019-05" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Traffic transformer: Capturing the continuity and periodicity of time series for traffic forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Cai</surname></persName>
		</author>
		<idno type="DOI">10.1111/tgis.12644</idno>
		<ptr target="https://onlinelibrary.wiley.com/doi/pdf/10.1111/tgis.12644" />
	</analytic>
	<monogr>
		<title level="j">Transactions in GIS</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="736" to="755" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29</title>
		<editor>D. D. Lee et al.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Time Series Prediction : Predicting Stock Price</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Elliot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng Hua</forename><surname>Hsu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05751</idno>
		<idno>arXiv: 1710.05751</idno>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">STANN: A Spatio-Temporal Attentive Neural Network for Traffic Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Yin</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Dong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="4795" to="4806" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">STCNN: A Spatio-Temporal Convolutional Neural Network for Long-Term Traffic Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Yin</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Dong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">20th IEEE International Conference on Mobile Data Management (MDM)</title>
		<meeting><address><addrLine>Hong Kong, Hong Kong</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="226" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Axial Attention in Multidimensional Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12180</idno>
		<idno>arXiv: 1912.12180</idno>
		<imprint>
			<date type="published" when="2019-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<idno>arXiv: 1609.02907</idno>
		<imprint>
			<date type="published" when="2016-09" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">STGAT: Spatial-Temporal Graph Attention Networks for Traffic Flow Forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyuan</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference Name: IEEE Access</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="134363" to="134372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">DDP-GCN: Multi-Graph Convolutional Network for Spatiotemporal Traffic Forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungeun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjong</forename><surname>Rhee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12256</idno>
		<idno>arXiv: 1905.12256</idno>
		<imprint>
			<date type="published" when="2020-02" />
		</imprint>
	</monogr>
	<note>cs, eess</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengzhang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanxing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09641</idno>
		<idno>arXiv: 2012.09641</idno>
		<title level="m">Spatial-Temporal Fusion Graph Neural Networks for Traffic Flow Forecasting</title>
		<imprint>
			<date type="published" when="2021-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaguang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01926</idno>
		<idno>arXiv: 1707.01926</idno>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Traffic Speed Prediction: An Attention-Based Method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duanyang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sensors 19</title>
		<imprint>
			<date type="published" when="2019-09" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">3836</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Visualizing Data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Graph-Partitioning-Based Diffusion Convolutional Recurrent Neural Network for Large-Scale Traffic Forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanwi</forename><surname>Mallick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11197</idno>
		<idno>arXiv: 1909.11197</idno>
		<imprint>
			<date type="published" when="2020-04" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Transfer Learning with Graph Neural Networks for Short-Term Highway Traffic Forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanwi</forename><surname>Mallick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08038</idno>
		<idno>arXiv: 2004.08038</idno>
		<imprint>
			<date type="published" when="2020-04" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ST-GRAT: A Novel Spatio-temporal Graph Attention Networks for Accurately Forecasting Dynamically Changing Road Speed</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheonbok</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management. CIKM &apos;20</title>
		<meeting>the 29th ACM International Conference on Information &amp; Knowledge Management. CIKM &apos;20<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-10" />
			<biblScope unit="page" from="1215" to="1224" />
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">High-Dimensional Multivariate Forecasting with Low-Rank Gaussian Copula Processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Salinas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03002</idno>
		<idno>arXiv: 1910.03002</idno>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs</orgName>
			</affiliation>
		</author>
		<idno type="arXiv">arXiv:1506.04214</idno>
		<idno>arXiv: 1506.04214</idno>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spatial-Temporal Synchronous Graph Convolutional Networks: A New Framework for Spatial-Temporal Network Data Forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020-04" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="914" to="921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Rethinking the Inception Architecture for Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs</orgName>
			</affiliation>
		</author>
		<idno type="arXiv">arXiv:1512.00567</idno>
		<idno>arXiv: 1512.00567</idno>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Efficient Transformers: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.06732[cs](Sept.2020</idno>
		<idno>arXiv: 2009.06732</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Attention Is All You Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<idno>arXiv: 1706.03762</idno>
		<imprint>
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A Learning Algorithm for Continually Running Fully Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zipser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Graph WaveNet for Deep Spatial-Temporal Graph Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00121</idno>
		<idno>arXiv: 1906.00121</idno>
		<imprint>
			<date type="published" when="2019-05" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Spatial-Temporal Transformer Networks for Traffic Flow Forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.02908</idno>
		<idno>arXiv: 2001.02908</idno>
		<imprint>
			<date type="published" when="2020-01" />
		</imprint>
	</monogr>
	<note>cs, eess</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoteng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanxing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.04875</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="page" from="3634" to="3640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Forecasting road traffic speeds by considering area-wide spatio-temporal dependencies based on a graph convolutional neural network (GCN)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeonghyeop</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keemin</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part C: Emerging Technologies</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="189" to="204" />
			<date type="published" when="2020-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Spatio-Temporal Graph Structure Learning for Traffic Forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<idno>34.01</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020-04" />
			<biblScope unit="page" from="1177" to="1185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">GMAN: A Graph Multi-Attention Network for Traffic Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanpan</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.08415</idno>
		<idno>arXiv: 1911.08415</idno>
		<imprint>
			<date type="published" when="2019-11" />
		</imprint>
	</monogr>
	<note>cs, eess</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
