<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SPSN: Superpixel Prototype Sampling Network for RGB-D Salient Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhyeok</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yonsei University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaewon</forename><surname>Park</surname></persName>
							<email>chaewon28@yonsei.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Yonsei University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suhwan</forename><surname>Cho</surname></persName>
							<email>chosuhwan@yonsei.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Yonsei University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangyoun</forename><surname>Lee</surname></persName>
							<email>syleee@yonsei.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Yonsei University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SPSN: Superpixel Prototype Sampling Network for RGB-D Salient Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>RGB-D salient object detection</term>
					<term>Superpixel</term>
					<term>Prototype learn- ing</term>
					<term>Reliance selection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>RGB-D salient object detection (SOD)  has been in the spotlight recently because it is an important preprocessing operation for various vision tasks. However, despite advances in deep learning-based methods, RGB-D SOD is still challenging due to the large domain gap between an RGB image and the depth map and low-quality depth maps. To solve this problem, we propose a novel superpixel prototype sampling network (SPSN) architecture. The proposed model splits the input RGB image and depth map into component superpixels to generate component prototypes. We design a prototype sampling network so that the network only samples prototypes corresponding to salient objects. In addition, we propose a reliance selection module to recognize the quality of each RGB and depth feature map and adaptively weight them in proportion to their reliability. The proposed method makes the model robust to inconsistencies between RGB images and depth maps and eliminates the influence of non-salient objects. Our method is evaluated on five popular datasets, achieving state-of-the-art performance. We prove the effectiveness of the proposed method through comparative experiments. Code and models are available at https://github.com/Hydragon516/SPSN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The salient object detection (SOD) task detects and segments objects that visually attract the most human interest from a single image or video. The SOD task is a useful preprocessing operation for various computer vision tasks such as few-shot learning, weakly-supervised semantic segmentation, object recognition, tracking, and image parsing. However, despite recent advances in deep learning, it is still challenging due to camouflaged objects, extreme lighting conditions, and scenes containing multiple objects with complex shapes. To potentially improve performance for such difficult scenes, RGB-D SOD, using an additional depth map, has recently been in the spotlight.</p><p>Recent deep learning-based studies <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b20">21]</ref> achieve significant RGB-D SOD performance by fusing RGB information and additional depth information. However, due to the large domain gap between an RGB image containing rich detail information and a depth image containing geometric information, previous works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b35">36]</ref> focus on the process of effectively fusing these two pieces of information. These methods show that they can effectively extract feature information about salient objects from RGB images and depth maps, but they have two major limitations.</p><p>First, they perform inconsistently due to mismatches between the RGB image and the depth map. For example, in the case of a picture hung on the wall, the depth map lacks saliency information compared to the RGB image due to the picture's thinness. Furthermore, the RGB image contains complex texture information about the background scene despite the particularly monotonous depth map background. This unnecessary additional information acts as noise in the network and makes it difficult to generate an accurate saliency mask. This often causes conventional methods to fail in challenging scenes involving complex background structures and multiple foreground objects.</p><p>Second, the quality of depth maps is inconsistent due to the limitations of the depth sensor. Some studies <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b41">42]</ref> suggest additional processes for depth map refinement to solve this problem. Although these methods can improve the consistency of low-quality depth maps, they are inefficient due to the additional network or computational costs.</p><p>To solve the problems described above, we propose a novel superpixel prototype sampling network (SPSN) architecture. <ref type="figure" target="#fig_0">Fig. 1</ref> shows the overall SPSN process. First, we note that RGB images and depth maps provide different kinds of information and can complement each other. RGB images have various detail and texture information in the foreground and background, which provides rich context information to the network as it passes through the encoder. In comparison, the depth map lacks detailed information, but it is more robust than an RGB image in extreme lighting conditions. For a preprocessing operation to effectively fuse and complement the advantages of an RGB image and depth map, we use the simple linear iterative clustering (SLIC) algorithm <ref type="bibr" target="#b1">[2]</ref> to segment the RGB image and depth map into superpixel components. Moreover, we propose a prototype sampling network module (PSNM) to solve the inconsistency problem between RGB images and depth maps and extract salient object features effectively. We generate component prototypes from superpixel components, in-spired by prototype learning, which is used extensively in few-shot segmentation tasks <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30]</ref>. PSNM, composed of transformers and graph convolutional layers, is trained to selectively sample only prototypes corresponding to salient objects among component prototypes. Therefore, the proposed method improves performance by minimizing the influence of the background and extracting consistent salient features from RGB images and depth maps. Furthermore, for the network to be flexible enough to handle low-quality depth maps, we propose a reliance selection module (RSM). The RSM is trained to evaluate the quality of the features generated from the RGB component prototypes and depth component prototypes. As a result, the RSM adaptively changes the RGB image and depth map dependence of the network. In other words, the proposed model minimizes performance degradation in situations such as low-quality depth maps and low-light RGB images and effectively creates a saliency mask.</p><p>The experimental results over five benchmark datasets show that our model significantly outperforms previous state-of-the-art approaches. Finally, we demonstrate the validity of our method through various ablation studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">RGB-D SOD</head><p>Recent RGB-based SOD methods <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b15">16]</ref> have demonstrated outstanding performance. However, they are still challenged by insufficient information to express the complex characteristics of scenes with multiple objects, transparent objects, ambiguous borders between the foreground and background, and extreme light conditions. Meanwhile, owing to the development of various consumer-grade depth cameras, additional depth cues of abundant structural and geometrical information have been enabled for SOD studies. Therefore, RGB-D SOD has gained significant attention and has been widely studied to supplement the limits of RGB-based methods on the scenarios mentioned above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Traditional RGB-D Methods</head><p>Traditional RGB-D SOD algorithms <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b26">27]</ref> focused on utilizing various hand-crafted features, such as contrast, center or boundary prior, and center-surround difference. Lang et al. <ref type="bibr" target="#b26">[27]</ref> introduced the depth prior by modeling the relationship between depth and saliency with a mixture of Gaussians. Additionally, Cheng et al. <ref type="bibr" target="#b10">[11]</ref> grouped the pixels in the input image with kmeans clustering and obtained three saliency cues-color contrast, depth contrast and spatial bias-from each cluster to generate saliency maps. Moreover, Ju et al. <ref type="bibr" target="#b24">[25]</ref> proposed an anisotropic center-surround difference based on the assumption that salient objects tend to stand out from the surroundings. Because these methods rely heavily on hand-crafted features of relatively limited information, their performance deteriorates in complex scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Deep Learning-Based RGB-D Methods</head><p>Existing deep learning-based RGB-D SOD methods focus more on fusing the complementary features extracted from the RGB and depth channels because the domain gap represented by each channel is significant. The merging strategies can be grouped into three categories based on when the fusion takes place: early fusion <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b40">41]</ref>, middle fusion <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, and late fusion <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b35">36]</ref>. Early fusion methods concatenate the RGB image and depth image at the earliest stage and regard the integrated four-channel matrix as a single input. For example, Qu et al. <ref type="bibr" target="#b38">[39]</ref> introduced this method by generating hand-crafted feature vectors from each RGB-D pair, which were fed as input to a CNN-based model. Middle fusion methods fuse the two different feature maps extracted from individual networks. For example, Chen et al. <ref type="bibr" target="#b6">[7]</ref> suggested a two-stream complementary-aware network in which the features from the same stages of each modality are fused with the help of a complementary-aware fusion block. Finally, late fusion methods produce individual saliency prediction maps from both the RGB and depth channels, and the two predicted maps are merged by a post-processing operation such as pixel-wise summation and multiplication. For example, Piao et al. <ref type="bibr" target="#b35">[36]</ref> proposed a depth-induced multiscale recurrent attention network to extract the features from an RGB image and depth image individually and designed depth refinement blocks for integration.</p><p>However, these methods neglect the problem of mismatches between the two modalities. For example, in some scenarios such as a thin calendar hung on the wall, the RGB image more accurately discriminates the salient object and the background, whereas all the pixel values are similar to each other in the depth image. To deal with this problem, several studies have proposed methods to enhance such unreliable input data by utilizing hand-crafted techniques to improve the accuracy. Zhao et al. <ref type="bibr" target="#b50">[51]</ref> suggested a contrast prior loss to increase the color difference between the foreground and background of the depth input. Ji et al. <ref type="bibr" target="#b22">[23]</ref> proposed an effective depth calibration strategy that corrects the latent bias of the raw depth maps. Furthermore, Zhang et al. <ref type="bibr" target="#b47">[48]</ref> presented a depth correction network to decrease the noise in unreliable depth data, assuming the object boundaries in the depth map align with those in the RGB map.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Feature Fusion Module</head><p>As shown in <ref type="figure" target="#fig_4">Fig. 3</ref> (a), FFM fuses multiscale features from the encoder. We extract three features</p><formula xml:id="formula_0">E 1 ? R C (1/8) ? H 8 ? W 8 , E 2 ? R C (1/16) ? H 16 ? W 16 , and E 3 ? R C (1/32) ? H 32 ? W 32</formula><p>from the encoder, where H and W are the height and width of the input image, respectively, and C (1/8) , C (1/16) , and C (1/32) are the number of channels of the multiscale encoder feature. Because the architectures of the RGB encoder and depth encoder are identical, the size of the extracted features is the same. The FFM consists of a 1 ? 1 convolution layer and upsampling layers, integrating the multiscale features of the encoder and extracting the global contextual information through atrous spatial pyramid pooling (ASPP) <ref type="bibr" target="#b8">[9]</ref> layer. As a result, The FFM generates RGB fusion feature <ref type="figure" target="#fig_2">Fig. 2</ref>. In addition, the channel-reduced encoder feature E cr after the 1 ? 1 convolution layer is used as the input for PSNM.</p><formula xml:id="formula_1">F RGB ? R 128? H 8 ? W 8 and depth fusion feature F D ? R 128? H 8 ? W 8 , as shown in</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Prototype Generating Module</head><p>The PGM aims to generate component prototypes from fusion features F RGB and F D , obtained from the FFM. As shown in <ref type="figure" target="#fig_4">Fig. 3</ref> (b), we first create a superpixel map S from each RGB image I RGB and depth map I D using the SLIC algorithm <ref type="bibr" target="#b1">[2]</ref>. Next, we create a superpixel mask group sm where each channel is a binary mask for each superpixel. Therefore, if the number of superpixels is N S , the size of sm is N S ? H ? W . sm is down-sampled to the same size as the fusion feature F (i.e. F RGB or F D ) generated by the FFM, so the size of the superpixel mask sm i constituting each channel of sm Like the prototype learning of few-shot segmentation tasks <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30]</ref>, the PGM creates a component prototype from each superpixel mask. Thus, the prototype P i generated by sm i is defined as</p><formula xml:id="formula_2">is 1 ? H 8 ? W 8 , where i = 1, 2, ..., N S .</formula><formula xml:id="formula_3">P i = M AP (F, sm i ), where M AP (.)</formula><p>is the masked average pooling operator. Finally, we define a prototype block PB ? R N S ?128 as a concatenation of the component prototypes generated. As shown in <ref type="figure" target="#fig_4">Fig. 3</ref> (b), we define prototypes created from superpixel masks on salient objects as salient component prototypes, and prototypes created from superpixel masks at other locations as non-salient component prototypes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Prototype Sampling Network Module</head><p>The PSNM aims to sample only the salient component prototypes from all the component prototypes P i created from the RGB images and depth maps. Therefore, the PSNM should focus on correlations between P i that contain consistent characteristics for salient objects, and it must be able to distinguish them from inconsistent background components. <ref type="figure" target="#fig_5">Fig. 4</ref> shows the structure of the proposed PSNM, which consists of Parts A, B, C, and D. Part A. Part A is a transformer module with multi-head attention to enhance the correlation between P i . Inspired by the previous key, query, and valuebased multi-head attention method <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b46">47]</ref>, we first generate PB K ? R N S ?64 , PB Q ? R N S ?64 , and PB V ? R N S ?64 from the prototype block PB using MLP blocks M LP K , M LP Q , and M LP V . By the MLP block, the length of prototypes is reduced by half, with each prototype block defined as</p><formula xml:id="formula_4">PB K = M LP K (PB), PB Q = M LP Q (PB), and PB V = M LP V (PB). The Part A output PB att ? R N S ?128</formula><p>is defined by the following equation:</p><formula xml:id="formula_5">PB att = PB + M LP W ? PB Q ? (PB K ) T ? d ? PB V ,<label>(1)</label></formula><p>where (.) T and ? (.) are the transpose and softmax operators, respectively. Furthermore, d = 64, the length of PB K , PB Q , and PB V . In addition, M LP W is</p><formula xml:id="formula_6">EdgeConv EdgeConv EdgeConv * EdgeConv MLP ( ) K-NN graph 1 3 2 . &amp; Sigmoid Part A Part B Part D Part C ( or ) Overlap &gt; 0.5</formula><p>Binary cross entropy loss ( ) an MLP block that increases the length of the reduced prototype to the length of the original. Part B. Part B is a network for sampling salient component prototypes from PB att with enhanced correlation between component prototypes. Since the component prototype is the result of masked average pooling of the encoder features, it is a one-dimensional vector representing each component feature. These feature shapes are similar to the embedded point features of 3D point cloud networks <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b45">46]</ref>. Therefore, we propose a graph convolution network based on the feature distance between the prototypes, inspired by EdgeConv <ref type="bibr" target="#b45">[46]</ref>, used in 3D point cloud networks. As shown in Part B of <ref type="figure" target="#fig_5">Fig. 4</ref>, the proposed module consists of three EdgeConv layers and one MLP block. First, we define the input component prototype block of EdgeConv as PB in ? R N S ?128 containing prototypes P in 1 , P in 2 , ..., P in N S . Then, EdgeConv uses the k-nearest-neighbor (k-NN) algorithm to create a graph between a target prototype P in i and a k prototypes P in j1 , P in j2 , ..., P in ja k that are most close to each other in the feature space. Next, as shown in <ref type="figure" target="#fig_5">Fig. 4</ref>, EdgeConv extracts edge features ? ijx between each node generated in the graph, where x = 1, 2, ..., a k . The edge features ? ijx are defined as follows:</p><formula xml:id="formula_7">? 1?32 ? 1?16 ? 1?8 ( or ) 1 2 3 ( or ) (? or ? ) * * * convolution</formula><formula xml:id="formula_8">? ijx = h ? P in i , P in jx ? P in i ,<label>(2)</label></formula><p>where h ? : R ce ?R ce ? R ce is a nonlinear function with a set of learnable parameters ?, and c e = 128. Therefore, as shown in <ref type="figure" target="#fig_5">Fig. 4</ref>, a total of a k ? ijx are generated, so the size of ? ij is a k ?N S ?128. This process is equivalent to generating dynamic graphs proposed by <ref type="bibr" target="#b45">[46]</ref>. The final output PB out ? R N S ?128 of the Edge-Conv layer is defined as PB out = M AX (? ij ), where M AX (.) is the channelwise symmetric aggregation operator max pooling, according to <ref type="bibr" target="#b45">[46]</ref>. The symmetric aggregation operator makes the network independent of the prototype order. As a result, Part B generates a prototype sampler vector S pred ? R N S , defined as follows:  where Sigmoid (.) is the sigmoid operator and M LP F is an MLP block that reduces the length of the prototype block. Therefore, as shown in <ref type="figure" target="#fig_5">Fig. 4</ref>, S pred has values between 0 and 1, and we multiply S pred by PB att to create a PB s , where only the salient object is sampled. Part C. Part C is an auxiliary module for training the network of Part B. It is used only in the training phase and is removed in the testing phase. As shown in Part C in <ref type="figure" target="#fig_5">Fig. 4</ref>, we compute the channel-wise sum of the multiplication of S Pred and m S to generate the auxiliary prediction superpixel map AM Pred ? R H?W . In other words, AM Pred is the set of superpixel masks sampled by S Pred . We also generate an auxiliary ground truth superpixel map AM GT ? R H?W from the sm and the ground truth salient object mask I GT . AM GT is the channelwise sum of sm satisfying (mS k ?I GT ) (mS k ) &gt; 0.5, where (.) is the sum of all pixel values. Therefore, as shown in Part C of <ref type="figure" target="#fig_5">Fig. 4</ref>, AM GT is similar to I GT . We use the binary cross-entropy loss between AM Pred and AM GT as an objective function so that Part B can learn to sample only the salient object prototypes. Part D. Part D generates correlation features for the salient objects from PB s and E cr . We treat each of the prototype blocks PB s 1 , PB s 2 , ..., PB s N S as a 1 ? 1 convolution kernel and perform convolution with E cr . As shown in Part D of <ref type="figure" target="#fig_5">Fig. 4</ref>, the correlation maps ? <ref type="bibr">(1/32)</ref> </p><formula xml:id="formula_9">S pred = Sigmoid M LP F PB out ,<label>(3)</label></formula><formula xml:id="formula_10">? R N S ? H 32 ? W 32 , ? (1/16) ? R N S ? H 16 ? W 16 , and ? (1/8) ? R N S ? H 8 ? W 8</formula><p>are generated by channel-wise concatenation of convolution results by multiscale E cr s and each 1 ? 1 kernel. This process makes it possible to exclude non-salient object features and generate correlation maps for salient objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Reliance Selection Module</head><p>As previously mentioned, reliable modality varies depending on the characteristics of the input image. Therefore, we propose the RSM to evaluate the quality of each RGB and the depth features generated from the component prototypes and adaptively weight them in proportion to their reliability. As shown in <ref type="figure" target="#fig_6">Fig. 5  (a</ref> is then fed as input to RSM. The RSM network consists of three convolutional layers. Each layer is composed of convolution, batch normalization, and ReLU <ref type="bibr" target="#b2">[3]</ref> activation. After extracting the features, we flatten the output of the last layer and apply linear function and sigmoid function. In this way, we obtain a vector RelyW ? R 2 of two reliance values RelyW R and RelyW D , lying between 0 and 1, which represent the reliability of each ? RGB and ? D respectively. The more reliable the feature is, the higher the reliance value. Finally, we obtain a reliance-weighted RGB-D feature matrix ? RGBD by multiplying RelyW R and RelyW D with ? RGBD . The equations are as follows:</p><formula xml:id="formula_11">? RGBD = RelyW R ? ? k RGBD , 0 ? k &lt; N s RelyW D ? ? k RGBD , N s ? k &lt; 2 ? N s ,<label>(4)</label></formula><p>where k indicates the channel dimension.</p><p>Ground truth for RSM. To optimize RSM, we generate a ground truth vector RSM gt . The process is demonstrated in <ref type="figure" target="#fig_6">Fig. 5 (b)</ref>. First, we process the channelwise summation of F RSM and apply min-max normalization. Thereby, we obtain a one-channel matrix PseudoGT ? R 1? H 8 ? W 8 which contains the channel-wise statistics for width ? height dimensions in F RSM . Because each channel of F RSM represents a candidate for the correlation map compressed to a small size, the bigger the pixel value is, the more likely that pixel belongs to the salient object. Next, we calculate the L 1 distance between PseudoGT and each channel of F RSM to obtain a distance matrix D RSM ? R (2?Ns)? H 8 ? W 8 . From D RSM , we obtain the mean distance values D R and D D by averaging the values where the channel index k is 0 ? k &lt; N s and N s ? k &lt; 2 ? N s , respectively. Finally, we acquire the two elements of RSM gt by the following equations:</p><formula xml:id="formula_12">RSM gt R = D D /(D R + D D ) (5) RSM gt D = D R /(D R + D D )<label>(6)</label></formula><p>Therefore, RSM gt R and RSM gt D represent the similarity between the RGB correlation maps and PseudoGT, and the similarity between the depth correlation maps and PseudoGT, respectively, which are in other words, the reliability. With the generated RSM gt , RSM is optimized by minimizing the L 1 distance between RelyW and RSM gt .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Model Optimization</head><p>We optimize the model with three object functions L mask , L P SN M and L RSM . First, L mask is the intersection over union (IOU) loss between the predicted saliency map I pred and the ground truth mask I GT , expressed as:</p><formula xml:id="formula_13">L mask (I pred , I GT ) = 1 ? x,y min I pred(x,y) , I GT (x,y) x,y max I pred(x,y) , I GT (x,y) ,<label>(7)</label></formula><p>where (x, y) are the pixel coordinates. Next, as described in section 3.4, L P SN M is the binary cross entropy loss between AM Pred and AM GT . Finally, the loss function L RSM for RSM is defined by measuring the L 1 distance between RelyW and RSM gt . L RSM is expressed as:</p><formula xml:id="formula_14">L RSM (RelyW, RSM gt ) = |RelyW, RSM gt |<label>(8)</label></formula><p>As a result, we combine all these constraints regarding PSNM, RSM, and I pred , and obtain the following objective function L:</p><formula xml:id="formula_15">L total = ? m L mask + ? p L P SN M + ? r L RSM ,<label>(9)</label></formula><p>where ? m , ? p , and ? r denote the weights controlling the contribution of each multiplied loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We perform our experiments on the following five popular RGB-D SOD benchmarks to validate the effectiveness of our proposed method: NJU2K <ref type="bibr" target="#b24">[25]</ref>, NLPR <ref type="bibr" target="#b34">[35]</ref>, STERE <ref type="bibr" target="#b32">[33]</ref>, DES <ref type="bibr" target="#b10">[11]</ref> and SIP <ref type="bibr" target="#b18">[19]</ref>. NJU2K <ref type="bibr" target="#b24">[25]</ref> and NLPR <ref type="bibr" target="#b34">[35]</ref> consists of 1985 and 1000 paired stereoscopic images, respectively. STERE <ref type="bibr" target="#b32">[33]</ref> consists of 1000 stereo images collected from the Internet. DES <ref type="bibr" target="#b10">[11]</ref>, which is also called RGBD135 in some other papers captures seven indoor scenes and contains 135 indoor images acquired by Microsoft Kinect. SIP <ref type="bibr" target="#b18">[19]</ref> is a high-quality dataset with 929 images. To make a fair comparison with previous works, we conduct experiments with two different training setups. First, we use 1485 samples from NJU2K <ref type="bibr" target="#b24">[25]</ref> and 700 samples from NLPR <ref type="bibr" target="#b34">[35]</ref> following the same setup as <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32]</ref>. Second, we follow the same training settings as existing works <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b22">23]</ref>, using 800 samples from DUT-RGBD <ref type="bibr" target="#b35">[36]</ref>, 1485 samples from NJU2K <ref type="bibr" target="#b24">[25]</ref> and 700 samples from NLPR <ref type="bibr" target="#b34">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metrics</head><p>We evaluate the performance of our method and other methods using five widely used evaluation metrics: the mean F-measure (F ? ) <ref type="bibr" target="#b0">[1]</ref>, mean absolute error (MAE, M ) <ref type="bibr" target="#b3">[4]</ref>, S-measure (S ? ) <ref type="bibr" target="#b16">[17]</ref>, E-measure (E ? ) <ref type="bibr" target="#b17">[18]</ref>, and precision-recall (PR) curve. <ref type="table">Table 1</ref>. Quantitative comparison on five representative large-scale benchmark datasets. ? indicates that higher is better and ? indicates that lower is better. * denotes the models are trained on NJU2K <ref type="bibr" target="#b24">[25]</ref> and NLPR <ref type="bibr" target="#b34">[35]</ref>; the rest are trained on DUT-RGBD <ref type="bibr" target="#b35">[36]</ref>, NJU2K <ref type="bibr" target="#b24">[25]</ref>, and NLPR <ref type="bibr" target="#b34">[35]</ref>. The best and second best are highlighted in red and blue, respectively.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p>We set the number of superpixels N S to 100 and a k of EdgeConv to 10. We also set ? m , ? p , and ? r in Equation 9 to 1, 1, and 10, respectively, for balanced training. We implement the proposed method using the open deep learning framework PyTorch. The backbone network is equipped with VGG-16 <ref type="bibr" target="#b39">[40]</ref>, with initial parameters pre-trained in ImageNet <ref type="bibr" target="#b13">[14]</ref>. All images are uniformly resized to 352 ? 352 pixels for training and inferring. For network training, we used the Adam optimizer <ref type="bibr" target="#b25">[26]</ref> with ? 1 = 0.9, ? 2 = 0.999, and = 10 ?8 . The learning rate decayed from 8 ? 10 ?5 to 8 ? 10 ?6 with the cosine annealing scheduler <ref type="bibr" target="#b30">[31]</ref>. The total number of epochs was set to 200 with batch size 16 with two NVIDIA RTX 3090 GPUs for all experiments in this study. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with State-of-the-Art Methods</head><p>Quantitative comparison. <ref type="table">Table 1</ref> shows our quantitative performance compared with 10 recently published state-of-the-art RGB-D SOD methods, DMRA <ref type="bibr" target="#b35">[36]</ref>, CPFP <ref type="bibr" target="#b50">[51]</ref>, CIM <ref type="bibr" target="#b49">[50]</ref>, CoN <ref type="bibr" target="#b23">[24]</ref>, CMWN <ref type="bibr" target="#b28">[29]</ref>, GAR <ref type="bibr" target="#b9">[10]</ref>, CasG <ref type="bibr" target="#b31">[32]</ref>, ATS <ref type="bibr" target="#b48">[49]</ref>, D2F <ref type="bibr" target="#b41">[42]</ref>, DCF <ref type="bibr" target="#b22">[23]</ref>, on five popular benchmark datasets. Because the training data in these comparative studies differ slightly, as some used NJU2K <ref type="bibr" target="#b24">[25]</ref> and NLPR <ref type="bibr" target="#b34">[35]</ref> whereas others also used DUT-RGBD <ref type="bibr" target="#b35">[36]</ref>, we show the performance on both settings for a fair comparison. It is observed that our model notably outperforms the other methods. In particular, our model exceeds the counterpart methods by a dramatic margin in terms of all four evaluation metrics on NJU2K <ref type="bibr" target="#b24">[25]</ref> and DES <ref type="bibr" target="#b10">[11]</ref>, which are considered more challenging than to the others due to the low contrast and objects cluttering the background. This result further indicates that our network can perform well on various complex scenes. Moreover, we plotted the PR curves in <ref type="figure" target="#fig_7">Fig. 6</ref> for a better comparison. The results show that ours lies above most of the methods compared. Qualitative comparison. In <ref type="figure" target="#fig_8">Fig. 7</ref>, we compare our qualitative results to those of eight top-ranking RGB-D SOD approaches on several challenging scenarios, including low contrast, reflection, thin objects, multiple objects, and long distance. Particularly for scenes with complex RGB maps caused by cluttered objects and patterns in the background (e.g., the second, third, and fourth row), our model utilized more information from the reliable depth maps to generate an accurate  saliency map. Furthermore, the accuracy of such scenes is boosted by the PSNM, which effectively discriminates the foreground from the background. Similarly, our model can handle samples with depth maps that are ambiguous because of light reflection and long distance (e.g., the sixth and eighth rows) because our model adaptively decides to rely on the more accurate RGB maps. Furthermore, it is observed that our model is robust to scenes with multiple objects (e.g., the fifth and ninth rows).</p><formula xml:id="formula_16">? S? ? F? ? M ? E? ? S? ? F? ? M ? E? ? S? ? F? ? M ? E? ? S? ? F? ? M ? E? ? S? ? F? ? M ? (a</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Analysis</head><p>We verify the performance of our model through various ablation studies. <ref type="table" target="#tab_3">Table 2</ref> shows the effects of the proposed modules in various combinations. RE and DE in <ref type="table" target="#tab_3">Table 2</ref> represent the VGG-16 <ref type="bibr" target="#b39">[40]</ref> encoders for the RGB images and depth maps, respectively. In addition, RS and DS are the set of FFM, PGM, and PSNM in the RGB and depth streams, respectively. The proposed RSM only applies when both RE and DE are used. <ref type="figure">Fig 8 also</ref> shows the performance of our model according to the number of superpixels. Impact of prototype sampling. As shown in <ref type="table" target="#tab_3">Table 2</ref>, (d) and (e), to which the prototype sampling method is applied, our method achieves better performance than (a) and (b) on all datasets. This is because the encoder-decoder-based network delivers not only the features for the salient object but also the background and non-salient object feature information extracted from the encoder to the decoder, preventing accurate mask generation. In contrast, the proposed SPSN model performs well because the network can selectively extract only the important salient object feature information by PSNM. Furthermore, <ref type="figure">Fig. 9</ref> shows the salient prototype sampling results of the proposed method. Impact of RSM. When the proposed RSM module is applied, as shown in <ref type="table" target="#tab_3">Table 2</ref> (c) and (f), it shows significant performance improvement when the RGB image and depth map are used together. The performance improves because RSM selects feature maps generated from RGB and depth streams based on their reliability. Therefore, as shown in <ref type="figure">Fig. 9</ref>, RelyW R is small for RGB images with camouflaged objects, and RelyW D is small for low-quality depth maps. This structure shows that the model reduces the biased dependence and makes it robust to low-quality depth maps. Number of superpixels. We conduct ablation studies to observe how the MAE and F ? values change according to the number of superpixels N S . <ref type="figure">Fig. 8</ref> shows the changes in performance using the NJU2K <ref type="bibr" target="#b24">[25]</ref>, STERE <ref type="bibr" target="#b32">[33]</ref>, and DES <ref type="bibr" target="#b10">[11]</ref> datasets according to the number of superpixels. As shown in <ref type="figure">Fig. 8</ref>, the proposed model performs best near N S = 100. Additionally, if N S is too small or too large, the performance will decrease. This degraded performance results because if N S is too small, the superpixel masks cannot effectively separate salient and nonsalient objects and cannot provide a sufficient number of component prototypes. Conversely, if N S is too large, it is difficult to create coherent features for the salient object by creating too small superpixel masks that are too small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we aim to segment salient objects by designing an SPSN, which suppresses the effects of background objects and effectively takes advantage of RGB and depth maps. Specifically, our network is composed of four novel modules-the FFM, which fuses the multiscale features extracted from the encoder; the PGM, which renders the fused feature maps to component prototypes; the PSNM, which discriminates the prototype that belongs to the salient object; and the RSM, which adaptively selects the contribution of RGB and depth features.</p><p>The results demonstrate the outstanding improvement of our method over the previous studies, indicating that our model can capture salient objects in various challenging scenes. Furthermore, extensive ablation studies show the contribution and effectiveness of each of the proposed modules.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The overall flow of the proposed model. Our model generates and samples component prototypes from superpixel maps. It also compares the reliability of correlation maps created from component prototypes to generate the predicted mask.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2</head><label>2</label><figDesc>shows the overall architecture of the proposed SPSN. The proposed model uses an RGB image I RGB , depth map I D , and their superpixel maps S RGB , S D as inputs. Our model is composed primarily of four parts: the feature fusion module (FFM), prototype generating module (PGM), prototype sampling network module (PSNM), and reliance selection module (RSM). The SPSN also has two encoders for RGB images and depth maps and one decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Overall architecture of the superpixel prototype sampling network (SPSN). The proposed network has one RGB encoder and one depth encoder. Our model consists of a feature fusion model (FFM) for effective fusion of encoder features, prototype generating module (PGM) for prototype extraction, prototype sampling network module (PSNM) for prototype sampling, and reliance selection module (RSM) for reliability selection of RGB and depth features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Structure of (a) FFM and (b) PGM. The FFM fuses the multiscale features of the encoder. The PGM generates a prototype block from the superpixel mask sm and the fusion feature F generated from the FFM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Structure of the PSNM, composed primarily of four subparts. The PSNM selectively samples only prototypes corresponding to salient objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>(a) Structure of the RSM and (b) ground truth generating process for the RSM. The RSM aims to discriminate the reliability of each RGB feature and depth feature to adaptively balance the contribution of the two when generating the final saliency map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Precision-recall curve comparison on five datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Qualitative comparison with eight state-of-the-art methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>24 Fig. 9 .</head><label>249</label><figDesc>Visualization of our results in several challenging situations. AM pred and AMGT are described in Section 3.4, and RelyWR and RelyWD are described in Section 3.5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>), the outputs of PSNM ? RGB and ? D processed in each encoder level are concatenated in the channel dimension for each level. These multiscale concatenated features ? RGBD(1/8) ? R (2?Ns)? H 8 ? W 8 , ? RGBD(1/16) ? R (2?Ns)? H 32 are then fused by applying 1 ? 1 convolution, upsampling, and element-wise summation. This fusion technique is mostly similar to FFM. The fused feature F RSM ? R (2?Ns)? H</figDesc><table><row><cell>16 ? W 16 ,</cell></row><row><cell>and ? RGBD(1/32) ? R (2?Ns)? H 32 ? W</cell></row><row><cell>8 ? W 8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Performance with different combinations of our contributions. RE and DE represent the encoders for the RGB and depth, respectively. RS and DS are the set of FFM, PGM, and PSNM in the RGB and depth streams, respectively.</figDesc><table><row><cell>Index</cell><cell>Method RE DE RS DS RSM E?</cell><cell>NJU2K [25]</cell><cell>NLPR [35]</cell><cell>STERE [33]</cell><cell>DES [11]</cell><cell>SIP [19]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>) .872 .836 .848 .058 .871 .852 .793 .044 .883 .852 .843 .061 .843 .775 .764 .048 .825 .738 .734 .069 (b) .904 .863 .869 .051 .912 .877 .842 .037 .908 .870 .864 .052 .888 .831 .820 .038 .868 .796 .793 .062 (c) .934 .888 .887 .044 .952 .905 .888 .028 .934 .889 .883 .042 .932 .887 .877 .031 .913 .855 .853 .057 (d) .937 .903 .901 .039 .950 .915 .896 .026 .938 .902 .894 .037 .950 .905 .900 .027 .918 .870 .878 .051 (e) .944 .912 .910 .035 .954 .919 .902 .025 .940 .905 .898 .036 .963 .922 .917 .022 .926 .881 .887 .047 (f) .950 .918 .920 .032 .958 .923 .910 .023 .943 .907 .900 .035 .974 .937 .936 .016 .934 .892 .899 .042</figDesc><table><row><cell></cell><cell>0.0375</cell><cell></cell><cell></cell><cell>0.904</cell><cell></cell><cell>0.0345</cell><cell></cell><cell></cell><cell>0.92</cell><cell>0.018</cell><cell>0.9395</cell></row><row><cell></cell><cell>0.037</cell><cell></cell><cell></cell><cell>0.902</cell><cell></cell><cell>0.034</cell><cell></cell><cell></cell><cell>0.919</cell><cell>0.939</cell></row><row><cell>MAE</cell><cell>0.0345 0.0365 0.036 0.035 0.0355</cell><cell></cell><cell></cell><cell>0.894 0.9 0.898 0.896</cell><cell>MAE</cell><cell>0.031 0.0335 0.033 0.0315 0.032 0.0325</cell><cell></cell><cell></cell><cell>0.914 0.917 0.918 0.915 0.916</cell><cell>MAE</cell><cell>0.016 0.0165 0.0175 0.017</cell><cell>0.9355 0.936 0.9365 0.937 0.9375 0.938 0.9385</cell></row><row><cell></cell><cell>0.0335 0.034</cell><cell cols="2">NJU2K</cell><cell>0.89 0.892</cell><cell></cell><cell>0.03 0.0305</cell><cell cols="2">STERE</cell><cell>0.912 0.913</cell><cell>0.0155</cell><cell>DES</cell><cell>0.9345 0.935</cell></row><row><cell></cell><cell></cell><cell>25</cell><cell>50</cell><cell>75 100 125 150 175 200</cell><cell></cell><cell></cell><cell>25</cell><cell>50</cell><cell>75 100 125 150 175 200</cell><cell>25 50 75 100 125 150 175 200</cell></row></table><note>MAE Fig. 8. Comparison of performance characteristics with respect to NS for the NJU2K [25], STERE [33], and DES [11] datasets.</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1597" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Slic superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>S?sstrunk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Agarap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08375</idno>
		<title level="m">Deep learning using rectified linear units (relu)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Salient object detection: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5706" to="5722" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">M 3 net: Multi-scale multi-path multi-modal fusion network and example application to rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4911" to="4916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attention-aware cross-modal cross-level fusion network for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6821" to="6826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Progressively complementarity-aware fusion network for rgbd salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3051" to="3060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Three-stream attention-aware network for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2825" to="2835" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Progressively guided alternate refinement network for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="520" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Depth enhanced saliency detection method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of international conference on internet multimedia computing and service</title>
		<meeting>international conference on internet multimedia computing and service</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="23" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Going from rgb to rgbd saliency: A depth-guided transformation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3627" to="3639" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Saliency detection for stereoscopic images based on depth confidence analysis and multiple cues fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="819" to="823" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Depth really matters: Improving visual salient region detection with depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Desingh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC. pp</title>
		<imprint>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Salient objects in clutter: Bringing salient object detection to the foreground</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="186" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Structure-measure: A new way to evaluate foreground maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4548" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Enhanced-alignment measure for binary foreground map evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10421</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rethinking rgb-d salient object detection: Models, data sets, and large-scale benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on neural networks and learning systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2075" to="2089" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Jl-dcf: Joint learning and densely-cooperative fusion framework for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3052" to="3062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Salient object detection for rgb-d image via saliency evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Calibrated rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9471" to="9481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Accurate rgb-d salient object detection via collaborative learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="52" to="69" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XVIII 16</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Depth saliency based on anisotropic center-surround difference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on image processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1115" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Depth matters: Influence of depth cues on visual saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Katti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yadati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kankanhalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="101" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adaptive prototype learning and allocation for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8334" to="8343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cross-modal weighting network for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="665" to="681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Part-aware prototype network for few-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="142" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<title level="m">Sgdr: Stochastic gradient descent with warm restarts</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cascade graph neural networks for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="346" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Leveraging stereopsis for saliency analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="454" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Saliency detection via global context enhanced feature fusion and edge weighted loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.06550</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rgbd salient object detection: a benchmark and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="92" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Depth-induced multi-scale recurrent attention network for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7254" to="7263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Rgbd salient object detection via deep fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2274" to="2285" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Depth-aware salient object detection and segmentation via multiscale discriminative saliency fusion and bootstrap learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Le Meur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4204" to="4216" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep rgb-d saliency detection with depth-sensitive attention and automatic multi-modal fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1407" to="1417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Panet: Few-shot image semantic segmentation with prototype alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Liew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9197" to="9206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Salient object detection with pyramid attention and salient edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1448" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Transactions On Graphics (tog)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7354" to="7363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Ucnet: Uncertainty inspired rgb-d saliency detection via conditional variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8582" to="8591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Asymmetric two-stream architecture for accurate rgb-d saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="374" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Select, supplement and focus for rgb-d saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3472" to="3481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Contrast prior and fluid pyramid integration for rgbd salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3927" to="3936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Egnet: Edge guidance network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8779" to="8788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Saliency detection by multi-context deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1265" to="1274" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
