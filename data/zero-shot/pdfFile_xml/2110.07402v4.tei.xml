<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Supervised Learning by Estimating Twin Class Distributions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
							<email>wang-f20@mails.</email>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
							<email>kongtao@bytedance.com</email>
							<affiliation key="aff1">
								<orgName type="department">ByteDance AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
							<email>cxrfzhang@tongji.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">Tongji University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaping</forename><surname>Liu</surname></persName>
							<email>hpliu@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">ByteDance AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Supervised Learning by Estimating Twin Class Distributions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present TWIST, a simple and theoretically explainable self-supervised representation learning method by classifying large-scale unlabeled datasets in an end-to-end way. We employ a siamese network terminated by a softmax operation to produce twin class distributions of two augmented images. Without supervision, we enforce the class distributions of different augmentations to be consistent. However, simply minimizing the divergence between augmentations will cause collapsed solutions, i.e., outputting the same class probability distribution for all images. In this case, no information about the input image is left. To solve this problem, we propose to maximize the mutual information between the input and the class predictions. Specifically, we minimize the entropy of the distribution for each sample to make the class prediction for each sample assertive and maximize the entropy of the mean distribution to make the predictions of different samples diverse. In this way, TWIST can naturally avoid the collapsed solutions without specific designs such as asymmetric network, stop-gradient operation, or momentum encoder. As a result, TWIST outperforms state-of-the-art methods on a wide range of tasks. Especially, TWIST performs surprisingly well on semi-supervised learning, achieving 61.2% top-1 accuracy with 1% ImageNet labels using a ResNet-50 as backbone, surpassing previous best results by an absolute improvement of 6.2%. Codes and pre-trained models are given on: https://github.com/bytedance/TWIST</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep neural networks learned from large-scale datasets have powered many aspects of machine learning. In computer vision, the neural networks trained on the ImageNet dataset <ref type="bibr" target="#b18">[18]</ref> can perform better than or as well as humans * This work was done during an internship at ByteDance. Correspondence to: Tao Kong and Huaping Liu in image classification <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b50">50]</ref>. The obtained representations can also be adapted to other downstream tasks <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b48">48]</ref>. However, learning from large-scale labeled data requires expensive annotations, making it difficult to scale.</p><p>Recently, self-supervised learning has achieved remarkable progress and largely closed the gap with supervised learning. The contrastive learning approaches <ref type="bibr">[11-13, 23, 24, 56]</ref> learn representations by maximizing the agreement of different augmentations and pushing away the representations of different images based on the instance discrimination pretext task <ref type="bibr" target="#b56">[56]</ref>. BYOL <ref type="bibr" target="#b23">[23]</ref> and SimSiam <ref type="bibr" target="#b12">[13]</ref> propose to abandon the negative samples, and design the asymmetric architecture and momentum encoder (or stopgradient) to avoid collapsed solutions. Clustering-based methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9]</ref> usually employ the clustering algorithms to generate supervision signals to guide the learning process, and constitute an unsupervised classification task.</p><p>This work focuses on the unsupervised classification pretext task and explores learning representations by classifying unlabeled images end-to-end. Without labels as accurate supervision, we learn from recent successful selfsupervised methods <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b24">24</ref>] that adequately utilize the consistency between augmentations. In the unsupervised classification pretext tasks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>, this is reflected by requiring different augmentations to have identical predictions. However, simply minimizing the divergence between the predictions of different augmentations will cause the collapsed solution problem, i.e., outputting the same class for all images. To solve this problem, clustering-based methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b37">37]</ref> adopt the clustering techniques such as K-means <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b37">37]</ref>, and Sinkhorn-Knopp algorithm <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9]</ref> to generate target assignments for each image as supervision. DINO <ref type="bibr" target="#b9">[10]</ref> proposes sharpening and centering technique, together with momentum encoder, to avoid generating collapsed solutions. Recent Self-Classifier <ref type="bibr" target="#b0">[1]</ref> avoids collapsed solutions by asserting the uniform prior to the predicted classes.</p><p>In this paper, we propose to solve the collapsed solution problem by maximizing the mutual information between in-ground-truth: steam locomotive ground-truth: barn ground-truth: spoonbill ground-truth: water ouzel 1.000 1.000 <ref type="figure">Figure 1</ref>. Examples of unsupervised top-3 predictions of TWIST. The predicted class indices are mapped to the labels in ImageNet by Kuhn-Munkres algorithm <ref type="bibr" target="#b34">[34]</ref>. Note that the labels are only used to map our predictions to the ImageNet labels, we do not use any label to participate in the training process. More examples are given in Appendix.</p><p>put images and output predictions, and our method is named TWIST (Twin Class Distribution Estimation). The motivation is based on the observation that the collapsed solution (i.e., outputting the same class for all images) will cause the class predictions to carry no information about the input images. Thus an intuitive and straightforward way is to maximize the shared information between image X and class prediction Y . The mutual information maximization between input and class predictions is a historical method which can be traced back to 30 years ago -Unsupervised classifiers, mutual information and 'phantom targets' <ref type="bibr" target="#b5">[6]</ref>. To the best of our knowledge, we are the first to successfully apply it to large-scale representation learning. Specifically, we minimize the entropy of the class distribution for each sample to make the class distribution sharp and maximize the entropy of the mean class distribution to make the predictions for different samples diverse. We theoretically prove that maximizing the mutual information is equal to the combination of the above two intuitive objectives. The above optimizing problem can be achieved by a unified objective function -TWIST loss. This makes TWIST more straightforward, eliminating the reliance on clustering techniques or complicated architecture designs. Besides, compared with clustering-based methods and DINO which explicitly generate targets for images, the target of TWIST is like a 'phantom' that is not known before optimizing and is decided by the cooperative learning of different views.</p><p>TWIST can not only successfully classify unlabeled images ( <ref type="figure">Fig. 1</ref>), but also derives high-quality features. We evaluate the performance of TWIST on many downstream tasks, surpassing state-of-the-art methods on a wide range of tasks including semi-supervised learning (+6.2 top-1 accuracy on 1% labeled data compared with previous best method), linear classification, transfer learning and dense tasks. These results show that TWIST successfully connects unsupervised classification and representation learning, and could serve as a strong baseline for both purposes. Overall, the contributions are summarized as follows:</p><p>? We propose a straightforward, theoretical explainable self-supervised learning method by classifying largescale unlabeled datasets, eliminating the requirements of complicated architecture designs.</p><p>? We show that TWIST itself is an efficient unsupervised classifier. Without special adaptation, we achieve the best unsupervised classification results compared with other methods.</p><p>? The representation quality of TWIST has been evaluated on various downstream tasks, including ImageNet linear and fine-tuning settings, semi-supervised learning, and dense predictive tasks, achieving state-of-theart results on most of them. The results indicate that TWIST can serve as a general pre-training method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Self-supervised Learning has been a promising paradigm to learn useful image representations. Previous selfsupervised methods try to design different handcrafted auxiliary tasks. Examples include context prediction <ref type="bibr" target="#b19">[19]</ref>, colorization <ref type="bibr" target="#b62">[62]</ref>, context encoder <ref type="bibr" target="#b47">[47]</ref>, jigsaw puzzle <ref type="bibr" target="#b45">[45]</ref>, and rotation prediction <ref type="bibr" target="#b22">[22]</ref>, etc. Recently, contrastive learning has drawn much attention. Representative methods include Instance Discrimination <ref type="bibr" target="#b56">[56]</ref>, MoCo <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b24">24]</ref>, and SimCLR <ref type="bibr" target="#b10">[11]</ref>. Contrastive methods learn an embedding space where features of different augmentations from the same image are attracted, and features of different images are separated. BYOL <ref type="bibr" target="#b23">[23]</ref> and SimSiam <ref type="bibr" target="#b12">[13]</ref> propose to abandon the negative samples and design some special techniques such as asymmetric architecture, momentum encoder and stop gradients to avoid the collapsed solution. Barlow Twins <ref type="bibr" target="#b61">[61]</ref> and VICReg <ref type="bibr" target="#b2">[3]</ref> propose to learn informative representations by reducing the redundancy or covariance of different dimensions.</p><p>The clustering-based methods <ref type="bibr">[2, 7-9, 37, 58, 59]</ref> have also exhibited remarkable progress. They usually use a clustering tool to generate pseudo-labels for images and then classify the images with the generated pseudo-labels. The two processes alternate with each other. DeepCluster <ref type="bibr" target="#b6">[7]</ref> uses the K-means algorithm to generate pseudolabels for every epoch. SwAV <ref type="bibr" target="#b8">[9]</ref> uses the Sinkhorn-Knopp algorithm <ref type="bibr" target="#b1">[2]</ref> to generate soft pseudo-labels and updates the pseudo-labels for every iteration. The most recent DINO <ref type="bibr" target="#b9">[10]</ref> updates pseudo-targets using the output of the momentum teacher together with the sharpening and centering operations. Self-Classifier <ref type="bibr" target="#b0">[1]</ref> proposes an end-toend method to classify unlabeled datasets, which is closely related to TWIST, while TWIST uses a different way to prevent the collapsed solution problem. Detailed comparisons with Self-Classifier are shown in Sec 4. SCAN <ref type="bibr" target="#b53">[53]</ref> advocates a two-step approach to mainly focus on unsupervised classification task. The mutual information maximization between input and class predictions is a historical method. Bridle et al <ref type="bibr" target="#b5">[6]</ref> first propose to utilize it as a clustering technique. IM-SAT <ref type="bibr" target="#b28">[28]</ref> uses it to encourage the predicted representations of augmented data to be close to those of the original data, and in the meantime regularizes the information dependency. IIC <ref type="bibr" target="#b30">[30]</ref> proposes a clustering objective to maximize the mutual information of different views. Previous work of Deep InfoMax <ref type="bibr" target="#b27">[27]</ref> proposes to learn representations by maximizing mutual information between different layers of features. The differences between TWIST and Deep Infomax are apparently: (1) The task of TWIST is to classify images by exploring their semantic relations, while the task of Deep InfoMax is to maximize the information between different neural layers. (2) TWIST measures the information between image and a discrete random variable, instead of high-dimensional continuous representation in Deep In-foMax. (3) We do not require the neural estimator <ref type="bibr" target="#b3">[4]</ref> to estimate the mutual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Our goal is to learn an end-to-end unsupervised classification network to make accurate predictions and learn good representations. Without labels, we design the TWIST loss to make the predictions of two augmented images be recog-nized as the same class. In the meantime, TWIST loss regularizes the class distribution to make it sharp and diverse, which helps the network maximize the shared information between input images and output predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Formulation</head><p>Given an unlabeled dataset S, we randomly sample a batch of images X ? S with a batch-size of B, and generate two augmented version X 1 and X 2 according to a predefined set of image augmentations. The two augmented images are fed into a siamese neural network composed of a backbone f ? with parameters ?, and a projection head terminated by a softmax operation g ? with parameters ?. The outputs of the neural networks g ? ? f ? are two probability distributions over C categories P k = g ? (f ? (X k )) (k ? {1, 2}). The process is shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. Note that P k ? R B?C , and P k i denotes the i-th row of P k , i.e., probability distribution of the i-th sample.</p><p>With the probability distributions of two augmented views P 1 and P 2 , we define the learning objective as the symmetric form</p><formula xml:id="formula_0">L(P 1 , P 2 ) = 1 2B B i=1 (DKL(P 1 i ||P 2 i ) + DKL(P 2 i ||P 1 i )) consistency term + ? 2 2 k=1 1 B B i=1 H(P k i ) sharpness term ? ? 2 2 k=1 H( 1 B B i=1 P k i ) diversity term ,<label>(1)</label></formula><p>where D KL (?||?) denotes the Kullback-Leibler divergence <ref type="bibr" target="#b35">[35]</ref> between two probability distributions. H(?) denotes the Entropy <ref type="bibr" target="#b49">[49]</ref> of a specific probability distribution. ? and ? are hyper-parameters to balance the two terms. Specifically, minimizing the consistency term makes the predictions of different views consistent, i.e., different augmentations of the same image are required to be recognized as the same class. For the sharpness term, we minimize the entropy of class distribution for each sample to regularize the output distribution to be sharp, which makes each sample have a deterministic assignment (i.e., one-hot vector in the ideal case). Besides, features of samples assigned to the same category will be more compact. For the diversity term, we try to make the predictions for different samples be diversely distributed to avoid the network assigning all images to the same class. This is achieved by maximizing the entropy of the mean distribution across different samples H( 1</p><formula xml:id="formula_1">B B i=1 P i ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Theoretical Explanation</head><p>In this part, we explain TWIST loss as an objective that optimizes two things: (1) minimizing the class prediction disagreements between augmentations through the consistency term. (2) maximizing the mutual information between the input image and the output class probability distributions, which is achieved by the sharpness term and diversity term. The mutual information between a random variable X representing input image and the predicted class Y is</p><formula xml:id="formula_2">?I(X, Y ) = H(Y |X) ? H(Y ) = E x ? y p(y|x)logp(y|x) ? H(Y ) ? ? 1 |X | x?X y p(y|x)logp(y|x) sharpness term ? H( 1 |X | x?X p(Y |x)) diversity term ,<label>(2)</label></formula><p>where H(Y |X) is the Conditional Entropy. The approximation is derived from the Monte Carlo estimation of the expectation on x. From Eq. 2, we observe that the first term is the sharpness term, and the second term is the diversity term. The Monte Carlo estimation of the diversity term is because of the following relation</p><formula xml:id="formula_3">p(Y ) = x p(Y |x)p(x)dx ? 1 |X | x?X p(Y |x).<label>(3)</label></formula><p>In this perspective, the TWIST loss can be interpreted as minimizing the disagreement of different augmentations and simultaneously maximizing the mutual information between the input image and output class predictions, when ? = ? = 1. From this view, we show the explanation why the TWIST loss can avoid the collapsed solutions. Specifically, TWIST loss requires the output class prediction Y to preserve information with the input image X as much as possible. When all images are predicted to the same class, the information shared with the input image will vanish.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Amplifying Variance for Better Optimization</head><p>In practice, directly optimizing the TWIST loss will derive sub-optimal solutions. Specifically, we find that the consistency term and the sharpness term are easy to minimize while the diversity term is difficult to maximize. We visualize the standard deviations of each row and each column of the features before softmax, as illustrated in <ref type="figure">Fig. 3</ref>. The column standard deviation keeps small during the training process, which makes the probability of each class tend to be similar across different samples in a mini-batch. This will cause the low diversity of classification results. To solve the problem, we add a batch normalization <ref type="bibr" target="#b29">[29]</ref> before the softmax to amplify the variance of probabilities in each class to force them to be separated. By adding the batch normalization layer, the diversity term is well optimized. The batch normalization before softmax brings about 5% improvements in ImageNet linear classification. More analyses and experiments are shown in the Ablation section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methodology Comparisons</head><p>In this section, we discuss the comparisons of TWIST to the most related works in details.</p><p>DINO <ref type="bibr" target="#b9">[10]</ref> utilizes the class distributions of the momentum encoder to form a self-distillation process, and achieves good performance. TWIST shares similarity with DINO for that TWIST also learns the class probability distributions. However, TWIST is different from DINO in the following aspects: (1) different objectives, (2) different mechanisms to avoid collapsed solutions, and (3) different training manners. We give the objective function of DINO as</p><formula xml:id="formula_4">LDINO = CE(stop(Pt(x1)), Ps(x2)) = DKL(stop(Pt(x1))||Ps(x2)) + h h h h h h h H(stop(Pt(x1))),<label>(4)</label></formula><p>where stop means the gradients is not back-propagated.</p><p>Here we re-write the DINO loss by the sum of KLdivergence plus a sample-wise entropy term, which we intend to make a clear comparison with TWIST loss in Eq. 5. We see that DINO loss is equal to the KL-divergence term in TWIST, without giving the loss-guided constrain to the sample-wise entropy (the slash term). In contrast, TWIST minimizes the entropy of each sample and simultaneously maximizes the diversity. DINO incorporates sharpening and centering operations to avoid assigning all samples to the same class, and it relies on momentum encoder to enable training. TWIST can work naturally with the unified loss function, eliminating the reliance on momentum encoder. TWIST uses a more explainable and straightforward way to avoid collapsed solutions. Empirically, TWIST performs much better at semisupervised settings and fine-tuning settings while achieving competitive results with DINO in the linear setting.</p><p>SwAV <ref type="bibr" target="#b8">[9]</ref> uses the Sinkhorn-Knopp algorithm <ref type="bibr" target="#b1">[2]</ref> to generate soft pseudo-labels for samples in a mini-batch. The differences between TWIST and SwAV are distinct. TWIST does not rely on the Sinkhorn-Knopp algorithm or any process to generate pseudo labels. Instead, the TWIST loss naturally helps the network learn meaningful assignments and representations.</p><p>Self-Classifier <ref type="bibr" target="#b0">[1]</ref> is an end-to-end self-supervised method that designs a classification network without relying on momentum-encoder or clustering techniques. The main differences between TWIST and Self-Classifier lie in the loss function. The loss function of self-classifier is</p><formula xml:id="formula_5">L SC (x 1 , x 2 ) = ? C y=1 p(x 2 |y)log(p(y|x 1 )).</formula><p>Training the loss function is equal to optimizing the cross entropy between p(y|x 1 ) and p(y|x 2 ), with the assumption that p(y) is a uniform distribution. The calculation about p(x 2 |y) include the softmax operation across samples in a mini-batch, which can be regarded as setting the iterations of Sinkhorn-Knopp algorithm to 1 <ref type="bibr" target="#b9">[10]</ref>. In contrast, our TWIST loss can measure and maximize the mutual infor-mation between x and y, making the method explainable. Moreover, TWIST performs much better than Self-Classifier on all benchmarks.</p><p>Barlow Twins <ref type="bibr" target="#b61">[61]</ref> proposes to learn representations by decorrelating different feature dimensions. TWIST shares the clean architectures as Barlow Twins. However, the learning objective and principle are quite different. We aim at learning an unsupervised classifier by our proposed loss function, while the objective of Barlow Twins is to minimize the redundancy between feature dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Main Results</head><p>We evaluate the performances of TWIST on ImageNet un/semi-supervised classification, transfer learning, and a wide range of downstream tasks. We set C = 4096 for the representation learning and C = 1000 for the ImageNet unsupervised classification in accordance with the standard ImageNet class number. We adopt the multi-crop <ref type="bibr" target="#b8">[9]</ref> augmentation strategy and also report performances without multi-crop for fair comparisons. We set ? = ? = 1 for ResNet and ? = 0.4, ? = 1 for ViT. More implementation details can be found in Appendix. For all downstream tasks, we strictly follow the common evaluation procedures. All TWIST models are trained on the train set of ImageNet ILSVRC-2012 which has ?1.28 million images <ref type="bibr" target="#b18">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Training Strategy</head><p>Self-labeling for ResNets: For ResNets, the multi-crop strategy helps improve the linear classification performance from 72.6% to 74.3%, shown in Tab. 9. However, compared with the performance improvement of SwAV (from 71.8% to 75.3%), the performance gain of TWIST is much smaller (3.5% v.s. 1.7%). Such phenomenon has also been observed by <ref type="bibr" target="#b9">[10]</ref>. Specifically, SwAV uses the global crops to generate relatively accurate pseudo-labels as supervision to train the local crops. However, in our method, the global crops and local crops are regarded equally. Thus the noisy local crops can also affect the accurate predictions of global crops, which will not happen in methods of SwAV and DINO. To take full advantage of the multi-crop strategy, we add a self-labeling stage after the regular training. Specifically, we use the outputs of the global crops as supervision to train other crops. Different with SwAV: (1) we directly use our network outputs as supervision, instead of the outputs of the Sinkhorn-Knopp algorithm, (2) we only use the samples in a mini-batch whose confidences surpass a predefined threshold. With only 50 epochs of self-labeling after finishing the regular training, we have another 1.2% performance gains (from 74.3% to 75.5%). We empirically show that the proposed self-labeling could not improve the performance of SwAV <ref type="bibr" target="#b8">[9]</ref> or DINO <ref type="bibr" target="#b9">[10]</ref>.</p><p>Momentum Encoder for ViT: For Vision Transformers <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b52">52]</ref>, we do not use the self-labeling process. In-stead, we adopt the momentum encoder design, which is widely adopted to train ViT-based models <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14]</ref>. Specifically, one tower of our siamese network is updated by the exponential moving average of the parameters from the other tower, similar as <ref type="bibr" target="#b24">[24]</ref> and <ref type="bibr" target="#b23">[23]</ref>. The whole network is updated by the TWIST loss. Although we use the momentum encoder as the default setting for ViT backbones, TWIST using ViT as backbone can also work without it and achieves 72.5% ImageNet Top-1 linear accuracy for Deit-S 300 epochs. The momentum encoder is only for accuracy gains. We do not use it for CNNs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Semi/Fully-supervised Fine-tuning</head><p>We fine-tune the pre-trained TWIST model on a subset of ImageNet, following the standard procedure <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">23]</ref>. From Tab. 1, we see that TWIST outperforms all other state-ofthe-art methods by large margins. With only 1% of labeled data, TWIST achieves 61.2% top-1 accuracy with ResNet-50, surpassing the previous best result by 6.2%. The trend is preserved when the backbone becomes larger. For example, TWIST achieves 67.2% (+5.0%) top-1 accuracy with ResNet-50w2. With 10% of labeled data, TWIST still achieves the best result.</p><p>We also fine-tune the pre-trained TWIST model on the full ImageNet. The results are shown in the last two columns of Tab. 1. With our pre-trained model as initialization, ResNet-50 achieves 78.4% top-1 accuracy, surpassing the model trained from scratch by a large margin (+1.9%).</p><p>Unsupervised Classification: Finally, we evaluate the unsupervised classification results using no label as supervision. For evaluation, the outputs of TWIST are directly mapped to the real labels by the Kuhn-Munkres <ref type="bibr" target="#b34">[34]</ref>   <ref type="table">Table 3</ref>. Linear classification results. We report and compare results with different backbones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Linear Classification</head><p>We evaluate the performance of linear classification on ImageNet. Specifically, we add a linear classifier on top of the frozen backbone network and measure the top-1 and top-5 center-crop classification accuracies, following previous works <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b62">62]</ref>. The results are shown in Tab. 3. TWIST outperforms other state-of-the-art methods, achieving 75.5% top-1 accuracy on ResNet-50. Besides, we also train TWIST with other backbones of neural networks, including wider ResNet-50 and Vision Transformers. For wider ResNet-50, the superiority of TWIST becomes more apparent. TWIST outperforms SwAV and BYOL by 0.4% and 0.3% respectively using ResNet-50w2 and 300 training epochs. For Vision Transformers, TWIST is also comparable with other state-of-the-art methods, achieving 78.3% Top-1 accuracy with 300 epochs. Besides, we find that ViT is very sensitive to hyper-parameters and training it is very costly, we believe better results can be achieved after welltuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Transfer Learning</head><p>To further validate the features learned by TWIST, we evaluate TWIST model on eleven different datasets, including Food101 <ref type="bibr" target="#b4">[5]</ref>, CIFAR10, CIFAR100 <ref type="bibr" target="#b32">[32]</ref>, SUN397 <ref type="bibr" target="#b57">[57]</ref>, Cars <ref type="bibr" target="#b31">[31]</ref>, FGVC-Aircraft <ref type="bibr" target="#b43">[43]</ref>, Pascal VOC2007 <ref type="bibr" target="#b21">[21]</ref>, Describable Textures Dataset (DTD) <ref type="bibr" target="#b14">[15]</ref>, Oxford-IIIT Pet <ref type="bibr" target="#b46">[46]</ref>, Caltech101 <ref type="bibr" target="#b36">[36]</ref>, and Flowers <ref type="bibr" target="#b44">[44]</ref>. Tab. 4 shows the results. We also report the results of linear classification models for a comprehensive comparison. TWIST performs competitively on these datasets. In some datasets, TWIST achieves improvements over 1%. TWIST outperforms the supervised models on seven out of eleven datasets. We also observe that our method shows more advantages over other methods in the fine-tune setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Detection and Segmentation</head><p>We evaluate the learned representations of TWIST on object detection and instance segmentation. We conduct experiments on Pascal VOC <ref type="bibr" target="#b21">[21]</ref> and MS COCO <ref type="bibr" target="#b39">[39]</ref>. We use ResNet-50 with Feature Pyramid Network (FPN) <ref type="bibr" target="#b38">[38]</ref> as the backbone architecture. For Pascal VOC, we use the Faster R-CNN <ref type="bibr" target="#b48">[48]</ref> as the detector. For MSCOCO, we follow the common practice to use the Mask R-CNN <ref type="bibr" target="#b25">[25]</ref>. In implementation, we use Detectron2 <ref type="bibr" target="#b55">[55]</ref>, with the same configurations as <ref type="bibr" target="#b51">[51]</ref> and <ref type="bibr" target="#b54">[54]</ref>. Tab. 10 shows the results. We can see that TWIST performs better on all three tasks, demonstrating the advantages of using TWIST as the pre-trained model in object detection and instance segmentation. Besides, we find that the FPN architecture is important for the category-level self-supervised learning methods to achieve good performance. Analysis is given in Appendix.</p><p>We also evaluate TWIST on semantic segmentation, using FCN <ref type="bibr" target="#b40">[40]</ref> as architectures. We use the MMSegmentation <ref type="bibr" target="#b15">[16]</ref> to train the architectures. Tab. 6 shows the results on Pascal VOC <ref type="bibr" target="#b21">[21]</ref> and Cityscapes <ref type="bibr" target="#b16">[17]</ref>. The results indicate that TWIST is competitive to other state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Ablation Study</head><p>Amplifying Variance before Softmax: We study the amplifying variance operation of the batch normalization before softmax (abbreviated as NBS). From the loss values in <ref type="table">Table 7</ref>, we can see that the model with NBS is optimized much better than the model without NBS. NBS brings 5.1% top-1 accuracy improvement and 12% NMI improvement  <ref type="table">Table 4</ref>. Transfer learning results on eleven datasets, including linear evaluation and fine-tuneing. We use ResNet-50 as backbone and pre-trained on ImageNet. We calculate the average of performanes on these datasets and report it at the last column. TWIST performs best on the fine-tuning setting, which is in accordance to the advantage on the semi-supervised fine-tuning setting. on ImageNet, demonstrating the effectiveness of NBS. To better understand how NBS works, we look at the behaviors of the models with and without NBS. <ref type="figure">Fig. 3 (a) and (b)</ref> show the row and column standard deviations of the output before softmax (input of BN for the NBS model). Although the intermediate processes are different, the row standard deviations are closing when training is finished. For the column deviations, it is not the case. The column standard deviation of NBS model is much larger than the model without NBS at the end of the training, indicating that the samples in the same batch tend to give similar predictions. This is also reflected in <ref type="figure">Fig. 3 (f)</ref>, from which we see that the diversity term of the model with NBS is better optimized than the model without NBS. The observation indicates that the model without NBS tends to be column-collapsed. Although the solution is not fully collapsed, it tends to output similar predictions for samples in a mini-batch. NBS can successfully avoid the degenerated solution because batch normalization will force the standard deviation of each column to be one. <ref type="figure">Fig. 3 (c)</ref> shows the magnitude and stability of the gradients from the optimization perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of Loss Terms:</head><p>We test the impact of the loss terms in TWIST. As shown in Tab. 8, the models trained without the sharpness term generate collapsed solutions: The entropy for each sample is as large as the entropy of   <ref type="figure">Figure 3</ref>. We show the statistical characteristics of the output before softmax operation with and without NBS and the training curves.</p><p>performances deteriorate significantly. Theoretically, models trained without the diversity term will also lead to collapsed solutions, i.e., outputting the same one-hot distributions. However, the batch-normalization before the softmax operation helps avoid the problem because it can separate the probabilities in different columns and force them to have a unit standard deviation. Therefore, all three terms are indispensable for TWIST.</p><p>Multi-crop and Self-labeling: Tab. 9 shows the ablation results on multi-crop and self-labeling, where the models are trained for 800 epochs. We observe that the multi-crop and self-labeling can improve the performance respectively, and the best result comes from the combination of both.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Classes:</head><p>We show the impact of class number C in <ref type="figure" target="#fig_2">Fig. 4 (a)</ref>. To make a comprehensive evaluation, we show the results of TWIST with and without multi-crop. The models are trained by setting the number of classes from 1000 to 32768. With multi-crop, TWIST performs best when the number of classes is 4096. Overall, the performances are quite stable and fluctuate within the range of 1%, particularly when without multi-crop and C &gt;= 2048.</p><p>Training Epochs: <ref type="figure" target="#fig_2">Fig. 4 (b)</ref> shows the performances of training TWIST with different epochs. Training longer improves the performance of TWIST model without multicrop, while has less impact on the TWIST model with multicrop (when the training epochs &gt; 400).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we have presented a novel self-supervised approach TWIST. With a single loss function, our method can learn to classify images without labels, reaching 40.6% top-1 accuracy on ImageNet. The learned representations can be used in a wide range of downstream tasks to achieve better results than existing state-of-the-art methods, including linear classification, semi-supervised classification, transfer learning, and dense prediction tasks such as object detection and instance segmentation. TWIST is simple and theoretically explainable. It does not rely on any clustering tools, making it easy to implement. There are many topics worth exploring in future work such as extensions to other modalities, and applications of TWIST to larger datasets.</p><p>Broader Impact: TWIST is a self-supervised method that tries to capture the intrinsic semantic structure from input datasets. Therefore, the learned model may be vulnerable to data distributions. With biased datasets, the model is likely to learn malicious information. The issue should be taken considered when using this method.</p><p>Limitation: When using ViTs as backbones, we adopt the momentum encoder to improve the performances, making the training strategy inconsistent with CNNs. In the future, we will explore strategies to remove the momentum encoder for ViTs without degrading performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. PyTorch-like Pseudo Code</head><p>We give the PyTorch-like pseudo code, shown in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 TWIST Pseudocode in a PyTorch-like style</head><p># loss function def twist_loss(p1, p2, alpha=1, beta=1):</p><p># calculate the consistency term: KL-divergence kl_div = ((p2 * p2.log()).sum(dim=1) -(p2 * p1.log()).sum(dim=1)).mean() # calculate the sharpness term mean_ent = -(p1 * p1.log()).sum(dim=1).mean() # calculate the diversity term mean_prob = p1.mean(dim=0) ent_mean = -(mean_prob * mean_prob.log()).sum() return kl_div + alpha * mean_ent -beta * ent_mean   <ref type="figure">5</ref> shows the similarities of features sampled from the same or different classes of ImageNet. Specifically, we collect the outputs of the backbone as features, and calculate the cosine similarities. For positive samples, we sample two images from the same ImageNet class. For negative samples, we sample two images from different ImageNet classes. We then l2-normalize the features and calculate the similarities of positive/negative samples. The similarity distributions are shown in <ref type="figure">Fig. 5</ref>. We compare TWIST with SimCLR <ref type="bibr" target="#b10">[11]</ref>, SwAV <ref type="bibr" target="#b8">[9]</ref> and Supervised models <ref type="bibr" target="#b26">[26]</ref>. From <ref type="figure">Fig. 5</ref>, we observe that the postive distributions and the negative distributions of TWIST are more separable than other self-supervised methods. scale to (0.4, 1.0) and local scale to (0.05, 0.4). For CNN backbones, we use 12 crops. For ViTs, we use 6 crops for DeiT-S, and 10 crops for ViT-B <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation Details</head><p>To train CNN backbones, we use LARS optimizer <ref type="bibr" target="#b60">[60]</ref> with a cosine annealing learning rate schedule <ref type="bibr" target="#b41">[41]</ref>. We use a batch-size of 2048 splitting over 16 Tesla-V100 GPUs for ResNet50, and a batch-size of 1920 splitting over 32 Tesla-V100 GPUs for ResNet50?2. The learning rate is set to 0.5? B /256. The weight decay is set to 1.5e-6. The computation and memory costs are mainly from the multi-crop augmentations. For model without multi-crop, 8 Tesla-V100 GPUs are enough to achieve 72.6% top-1 linear accuracy on ImageNet. For self-labeling, we choose the samples in a mini-batch whose classification confidence (the maximum of softmax output) is larger than a predefined threshold. In practice, we use a cosine annealing schedule to choose the top 50% to 60% confident samples. We then use the chosen samples to generate hard labels for the subsequent fine-tuning process. For augmentation, we use multicrop augmentations same as the regular training setting, except for that we change the global crop scale to (0.14, 0.4) and the local crop scale to (0.05, 0.14). We use the predictions of global crops as labels to guide the learning of local crops.</p><p>For ViT-based models, we use the momentum encoder to enable stable training. The momentum is set to a variable value raised from 0.996 to 1 with a cosine annealing schedule. We change the weight of the three different terms of TWIST loss. Specifically, we set the coefficient as ? = 0.4, ? = 1.0 for the sharpness term and diversity term, respectively. We evaluate the linear performance using the momentum encoder, similar as <ref type="bibr" target="#b9">[10]</ref> (with 0.1 performance improvement compared with the online network). With momentum encoder, the training objective is the assymetric form of:</p><formula xml:id="formula_6">L(P 1 , P 2 ) = 1 B B i=1 DKL(P 1 i ||P 2 i ) consistency term + ? B B i=1 H(P 2 i ) sharpness term ?? H( 1 B B i=1 P 2 i ) diversity term ,<label>(5)</label></formula><p>whereP 1 is the output of the momentum encoder, andP 2 is the output of the online network. The optimizer is AdamW <ref type="bibr" target="#b42">[42]</ref>. We use a batch-size of 1024 splitting over 16 Tesla-V100 GPUs. The learning rate is set to 0.0003? B /256 for DeiT-S and 0.00075? B /256 for ViT-B. The weight decay is set to 0.06-0.12 with a cosine annealing strategy for Deit-S and 0.06 for ViT-B, we use no drop path for DeiT-S and drop path rate 0.1 for ViT-B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Unsupervised Classification</head><p>Our TWIST model can be regarded as a clustering function which takes images as input and directly outputs the assignments. Therefore, we adopt the measures for clustering in evaluation, including normalized mutual information (NMI), adjusted mutual information (AMI), and adjusted rand index (ARI). Besides, we map the predicted assignments to the class labels of ImageNet to evaluate the unsupervised classification accuracy. We use the Kuhn-Munkres algorithm <ref type="bibr" target="#b34">[34]</ref> to find the best one-to-one permutation mapping, following the settings in IIC <ref type="bibr" target="#b30">[30]</ref> and SCAN <ref type="bibr" target="#b53">[53]</ref>. Though this process uses the real labels, they do not participate in any training process and are only used to map the prediction to the meaningful ImageNet labels. We use ResNet-50 as backbone and set C = 1000 in accordance with ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Examples for unsupervised classification</head><p>To give a qualitative impression on the performance of unsupervised classification, we display the learned partitions as <ref type="figure">Fig. 6</ref>. We also display the top-5 predictions of some randomly selected pictures, shown as <ref type="figure" target="#fig_6">Fig. 7</ref>. Specifically, the labels are mapped to the labels in ImageNet by Kuhn-Munkres algorithm. Note that the labels are only used to map our predictions to the meaningful ImageNet label descriptions, we do not use any label to participate in the training process.</p><p>E. Results and Analyses on Dense Tasks. <ref type="table" target="#tab_0">Table 10</ref> shows the object detection performance on Pascal VOC and COCO datasets and the instance segmentation performance on COCO. We compare different architectures, namely C4 and FPN. From <ref type="table" target="#tab_0">Table 10</ref>, we find some interesting phenomenons. (1) With architectures using feature pyramid network, TWIST achieves state-of-the-art results. Clustering-based methods also perform pretty well.</p><p>(2) For C4 architectures, TWIST and clustering-based methods perform worse than contrastive learning methods like MoCo-v2.</p><p>We thought the reason of the above phenomenons is that the classification-based methods (TWIST and clusteringbased methods) tend to capture category-level invariances instead of instance-level invariances, which makes the out-puts of the last convolutional layer discard intra-class variations that is useful for dense predictive tasks. When using FPN-based detectors, features of different layers are combined to compensate for the discarded information from the last layer. Less work concentrates on the effect of the intermediate layers of self-supervised models, while we find the intermediate features may preserve useful information for dense tasks. When using the FPN detectors, TWIST even outperforms those self-supervised methods designed specifically for the dense tasks, such as DenseCL <ref type="bibr" target="#b54">[54]</ref>, i.e., +1.6 AP on COCO detection.</p><p>We find the similar phenomenon on semantic segmentation, shown in <ref type="table" target="#tab_0">Table 11</ref>. We give results of semantic segmentation with different architectures. The FCN architecture has no fusion of different layers, while FCN-FPN and DeepLab v3+ have the fusion operation. From <ref type="table" target="#tab_0">Table 11</ref>, we could observe that when combining with feature pyramid network, our method achieves best or competitive results. Without fusion of different layers of features, TWIST and other clustering-based models perform worse than contrastive learning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Change Log</head><p>The results of ViTs are updated in Section 5. The linear result of DeiT-S/16 is improved from 75.6% to 76.3%, and the linear result of Vit-B is improved from 77.3% to 78.4%. The improved results are due to the more adequate hyperparameter searching. Specifically for DeiT-S:</p><p>? We change the batch-size from 2048 to 1024.</p><p>? The learning rate is changed from 0.0005 to 0.0003.</p><p>? The weight decay is changed from 0.06 to a cosine scheduler from 0.06 to 0.12.</p><p>For ViT-B:</p><p>? We change the batch-size from 2048 to 1024.</p><p>? We change drop path rate from 0.0 to 0.1. <ref type="figure">Figure 6</ref>. Randomly chosen classes. We randomly choose 24 classes for visualization. For each class, we randomly choose 25 pictures to display. Note we did not make any selections on the pictures or the categories, all the categories and images are randomly chosen to give readers the accurate impression.   <ref type="table" target="#tab_0">Table 10</ref>. Object detection and instance segmentation results. For methods marked with ? , we download the pre-trained models and run the detection and segmentation by ourselves. We report results both with C4 architecture and FPN architecture. For VOC dataset, we run 5 times and report the average.  <ref type="table" target="#tab_0">Table 11</ref>. Semantic segmentation with different architectures. All results are averaged over 5 trials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Network architecture of TWIST.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>NBS Loss ACC NMI std c std r -5.05 70.6 59.0 2.37 1.12 -3.78 65.5 47.0 0.26 1.14</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>(a) Effect of different numbers of classes in TWIST. (b) Effect of different training epochs in TWIST, where "mc" denotes multi-crop and "sl" denotes self-labeling. All results are ImageNet one-crop top-1 accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc># f: encoder network: backbone + projection head. # bn: batch normalization operation with no affine parameters. # B: batch-size. # C: class number. for x in loader: # load a minibatch x1, x2 = aug(x), aug(x) # random augmentation feat1, feat2 = f(x1), f(x2) p1 = softmax(bn(feat1), dim=1) p2 = softmax(bn(feat2), dim=1) # symmetric twist loss L = 0.5 * (twist_loss(p1, p2) + twist_loss(p2, p1) L.backward() # back-propagate update(f.param) # SGD update B. Visualization of feature similarity</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig.</head><label></label><figDesc>Fig. 5 shows the similarities of features sampled from the same or different classes of ImageNet. Specifically, we collect the outputs of the backbone as features, and calculate the cosine similarities. For positive samples, we sample two images from the same ImageNet class. For negative samples, we sample two images from different ImageNet classes. We then l2-normalize the features and calculate the similarities of positive/negative samples. The similarity distributions are shown in Fig. 5. We compare TWIST with SimCLR [11], SwAV [9] and Supervised models [26]. From Fig. 5, we observe that the postive distributions and the negative distributions of TWIST are more separable than other self-supervised methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>C. 1 .Figure 5 .</head><label>15</label><figDesc>Self-supervised Pre-trainingThe projection head is an non-linear MLP consisting of three layers with dimensions of 4096 ? 4096 ? C. The first two layers of the projection head are followed by a batch normalization and rectified linear units. After the projection head, we add a batch-normalization layer without affine parameters and finally a softmax operation to calculate the probability distributions. For multi-crop, we set the global TWIST Twist Twist The distributions of positive/negative similarities. IoU is the area of overlap between positive and negative distributions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Top-5 ImageNet accuracy of unsupervised classification. The labels are mapped by the Kuhn-Munkres algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>Method</cell><cell cols="3">1% Labels Top1 Top5 Top1 Top5 Top1 Top5 10% Labels 100% Labels</cell></row><row><cell>ResNet-50</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SUP</cell><cell cols="2">25.4 48.4 56.4 80.4 76.5</cell><cell>-</cell></row><row><cell>SimCLR</cell><cell cols="3">48.3 75.5 65.6 87.8 76.5 93.5</cell></row><row><cell>BYOL</cell><cell cols="3">53.2 78.4 68.8 89.0 77.7 93.9</cell></row><row><cell>SwAV</cell><cell>53.9 78.5 70.2 89.9</cell><cell>-</cell><cell>-</cell></row><row><cell>DINO</cell><cell>52.2 78.2 68.2 89.1</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">BarlowTwins 55.0 79.2 69.7 89.3</cell><cell>-</cell><cell>-</cell></row><row><cell>TWIST</cell><cell cols="3">61.2 84.2 71.7 91.0 78.4 94.6</cell></row><row><cell>ResNet-50?2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SimCLR</cell><cell>58.5 83.0 71.7 91.2</cell><cell>-</cell><cell>-</cell></row><row><cell>BYOL</cell><cell>62.2 84.1 73.5 91.7</cell><cell>-</cell><cell>-</cell></row><row><cell>TWIST</cell><cell cols="3">67.2 88.2 75.3 92.8 80.3 95.4</cell></row><row><cell>ViT-B/16</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DINO</cell><cell cols="2">67.3 88.2 74.6 92.0 82.8</cell><cell>-</cell></row><row><cell>TWIST</cell><cell cols="3">69.6 89.7 76.5 93.1 82.8 96.3</cell></row></table><note>. Semi-supervised classification results on ImageNet. We report top-1 and top-5 center-crop accuracies, from 1% to 100%.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>algorithm. Tab. 2 shows the results. TWIST with ResNet-50 Method NMI ARI AMI ACC SCAN 72.0 27.5 51.2 39.9 SeLa 65.7 16.2 42.0 -SelfClassifier 64.7 13.2 46.2 -TWIST 74.3 30.0 57.7 40.6 Unsupervised classification results on ImageNet. All numbers are reported on the validation set of ImageNet. Comparison methods include SCAN<ref type="bibr" target="#b53">[53]</ref>, SeLa<ref type="bibr" target="#b1">[2]</ref>, and Self Classifier<ref type="bibr" target="#b0">[1]</ref>. backbone outperforms previous best results by 2.3% NMI. Details are shown in appendix.</figDesc><table><row><cell>Method</cell><cell cols="5">Network Param Epoch Top1 Top5</cell></row><row><cell cols="2">ResNet-50 without multi-crop</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MoCo v2</cell><cell>RN50</cell><cell>24M</cell><cell>800</cell><cell>71.1</cell><cell>90.1</cell></row><row><cell>SimCLR</cell><cell>RN50</cell><cell>24M</cell><cell>1000</cell><cell>69.3</cell><cell>89.0</cell></row><row><cell>BarlowTwins</cell><cell>RN50</cell><cell>24M</cell><cell>1000</cell><cell>73.2</cell><cell>91.0</cell></row><row><cell>BYOL</cell><cell>RN50</cell><cell>24M</cell><cell>1000</cell><cell>74.3</cell><cell>91.6</cell></row><row><cell>SelfClassifier</cell><cell>RN50</cell><cell>24M</cell><cell>800</cell><cell>69.7</cell><cell>89.3</cell></row><row><cell>SwAV</cell><cell>RN50</cell><cell>24M</cell><cell>800</cell><cell>71.8</cell><cell>-</cell></row><row><cell>TWIST</cell><cell>RN50</cell><cell>24M</cell><cell>800</cell><cell>72.6</cell><cell>91.0</cell></row><row><cell cols="2">ResNet-50 with multi-crop</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SwAV</cell><cell>RN50</cell><cell>24M</cell><cell>800</cell><cell>75.3</cell><cell>-</cell></row><row><cell>DINO</cell><cell>RN50</cell><cell>24M</cell><cell>800</cell><cell>75.3</cell><cell>92.5</cell></row><row><cell>TWIST</cell><cell>RN50</cell><cell>24M</cell><cell>300</cell><cell>75.0</cell><cell>92.4</cell></row><row><cell>TWIST</cell><cell>RN50</cell><cell>24M</cell><cell>800</cell><cell>75.5</cell><cell>92.5</cell></row><row><cell>Wider ResNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SimCLR</cell><cell>RN50w2</cell><cell>94M</cell><cell>1000</cell><cell>74.2</cell><cell>92.0</cell></row><row><cell>CMC</cell><cell>RN50w2</cell><cell>94M</cell><cell>-</cell><cell>70.6</cell><cell>89.7</cell></row><row><cell>SwAV</cell><cell>RN50w2</cell><cell>94M</cell><cell>800</cell><cell>77.3</cell><cell>-</cell></row><row><cell>BYOL</cell><cell>RN50w2</cell><cell>94M</cell><cell>1000</cell><cell>77.4</cell><cell>93.6</cell></row><row><cell>TWIST</cell><cell>RN50w2</cell><cell>94M</cell><cell>300</cell><cell>77.7</cell><cell>93.9</cell></row><row><cell cols="2">Vision Transformer</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MoCo-v3</cell><cell>Deit-S/16</cell><cell>21M</cell><cell>300</cell><cell>72.5</cell><cell>-</cell></row><row><cell>DINO</cell><cell>Deit-S/16</cell><cell>21M</cell><cell>300</cell><cell>75.9</cell><cell>-</cell></row><row><cell>TWIST</cell><cell>Deit-S/16</cell><cell>21M</cell><cell>300</cell><cell>76.3</cell><cell>92.7</cell></row><row><cell>MoCo-v3</cell><cell>ViT-B/16</cell><cell>86M</cell><cell>300</cell><cell>76.5</cell><cell>-</cell></row><row><cell>DINO</cell><cell>ViT-B/16</cell><cell>86M</cell><cell>800</cell><cell>78.2</cell><cell>93.9</cell></row><row><cell>TWIST</cell><cell>ViT-B/16</cell><cell>86M</cell><cell>300</cell><cell>78.4</cell><cell>93.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Method Food Cifar10 Cifar100 Sun397 Cars Aircraft VOC DTD Pets Caltech Flowers Avg</figDesc><table><row><cell cols="2">Linear evaluation:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">SimCLR 68.4</cell><cell>90.6</cell><cell>71.6</cell><cell>58.8</cell><cell>50.3</cell><cell>50.3</cell><cell>80.5</cell><cell>74.5 83.6</cell><cell>90.3</cell><cell>91.2</cell><cell>73.6</cell></row><row><cell>BYOL</cell><cell>75.3</cell><cell>91.3</cell><cell>78.4</cell><cell>62.2</cell><cell>67.8</cell><cell>60.6</cell><cell>82.5</cell><cell>75.5 90.4</cell><cell>94.2</cell><cell>96.1</cell><cell>79.5</cell></row><row><cell>SUP</cell><cell>72.3</cell><cell>93.6</cell><cell>78.3</cell><cell>61.9</cell><cell>66.7</cell><cell>61.0</cell><cell>82.8</cell><cell>74.9 91.5</cell><cell>94.5</cell><cell>94.7</cell><cell>79.3</cell></row><row><cell>TWIST</cell><cell>78.0</cell><cell>91.2</cell><cell>74.4</cell><cell>66.8</cell><cell>55.2</cell><cell>53.6</cell><cell>85.7</cell><cell>76.6 91.6</cell><cell>91.1</cell><cell>93.4</cell><cell>78.0</cell></row><row><cell>Fine-tune:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Random</cell><cell>86.9</cell><cell>95.9</cell><cell>80.2</cell><cell>53.6</cell><cell>91.4</cell><cell>85.9</cell><cell>67.3</cell><cell>64.8 81.5</cell><cell>72.6</cell><cell>92.0</cell><cell>79.3</cell></row><row><cell cols="2">SimCLR 87.5</cell><cell>97.4</cell><cell>85.3</cell><cell>63.9</cell><cell>91.4</cell><cell>87.6</cell><cell>84.5</cell><cell>75.4 89.4</cell><cell>91.7</cell><cell>96.6</cell><cell>86.4</cell></row><row><cell>BYOL</cell><cell>88.5</cell><cell>97.8</cell><cell>86.1</cell><cell>63.7</cell><cell>91.6</cell><cell>88.1</cell><cell>85.4</cell><cell>76.2 91.7</cell><cell>93.8</cell><cell>97.0</cell><cell>87.3</cell></row><row><cell>SUP</cell><cell>88.3</cell><cell>97.5</cell><cell>86.4</cell><cell>64.3</cell><cell>92.1</cell><cell>86.0</cell><cell>85.0</cell><cell>74.6 92.1</cell><cell>93.3</cell><cell>97.6</cell><cell>87.0</cell></row><row><cell>TWIST</cell><cell>89.3</cell><cell>97.9</cell><cell>86.5</cell><cell>67.4</cell><cell>91.9</cell><cell>85.7</cell><cell>86.5</cell><cell>76.4 94.5</cell><cell>93.5</cell><cell>97.1</cell><cell>87.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .Table 6</head><label>56</label><figDesc>Detection and instance segmentation. ? means that we download the pre-trained models and conduct the experiments. For the VOC dataset, we run five trials and report the average. The performance is measured by Average Precision (AP). DC-v2 denotes the DeepCluster-v2.</figDesc><table><row><cell></cell><cell cols="6">VOC07+12 detection AP all AP50 AP75 AP bb COCO detection all AP bb 50 AP bb 75</cell><cell cols="3">COCO instance seg AP mk all AP mk 50 AP mk 75</cell><cell>Method</cell><cell cols="2">FCN-FPN VOC Cityscapes</cell></row><row><cell>Moco-v2</cell><cell>56.4</cell><cell>81.6</cell><cell>62.4</cell><cell>39.8</cell><cell>59.8</cell><cell>43.6</cell><cell>36.1</cell><cell>56.9</cell><cell>38.7</cell><cell>Sup</cell><cell>67.7</cell><cell>75.4</cell></row><row><cell>SimCLR  ?</cell><cell>58.2</cell><cell>83.8</cell><cell>65.1</cell><cell>41.6</cell><cell>61.8</cell><cell>45.6</cell><cell>37.6</cell><cell>59.0</cell><cell>40.5</cell><cell>Moco-v2</cell><cell>67.5</cell><cell>75.4</cell></row><row><cell>SwAV</cell><cell>57.2</cell><cell>83.5</cell><cell>64.5</cell><cell>41.6</cell><cell>62.3</cell><cell>45.7</cell><cell>37.9</cell><cell>59.3</cell><cell>40.8</cell><cell>SimCLR</cell><cell>72.8</cell><cell>74.9</cell></row><row><cell>DC-v2  ?</cell><cell>57.0</cell><cell>83.7</cell><cell>64.1</cell><cell>41.0</cell><cell>61.8</cell><cell>45.1</cell><cell>37.3</cell><cell>58.7</cell><cell>39.9</cell><cell>SwAV</cell><cell>71.9</cell><cell>74.4</cell></row><row><cell>DINO  ?</cell><cell>57.2</cell><cell>83.5</cell><cell>63.7</cell><cell>41.4</cell><cell>62.2</cell><cell>45.3</cell><cell>37.5</cell><cell>58.8</cell><cell>40.2</cell><cell>DC-v2</cell><cell>72.1</cell><cell>73.8</cell></row><row><cell>DenseCL</cell><cell>56.9</cell><cell>82.0</cell><cell>63.0</cell><cell>40.3</cell><cell>59.9</cell><cell>44.3</cell><cell>36.4</cell><cell>57.0</cell><cell>39.2</cell><cell>DINO</cell><cell>71.9</cell><cell>73.8</cell></row><row><cell>TWIST</cell><cell>58.1</cell><cell>84.2</cell><cell>65.4</cell><cell>41.9</cell><cell>62.6</cell><cell>45.7</cell><cell>37.9</cell><cell>59.7</cell><cell>40.6</cell><cell>TWIST</cell><cell>73.3</cell><cell>74.6</cell></row></table><note>. Results of semantic segmen- tation with FCN-FPN backbone. All results are averaged over five trials.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 .Table 8 .Table 9 .</head><label>789</label><figDesc>Ablation study on batch normalization before softmax. Ablation study on the loss terms. Here Ls and L d denote the sharpness and diversity term respectively. |g| denotes the mean magnitude of gradients before the last batch normalization and "acc" is the linear accuracy. Models are trained for 50 epochs. Ablation study on multi-crop and self-labeling. We report the linear accuracy. Models are trained for 800 epochs.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>|g|</cell><cell>acc</cell></row><row><cell></cell><cell>8.28</cell><cell>8.28</cell><cell>0</cell><cell>0.1</cell></row><row><cell></cell><cell>8.27</cell><cell>8.28</cell><cell>0</cell><cell>0.1</cell></row><row><cell></cell><cell>2.59</cell><cell cols="3">6.42 0.01 56.1</cell></row><row><cell></cell><cell>1.51</cell><cell cols="3">7.87 0.02 70.9</cell></row><row><cell>multi-crop</cell><cell></cell><cell></cell><cell></cell></row><row><cell>self-labeling</cell><cell></cell><cell></cell><cell></cell></row><row><cell>acc</cell><cell cols="4">75.5 74.0 73.8 72.6</cell></row></table><note>L s L d L s = L d =a uniform distribution, and the gradient magnitude rapidly decreases to 0. In contrast, the models trained without the diversity term do not generate collapsed solutions, but their</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>AP 50 AP 75 AP bb all</figDesc><table><row><cell>Method</cell><cell cols="5">VOC07+12 det AP all AP bb COCO det 50</cell><cell>AP bb 75</cell><cell cols="3">COCO instance seg AP mk all AP mk 50 AP mk 75</cell></row><row><cell>C4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sup</cell><cell>53.5</cell><cell>81.3</cell><cell>58.8</cell><cell>38.2</cell><cell>58.2</cell><cell>41.2</cell><cell>33.3</cell><cell>54.7</cell><cell>35.2</cell></row><row><cell>Moco-v2</cell><cell>57.4</cell><cell>82.5</cell><cell>64.0</cell><cell>39.3</cell><cell>58.9</cell><cell>42.5</cell><cell>34.4</cell><cell>55.8</cell><cell>36.5</cell></row><row><cell>SimCLR  ?</cell><cell>57.0</cell><cell>82.4</cell><cell>63.5</cell><cell>38.5</cell><cell>58.5</cell><cell>41.7</cell><cell>33.8</cell><cell>55.1</cell><cell>36.0</cell></row><row><cell>SwAV</cell><cell>56.1</cell><cell>82.6</cell><cell>62.7</cell><cell>38.4</cell><cell>58.6</cell><cell>41.3</cell><cell>33.8</cell><cell>55.2</cell><cell>35.9</cell></row><row><cell>DINO  ?</cell><cell>55.2</cell><cell>81.8</cell><cell>61.3</cell><cell>37.4</cell><cell>57.8</cell><cell>40.0</cell><cell>33.0</cell><cell>54.3</cell><cell>34.9</cell></row><row><cell>DC-v2  ?</cell><cell>54.2</cell><cell>81.6</cell><cell>59.9</cell><cell>37.0</cell><cell>57.7</cell><cell>39.5</cell><cell>32.8</cell><cell>54.2</cell><cell>34.4</cell></row><row><cell>SimSiam</cell><cell>57.0</cell><cell>82.4</cell><cell>63.7</cell><cell>39.2</cell><cell>59.3</cell><cell>42.1</cell><cell>34.4</cell><cell>56.0</cell><cell>36.7</cell></row><row><cell cols="2">BarlowTwins 56.8</cell><cell>82.6</cell><cell>63.4</cell><cell>39.2</cell><cell>59.0</cell><cell>42.5</cell><cell>34.3</cell><cell>56.0</cell><cell>36.5</cell></row><row><cell>TWIST</cell><cell>55.3</cell><cell>82.2</cell><cell>61.2</cell><cell>38.0</cell><cell>58.4</cell><cell>40.8</cell><cell>33.5</cell><cell>54.9</cell><cell>35.5</cell></row><row><cell>FPN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Moco-v2</cell><cell>56.4</cell><cell>81.6</cell><cell>62.4</cell><cell>39.8</cell><cell>59.8</cell><cell>43.6</cell><cell>36.1</cell><cell>56.9</cell><cell>38.7</cell></row><row><cell>SimCLR  ?</cell><cell>58.2</cell><cell>83.8</cell><cell>65.1</cell><cell>41.6</cell><cell>61.8</cell><cell>45.6</cell><cell>37.6</cell><cell>59.0</cell><cell>40.5</cell></row><row><cell>SwAV</cell><cell>57.2</cell><cell>83.5</cell><cell>64.5</cell><cell>41.6</cell><cell>62.3</cell><cell>45.7</cell><cell>37.9</cell><cell>59.3</cell><cell>40.8</cell></row><row><cell>DC-v2  ?</cell><cell>57.0</cell><cell>83.7</cell><cell>64.1</cell><cell>41.0</cell><cell>61.8</cell><cell>45.1</cell><cell>37.3</cell><cell>58.7</cell><cell>39.9</cell></row><row><cell>DINO  ?</cell><cell>57.2</cell><cell>83.5</cell><cell>63.7</cell><cell>41.4</cell><cell>62.2</cell><cell>45.3</cell><cell>37.5</cell><cell>58.8</cell><cell>40.2</cell></row><row><cell>DenseCL</cell><cell>56.9</cell><cell>82.0</cell><cell>63.0</cell><cell>40.3</cell><cell>59.9</cell><cell>44.3</cell><cell>36.4</cell><cell>57.0</cell><cell>39.2</cell></row><row><cell>TWIST</cell><cell>58.1</cell><cell>84.2</cell><cell>65.4</cell><cell>41.9</cell><cell>62.6</cell><cell>45.7</cell><cell>37.9</cell><cell>59.7</cell><cell>40.6</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Self-supervised classification network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Amrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10994</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Self-labelling via simultaneous clustering and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><forename type="middle">Markus</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Vicreg: Variance-invariance-covariance regularization for selfsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Bardes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.04906</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mine: mutual information neural estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><forename type="middle">Ishmael</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aristide</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai</forename><surname>Rajeswar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R Devon</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Food-101 -mining discriminative components with random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised classifiers, mutual information and &apos;phantom targets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>John S Bridle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David Jc</forename><surname>Heading</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised pre-training of image features on non-curated data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2959" to="2968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14294</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="15750" to="15758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">An empirical study of training self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.02057</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark</title>
		<ptr target="https://github.com/open-mmlab/mmsegmentation,2020.6" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<imprint>
			<pubPlace>Uwe</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2021</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The pascal visual object classes (voc) challenge. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">M</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="303" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><forename type="middle">Daniel</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Azar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2020. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>R Devon Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning discrete representations via information maximizing self-augmented training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seiya</forename><surname>Tokui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichi</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1558" to="1567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Invariant information clustering for unsupervised image classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Collecting a large-scale dataset of fine-grained cars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harold W Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Naval research logistics quarterly</title>
		<imprint>
			<date type="published" when="1955" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">On information and sufficiency. The annals of mathematical statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Solomon</forename><surname>Kullback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leibler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1951" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Prototypical contrastive learning of unsupervised representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2021</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-Elena</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth Indian Conference on Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jawahar</surname></persName>
		</author>
		<title level="m">Cats and dogs. CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3498" to="3505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">A mathematical theory of communication. The Bell system technical journal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claude</forename><forename type="middle">Elwood</forename><surname>Shannon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1948" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="379" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10243</idno>
		<title level="m">What makes for good views for contrastive learning? arXiv preprint</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Scan: Learning to classify images without labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Wouter Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Dense contrastive learning for self-supervised visual pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron2</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Sun database: Large-scale scene recognition from abbey to zoo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krista</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3485" to="3492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Unsupervised deep embedding for clustering analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="478" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Joint unsupervised learning of deep representations and image clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5147" to="5156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Scaling sgd batch size to 32k for imagenet training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03888</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Barlow twins: Self-supervised learning via redundancy reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Deny</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03230</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
