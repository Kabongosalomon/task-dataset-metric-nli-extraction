<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Optimize Domain Specific Normalization for Domain Generalization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonguk</forename><surname>Seo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yumin</forename><surname>Suh</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">NEC Laboratories America</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongwan</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geeho</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwoo</forename><surname>Han</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">LG Electronics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Optimize Domain Specific Normalization for Domain Generalization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Domain generalization</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a simple but effective multi-source domain generalization technique based on deep neural networks by incorporating optimized normalization layers that are specific to individual domains. Our approach employs multiple normalization methods while learning separate affine parameters per domain. For each domain, the activations are normalized by a weighted average of multiple normalization statistics. The normalization statistics are kept track of separately for each normalization type if necessary. Specifically, we employ batch and instance normalizations in our implementation to identify the best combination of these two normalization methods in each domain. The optimized normalization layers are effective to enhance the generalizability of the learned model. We demonstrate the state-of-the-art accuracy of our algorithm in the standard domain generalization benchmarks, as well as viability to further tasks such as multi-source domain adaptation and domain generalization in the presence of label noise.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Domain generalization aims to learn generic feature representations agnostic to domains and make trained models perform well in completely new domains. To achieve this challenging goal, one needs to train models that can capture useful information observed commonly in multiple domains and recognize semantically related but visually inconsistent examples effectively. Many real-world problems have similar objectives so this task can be widely used in various practical applications. Domain generalization is closely related to unsupervised domain adaptation but there is a critical difference regarding the availability of target domain data; contrary to unsupervised domain adaptation, domain generalization cannot access any examples in target domain during training but is still required to capture transferable information across domains. Due to this constraint, the domain generalization problem is typically considered to be more challenging, so multiple source domains are usually involved to make the problem more feasible.</p><p>Domain generalization techniques are classified into several groups depending on their approaches. Some algorithms define novel loss functions to learn domain-agnostic representations <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b6">7]</ref> while others are more interested in designing deep neural network architectures to achieve similar goals <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b16">17]</ref>. The algorithms based on meta-learning have been proposed under the assumption that there exists a held-out validation set <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>Our algorithm belongs to the second category, i.e. network architecture design methods. In particular, we are interested in exploiting normalization layers in deep neural networks to handle the domain generalization task. A na?ve approach would be to train a single deep neural network with batch normalization using all training examples regardless of their domain memberships. This method works fairly well partly because batch normalization regularizes feature representations from heterogeneous domains and the trained model is often capable of adapting to unseen domains. However, the benefit of batch normalization is limited when domain shift is significant, and we are often required to remove domain-specific styles for better generalization. Instance normalization <ref type="bibr" target="#b26">[27]</ref> turns out to be an effective scheme for the goal and incorporating both batch and instance normalization techniques further improves accuracy by a data-driven balancing of two normalization methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>. Our approach also employs the two normalizations but proposes a more sophisticated algorithm designed for domain generalization.</p><p>We explore domain-specific normalizations to learn representations that are both domain-agnostic and semantically discriminative by discarding domainspecific ones. The goal of our algorithm is to optimize the combination of normalization techniques in each domain while different domains learn separate parameters for the mixture of normalizations. The intuition behind this approach is that we can learn domain-invariant representations by controlling types of normalization and parameters in normalization layers. Note that all other parameters, including the ones in convolutional layers, are shared across domains. Although our approach is somewhat similar to <ref type="bibr" target="#b15">[16]</ref> in that the optimal mixing weights between normalization types are learned, we emphasize that the motivations are different; <ref type="bibr" target="#b15">[16]</ref> aims for a differentiable normalization for universal tasks while we set our sights on how to remove style information without losing semantics to generalize on unseen domains. In addition, our domain-specific propertieslearning normalization parameters, batch statistics and mixture weights for each domain separately-makes it unique and more effective to construct domainagnostic representations, thereby outperforming all the established normalization techniques. We illustrate the main idea of our approach in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>Our contributions are as follows:</p><p>? Our approach leverages instance normalization to optimize the trade-off between cross-category variance and domain invariance, which is desirable for domain generalization in unseen domains. ? We propose a simple but effective domain generalization technique combining heterogeneous normalization methods specific to individual domains, which facilitates the extraction of domain-agnostic feature representations by removing domain-specific information effectively. ? The proposed algorithm achieves the state-of-the-art accuracy in multiple standard benchmark datasets and outperforms all established normalization methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>This section discusses existing domain generalization approaches and reviews two related problems, multi-source domain adaptation and normalization techniques in deep neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Domain Generalization</head><p>Domain generalization algorithms learn domain-invariant representations given input examples regardless of their domain memberships. Since target domain information is not available at training time, they typically rely on multiple source domains to extract knowledge applicable to any unseen domain. The existing domain generalization approaches can be roughly categorized into three classes. The first group of methods proposes novel loss functions that encourage learned representations to generalize well to new domains. Muandet et al. <ref type="bibr" target="#b20">[21]</ref> propose domain-invariant component analysis, which generates invariant feature representation via dimensionality reduction. A few recent works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b6">7]</ref> also attempt to learn a shared embedding space appropriate for semantic matching across domains. Another kind of approach tackles the domain generalization task by manipulating deep neural network architectures. Domain-specific information is handled by designated modules within deep neural networks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b17">18]</ref> while <ref type="bibr" target="#b16">[17]</ref> proposes a soft model selection technique to obtain generalized representations. Recently, meta-learning based techniques start to be used to solve domain generalization problems. MLDG <ref type="bibr" target="#b11">[12]</ref> extends MAML <ref type="bibr" target="#b7">[8]</ref> to domain generalization task. Balaji et al. <ref type="bibr" target="#b1">[2]</ref> points out the limitation of <ref type="bibr" target="#b11">[12]</ref> and proposes a regularizer to address domain generalization in a meta-learning framework directly. Also, <ref type="bibr" target="#b13">[14]</ref> presents an episodic training technique appropriate for domain generalization. Note that, to the best of our knowledge, none of the existing methods exploit normalization types and their optimization for domain generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multi-Source Domain Adaptation</head><p>Multi-source domain adaptation can be considered as the middle-ground between domain adaptation and generalization, where data from multiple source domains are used for training in addition to examples in an unlabeled target domain. Although unsupervised domain adaptation is a very popular problem, its multi-source version is relatively less investigated. Zhao et al. <ref type="bibr" target="#b30">[31]</ref> propose to learn features that are invariant to multiple domain shifts through adversarial training, and Guo et al. <ref type="bibr" target="#b8">[9]</ref> use a mixture-of-experts approach by modeling the inter-domain relationships between source and target domains. A recent work using domain-specific batch normalization (DSBN) <ref type="bibr" target="#b4">[5]</ref> has shown competitive performance in multi-source domain adaptation by aligning the representations in heterogeneous domains to a single common feature space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Normalization in Neural Networks</head><p>Normalization techniques in deep neural networks are originally designed for regularizing trained models and improving their generalization performance. Various normalization techniques <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b4">5]</ref> have been studied actively in recent years. The most popular technique is batch normalization (BN) <ref type="bibr" target="#b10">[11]</ref>, which normalizes activations over individual channels using data in a mini-batch while instance normalization (IN) <ref type="bibr" target="#b26">[27]</ref> performs the same operation per instance instead of mini-batch. In general, IN is effective to remove instance-specific characteristics (e.g. style in an image) and adding IN makes a trained model focus on instance-invariant information and increases generalization capability of the model to an unseen domain. Other normalizations such as layer normalization (LN) <ref type="bibr" target="#b0">[1]</ref> and group normalization (GN) <ref type="bibr" target="#b28">[29]</ref> have the same concept while weight normalization <ref type="bibr" target="#b23">[24]</ref> and spectral normalization <ref type="bibr" target="#b18">[19]</ref> normalize weights over parameter space.</p><p>Recently, batch-instance normalization (BIN) <ref type="bibr" target="#b21">[22]</ref>, switchable normalization (SN) <ref type="bibr" target="#b15">[16]</ref>, and sparse switchable normalization (SSN) <ref type="bibr" target="#b24">[25]</ref> employ the combinations of multiple normalization types to maximize the benefit. Note that BIN considers batch and instance normalizations while SN uses LN additionally. On the other hand, DSBN <ref type="bibr" target="#b4">[5]</ref> adopts separate batch normalization layers for each domain to deal with domain shift and generate domain-invariant representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Domain-Specific Optimized Normalization for Domain Generalization</head><p>This section describes our main algorithm called domain-specific optimized normalization (DSON) in details and also presents how the proposed method is employed to solve domain generalization problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>Domain generalization aims to learn a domain-agnostic model that can be applied to an unseen domain by leveraging multiple source domains. Consider a set of training examples X s with its corresponding label set Y s in a source domain s. Our goal is to train a classifier using the data in multiple source domains {X s } S s=1 to correctly classify an image x t ? X t in a target domain t, which is unavailable during training.</p><p>In our approach, we aim to learn a joint embedding space across all source domains, which is expected to be valid in target domains as well. To this end, we train domain-invariant classifiers from each of the source domains and ensemble their predictions. To embed each example onto a domain-invariant feature space, we employ domain-specific normalization, which is to be described in the following sections.</p><p>Our classification network consists of a set of feature extractors {F s } S s=1 and a single fully connected layer D. Specifically, the feature extractors share all parameters across domains except for the ones in the normalization layers. For each source domain s, loss function is defined as</p><formula xml:id="formula_0">L C (X s , Y s ) = 1 |X s | x?Xs,y?Ys (y, D(F s (x)),<label>(1)</label></formula><p>where (?, ?) is the cross-entropy loss. All the parameters are jointly optimized to minimize the sum of classification losses of source domains:</p><formula xml:id="formula_1">L = S s=1 L C (X s , Y s ).<label>(2)</label></formula><p>Our domain-specific deep neural network model is obtained by minimizing the total loss L. To facilitate generalization, in the validation phase, we follow the leave-one-domain-out validation strategy proposed in <ref type="bibr" target="#b5">[6]</ref>; the label of a validation example from domain s is predicted by averaging predictions from all domainspecific classifiers, except for the one with domain s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Instance Normalization for Domain Generalization</head><p>Normalization techniques <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b28">29]</ref> are widely applied in recent network architectures for better optimization and regularization. Particularly, batch normalization (BN) <ref type="bibr" target="#b10">[11]</ref> improves performance and generalization ability in training  neural networks and has become indispensable in many deep neural networks. However, when BN is applied to cross-domain scenarios, it may not be optimal <ref type="bibr" target="#b2">[3]</ref>. <ref type="table" target="#tab_0">Table 1</ref> empirically validates our claim about the effects of training BN in domain generalization. We employ a pretrained ResNet-18 on the ImageNet dataset as the backbone network, and fine-tune it on the PACS dataset with two different settings; fixing the BN parameters and statistics or fine-tuning them.</p><p>Although BN generally works well in a variety of vision tasks, it consistently degrades performance when it is trained in the presence of a large domain divergence. This is because the batch statistics overfit to the particular training domains, resulting in poor generalization performance in unseen domains. This motivates us to construct a domain-agnostic feature extractor. To achieve our goal, we combine BN with instance normalization (IN) to obtain domain-agnostic features. Our intuition about the two normalization methods is as follows. Instance normalization has been widely adopted in many works regarding style transfer due to its ability to perform style normalization <ref type="bibr" target="#b9">[10]</ref>. Inspired by this, we employ IN as a means of reducing the inherent style information in each domain. In addition, IN does not depend on mini-batch construction or batch statistics, which can be helpful to extrapolate on unseen domains. These properties allow the network to learn feature representations that less overfit to a particular domain. The downside of IN, however, is that it makes the features less discriminative with respect to object categories. This is illustrated in a simplified setting <ref type="figure" target="#fig_1">(Fig. 2)</ref>, where we represent an instance by a cluster of data points and the corresponding classes by color. Unlike BN, which retains variation across the different classes, IN largely reduces the inter-class variance. To reap the benefits of IN while maintaining good classification performance, we utilize a mixture of IN and BN by optimizing the tradeoff between cross-category variance and domain invariance. More specifically, we fuse IN into all the BN layers of our network by linearly interpolating the means and variances of the two normalization statistics. The combination serves as a regularization method which results in a strong classifier that tends to focus on high-level semantic information but is much less vulnerable to domain shifts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Optimization for Domain-Specific Normalization</head><p>Based on the intuitions above, we propose a domain-specific optimized normalization (DSON) for domain generalization. Given an example from domain d, the proposed domain-specific normalization layer transforms channel-wise whitened activations using affine parameters ? d and ? d . Note that whitening is also performed for each domain. At each channel, the activations x d ? R H?W ?N are transformed as</p><formula xml:id="formula_2">DSON d (x d [i, j, n]; ? d , ? d ) = ? d ?x d [i, j, n] + ? d ,<label>(3)</label></formula><p>where the whitening is performed using the domain-specific mean and variance, ? dn and ? 2 dn ,x</p><formula xml:id="formula_3">d [i, j, n] = x d [i, j, n] ? ? dn ? 2 dn + .<label>(4)</label></formula><p>We combine batch normalization (BN) with instance normalization (IN) in a similar manner to <ref type="bibr" target="#b15">[16]</ref> as</p><formula xml:id="formula_4">? dn = w d ? bn d + (1 ? w d )? in n ,<label>(5)</label></formula><formula xml:id="formula_5">? 2 dn = w d ? bn d 2 + (1 ? w d )? in n 2 ,<label>(6)</label></formula><p>where both are calculated separately in each domain as</p><formula xml:id="formula_6">? bn d = n i,j x d [i, j, n] N ? H ? W and ? bn d 2 = n i,j x d [i, j, n] ? ? bn d 2 N ? H ? W ,<label>(7)</label></formula><p>and</p><formula xml:id="formula_7">? in n = i,j x d [i, j, n] H ? W and ? in n 2 = i,j x d [i, j, n] ? ? in n 2 H ? W .<label>(8)</label></formula><p>The optimal mixture weight, w d , between BN and IN are trained to minimize the loss in Equation 2. Note that our domain-specific mixture weights are shared across all layers for each domain, which facilitates to find the optimal point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Inference</head><p>A test example x in a target domain is unknown during training. Hence, for inference, we feed the example to the feature extractors of all domains. The final label prediction is given by computing the logits using the fully connected layer D, averaging the logits, i.e. 1 S S s=1 D(F s (x)), and finally applying a softmax function.</p><p>One potential issue in the inference step is whether target domains can rely on the model trained only on source domains. This is the main challenge in domain generalization, which assumes that reasonably good representations of target domains can be obtained from the information in source domains only. In our algorithm, instance normalization in each domain has the capability to remove domain-specific styles and standardize the representation. Since each domain has different characteristics, we learn the relative weights of instance normalization in each domain separately. Thus, predictions in each domain should be accurate enough even for the data in target domains. Additionally, the accuracy given by aggregating the predictions of multiple networks trained on different source domains should further improve accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To depict the effectiveness of domain-specific optimized normalization (DSON), we implement it on domain generalization benchmarks and provide an extensive ablation study of the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>Datasets We evaluate the proposed method on three domain generalization benchmarks. The PACS dataset <ref type="bibr" target="#b12">[13]</ref> is commonly used in domain generalization and is favored due to its large inter-domain shift across four domains: Photo, Art Painting, Cartoon, and Sketch. It contains a total of 9,991 images in 7 categories, with an image resolution of 227 ? 227. We follow the experimental protocol in <ref type="bibr" target="#b12">[13]</ref>, where the model is trained on any three of the four domains (source domains), and then tested on the remaining domain (target domain). Office-Home <ref type="bibr" target="#b27">[28]</ref> is a popular domain adaptation dataset, which consists of four distinct domains: Artistic Images, Clip Art, Product, and Real-world Images. Each domain contains 65 categories, with around 15,500 images in total. While the dataset is mostly used in the domain adaptation context, it can easily be repurposed for domain generalization by following the same protocol used in the PACS dataset. Finally, we employ five datasets-MNIST, MNIST-M, USPS, SVHN and Synthetic Digits-for digit recognition and split training and testing subsets following <ref type="bibr" target="#b29">[30]</ref>.</p><p>Implementation details For the fair comparison with prior arts <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b13">14]</ref>, we employ ResNet as the backbone network in all experiments. The convolutional and BN layers are initialized with ImageNet pretrained weights. We use <ref type="table">Table 2</ref>. Comparision with the state-of-the-art domain generalization methods (%) on the PACS dataset using ResNet-18 and ResNet-50 architectures. Column title indicates the target domain. *All experiments use the "train" split for the source domains, except MetaReg <ref type="bibr" target="#b1">[2]</ref>, which uses both "train" and "validation" splits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Archictecture Method</head><p>Art painting Cartoon Sketch Photo Avg. a batch size of 32 images per source domain, and optimize the network parameters over 10K iterations using SGD-M with a momentum 0.9 and an initial learning rate ? 0 = 0.02. As suggested in <ref type="bibr" target="#b30">[31]</ref>, the learning rate is annealed by ? p = ?0 (1+?p) ? , where ? = 10, ? = 0.75, and p increases linearly from 0 to 1 as training progresses. We follow the domain generalization convention by training with the "train" split from each of the source domains, then testing on the combined "train" and "validation" splits of the target domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ResNet</head><p>We made the mixture weights shared across all layers in our network to facilitate optimization. In our experiments, the convergence rates of local mixture weights in lower layers were significantly slower than higher layers. We sidestepped this issue by sharing the mixture weights across all the layers. This strategy improved accuracy substantially and consistently in all settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with Other Methods</head><p>In this section, we compare our method with other domain generalization methods on PACS and Office-Home datasets. PACS <ref type="table">Table 2</ref> portrays the domain generalization accuracy on the PACS dataset. The proposed algorithm is compared with several existing methods, which include JiGen <ref type="bibr" target="#b3">[4]</ref>, D-SAM <ref type="bibr" target="#b5">[6]</ref>, Epi-FCR <ref type="bibr" target="#b13">[14]</ref>, MetaReg <ref type="bibr" target="#b1">[2]</ref>, MASF <ref type="bibr" target="#b6">[7]</ref>, and MMLD <ref type="bibr" target="#b17">[18]</ref>. Our method outperforms both the baseline and other state-of-theart techniques by significant margins, which is particularly effective for hard domain (Sketch). When ResNet-50 is employed as a backbone architecture, our method still achieves better performance than other baselines; this result implies the scalability and stability of DSON when it is incorporated into a larger model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Office-Home</head><p>We also evaluate DSON on the Office-Home dataset, and the results are presented in <ref type="table">Table 3</ref>. As in PACS, DSON outperforms the recently proposed JiGen <ref type="bibr" target="#b3">[4]</ref> and D-SAM <ref type="bibr" target="#b5">[6]</ref> as well as our baseline. We find that DSON achieves the best score on all target domains. Again, DSON is more advantageous in hard domain (Clipart).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>PACS and Office-Home Dataset We conduct an ablation study to assess the contribution of individual components within our full algorithm on the PACS and Office-Home datasets. <ref type="table" target="#tab_2">Table 4</ref> presents the results, where our complete method is denoted by DSON. It also presents accuracies of its variants given by different implementations of normalization layers. We first present results from the baseline method, where the model is trained na?vely with BN layers that are not specific to any single domain. Then, to examine the effects of domain-specific normalization layers, the BN layers are made specific to each of the source domains, which is denoted by DSBN <ref type="bibr" target="#b4">[5]</ref>. We also examine the suitability of SN <ref type="bibr" target="#b15">[16]</ref> by replacing BN layers with adaptive mixtures of BN, IN and LN. IBN-Net <ref type="bibr" target="#b22">[23]</ref> concatenates instance normalization and batch normalization in a channel axis, but its improvement is marginal. We do not include batch-instance normalization (BIN) <ref type="bibr" target="#b21">[22]</ref> in our experiment because it is hard to optimize and the results are unstable. The ablation study clearly illustrates the benefits of individual components in our algorithm: optimization of multiple normalization methods and domain-specific normalization. Note that other normalization methods can degrade the performance compared to baseline depending on the dataset, while DSON consistently displays superior results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Digits Dataset</head><p>The results on five digits datasets are shown in <ref type="table">Table 5</ref>. Our model achieves 87.32% of average accuracy, outperforming all other baselines by large margins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Additional Experiments</head><p>Robustness against Label Noise The performance of the proposed algorithm on the PACS dataset is tested in the presence of label noise, and the results are investigated against other approaches. Two different noise levels are tested (0.2 and 0.5), and the results are presented in <ref type="table" target="#tab_3">Table 6</ref>. Although all algorithms undergo performance degradation, the amount of accuracy drops is marginal in general and DSON turns out to be more reliable with respect to label noise compared to other models. This is partly because DSON makes the network less overfit to class discrimination, reducing the effects of noise.  Multi-Source Domain Adaptation DSON can be extended to the multisource domain adaptation task, where we gain access to unlabeled data from the target domain. To compare the effect of different normalization methods, we adopt the algorithm proposed by Shu et al. <ref type="bibr" target="#b25">[26]</ref> as the baseline method and vary the normalization method only. The results are shown in <ref type="table" target="#tab_4">Table 7</ref>, where we compare DSON with baseline, SN <ref type="bibr" target="#b15">[16]</ref>, and DSBN <ref type="bibr" target="#b4">[5]</ref>. All compared methods illustrate a large improvement over the baseline. In direct contrast to the results from the ablation analysis in <ref type="table" target="#tab_2">Table 4</ref>, DSBN is clearly superior to SN. This is unsurprising, given that DSBN is focused specifically on the domain adaptation task. Interestingly, we find that DSON outperforms not only the baseline but also DSBN, which demonstrates how effectively DSON can be extended to the domain adaptation task. Domain-specific models consistently outperform their domain-agnostic counterparts in this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Analysis</head><p>Mixture Weights For a trained network with DSON in domain generalization tasks, the mixture weights of IN and BN are 3:7 and 1:9 on average for PACS and Office-Home datasets, respectively. Additionally, we analyzed the effect that the number of source domains has on the value of the mixture weights. To this end, we tested two scenarios; single-source and multi-source. For each domain, single-source denotes that only the specified domain was used for training, while multi-source indicates that other domains, along with the specified domain, were used for training. The graphs in <ref type="figure" target="#fig_2">Fig. 3</ref> show the weight ratio of IN to BN for each domain in the two scenarios. As shown in the plot, training with multisource domains led to a large and consistent increase in the usage of IN for all domains, because the multi-source scenario requires more domain-invariant representations than the single-source scenario. We also validated the effectiveness of separating mixture weights for each domain independently in <ref type="table">Table 8</ref>, in which domain-specific mixture weights boosts the performance compared to the domain-agnostic ones.</p><p>Effects of Instance Normalization To analyze the effects of combining instance normalization, we tested single-source domain generalization on every source-target combination of domains, as shown in <ref type="table">Table 9</ref>. Rows and columns denote source domains and target domains, respectively. For evaluation, we used the validation split of the target domain in case source and target domains are the same. Although DSON marginally sacrifices the accuracy compared to BN when source and target domains are the same, it brought a significant performance gain in most cross-domain scenarios. This presents a desirable trade-off in combining instance normalization; cross-category variance for domain invariance. Results from Single Source Domain Branch We present the classification results using each single domain branch of our model on the PACS dataset in <ref type="table" target="#tab_0">Table 10</ref>. Columns denote target domains and rows denote each single source domain branch of our model. It shows that the results from single domain branches differ slightly from each other in accuracy, and integrating them gives consistent performance gain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We presented a simple but effective domain generalization algorithm based on domain-specific optimized normalization layers. The proposed algorithm uses multiple normalization methods while learning a separate affine parameter per domain. The mixing weights are employed to compute the weighted average of multiple normalization statistics for each domain separately. This strategy turns out to be helpful for learning domain-invariant representations since instance normalization removes domain-specific style while preserving semantic category information effectively. The proposed algorithm achieves the state-of-the-art accuracy consistently on multiple standard benchmarks even with substantial label noise. We showed that the algorithm is well-suited for unsupervised domain adaptation as well. Finally we analyzed the characteristics and effects of our method with diverse ablative study.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Illustration of Domain Specific Optimized Normalization (DSON). Each domain maintains domain-specific batch normalization statistics and affine parameters, as well as mixture weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Comparing feature distributions of three classes, where color represents the class label and each dot represents a feature map with two channels. where each axis corresponds to one channel. For given (a) input activations, (c) instance normalization makes the features less discriminative over classes when compared to (b) batch normalization. Although instance normalization loses discriminability, it makes the normalized representations less overfit to a particular domain and eventually improves the quality of features when combined with batch normalization. (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Analysis of the mixture weights with single-source and multi-source scenarios on the PACS (left) and Office-Home (right) datasets. We present the weight ratio of IN to BN in our DSON module. (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Effects of training batch normalization on the PACS dataset using a ResNet-18 architecture. Each column shows the performance on the target domain when a network is trained using the remaining domains as sources. Fine-tuning BN parameters degrades the generalization performance by overfitting to source domains.</figDesc><table><row><cell></cell><cell cols="5">Art painting Cartoon Sketch Photo Avg.</cell></row><row><cell>BN fixed</cell><cell>79.25</cell><cell>74.61</cell><cell cols="3">71.52 95.99 80.34</cell></row><row><cell>BN finetuned</cell><cell>78.47</cell><cell>70.41</cell><cell>70.68</cell><cell>95.87</cell><cell>78.86</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Domain generalization accuracy (%) in ablation study. We compare DSON (ours) with its variants given by different implementations of normalization layers. 'Art.' denotes 'Art painting' domain in the PACS dataset.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">PACS</cell><cell></cell><cell></cell><cell cols="2">Office-Home</cell></row><row><cell></cell><cell cols="7">Art. Cartoon Sketch Photo Avg. Art Clipart Product Real-World Avg.</cell></row><row><cell>Baseline</cell><cell cols="5">78.47 70.41 70.68 95.87 78.86 58.71 44.20</cell><cell>71.75</cell><cell>73.19</cell><cell>61.96</cell></row><row><cell>IBN [23]</cell><cell cols="5">75.29 72.95 77.42 92.04 79.43 55.41 44.82</cell><cell>68.28</cell><cell>71.95</cell><cell>60.09</cell></row><row><cell>DSBN [5]</cell><cell cols="6">78.61 66.17 70.15 95.51 77.61 59.04 45.02 72.67</cell><cell>71.98</cell><cell>62.18</cell></row><row><cell>SN [16]</cell><cell cols="5">82.50 76.80 80.77 93.47 83.38 54.10 44.97</cell><cell>64.54</cell><cell>71.40</cell><cell>58.75</cell></row><row><cell cols="7">DSON (Ours) 84.67 77.65 82.23 95.87 85.11 59.37 45.70 71.84</cell><cell>74.68</cell><cell>62.90</cell></row><row><cell cols="8">Table 5. Domain generalization accuracy (%) on Digits datasets using a ResNet-18</cell></row><row><cell cols="8">architecture. We compare DSON (ours) with its variants given by different implemen-</cell></row><row><cell cols="3">tations of normalization layers.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Digits</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>MNIST</cell><cell>MNIST-M</cell><cell>USPS</cell><cell>SVHN</cell><cell>Synthetic</cell><cell>Avg.</cell></row><row><cell>Baseline</cell><cell></cell><cell>86.15</cell><cell>74.44</cell><cell>90.07</cell><cell>81.29</cell><cell>94.46</cell><cell>85.28</cell></row><row><cell cols="2">DSBN [5]</cell><cell>87.01</cell><cell>71.20</cell><cell>91.18</cell><cell>78.23</cell><cell>94.30</cell><cell>84.38</cell></row><row><cell>SN [16]</cell><cell></cell><cell>89.28</cell><cell>78.40</cell><cell>88.54</cell><cell>79.12</cell><cell>95.66</cell><cell>86.20</cell></row><row><cell cols="2">DSON (Ours)</cell><cell>89.62</cell><cell>79.00</cell><cell>91.63</cell><cell>81.02</cell><cell>95.34</cell><cell>87.32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 .</head><label>6</label><figDesc>Domain generalization accuracy (%) using ResNet-18 in the presence of label noise on the PACS dataset. Note that ?Avg. denotes the amount of accuracy drop with respect to the results from clean data.</figDesc><table><row><cell cols="2">Noise level Method</cell><cell cols="4">Art painting Cartoon Sketch Photo</cell><cell>Avg.</cell><cell>?Avg.</cell></row><row><cell></cell><cell>Baseline (BN)</cell><cell>75.16</cell><cell>70.41</cell><cell>68.17</cell><cell>92.13</cell><cell>76.47</cell><cell>-2.89</cell></row><row><cell></cell><cell>IBN [23]</cell><cell>77.25</cell><cell>69.75</cell><cell>69.53</cell><cell>90.60</cell><cell>76.78</cell><cell>-2.65</cell></row><row><cell>0.2</cell><cell>DSBN [5]</cell><cell>77.10</cell><cell>66.00</cell><cell>59.43</cell><cell>94.85</cell><cell>74.35</cell><cell>-3.27</cell></row><row><cell></cell><cell>SN [16]</cell><cell>78.56</cell><cell>75.21</cell><cell>77.42</cell><cell>91.08</cell><cell>80.57</cell><cell>-2.57</cell></row><row><cell></cell><cell>DSON (Ours)</cell><cell>83.11</cell><cell>79.07</cell><cell>80.15</cell><cell>94.79</cell><cell>84.03</cell><cell>-1.08</cell></row><row><cell></cell><cell>Baseline (BN)</cell><cell>73.49</cell><cell>64.68</cell><cell>58.95</cell><cell>89.22</cell><cell>71.59</cell><cell>-7.77</cell></row><row><cell></cell><cell>IBN [23]</cell><cell>67.60</cell><cell>63.58</cell><cell>65.08</cell><cell>86.53</cell><cell>70.70</cell><cell>-8.73</cell></row><row><cell>0.5</cell><cell>DSBN [5]</cell><cell>75.05</cell><cell>57.98</cell><cell>59.99</cell><cell>93.35</cell><cell>71.59</cell><cell>-6.02</cell></row><row><cell></cell><cell>SN [16]</cell><cell>78.27</cell><cell>74.06</cell><cell>72.23</cell><cell>88.86</cell><cell>78.36</cell><cell>-4.78</cell></row><row><cell></cell><cell>DSON (Ours)</cell><cell>80.22</cell><cell>73.85</cell><cell cols="3">77.37 94.91 81.59</cell><cell>-3.52</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 .</head><label>7</label><figDesc>Multi-source domain adaptation results on the PACS dataset using<ref type="bibr" target="#b25">[26]</ref> as a backbone with a ResNet-18 architecture.</figDesc><table><row><cell></cell><cell cols="2">Art painting</cell><cell>Cartoon</cell><cell>Sketch</cell><cell>Photo</cell><cell>Avg.</cell></row><row><cell>Baseline</cell><cell>78.85</cell><cell></cell><cell>76.79</cell><cell>78.14</cell><cell>99.42</cell><cell>83.30</cell></row><row><cell>DSBN [5]</cell><cell cols="2">88.94</cell><cell>83.54</cell><cell>77.39</cell><cell>99.42</cell><cell>87.32</cell></row><row><cell>SN [16]</cell><cell>79.00</cell><cell></cell><cell>77.66</cell><cell>79.11</cell><cell>97.78</cell><cell>83.39</cell></row><row><cell>DSON (Ours)</cell><cell>86.54</cell><cell></cell><cell>88.61</cell><cell>86.93</cell><cell>99.42</cell><cell>90.38</cell></row><row><cell>Art painting Cartoon</cell><cell>Sketch</cell><cell>Photo</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 8 .Table 9 .</head><label>89</label><figDesc>Effects of separating mixture weights for each domain in our DSON module on the PACS dataset with multi-source domain generalization scenario. Domain-agnostic denotes the mixture weights are shared across domains. Single-source domain generalization accuracy on the PACS dataset. Rows and columns denote source and target domains, respectively. We compare DSON (ours) with BN in terms of the amount of change (%p).</figDesc><table><row><cell cols="2">Mixture weights</cell><cell></cell><cell cols="2">Art painting</cell><cell>Cartoon</cell><cell>Sketch</cell><cell>Photo</cell><cell>Avg.</cell></row><row><cell cols="2">Domain-agnostic</cell><cell></cell><cell>82.13</cell><cell></cell><cell>74.10</cell><cell>80.02</cell><cell>95.03</cell><cell>82.82</cell></row><row><cell cols="3">Domain-specific (Ours)</cell><cell>84.67</cell><cell></cell><cell>77.65</cell><cell>82.23</cell><cell>95.87</cell><cell>85.11</cell></row><row><cell></cell><cell></cell><cell cols="2">BN (%)</cell><cell></cell><cell></cell><cell cols="2">DSON (%p)</cell></row><row><cell></cell><cell>A</cell><cell>C</cell><cell>S</cell><cell>P</cell><cell>A</cell><cell>C</cell><cell>S</cell><cell>P</cell></row><row><cell cols="6">Art painting 89.42 60.71 48.74 94.25 -1.20</cell><cell cols="3">+ 7.42 + 20.36 -0.44</cell></row><row><cell>Cartoon</cell><cell cols="5">69.68 94.51 71.60 81.56 + 2.83</cell><cell>-0.11</cell><cell cols="2">+ 0.88 + 3.65</cell></row><row><cell>Sketch</cell><cell cols="6">44.34 63.65 93.50 49.28 + 8.64 + 0.64</cell><cell>-0.67</cell><cell>+ 9.82</cell></row><row><cell>Photo</cell><cell cols="7">61.72 29.10 33.98 97.86 + 3.56 + 16.85 + 1.19</cell><cell>-0.52</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 10 .</head><label>10</label><figDesc>Results from single source domain branch on the PACS dataset. Columns denote target domains and rows denote the single source domain branch of our model.</figDesc><table><row><cell></cell><cell>Art painting</cell><cell>Cartoon</cell><cell>Sketch</cell><cell>Photo</cell></row><row><cell>Art painting</cell><cell>-</cell><cell>73.68</cell><cell>79.51</cell><cell>95.41</cell></row><row><cell>Cartoon</cell><cell>80.96</cell><cell>-</cell><cell>75.87</cell><cell>93.77</cell></row><row><cell>Sketch</cell><cell>78.71</cell><cell>71.72</cell><cell>-</cell><cell>91.98</cell></row><row><cell>Photo</cell><cell>79.49</cell><cell>75.71</cell><cell>76.38</cell><cell>-</cell></row><row><cell>DSON (all)</cell><cell>84.67</cell><cell>77.65</cell><cell>82.23</cell><cell>95.87</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer normalization</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Metareg: Towards domain generalization using meta-regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07275</idno>
		<title level="m">Universal representations: The missing link between faces, text, planktons, and cat breeds</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Domain generalization by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>D&amp;apos;innocente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Domain specific batch normalization for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">G</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Domain generalization with domain-specific aggregation modules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dainnocente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">GCPR</title>
		<imprint>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Domain generalization via model-agnostic learning of semantic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>De Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeruIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-source domain adaptation with mixture of experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to generalize: Meta-learning for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deeper, broader and artier domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Episodic training for domain generalization (2019) 2, 4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Domain generalization via conditional invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Differentiable learning-to-normalize via switchable normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note>In: ICLR (2019) 2, 4, 7, 10</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Best sources forward: domain generalization through source-specific nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Domain generalization using a mixture of multiple latent domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matsuura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI (2020)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoshida</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Unified deep supervised domain adaptation and generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Motiian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Piccirilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Adjeroh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Doretto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Domain generalization via invariant feature representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Batch-instance normalization for adaptively style-invariant neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Two at once: Enhancing learning and generalization capacities via ibn-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: NIPS</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Ssn: Learning sparse switchable normalization via sparsestmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Transferable curriculum for weaklysupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improved Texture Networks: Maximizing Quality and Diversity in Feed-Forward Stylization and Texture Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Deep Hashing Network for Unsupervised Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Panchanathan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Group Normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Deep cocktail network: Multi-source unsupervised domain adaptation with category shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Adversarial multiple source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M F</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Costeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
