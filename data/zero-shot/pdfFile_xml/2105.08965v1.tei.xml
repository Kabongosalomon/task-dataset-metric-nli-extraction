<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Railroad is not a Train: Saliency as Pseudo-pixel Supervision for Weakly Supervised Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungho</forename><surname>Lee</surname></persName>
							<email>seungholee@yonsei.ac.kr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhyun</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwuk</forename><surname>Lee</surname></persName>
							<email>jongwuklee@skku.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjung</forename><surname>Shim</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Sungkyunkwan University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Railroad is not a Train: Saliency as Pseudo-pixel Supervision for Weakly Supervised Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing studies in weakly-supervised semantic segmentation (WSSS) using image-level weak supervision have several limitations: sparse object coverage, inaccurate object boundaries, and co-occurring pixels from non-target objects. To overcome these challenges, we propose a novel framework, namely Explicit Pseudo-pixel Supervision (EPS), which learns from pixel-level feedback by combining two weak supervisions; the image-level label provides the object identity via the localization map and the saliency map from the off-the-shelf saliency detection model offers rich boundaries. We devise a joint training strategy to fully utilize the complementary relationship between both information. Our method can obtain accurate object boundaries and discard co-occurring pixels, thereby significantly improving the quality of pseudo-masks. Experimental results show that the proposed method remarkably outperforms existing methods by resolving key challenges of WSSS and achieves the new state-of-the-art performance on both PAS-CAL VOC 2012 and MS COCO 2014 datasets. The code is available at https://github.com/halbielee/EPS.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Weakly-supervised semantic segmentation (WSSS) utilizes weak supervision (e.g., image-level labels <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>, scribbles <ref type="bibr" target="#b28">[29]</ref>, or bounding boxes <ref type="bibr" target="#b21">[22]</ref>) and aims at achieving competitive performances to the fully-supervised model, which requires pixel-level labels. Most existing studies adopt image-level labels as the weak supervision of the segmentation model. The overall pipeline of WSSS consists of two stages. Firstly, pseudo-masks are generated for * indicates an equal contribution. ? Hyunjung Shim is a corresponding author.  <ref type="bibr" target="#b50">[51]</ref>, (c) localization map via CAM <ref type="bibr" target="#b51">[52]</ref> and (d) our EPS utilizing both the saliency map and the localization map for training a classifier. Note that the saliency map cannot capture person and car while our result can correctly restore them, and the localization map overly captures two objects.</p><p>target objects using an image classifier. Then, the segmentation model is trained using the pseudo-masks as supervision. The prevalent technique for generating pseudo-masks is class activation mapping (CAM) <ref type="bibr" target="#b51">[52]</ref>, which provides object localization maps corresponding to their image-level labels. Because of the supervision gap between the fully (i.e., pixel-level annotations) and weakly (i.e., image-level labels) supervised semantic segmentation, WSSS has the following key challenges: 1) the localization map only captures a small fraction of target objects <ref type="bibr" target="#b51">[52]</ref>, 2) it suffers from the boundary mismatch of the objects <ref type="bibr" target="#b22">[23]</ref>, and 3) it hardly separates co-occurring pixels from target objects (e.g., the railroad from a train) <ref type="bibr" target="#b24">[25]</ref>.</p><p>To address these problems, existing studies can be categorized into three pillars. The first approach expands object coverage to capture the full extent of objects by erasing pixels <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28]</ref>, ensembling score maps <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b26">27]</ref>, or using self-supervised signal <ref type="bibr" target="#b40">[41]</ref>. However, they fail to determine accurate object boundaries of the target object because they have no clue to guide the object's shape. The second approach focuses on improving the object boundaries of pseudo-masks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b31">32]</ref>. Since they effectively learn object boundaries, they naturally expand pseudo-masks until boundaries. However, they still fail to distinguish coincident pixels of non-target objects from a target object. It is because the strong correlation between the foreground and the background (i.e., co-occurrence) is almost indistinguishable from an inductive bias (i.e., the frequency of observing the target object and its coincident pixels), as shown in <ref type="bibr" target="#b9">[10]</ref>. Lastly, the third approach aims to mitigate the cooccurrence problem using extra groundtruth masks <ref type="bibr" target="#b23">[24]</ref>, or the saliency map <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b46">47]</ref>. However, <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b27">28]</ref> require strong pixel-level annotations, which are far from a weakly supervised learning paradigm. <ref type="bibr" target="#b34">[35]</ref> is sensitive to the errors of the saliency map. Also, <ref type="bibr" target="#b46">[47]</ref> does not cover the full extent of objects and suffers from the boundary mismatch.</p><p>In this paper, our goal is to overcome the three challenges of WSSS by fully utilizing both the localization map (i.e., CAM from the image classifier trained with image-level labels) and the saliency map (i.e., the output of the off-theshelf saliency detection model <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b50">51]</ref>). We focus on a complementary relationship in the localization map and the saliency map. As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, the localization map can distinguish different objects but does not separate their boundaries effectively. Contrarily, while the saliency map provides rich boundary information, it does not reveal object identity. In this sense, we argue that our method using two complementary pieces of information can resolve the performance bottleneck of WSSS.</p><p>To this end, we propose a novel framework for WSSS, called Explicit Pseudo-pixel Supervision (EPS). To fully utilize the saliency map (i.e., both the foreground and the background), we design a classifier to predict C + 1 classes, consisting of C target classes and the background class. We leverage C localization maps and the background localization map to estimate a saliency map. Then, the saliency loss is defined as the pixel-wise difference between the saliency map and our estimated saliency map. By introducing the saliency loss, the model can be supervised by pseudo-pixel feedback across all classes. We also use the multi-label classification loss to predict image-level labels. Therefore, we train the classifier to optimize both the saliency loss and the multi-label classification loss, synergizing the predictions for both the background and foreground pixels-we find that our strategy can improve both the saliency map (Section 3.3 and We stress that, because the saliency loss penalizes boundary mismatches via pseudo-pixel feedback, it can enforce our method to learn the object's accurate boundaries. As a byproduct, we can also capture the entire object by expanding the map until the boundaries. Because the saliency loss helps separate the foreground (e.g., a train) from the background, our method can assign the co-occurring pixels (e.g., a railroad) to the background class. Experimental results show that our EPS achieves remarkable segmentation performances, recording new state-of-the-art accuracies on PASCAL VOC 2012 and MS COCO 2014 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Weakly-supervised semantic segmentation. The general pipeline of WSSS is to generate pseudo-masks from a classification network and to use the pseudo-masks as supervision to train a segmentation network. Due to the scarcity of boundary information in the image-level label, many existing methods suffer from inaccurate pseudo-masks. To address this problem, cross-image affinity <ref type="bibr" target="#b14">[15]</ref>, knowledge graph <ref type="bibr" target="#b30">[31]</ref> and contrastive optimization <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b49">50]</ref> are used to improve the quality of pseudo-masks. <ref type="bibr" target="#b4">[5]</ref> proposes a self-supervised task to discover sub-categories to enforce the classifier to improve CAM. <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> implicitly exploit the boundary information by calculating affinities between pixels. <ref type="bibr" target="#b48">[49]</ref> focuses on producing reliable pixel-level annotations and designs an end-to-end network for generating segmentation maps. <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">25]</ref> train the segmentation network by utilizing a boundary loss. Recently, <ref type="bibr" target="#b2">[3]</ref> uses a single segmentation-based model with a self-supervised training scheme. <ref type="bibr" target="#b13">[14]</ref> focuses on the robustness of the segmentation network by utilizing multiple incomplete pseudo-masks.</p><p>Saliency-guided semantic segmentation. Saliency detection (SD) methods generate the saliency map that distinguishes between the foreground and the background in an image via external saliency datasets with pixel-level annotations <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b50">51]</ref>, or image-level annotations <ref type="bibr" target="#b38">[39]</ref>. Many WSSS methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44]</ref> exploit the saliency map as the background cues of pseudo-masks. <ref type="bibr" target="#b42">[43]</ref> utilizes the saliency map as the full supervision of singleobject images. <ref type="bibr" target="#b15">[16]</ref> uses an instance-level saliency map to learn the similarity graph for objects. <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b46">47]</ref> combine saliency maps with class-specific attention cues to generate reliable pseudo-masks. <ref type="bibr" target="#b47">[48]</ref> jointly solves WSSS and SD using a single network to improve the performance of both tasks. Our EPS can be categorized into the saliency-guided method but is clearly distinguished from all others in the following reason. Most existing methods exploit the saliency map as a part of pseudo-masks or as the implicit guidance for refining the intermediate feature of the classifier. Contrarily, our method utilizes the saliency map as pseudo-pixel</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classification loss</head><p>Image-level labels C x 1  <ref type="figure">Figure 2</ref>. The overall framework of our EPS. C + 1 localization maps are generated from a backbone network. The actual saliency map is generated from the off-the-shelf saliency detection model. Some localization maps for target labels are selectively used to generate an estimated saliency map (Section 3.2). The overall framework is jointly trained with the saliency loss and the classification loss (Section 3.3). feedback for localization maps. Although <ref type="bibr" target="#b47">[48]</ref> is the most similar work to ours in the sense of utilizing two complementary information, they neither address the co-occurring problem nor handle the noisy saliency map issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Saliency loss</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>In this section, we propose a new framework for Weaklysupervised semantic segmentation (WSSS), called Explicit Pseudo-pixel Supervision (EPS). Considering two stages in WSSS, the first stage is to generate pseudo-masks and the second stage is to train the segmentation model. Here, our main contribution is to generate accurate pseudo-masks. Following the WSSS convention <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref>, we then train a segmentation model, where the generated pseudo-masks in the first stage are used as supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Motivation</head><p>Our key insight of EPS is to fully exploit two complementary information, i.e., the object identity from the localization map and boundary information from the saliency map. To this end, we utilize the saliency map as pseudopixel feedback to the localization map for both target labels and the background. We devise a classifier with an additional background class, leading to predict a total of C + 1 classes, as shown in <ref type="figure">Figure 2</ref>. Using the classifier, we can learn C + 1 localization maps, i.e., C localization maps for target labels and a background localization map.</p><p>We then explain how EPS can tackle both the boundary mismatch and co-occurrence problems in WSSS. To manage the boundary mismatch problem, we estimate the foreground map from C localization maps and match it with the foreground of the saliency map. In this way, the localization maps for target labels can receive pseudo-pixel feed- back from the saliency map, thereby improving the boundaries of objects. To mitigate the co-occurring pixels of nontarget objects, we also match the localization map for the background with the saliency map. Since the localization map for the background also receives pseudo-pixel feedback from the saliency map, the co-occurring pixels can be successfully assigned to the background; the co-occurring pixels of non-target objects mostly overlap with the background. It is why our method can separate the co-occurring pixels from target objects. Lastly, the objective function of EPS is formulated with two parts: the saliency loss L sal (marked by red box/arrow in <ref type="figure">Figure 2</ref>) via the saliency map, and the multi-label classification loss L cls (marked by blue box/arrow in <ref type="figure">Figure 2</ref>) via image-level labels. By jointly training the two objectives, we can synergize the localization map and the saliency map with complementary information-we observe that noisy and missing information of each other is complemented via our joint training strategy, as illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>. For example, the original saliency map obtained from the off-the-shelf model <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b50">51]</ref> has missing and noisy information. On the other hand, our results successfully restore missing objects (e.g., boats or chairs) and remove the noise (e.g., water bubbles or contrail), which are evidently better than the original saliency map. Consequently, EPS can capture more accurate object boundaries and separate the co-occurring pixels from target objects. These advantages result in remarkable performance gains; <ref type="table">Table 6</ref> reports that EPS remarkably outperforms existing models up to 3.8-10.6% gains in terms of the segmentation accuracy.</p><formula xml:id="formula_0">(a) (b) (c) (d)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Explicit Pseudo-pixel Supervision</head><p>We explain how to utilize the saliency map for pseudopixel supervision. The key advantage of the saliency map is to provide an object silhouette, which can better reveals object boundaries. To make use of this property, we match the saliency map with two cases: the foreground and the background. To make class-wise localization maps comparable with the saliency map, we merge the localization maps for target labels and generate a foreground map, M f g ? R H?W . We can also represent the foreground by performing the inversion of a background map which is the localization map for the background label M bg ? R H?W . (Later, we explain how to refine the foreground map to address noisy saliency maps.)</p><p>Specifically, we estimate the saliency mapM s using M f g and M bg as follows:</p><formula xml:id="formula_1">Ms = ?M f g + (1 ? ?)(1 ? M bg ),<label>(1)</label></formula><p>where ? ? [0, 1] is a hyperparameter to adjust a weighted sum of the foreground map and the inversion of the background map. (By default, we set ? to 0.5 in our experiments and an additional ablation study for ? is found in the supplementary material.) Then, we define the saliency loss L sal as the sum of pixel-wise differences between our estimated saliency map and an actual saliency map. (The formal definition of L sal is presented in Section 3.3.)</p><p>It is worth noting that using the pre-trained model is regarded as weakly supervised learning, thus utilizing the saliency map has been widely accepted as a common practice in WSSS. Despite its popularity, adopting the fully supervised saliency detection model can be arguable in that they use pixel-level annotations from different datasets. In this paper, we investigate the effect of different saliency detection methods; 1) unsupervised and 2) fully supervised saliency detection models (see Section 5.3), and empirically show our method using any of them outperforms all other methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b46">47]</ref> using fully supervised saliency models. Whereas existing methods are limited to fully take advantage of the saliency map, our method incorporates the saliency map as pseudo-pixel supervision and exploits it as the cues for boundaries and co-occurring pixels. Map selection for handling saliency bias. Previously, we assume that the foreground map can be the union of the localization maps for target labels; the background map can be the localization map of the background label. However, such a na?ve selection rule may not be compatible with the saliency map computed by the off-the-shelf model. For example, the saliency map from <ref type="bibr" target="#b50">[51]</ref> often ignores some objects as salient objects (e.g., small people nearby a train in <ref type="figure" target="#fig_0">Figure 1</ref>). This systematic error is inevitable because the saliency model learns the statistics of different datasets. Unless considering this error, the same error may propagate to our model and lead the performance degradation.</p><p>To tackle the systematic error, we develop an effective strategy using the overlapping ratio between the localization map and the saliency map. Specifically, the i-th localization map M i is assigned to the foreground if M i is overlapped with the saliency map more than ? %, otherwise the background. Formally, the foreground and the background map are computed by:</p><formula xml:id="formula_2">M f g = C i=1 yi ? Mi ? 1[O(Mi, Ms) &gt; ? ], M bg = C i=1 yi ? Mi ? 1[O(Mi, Ms) ? ? ] + MC+1,<label>(2)</label></formula><p>where y ? R C is the binary image-level label and O(M i , M s ) is the function to compute the overlapping ratio between M i and M s . For that, we first binarize the localization map and the saliency map such that: for pixel p, B k (p) = 1 if M k (p) &gt; 0.5; B k (p) = 0, otherwise. B i and B s are the binarized maps corresponding to M i and M s , respectively. We then compute the overlapping ratio between M i and M s , i.e., O(M i , M s ) = |B i ? B s |/|B i |. We set ? = 0.4 regardless of datasets and backbone models. In the supplementary material, we show that our method is robust against the choice of ? (i.e., ? within [0.3, 0.5] shows the comparable performance).</p><p>Instead of a single localization map for the background label, we combine the localization map for the background label with the localization maps not selected as the foreground. Although it is simple, we can bypass the error of the saliency map and effectively train some objects neglected from the saliency map. (In <ref type="table" target="#tab_2">Table 3</ref>, we report the effectiveness of the proposed strategy to overcome the error of the saliency map.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Joint Training Procedure</head><p>Using the saliency map and image-level labels, the overall training objective of EPS consists of two parts, the saliency loss L sal and the classification loss L cls . First, the saliency loss L sal is formulated by measuring the average pixel-level distance between the actual saliency map M s and the estimated saliency mapM s .</p><formula xml:id="formula_3">L sal = 1 H ? W ||M s ?M s || 2 ,<label>(3)</label></formula><p>where M s is obtained from the off-the-shelf saliency detection model-PFAN <ref type="bibr" target="#b50">[51]</ref> trained on DUTS dataset <ref type="bibr" target="#b38">[39]</ref>. Note that our method consistently outperforms all previous arts regardless of the saliency detection models. Next, the classification loss is computed by a multi-label soft margin loss between the image-level label y and its prediction? ? R C , which is the result of the global average pooling on the localization map for each target class.</p><formula xml:id="formula_4">L cls = ? 1 C C i=1 yi log ?(?i) + (1 ? yi) log (1 ? ?(?i)),<label>(4)</label></formula><p>where ?(?) is the sigmoid function. Finally, the total training loss is the sum of the multi-label classification loss and the saliency loss, i.e., L total = L cls + L sal . As shown in <ref type="figure">Figure 2</ref>, L sal is involved in updating the parameters of C + 1 classes, including target objects and the background. Meanwhile, L cls only evaluates the label prediction for C classes, excluding the background classthe gradient from L cls does not flow into the background class. However, the prediction of the background class can be implicitly affected by L cls because it supervises classifier training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Setup</head><p>Datasets. We conduct an empirical study on two popular benchmark datasets, PASCAL VOC 2012 <ref type="bibr" target="#b11">[12]</ref> and MS COCO 2014 <ref type="bibr" target="#b29">[30]</ref>. PASCAL VOC 2012 consists of 21 classes (i.e., 20 objects and the background) with 1,464, 1,449, and 1,456 images for training, validation, and test set, respectively. Following the common practice in semantic segmentation, we use the augmented training set with 10,582 images <ref type="bibr" target="#b16">[17]</ref>. Next, COCO 2014 consists of 81 classes, including a background, with 82,081 and 40,137 images for training and validation, where images with no target classes are excluded as done in <ref type="bibr" target="#b8">[9]</ref>. Because the groundtruth segmentation labels of some objects overlap each other, we adopt the groundtruth segmentation labels from COCO-Stuff <ref type="bibr" target="#b3">[4]</ref>, which solves the overlapping problem on the same COCO dataset. Evaluation protocol. We validate our method with the validation and the test set on PASCAL VOC 2012, and the validation set on COCO 2014. The evaluation results on the test set of PASCAL VOC 2012 is obtained from the official PASCAL VOC evaluation server. Also, we adopt mean intersection-over-union (mIoU) to measure the accuracy of segmentation models.  Implementation details. We choose ResNet38 <ref type="bibr" target="#b44">[45]</ref> as the backbone network of our method with the output stride of 8. All backbone models are pre-trained on ImageNet <ref type="bibr" target="#b10">[11]</ref>. We use the SGD optimizer with a batch size of 8. Our method is trained until 20k iterations with learning rate 0.01 (0.1 for the last convolutional layer). For data augmentation, we use a random scaling, random flipping, and random crop into 448 ? 448. For the segmentation networks, we adopt DeepLab-LargeFOV (V1) <ref type="bibr" target="#b6">[7]</ref> and DeepLab-ASPP (V2) <ref type="bibr" target="#b7">[8]</ref>, and VGG16 and ResNet101 for their backbone networks. Specifically, we use four segmentation networks: VGG16-based DeepLab-V1 and DeepLab-V2, ResNet101 based DeepLab-V1 and DeepLab-V2. More detailed setting is in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Handling Boundary and Co-occurrence</head><p>Boundary mismatch problem. To validate the boundary of pseudo-masks, we compare the quality of boundaries with the state-of-the-art methods <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b51">52]</ref>. We utilize SBD <ref type="bibr" target="#b16">[17]</ref>, which provides boundary annotations and the boundary benchmark in PASCAL VOC 2011. As done in <ref type="bibr" target="#b31">[32]</ref>, the quality of the boundary is evaluated in a classagnostic manner by computing the edges of pseudo-masks from the Laplacian edge detector. Then, the boundary quality is evaluated by measuring recall, precision, and F1score, comparing the predicted and groundtruth boundaries. <ref type="table">Table 1</ref> reports that our method largely outperforms other methods in all three metrics. The qualitative examples in <ref type="figure" target="#fig_4">Figure 4</ref> show that our method can capture more accurate boundaries than all the other methods. Co-occurrence problem. As discussed in several stud- ies <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b34">35]</ref>, we observe that some background classes frequently appear with target objects in PASCAL VOC 2012. We quantitatively analyze the frequency of cooccurred objects by employing the PASCAL-CONTEXT dataset <ref type="bibr" target="#b32">[33]</ref>, which provides pixel-level annotations for a whole scene (e.g., water and railroad). We choose three co-occurring pairs; boat with water, train with railroad, and train with platform. We compare IoU for the target class and the confusion ratio between a target class and its coincident class. The confusion ratio measures how much the coincident class is incorrectly predicted as the target class. The confusion ratio m k,c is calculated by m k,c = F P k,c /T P c , where F P k,c is the number of pixels mis-classified as the target class c for the coincident class k, and T P c is the number of true-positive pixels for the target class c. More detailed analysis on the co-occurrence problem is in the supplementary materials. <ref type="table">Table 2</ref> reports that EPS consistently shows a lower confusion ratio than other methods. SGAN <ref type="bibr" target="#b46">[47]</ref> has quite a similar confusion ratio with ours, but our method captures the target class much accurately in terms of IoU. Interestingly, SEAM shows a high confusion ratio and even worse than CAM. It is because SEAM <ref type="bibr" target="#b40">[41]</ref> learns to cover the full extent of target objects by applying self-supervised training, which is easily fooled by the coincident pixels of target objects. Meanwhile, CAM only captures the most discriminative region of target objects and does not cover the less discriminative parts, e.g., the coincident class. We can also observe this phenomenon in <ref type="figure" target="#fig_4">Figure 4</ref>.  <ref type="table">Table 4</ref>. Accuracy (mIoU) of pseudo-masks evaluated on the PAS-CAL VOC 2012 train set. Note that * indicates that low-confident pixels are ignored; other methods use all pixels for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Effect of Map Selection Strategies</head><p>We evaluate the effectiveness of our map selection strategy to mitigate the error of the saliency map. We compare three different map selection strategies to the baseline, which does not use the map selection module. As the na?ve strategy, the foreground map is the union of all object localization maps; the background map equals the localization map of the background class (i.e., na?ve strategy). Next, we follow the na?ve strategy with the following exceptions. The localization maps of several pre-determined classes (e.g., sofa, chair, and dining table) are assigned to the background map (i.e., pre-defined class strategy). Lastly, the proposed selection method utilizes the overlapping ratio between the localization map and the saliency map, as explained in Section 3.2 (i.e., our adaptive strategy). <ref type="table" target="#tab_2">Table 3</ref> shows that our adaptive strategy can effectively handle the systematic bias of the saliency map. The na?ve strategy implies no bias consideration when generating the estimated saliency map from the localization maps. In this case, the performance of pseudo-masks is degraded, especially on sofa, chair or dining table classes. The performance of using pre-defined classes shows that the bias can be mitigated by neglecting missing classes in the saliency map. However, as it requires manual selection by human observers, it is less practical and cannot make an optimal decision per image. Meanwhile, our adaptive strategy can handle the bias automatically and makes more effective decisions for a given saliency map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison with state-of-the-arts</head><p>Accuracy of pseudo-masks. We adopt a multi-scale inference by aggregating the prediction results from images with different scales, which is a common practice utilized in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b40">41]</ref>. Then, We evaluate the accuracies of pseudomasks in the train set by comparing our EPS with the baseline CAM <ref type="bibr" target="#b51">[52]</ref> and three state-of-the-art methods, i.e., SEAM <ref type="bibr" target="#b40">[41]</ref>, ICD <ref type="bibr" target="#b12">[13]</ref>, and SGAN <ref type="bibr" target="#b46">[47]</ref>. Here, measuring the accuracy of the pseudo-masks in the train set is a common protocol in WSSS because the pseudo-masks of the train set are used to supervise the segmentation model. <ref type="table">Table 4</ref> summarizes the accuracies of pseudo-masks and indicates that our method clearly outperforms all existing methods by large margins (i.e., 7-21% gaps). <ref type="figure" target="#fig_4">Figure 4</ref> visualizes the qualitative examples of pseudo-masks, confirming that our method remarkably improves the object boundary and significantly outperforms three state-of-the-art methods in terms of the quality of pseudo-masks. Our method can capture the precise boundaries of objects (2nd row) and thus naturally cover the full extent of objects (3rd row), and also mitigate the coincident pixels (1st row). More examples and failure cases of our method are provided in the supplementary material.</p><p>Accuracy of segmentation maps. Previous methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b40">41]</ref> generate pseudo-masks and refine them with the CRF post-processing algorithm <ref type="bibr" target="#b25">[26]</ref> or affinity network <ref type="bibr" target="#b1">[2]</ref>. Meanwhile, as shown in <ref type="table">Table 4</ref>, our generated pseudomasks are accurate enough, thereby we train a segmentation network without any additional refinement for pseudomasks. We extensively evaluate and precisely compare our method with others on the four segmentation networks in the Pascal VOC 2012 dataset.</p><p>Our method performs remarkably better than other methods regardless of segmentation networks. <ref type="table" target="#tab_3">Table 5</ref> reports that our method is more accurate than other methods with the same VGG16 backbone. Besides, our results on the VGG16 are comparable or even superior to other existing methods based on a more powerful backbone (i.e. ResNet101 in <ref type="table">Table 6</ref>). Our method also shows a clear improvement over existing methods. Finally, <ref type="table">Table 6</ref>  achieved by the existing state-of-the-art models were approximately 1%. Meanwhile, our method achieves more than 3% higher gains than the previous best record. <ref type="figure" target="#fig_5">Figure 5</ref> visualizes the qualitative examples of our segmentation results on PASCAL VOC 2012. These results confirm that our method provides accurate boundaries and successfully resolves the co-occurrence problem. In <ref type="table">Table 7</ref>, we further evaluate our method in the COCO 2014 dataset. We use VGG16 based DeepLab-V2 as the segmentation network to compare with SGAN <ref type="bibr" target="#b46">[47]</ref>, which is the state-of-the-art WSSS model in the COCO dataset. Our method achieves 35.7 mIoU in the validation set, and it is 1.9% higher than SGAN <ref type="bibr" target="#b46">[47]</ref>. Consequently, we achieve the new state-of-the-art accuracy in the COCO 2014 dataset. These outstanding performances over the existing state-ofthe-arts on both datasets confirm the effectiveness of our  method; by fully utilizing both localization maps and the saliency map, it successfully captures the integral of target objects correctly and remedies the shortcomings of existing models. <ref type="figure" target="#fig_6">Figure 6</ref> shows the qualitative examples of segmentation results on the COCO 2014 dataset. Our method performs well when a few objects appear without occlusions but less effective in handling many small objects. More examples and failure cases of our method are provided in the supplementary material.</p><p>Effect of saliency detection models. To investigate the ef-fect of different saliency detection models, we adopt three saliency models; PFAN <ref type="bibr" target="#b50">[51]</ref> (our default), DSS <ref type="bibr" target="#b17">[18]</ref> used by OAA <ref type="bibr" target="#b20">[21]</ref> and ICD <ref type="bibr" target="#b12">[13]</ref>, and USPS <ref type="bibr" target="#b33">[34]</ref> (i.e., the unsupervised detection model). The segmentation results (mIoU) under Resnet101 based DeepLab-V1 are 71.0/71.8 with PFAN, 70.0/70.1 with DSS, and 68.8/69.9 with USPS (validation set and test set), respectively. These scores support that our EPS using any of three different saliency models is still more accurate than all the other methods in <ref type="table">Table 6</ref>. Notably, our EPS using the unsupervised saliency model outperforms all existing methods using the supervised saliency model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We propose a novel weakly supervised segmentation framework, namely explicit pseudo-pixel supervision (EPS). Motivated by the complementary relationship between the localization map and the saliency map, our EPS learns from pseudo-pixel feedback combining with the saliency map and the localization map. Owing to our joint training scheme, we successfully complement noise or missing information on both sides. Consequently, our EPS can capture precise object boundaries and discard cooccurring pixels of non-target objects, remarkably improving the quality of pseudo-masks. Extensive evaluations and various case studies demonstrate the effectiveness of our EPS and the outstanding performances, the new state-ofthe-art accuracies for WSSS on both PASCAL VOC 2012 and MS COCO 2014 datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Motivating example of utilizing both the saliency map and the localization map for WSSS. (a) Groundtruth, (b) saliency map via PFAN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 )</head><label>3</label><figDesc>and the pseudo-mask (Section 5.1 and Fig-ure 4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Qualitative examples of estimated saliency maps on PASCAL VOC 2012. (a) Input images, (b) groundtruth, (c) saliency maps from [51] and (d) our estimated saliency maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative comparison for pseudo-masks on PASCAL VOC 2012. (a) Input images, (b) groundtruth, (c) CAM, (d) SEAM, (e) ICD, (f) SGAN and (g) our EPS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative examples of segmentation results on PASCAL VOC 2012. (a) Input images, (b) groundtruth and (c) our EPS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Qualitative examples of segmentation results on MS COCO 2014. (a) Input images, (b) groundtruth and (c) our EPS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .Table 2</head><label>12</label><figDesc>Boundary accuracy evaluated on the SBD trainval set. Note that the results of BES are measured from the boundary prediction network proposed in<ref type="bibr" target="#b31">[32]</ref>.</figDesc><table><row><cell>Method</cell><cell cols="3">Recall (%) Precision (%) F1-score (%)</cell></row><row><cell>CAM [52]CVPR'16</cell><cell>22.3</cell><cell>35.8</cell><cell>27.5</cell></row><row><cell>SEAM [41]CVPR'20</cell><cell>40.2</cell><cell>45.0</cell><cell>42.5</cell></row><row><cell>BES [32]ECCV'20</cell><cell>45.5</cell><cell>46.4</cell><cell>45.9</cell></row><row><cell>Our EPS</cell><cell>60.0</cell><cell>73.1</cell><cell>65.9</cell></row><row><cell>Method</cell><cell>boat w/ water</cell><cell>train w/ railroad</cell><cell>train w/ platform</cell></row><row><cell>CAM [52]CVPR'16</cell><cell cols="3">0.74 (33.1) 0.11 (52.9) 0.09 (49.6)</cell></row><row><cell>SEAM [41]CVPR'20</cell><cell cols="3">1.13 (30.7) 0.24 (48.6) 0.20 (45.5)</cell></row><row><cell>ICD [13]CVPR'20</cell><cell cols="3">0.47 (41.4) 0.11 (56.7) 0.09 (49.2)</cell></row><row><cell cols="4">SGAN [47]ACCESS'20 0.10 (42.3) 0.02 (48.8) 0.01 (36.3)</cell></row><row><cell>Our EPS</cell><cell cols="3">0.10 (55.0) 0.02 (78.1) 0.01 (73.0)</cell></row></table><note>. Comparison with representative existing methods han- dling the co-occurrence problem. Each entry is m k,c in blue (the lower the better) and IoU in the bracket (the higher the better).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Effect of map selection strategies. The accuracies of pseudo-masks using different map selection strategies are evaluated on the PASCAL VOC 2012 train set.</figDesc><table><row><cell></cell><cell cols="4">Baseline Na?ve Pre-defined Our adaptive</cell></row><row><cell>mIoU</cell><cell>66.1</cell><cell>66.5</cell><cell>67.9</cell><cell>69.4</cell></row><row><cell cols="2">Method</cell><cell cols="3">w/o refinement CRF [26] AffinityNet [2] w/ w/</cell></row><row><cell cols="2">CAM [52]CVPR'16</cell><cell>48.0</cell><cell>-</cell><cell>58.1</cell></row><row><cell cols="2">SEAM [41]CVPR'20</cell><cell>55.4</cell><cell>56.8</cell><cell>63.6</cell></row><row><cell cols="2">ICD [32]CVPR'20*</cell><cell>59.9</cell><cell>62.2</cell><cell>-</cell></row><row><cell cols="2">SGAN [47]ACCESS'20*</cell><cell>62.8</cell><cell>-</cell><cell>-</cell></row><row><cell>Our EPS</cell><cell></cell><cell>69.4</cell><cell>71.4</cell><cell>71.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>demonstrates that our method (under ResNet101 based DeepLab-V1 with saliency map) achieves the new state-of-the-art performance (71.0 for validation and 71.8 for test set) in the PASCAL VOC 2012 dataset. We highlight that the gains Segmentation results (mIoU) on PASCAL VOC 2012. All results are based on VGG16. The best score is in bold throughout all experiments.</figDesc><table><row><cell>Method</cell><cell cols="2">Seg. Sup.</cell><cell>val</cell><cell>test</cell></row><row><cell>SEC [25]ECCV'16</cell><cell>V1</cell><cell>I.</cell><cell cols="2">50.7 51.7</cell></row><row><cell>AffinityNet [2]CVPR'18</cell><cell>V1</cell><cell>I.</cell><cell cols="2">58.4 60.5</cell></row><row><cell>ICD [13]CVPR'20</cell><cell>V1</cell><cell>I.</cell><cell cols="2">61.2 60.9</cell></row><row><cell>BES [32]ECCV'20</cell><cell>V1</cell><cell>I.</cell><cell cols="2">60.1 61.1</cell></row><row><cell>GAIN [28]CVPR'18</cell><cell>V1</cell><cell cols="3">I.+S. 55.3 56.8</cell></row><row><cell>MCOF [40]CVPR'18</cell><cell>V1</cell><cell cols="3">I.+S. 56.2 57.6</cell></row><row><cell>SSNet [48]ICCV'19</cell><cell>V1</cell><cell cols="3">I.+S. 57.1 58.6</cell></row><row><cell>DSRG [20]CVPR'18</cell><cell>V2</cell><cell cols="3">I.+S. 59.0 60.4</cell></row><row><cell>SeeNet [19]NeurIPS'18</cell><cell>V1</cell><cell cols="3">I.+S. 61.1 60.7</cell></row><row><cell>MDC [44]CVPR'18</cell><cell>V1</cell><cell cols="3">I.+S. 60.4 60.8</cell></row><row><cell>FickleNet [27]CVPR'18</cell><cell>V2</cell><cell cols="3">I.+S. 61.2 61.9</cell></row><row><cell>OAA [21]ICCV'19</cell><cell>V1</cell><cell cols="3">I.+S. 63.1 62.8</cell></row><row><cell>ICD [13]CVPR'20</cell><cell>V1</cell><cell cols="3">I.+S. 64.0 63.9</cell></row><row><cell>Multi-Est. [14]ECCV'20</cell><cell>V1</cell><cell cols="3">I.+S. 64.6 64.2</cell></row><row><cell>Split. &amp; Merge. [50]ECCV'20</cell><cell>V2</cell><cell cols="3">I.+S. 63.7 64.5</cell></row><row><cell>SGAN [47]ACCESS'20</cell><cell>V2</cell><cell cols="3">I.+S. 64.2 65.0</cell></row><row><cell>Our EPS</cell><cell>V1 V2</cell><cell cols="3">I.+S. 66.6 67.9 I.+S. 67.0 67.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .Table 7 .</head><label>67</label><figDesc>Segmentation results (mIoU) on PASCAL VOC 2012. All results are based on ResNet101. Segmentation results (mIoU) on MS COCO 2014. All results are based on VGG16.</figDesc><table><row><cell>Method</cell><cell cols="2">Seg. Sup.</cell><cell>val</cell><cell>test</cell></row><row><cell>ICD [13]CVPR'20</cell><cell>V1</cell><cell>I.</cell><cell cols="2">64.1 64.3</cell></row><row><cell>SC-CAM [5]CVPR'20</cell><cell>V1</cell><cell>I.</cell><cell cols="2">66.1 65.9</cell></row><row><cell>BES [32]ECCV'20</cell><cell>V2</cell><cell>I.</cell><cell cols="2">65.7 66.6</cell></row><row><cell>LIID [31]TPAMI'20</cell><cell>V2</cell><cell>I.</cell><cell cols="2">66.5 67.5</cell></row><row><cell>MCOF [40]CVPR'18</cell><cell>V1</cell><cell cols="3">I.+S. 60.3 61.2</cell></row><row><cell>SeeNet [19]NeurIPS'18</cell><cell>V1</cell><cell cols="3">I.+S. 63.1 62.8</cell></row><row><cell>DSRG [20]CVPR'18</cell><cell>V2</cell><cell cols="3">I.+S. 61.4 63.2</cell></row><row><cell>FickleNet [27]CVPR'18</cell><cell>V2</cell><cell cols="3">I.+S. 64.9 65.3</cell></row><row><cell>OAA [21]ICCV'19</cell><cell>V1</cell><cell cols="3">I.+S. 65.2 66.4</cell></row><row><cell>Multi-Est. [14]ECCV'19</cell><cell>V1</cell><cell cols="3">I.+S. 67.2 66.7</cell></row><row><cell>MCIS [38]ECCV'20</cell><cell>V1</cell><cell cols="3">I.+S. 66.2 66.9</cell></row><row><cell>SGAN [47]ACCESS'20</cell><cell>V2</cell><cell cols="3">I.+S. 67.1 67.2</cell></row><row><cell>ICD [13]CVPR'20</cell><cell>V1</cell><cell cols="3">I.+S. 67.8 68.0</cell></row><row><cell>Our EPS</cell><cell>V1 V2</cell><cell cols="3">I.+S. 71.0 71.8 I.+S. 70.9 70.8</cell></row><row><cell>Method</cell><cell cols="2">Seg. Sup.</cell><cell>val</cell></row><row><cell>SEC [25]ECCV'16</cell><cell>V1</cell><cell>I.</cell><cell>22.4</cell></row><row><cell>DSRG [20]CVPR'18</cell><cell>V2</cell><cell cols="2">I.+S. 26.0</cell></row><row><cell>ADL [9]TPAMI'20</cell><cell>V1</cell><cell cols="2">I.+S. 30.8</cell></row><row><cell>SGAN [47]ACESS'20</cell><cell>V2</cell><cell cols="2">I.+S. 33.6</cell></row><row><cell>Our EPS</cell><cell>V2</cell><cell cols="2">I.+S. 35.7</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of instance segmentation with inter-pixel relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwoon</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2209" to="2218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning pixel-level semantic affinity with image-level supervision for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwoon</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Single-stage semantic segmentation from image labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Araslanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="4253" to="4262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cocostuff: Thing and stuff classes in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1209" to="1218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Weaklysupervised semantic segmentation via sub-category exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ting</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaosong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robinson</forename><surname>Piramuthu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="8991" to="9000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Discovering class-specific pixels for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arslan</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Puneet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Dokania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Attentionbased dropout layer for weakly supervised single object localization and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjung</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Evaluating weakly supervised object localization methods right</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungho</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjung</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3133" to="3142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning integral objects with intra-class discriminator for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4283" to="4292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Employing multi-estimations for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cian: Cross-image affinity net for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="10762" to="10769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Associating inter-image salient instances for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruochen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Ralph R Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="367" to="383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="991" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deeply supervised salient object detection with short connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3203" to="3212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Self-erasing network for integral object attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengtao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="549" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Weakly-supervised semantic segmentation network with deep seeded region growing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="7014" to="7023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Integral object mining via online attention accumulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Tao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Kai</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Simple does it: Weakly supervised instance and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="876" to="885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Donggeun Yoo, and In So Kweon. Two-phase learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyeon</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3534" to="3543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improving weakly-supervised object localization by micro-annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<editor>Edwin R. Hancock Richard C. Wilson and William A. P. Smith</editor>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="92" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Seed, expand and constrain: Three principles for weakly-supervised image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="695" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ficklenet: Weakly and semi-supervised semantic image segmentation using stochastic inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungbeom</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunji</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jangho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5267" to="5276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Tell me where to look: Guided attention inference network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuan-Chuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="9215" to="9223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Scribblesup: Scribble-supervised convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3159" to="3167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Leveraging instance-, imageand dataset-level information for weakly supervised instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Huan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Song</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Jun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Weakly supervised semantic segmentation with boundary exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Weiwei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Gyu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Whan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="891" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deepusps: Deep robust unsupervised saliency prediction via self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tam</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Dax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaithanya</forename><surname>Kumar Mummadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nhung</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thi Hoai Phuong</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyu</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="204" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Exploiting saliency for object segmentation from image level labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Constrained convolutional neural networks for weakly supervised segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1796" to="1804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">From image-level to pixel-level labeling with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1713" to="1721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mining cross-image semantics for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to detect salient objects with image-level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baocai</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Weaklysupervised semantic segmentation by iteratively mining common object features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaodi</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1354" to="1362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Self-supervised equivariant attention mechanism for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yude</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meina</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Object region mining with adversarial erasing: A simple classification to semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Stc: A simple to complex framework for weaklysupervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Revisiting dilated convolution: A simple approach for weakly-and semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="119" to="133" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep salient object detection with dense connections and distraction diagnosis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maojun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3239" to="3251" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Saliency guided self-attention network for weakly and semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="14413" to="14423" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Joint learning of saliency detection and weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunzhi</forename><surname>Zhuge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Reliability does matter: An end-to-end weakly supervised semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaizhu</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12765" to="12772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Splitting vs. merging: Mining object regions with discrepancy and intersection loss for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weide</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision, 2020</title>
		<meeting>the European Conference on Computer Vision, 2020</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Pyramid feature attention network for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangqian</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3085" to="3094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
