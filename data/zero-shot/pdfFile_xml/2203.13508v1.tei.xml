<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BDDM: BILATERAL DENOISING DIFFUSION MODELS FOR FAST AND HIGH-QUALITY SPEECH SYNTHESIS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><forename type="middle">W Y</forename><surname>Lam</surname></persName>
							<email>maxwylam@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Tencent AI Lab Shenzhen</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
							<email>joinerwang@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Tencent AI Lab Shenzhen</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Su</surname></persName>
							<email>dansu@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Tencent AI Lab Shenzhen</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab Bellevue</orgName>
								<address>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">BDDM: BILATERAL DENOISING DIFFUSION MODELS FOR FAST AND HIGH-QUALITY SPEECH SYNTHESIS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2022</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Diffusion probabilistic models (DPMs) and their extensions have emerged as competitive generative models yet confront challenges of efficient sampling. We propose a new bilateral denoising diffusion model (BDDM) that parameterizes both the forward and reverse processes with a schedule network and a score network, which can train with a novel bilateral modeling objective. We show that the new surrogate objective can achieve a lower bound of the log marginal likelihood tighter than a conventional surrogate. We also find that BDDM allows inheriting pre-trained score network parameters from any DPMs and consequently enables speedy and stable learning of the schedule network and optimization of a noise schedule for sampling. Our experiments demonstrate that BDDMs can generate high-fidelity audio samples with as few as three sampling steps. Moreover, compared to other state-of-the-art diffusion-based neural vocoders, BDDMs produce comparable or higher quality samples indistinguishable from human speech, notably with only seven sampling steps (143x faster than WaveGrad and 28.6x faster than DiffWave). We release our code at https://github.com/tencent-ailab/bddm.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Deep generative models have shown a tremendous advancement in speech synthesis (van den <ref type="bibr" target="#b44">Oord et al., 2016;</ref><ref type="bibr" target="#b14">Kalchbrenner et al., 2018;</ref><ref type="bibr" target="#b30">Prenger et al., 2019;</ref><ref type="bibr" target="#b22">Kumar et al., 2019a;</ref><ref type="bibr" target="#b19">Kong et al., 2020b;</ref><ref type="bibr" target="#b20">Kong et al., 2021)</ref>. Successful generative models can be mainly divided into two categories: generative adversarial network (GAN) <ref type="bibr" target="#b8">(Goodfellow et al., 2014)</ref> based and likelihoodbased. The former is based on adversarial learning, where the objective is to generate data indistinguishable from the training data. Yet, the training GANs can be very unstable, and the relevant training objectives are not suitable to compare against different GANs. The latter uses log-likelihood or surrogate objectives for training, but they also have intrinsic limitations regarding generation speed or quality. For example, the autoregressive models (van den <ref type="bibr" target="#b44">Oord et al., 2016;</ref><ref type="bibr" target="#b14">Kalchbrenner et al., 2018)</ref>, while being capable of generating high-fidelity data, are limited by their inherently slow sampling process and the poor scaling properties on high-dimensional data. Likewise, the flowbased models <ref type="bibr" target="#b6">(Dinh et al., 2016;</ref><ref type="bibr" target="#b15">Kingma &amp; Dhariwal, 2018;</ref><ref type="bibr" target="#b4">Chen et al., 2018;</ref><ref type="bibr" target="#b29">Papamakarios et al., 2021)</ref> rely on specialized architectures to build a normalized probability model, whose training is less parameter-efficient. Other prior works use surrogate objectives, such as the evidence lower bound in variational auto-encoders <ref type="bibr" target="#b16">(Kingma &amp; Welling, 2013;</ref><ref type="bibr" target="#b32">Rezende et al., 2014;</ref><ref type="bibr" target="#b26">Maal?e et al., 2019)</ref> and the contrastive divergence in energy-based models <ref type="bibr" target="#b9">(Hinton, 2002;</ref><ref type="bibr" target="#b2">Carreira-Perpinan &amp; Hinton, 2005)</ref>. These models, despite showing improved speed, typically only work well for lowdimensional data, and, in general, the sample qualities are not competitive to the GAN-based and the autoregressive models <ref type="bibr" target="#b1">(Bond-Taylor et al., 2021</ref>).</p><p>An up-and-coming class of likelihood-based models is the diffusion probabilistic models (DPMs) <ref type="bibr" target="#b36">(Sohl-Dickstein et al., 2015)</ref>, which introduces the idea of using a forward diffusion process to sequentially corrupt a given distribution and learning the reversal of such diffusion process to restore the data distribution for sampling. From a similar perspective, <ref type="bibr" target="#b38">Song &amp; Ermon (2019)</ref> proposed the score-based generative models by applying the score matching technique <ref type="bibr">(Hyvarinen &amp; Dayan,</ref> Published as a conference paper at ICLR 2022 2005) to train a neural network such that samples can be generated via Langevin dynamics. Along these two lines of research, <ref type="bibr" target="#b11">Ho et al. (2020)</ref> proposed the denoising diffusion probabilistic models (DDPMs) for high-quality image syntheses.  demonstrated that improved DDPMs  are capable of generating high-quality images of comparable or even superior quality to the state-of-the-art (SOTA) GAN-based models. For speech syntheses, DDPMs were also applied in Wavegrad  and DiffWave <ref type="bibr" target="#b20">(Kong et al., 2021)</ref> to produce higher-fidelity audio samples than the conventional non-autoregressive models <ref type="bibr" target="#b48">(Yamamoto et al., 2020;</ref><ref type="bibr" target="#b23">Kumar et al., 2019b;</ref><ref type="bibr">Bi?kowski et al., 2020)</ref> and matched the quality of the SOTA autoregressive methods .</p><p>Despite the compelling results, the diffusion generative models are two to three orders of magnitude slower than other generative models such as GANs and VAEs. Their primary limitation is that they require up to thousands of diffusion steps during training to learn the target distribution. Therefore a large number of reverse steps are often required at sampling time. Recently, extensive investigations have been conducted to reduce the sampling steps for efficiently generating high-quality samples, which we will discuss in the related work in Section 2. Distinctively, we conceived that we might train a neural network to efficiently and adaptively estimate a much shorter noise schedule for sampling while achieving generation performances comparable or superior to the conventional DPMs. With such an incentive, after introducing the conventional DPMs as our background in Section 3, we propose in Section 4 bilateral denoising diffusion models (BDDMs), named after a bilateral modeling perspective -parameterizing the forward and reverse processes with a schedule network and a score network, respectively. We theoretically derive that the schedule network should be trained after the score network is optimized. For training the schedule network, we propose a novel objective to minimize the gap between a newly derived lower bound and the log marginal likelihood. We describe the training algorithm as well as the fast and high-quality sampling algorithm in Section 5. The training of the schedule network converges very fast using our newly derived objective, and its training only adds negligible overhead to DDPM's. In Section 6, our neural vocoding experiments demonstrated that BDDMs could generate high-fidelity samples with as few as three sampling steps. Moreover, our method can produce speech samples indistinguishable from human speech with only seven sampling steps (143x faster than WaveGrad and 28.6x faster than DiffWave).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Prior works showed that noise scheduling is crucial for efficient and high-fidelity data generation in DPMs. DDPMs <ref type="bibr" target="#b11">(Ho et al., 2020)</ref> used a shared linear noise schedule for both training and sampling, which, however, requires thousands of sampling iterations to obtain competitive results. To speed up the sampling process, one class of related work, including <ref type="bibr" target="#b20">Kong et al., 2021;</ref>, attempts to use a different, shorter noise schedule for sampling. For clarity, we thereafter denote the training noise schedule as ? ? R T and the sampling noise schedule as? ? R N with N &lt; T . In particular,  applied a grid search (GS) algorithm to select?. Unfortunately, GS becomes prohibitively slow when N grows large, e.g., N = 6 took more than a day on a single NVIDIA Tesla P40 GPU. This is because the time costs of GS algorithm grow exponentially with N , i.e., O(9 N ) with 9 bins as the default setting in . Instead of searching, <ref type="bibr" target="#b20">Kong et al. (2021)</ref> devised a fast sampling (FS) algorithm based on an expert-defined 6-step noise schedule for their score network. However, this specifically tuned noise schedule is hard to generalize to other score networks, tasks, or datasets.</p><p>Another class of noise scheduling methods searches for a subsequence of time indices of the training noise schedule, which we call the time schedule. DDIMs <ref type="bibr" target="#b37">(Song et al., 2021a)</ref> introduced an accelerated reverse process that relies on a pre-specified time schedule. A linear and a quadratic time schedule were used in DDIMs and showed superior generation quality over DDPMs within 10 to 100 sampling steps.  proposed a re-scaled noise schedule for fast sampling, but this also requires pre-specifying the time schedule and the training noise schedule.  also proposed learning variances for the reverse processes, whereas the variances of the forward processes, i.e., the noise schedule, which affected both the means and variances of the reverse processes, were not learnable. According to the results of <ref type="bibr" target="#b37">(Song et al., 2021a;</ref>, using a linear or quadratic time schedule resulted in quite different performances in different datasets, implying that the optimal choice of schedule varies with the datasets. So, there remains a challenge in finding a short and effective schedule for fast sampling on different datasets.</p><p>Notably, <ref type="bibr" target="#b20">Kong &amp; Ping (2021)</ref> proposed a method to map a noise schedule to a time schedule for fast sampling. In this sense, searching for a time schedule becomes a sub-set of the noise scheduling problem, which resembles the above category of methods.</p><p>Although DPMs <ref type="bibr" target="#b36">(Sohl-Dickstein et al., 2015)</ref> and DDPMs <ref type="bibr" target="#b10">(Ho et al., 2019)</ref> mentioned that the noise schedule could be learned by re-parameterization, the approach was not investigated in their works. Closely related works that learn a noise schedule emerged until very recently. <ref type="bibr" target="#b34">San-Roman et al. (2021)</ref> proposed a noise estimation (NE) method, which trained a neural net with a regression loss to estimate the noise scale from the noisy sample at each time point, and then predicted the next noise scale. However, NE requires a prior assumption of the noise schedule following a linear or Fibonacci rule. Most recently, a concurrent work to ours by <ref type="bibr" target="#b17">Kingma et al. (2021)</ref> jointly trained a neural net to predict the signal-to-noise ratio (SNR) by maximizing the variational lower bound. The SNR was then used for noise scheduling. Different from ours, this scheduling neural net only took t as input and is independent of the noisy sample generated during the loop of sampling process. Intrinsically, with limited information about the sampled data, the predicted SNR could deviate from the actual SNR of the noisy data during sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">DIFFUSION PROBABILISTIC MODELS (DPMS)</head><p>Given i.i.d. samples {x 0 ? R D } from an unknown data distribution p data (x 0 ), diffusion probabilistic models (DPMs) <ref type="bibr" target="#b36">(Sohl-Dickstein et al., 2015)</ref> define a forward process q(x 1:T |x 0 ) = T t=1 q(x t |x t?1 ) that converts any complex data distribution into a simple, tractable distribution after T steps of diffusion. A reverse process p ? (x t?1 |x t ) parameterized by ? is used to model the data distribution:</p><formula xml:id="formula_0">p ? (x 0 ) = ?(x T ) T t=1 p ? (x t?1 |x t )dx 1:T , where ?(x T )</formula><p>is the prior distribution for starting the reverse process. Then, the variational parameters ? can be learned by maximizing the standard log evidence lower bound (ELBO):</p><formula xml:id="formula_1">F elbo := E q log p ? (x 0 |x 1 ) ? T t=2 D KL (q(x t?1 |x t , x 0 )||p ? (x t?1 |x t )) ? D KL (q(x T |x 0 )||?(x T )) .</formula><p>(1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">DENOISING DIFFUSION PROBABILISTIC MODELS (DDPMS)</head><p>As an extension to DPMs, denoising diffusion probabilistic models (DDPMs) <ref type="bibr" target="#b11">(Ho et al., 2020)</ref> applied the score matching technique <ref type="bibr" target="#b12">(Hyvarinen &amp; Dayan, 2005;</ref><ref type="bibr" target="#b38">Song &amp; Ermon, 2019)</ref> to define the reverse process. In particular, DDPMs considered a Gaussian diffusion process parameterized by a noise schedule ? ? R T with 0 &lt; ? 1 , . . . , ? T &lt; 1:</p><formula xml:id="formula_2">q ? (x 1:T |x 0 ) := T t=1 q ?t (x t |x t?1 ), where q ?t (x t |x t?1 ) := N ( 1 ? ? t x t?1 , ? t I).</formula><p>(</p><p>Based on the nice property of isotropic Gaussians, one can express x t directly conditioned on x 0 :</p><formula xml:id="formula_4">q ? (x t |x 0 ) = N (? t x 0 , (1 ? ? 2 t )I), where ? t = t i=1 1 ? ? i .<label>(3)</label></formula><p>To revert this forward process, DDPMs employ a score network 1 ? (x t , ? t ) to define <ref type="figure">Figure 1</ref>: A bilateral denoising diffusion model (BDDM) introduces a junctional variable x t and a schedule network ?. The schedule network can optimize the shortened noise schedule? n (?) if we know the score of the distribution at the junctional step, using the KL divergence to directly compare p ? * (x n?1 |x n = x t ) against the re-parameterized forward process posteriors.</p><formula xml:id="formula_5">p ? (x t?1 |x t ) := N 1 ? 1 ? ? t x t ? ? t 1 ? ? 2 t ? (x t , ? t ) , ? t ,<label>(4)</label></formula><p>where ? t is the co-variance matrix defined for the reverse process. <ref type="bibr" target="#b11">Ho et al. (2020)</ref> showed that</p><formula xml:id="formula_6">setting ? t =? t I = 1?? 2 t?1 1?? 2 t</formula><p>? t I is optimal for a deterministic x 0 , while setting ? t = ? t I is optimal for a white noise x 0 ? N (0, I). Alternatively,  proposed learnable variances by interpolating the two optimals with a jointly trained neural network, i.e., ? t,? (</p><formula xml:id="formula_7">x) := diag(exp(v ? (x) log ? t + (1 ? v ? (x)) log? t )), where v ? (x) ? R D is a trainable network.</formula><p>Note that the calculation of the complete ELBO in Eq. (1) requires T forward passes of the score network, which would make the training computationally prohibitive for a large T . To feasibly train the score network, instead of computing the complete ELBO, <ref type="bibr" target="#b11">Ho et al. (2020)</ref> proposed an efficient training mechanism by sampling from a discrete uniform distribution: t ? U{1, ..., T }, x 0 ? p data (x 0 ), t ? N (0, I) at each training iteration to compute the training loss:</p><formula xml:id="formula_8">L (t) ddpm (?) := t ? ? ? t x 0 + 1 ? ? 2 t t , ? t 2 2 ,<label>(5)</label></formula><p>which is a re-weighted form of D KL (q ? (x t?1 |x t , x 0 )||p ? (x t?1 |x t )). <ref type="bibr" target="#b11">Ho et al. (2020)</ref> reported that the re-weighting worked effectively for learning ?. Yet, we demonstrate it is deficient for learning the noise schedule ? in our ablation experiment in Section 6.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">BILATERAL DENOISING DIFFUSION MODELS (BDDMS)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">PROBLEM FORMULATION</head><p>For fast sampling with DPMs, we strive for a noise schedule? for sampling that is much shorter than the noise schedule ? for training. As shown in <ref type="figure">Fig. 1</ref>, we define two separate diffusion processes corresponding to the noise schedules, ? and?, respectively. The upper diffusion process parameterized by ? is the same as in Eq.</p><p>(2), whereas the lower process is defined as q?(x 1:N |x 0 ) = N n=1 q? n (x n |x n?1 ) with much fewer diffusion steps (N T ). In our problem formulation, ? is given, but? is unknown. The goal is to find a? for the reverse process p ? (x n?1 |x n ;? n ) such thatx 0 can be effectively recovered fromx N with N reverse steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">MODEL DESCRIPTION</head><p>Although many prior arts <ref type="bibr" target="#b11">(Ho et al., 2020;</ref><ref type="bibr" target="#b37">Song et al., 2021a;</ref><ref type="bibr" target="#b34">San-Roman et al., 2021)</ref> directly applied a shortened linear or Fibonacci noise schedule to the reverse process, we argue that these are sub-optimal solutions. Theoretically, the diffusion process specified by a new shortened noise schedule is essentially different from the one used to train the score network ?. Therefore, ? is not guaranteed suitable for reverting the shortened diffusion process. This issue motivated a novel modeling perspective to establish a link between the shortened schedule? and the score network ?, i.e., to have? optimized according to ?.</p><p>As a starting point, we consider an N = T /? , where 1 ? ? &lt; T is a hyperparameter controlling the step size such that each diffusion step between two consecutive variables in the shorter diffusion process corresponds to ? diffusion steps in the longer one. Based on Eq.</p><p>(2), we define the following:</p><formula xml:id="formula_9">q? n+1 (x n+1 |x n = x t ) := q ? (x t+? |x t ) = N ? ? ? 2 t+? ? 2 t x t , 1 ? ? 2 t+? ? 2 t I ? ? ,<label>(6)</label></formula><p>where x t is an intermediate diffused variable we introduced to link the two differently indexed diffusion sequences. We call it a junctional variable, which can be easily generated given x 0 and ? during training: x t = ? t x 0 + 1 ? ? 2 t n . Unfortunately, for the reverse process when x 0 is not given, the junctional variable is intractable. However, our key observation is that while using the score by a score network ? * trained for the long ?-parameterized diffusion process, a short noise schedule?(?) can be optimized accordingly by introducing a schedule network ?. We provide its mathematical derivations in Appendix A.3. Next, we present a formal definition of BDDM and derive its training objectives, L (n) score (?) and L (n) step (?; ? * ), for the score network and the schedule network, respectively, in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">SCORE NETWORK</head><p>Recall that a DDPM starts the reverse process with a white noise x T ? N (0, I) and takes T steps to recover the data distribution:</p><formula xml:id="formula_10">p ? (x 0 ) DDPM := E N (0,I) E p ? (x 1:T ?1 |x T ) [p ? (x 0 |x 1:T )] .<label>(7)</label></formula><p>A BDDM, in contrast, starts from the junctional variable x t , and reverts a shorter sequence of diffusion random variables with only n steps:</p><formula xml:id="formula_11">p ? (x 0 ) BDDM := E q?(xn?1;xt, n ) E p ? (x1:n?2|xn?1) [p ? (x 0 |x 1:n?1 )] , 2 ? n ? N,<label>(8)</label></formula><p>where q?(x n?1 ; x t , n ) is defined as a re-parameterization on the posterior:</p><formula xml:id="formula_12">q?(x n?1 ; x t , n ) :=q? x n?1 x n = x t ,x 0 = x t ? 1 ?? 2 n n ? n (9) =N ? ? 1 1 ?? n x t ?? n (1 ?? n )(1 ?? 2 n ) n , 1 ?? 2 n?1 1 ?? 2 n? n I ? ? ,<label>(10)</label></formula><formula xml:id="formula_13">where? n = n i=1 1 ?? i , x t = ? t x 0 + 1 ? ? 2</formula><p>t n is the junctional variable that maps x t tox n given an approximate index t ? U{(n ? 1)?, ..., n? ? 1, n? } and a sampled white noise n ? N (0, I). Detailed derivation from Eq. (9) to (10) is provided in Appendix A.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">TRAINING OBJECTIVE FOR SCORE NETWORK</head><p>With the above definition, a new form of lower bound to the log marginal likelihood can be derived</p><formula xml:id="formula_14">such that log p ? (x 0 ) ? F (n) score (?) := ?L (n) score (?) ? R ? (x 0 , x t ), where L (n) score (?) :=D KL p ? (x n?1 |x n = x t )||q?(x n?1 ; x t , n ) ,<label>(11)</label></formula><formula xml:id="formula_15">R ? (x 0 , x t ) := ? E p ? (x1|xn=xt) [log p ? (x 0 |x 1 )] .<label>(12)</label></formula><p>See detailed derivation in Proposition 1 in Appendix A.2. In the following Proposition 2, we prove that via the junctional variable x t , the solution ? * for optimizing the objective L (t) ddpm (?), ?t ? {1, ..., T } is also the solution for optimizing L (n) score (?), ?n ? {2, ..., N }. Thereby, we show that the score network ? can be trained with L (t) ddpm (?) and re-used for reverting the short diffusion process overx N :0 . Although the newly derived lower bound result in the same objective as the conventional score network, it for the first time establishes a link between the score network ? andx N :0 . The connection is essential for learning?, which we will describe next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">SCHEDULE NETWORK</head><p>In BDDMs, a schedule network is introduced to the forward process by re-parameterizing? n a? ? n (?) = f ? x t ;? n+1 , and recall that during training, we can use x t = ? t x 0 + 1 ? ? 2 t n and</p><formula xml:id="formula_16">? n+1 = 1 ? ? 2 t+? ? 2 t .</formula><p>Through the re-parameterization, the task of noise scheduling, i.e., searching for?, can now be reformulated as training a schedule network f ? that ancestrally estimates datadependent variances. The schedule network learns to predict? n based on the current noisy sample x t -this makes our method fundamentally different from existing and concurrent work, including Kingma et al. <ref type="formula" target="#formula_19">(2021)</ref> -as we reveal that, aside from? n+1 , t, or n that reflects diffusion step information, x t is also essential for noise scheduling from a reverse direction at inference time.</p><p>Specifically, we adopt the ancestral step information (? n+1 ) to derive an upper bound for the current step while leaving the schedule network only to take the current noisy sample x t as input to predict a relative change of noise scales against the ancestral step. First, we derive an upper bound of? n by proving 0 &lt;? n &lt; min 1 ?? 2 n+1 1??n+1 ,? n+1 in Appendix A.1. Then, by multiplying the upper bound by a ratio estimated by a neural network ? ? :</p><formula xml:id="formula_17">R D ? (0, 1), we define f ? (x t ;? n+1 ) := min 1 ?? 2 n+1 1 ?? n+1 ,? n+1 ? ? (x t ),<label>(13)</label></formula><p>where the network parameter set ? is learned to estimate the ratio between two consecutive noise scales (? n and? n+1 ) from the current noisy input x t .</p><p>Finally, at inference time for noise scheduling, starting from a maximum reverse steps (N ) and two hyperparameters (? N ,? N ), we ancestrally predict the noise scale? n (?) = f ? x n ;? n+1 , for n from N to 1, and cumulatively update the product? n =? n+1 ? 1??n+1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">TRAINING OBJECTIVE FOR SCHEDULE NETWORK</head><p>Here we describe how to learn the network parameters ? effectively. First, we demonstrated that ? should be trained after ? is well-optimized, referring to Proposition 3 in Appendix A.3. The Proposition also shows that we are minimizing the gap between the lower bound F (n) score (? * ) and log p ? * (x 0 ), i.e., log p ? * (x 0 ) ? F (n) score (? * ), by minimizing the following objective</p><formula xml:id="formula_18">L (n) step (?; ? * ) :=D KL p ? * (x n?1 |x n = x t )||q? n(?) (x n?1 ; x 0 , ? t ) ,<label>(14)</label></formula><p>which is defined as a KL divergence to directly compare p ? * (x n?1 |x n = x t ) against the reparameterized forward process posteriors, which are tractable when conditioned on the junctional noise scale ? t and x 0 .</p><p>The detailed derivation of Eq. <ref type="formula" target="#formula_5">(14)</ref> is also provided in the proof of Proposition 3 to get its concrete formulas as shown in Step (8-10) in Alg. 2. 5 ALGORITHMS: TRAINING, NOISE SCHEDULING, AND SAMPLING</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">TRAINING SCORE AND SCHEDULE NETWORKS</head><p>Following the theoretical result in Appendix A.3, ? should be optimized before learning ?. Thereby first, to train the score network ? , we refer to the settings in <ref type="bibr" target="#b11">(Ho et al., 2020;</ref><ref type="bibr" target="#b37">Song et al., 2021a)</ref> to define ? as a linear noise schedule: ? t = ? start + t T (? end ? ? start ), for 1 ? t ? T, where ? start and ? end are two hyperparameter that specifies the start value and the end value. This results in Algorithm 1, which resembles the training algorithm in <ref type="bibr" target="#b11">(Ho et al., 2020)</ref>.</p><p>Next, based on the converged score network ? * , we train the schedule network ?. We draw an n ? U {2, . . . , N } at each training step, and then draw a t ? U {(n ? 1)?, ..., n? }. These together can be re-formulated as directly drawing t ? U{?, ..., T ? ? } for a finer-scale time step. Then, we Algorithm 1 Training Score Network (?) sequentially compute the variables needed for calculating L (n) step (?; ? * ), as presented in Algorithm 2. We observed that, although a linear schedule is used to define ?, the noise schedule of? predicted by f ? is not limited to but rather different from a linear one.</p><formula xml:id="formula_19">1: Given T, {?t} T t=1 2: {?t} T t=1 = { t i=1 ? 1 ? ?t} T t=1 3: repeat 4: x0 ? pdata(x0) 5: t ? U{1, . . . , T } 6: t ? N (0, I) 7: xt = ?tx0 + 1 ? ? 2 t t 8: L (t) ddpm = t ? ? (xt, ?t) 2 2 9: Take a gradient descent step on ? ? L (t) ddpm 10: until converged Algorithm 3 Noise Scheduling 1: Given ? * ,?N ,?N , xN ? N (0, I) 2: for n = N to 2 do 3:xn?1 ? p ? * (xn?1|xn;?n,?n) 4:?n?1 =? n ? 1??n 5:?n?1 = min{1 ?? 2 n?1 ,?n}? ? (xn?1) 6: if?n?1 &lt; ?1 then 7: return?n, . . . ,?N 8: end if 9: end for 10: return?1, . . . ,?N Algorithm 2 Training Schedule Network (?) 1: Given ? * , ?, T, {?t, ?t} T t=1 2: repeat 3: x0 ? pdata(x0) 4: t ? U{?, . . . , T ? ? } 5: ?t = 1 ? ? 2 t 6: n ? N (0, I) 7: xt = ?tx0 + ? ?t n 8:?n = min ?t, 1 ? ? 2 t+? ? 2 t ? ? (xt) 9: C = 4 ?1 log(?t/?n) + 2 ?1 D ? n/?t ? 1 10: L (n) step = ? t 2(? t ??n) n ?? n ? t ? * (xt, ?t) 2 2 + C 11: Take a gradient descent step on ? ? L<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">NOISE SCHEDULING FOR FAST AND HIGH-QUALITY SAMPLING</head><p>After the score network and the schedule network are trained, the inference procedure can divide into two phases: (1) the noise scheduling phase and (2) the sampling phase.</p><p>First, we run the noise scheduling process similarly to a sampling process with N iterations maximum. Different from training, where ? t is forward-computed,? n is instead a backward-computed variable (from N to 1) that may deviate from the forward one because {? i } n?1 i are unknown in the noise scheduling phase during inference. To start noise scheduling, we first set two hyperparameters:? N and? N . We use ? 1 , the smallest noise scale seen in training, as a threshold to early stop the noise scheduling process so that we can ignore small noise scales (&lt; ? 1 ) that were never seen by the score network. Overall, the noise scheduling process presents in Algorithm 3.</p><p>In practice, we apply a grid search algorithm of M bins to Algorithm 3, which takes O(M 2 ) time, to find proper values for (? N ,? N ). We used M = 9 as in . The grid search for our noise scheduling algorithm can be evaluated on a small subset of the training samples. Empirically, even as few as 1 sample for evaluation works well in our algorithm. Finally, given the predicted noise schedule? ? R Ns , we generate samples with N s sampling steps, as shown in Algorithm 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>We conducted a series of experiments on neural vocoding tasks to evaluate the proposed BDDMs. First, we compared BDDMs against several strongest models that have been published: the mixture of logistics (MoL) WaveNet  implemented in <ref type="bibr" target="#b48">(Yamamoto, 2020)</ref>, the WaveGlow <ref type="bibr" target="#b30">(Prenger et al., 2019)</ref> implemented in <ref type="bibr">(Valle, 2020)</ref>, the MelGAN <ref type="bibr" target="#b22">(Kumar et al., 2019a)</ref> implemented in <ref type="bibr" target="#b24">(Kumar, 2019)</ref>, the HiFi-GAN <ref type="bibr" target="#b19">(Kong et al., 2020b)</ref> implemented in <ref type="bibr" target="#b18">(Kong et al., 2020a)</ref> and the two most recently proposed diffusion-based vocoders, i.e., WaveGrad  and DiffWave <ref type="bibr" target="#b20">(Kong et al., 2021)</ref>, both re-implemented in our code. The hyperparameter settings of BDDMs and all these models are detailed in Appendix B.</p><p>In addition, we also compared BDDMs to a variety of scheduling and acceleration techniques applicable to DDPMs, including the grid search (GS) approach in WaveGrad, the fast sampling (FS) 1: Comparison of neural vocoders in terms of MOS with 95% confidence intervals, real-time factor (RTF) and model size in megabytes (MB) for inference. The highest score and the scores that are not significantly different from the highest score (p-values ? 0.05) are bold-faced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neural Vocoder MOS RTF Size</head><p>Ground-truth 4.64 ? 0.08 --WaveNet (MoL)  3.52 ? 0.16 318.6 282MB WaveGlow <ref type="bibr" target="#b30">(Prenger et al., 2019)</ref> 3.03 ? 0.15 0.0198 645MB MelGAN <ref type="bibr" target="#b22">(Kumar et al., 2019a)</ref> 3.48 ? 0.14 0.00396 17MB HiFi-GAN <ref type="bibr" target="#b19">(Kong et al., 2020b)</ref> 4.33 ? 0.12 0.0134 54MB WaveGrad -1000 steps  4.36 ? 0.13 38.2 183MB DiffWave -200 steps <ref type="bibr" target="#b20">(Kong et al., 2021)</ref> 4.49 ? 0.13 7.30 27MB</p><p>BDDM -3 steps (? N = 0.68,? N = 0.53) 3.64 ? 0.13 0.110 27MB BDDM -7 steps (? N = 0.62,? N = 0.42)</p><p>4.43 ? 0.11 0.256 27MB BDDM -12 steps (? N = 0.67,? N = 0.12) 4.48 ? 0.12 0.438 27MB <ref type="table">Table 2</ref>: Comparison of sampling acceleration methods with the same score network and the same number of steps. The highest score and the scores that are not significantly different from the highest score (p-values ? 0.05) are bold-faced.</p><p>Steps Acceleration Method STOI PESQ MOS 3 GS  0.965 ? 0.009 3.66 ? 0.20 3.61 ? 0.12 FS <ref type="bibr" target="#b20">(Kong et al., 2021)</ref> 0.939 ? 0.023 3.09 ? 0.23 3.10 ? 0.12 DDIM <ref type="bibr" target="#b37">(Song et al., 2021a)</ref> 0  <ref type="bibr" target="#b13">(Ito &amp; Johnson, 2017)</ref>, which consists of 13,100 22kHz audio clips of a female speaker. All diffusion models were trained on the same training split as in . We also replicated the comparative experiment of neural vocoding using a multi-speaker VCTK dataset <ref type="bibr" target="#b46">(Yamagishi et al., 2019)</ref> as presented in Appendix C and obtained a result consistent with that obtained from the LJSpeech dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">SAMPLING QUALITY IN OBJECTIVE AND SUBJECTIVE METRICS</head><p>To assess the quality of each generated audio sample, we used both objective and subjective measures for comparing different neural vocoders given the same ground-truth spectrogram s as the condition, i.e., ? (x, s, ? t ). Specifically, we used two scale-invariant metrics: the perceptual evaluation of speech quality (PESQ) <ref type="bibr" target="#b33">(Rix et al., 2001)</ref> and the short-time objective intelligibility (STOI) <ref type="bibr" target="#b42">(Taal et al., 2010)</ref> to measure the noisiness and the distortion of the generated speech relative to the reference speech. Mean opinion score (MOS) was also used as a subjective metric for evaluating the naturalness of the generated speech. The assessment scheme of MOS is included in Appendix B.</p><p>In <ref type="table">Table 1</ref>, we compared BDDMs against the state-of-the-art (SOTA) vocoders. To predict noise schedules with different sampling steps (3, 7, and 12), we set three pairs of {? N ,? N } for BDDMs by running on Algorithm 3 a quick hyperparameter grid search, which is detailed in Appendix B. Among the 9 evaluated vocoders, only our proposed BDDMs with 7 and 12 steps and DiffWave with 200 steps showed no statistic-significant difference from the ground-truth in terms of MOS. Moreover, BDDMs significantly outspeeded DiffWave in terms of RTFs. Notably, previous diffusion- based vocoders achieved high MOS scores at the cost of an unacceptable RTF for industrial deployment. In contrast, BDDMs managed to achieve a high standard of generation quality with only 7 sampling steps (143x faster than WaveGrad and 28.6x faster than DiffWave).</p><p>In <ref type="table">Table 2</ref>, we evaluated BDDMs and alternative accelerated sampling methods, which used the same score network for a pair-to-pair comparison. The GS method performed stably when the step number was small (i.e., N ? 6) but not scalable to more step numbers, which were therefore bypassed in the comparisons of 7 and 12 steps. The FS method by <ref type="bibr" target="#b37">Song et al. (2021a)</ref> was linearly interpolated to 3 and 7 steps for a fair comparison. Comparing its 7-step and 3-step results, we observed that the FS performance degraded drastically. Both the DDIM and the NE methods were stable across all the steps but were not performing competitively enough. In comparison, BDDMs consistently attained the leading scores across all the steps. This evaluation confirmed that BDDM was superior to other acceleration methods for DPMs in terms of both stability and quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">ABLATION STUDY AND ANALYSIS</head><p>We attribute the primary advantage of BDDMs to the newly derived objective L (n) step for learning ?. To better reason about this, we performed an ablation study, where we substituted the proposed loss with the standard negative ELBO for learning ? as mentioned by <ref type="bibr" target="#b36">Sohl-Dickstein et al. (2015)</ref>. We plotted the network outputs with different training losses in <ref type="figure" target="#fig_0">Fig. 2</ref>. It turned out that, when using L (n) elbo to learn ?, the network output rapidly collapsed to zero within several training steps; whereas, the network trained with L (n) step produced fluctuating outputs. The fluctuation is a desirable property showing the network properly predicts t-dependent noise scales, as t is a random time step drawn from a uniform distribution in training.</p><p>By setting? = ?, we empirically validated that F</p><formula xml:id="formula_20">(t) bddm := F (t) score +L (t) step ? F (t)</formula><p>elbo with their respective values at t ? [20, 180] using the same optimized ? * . Each value is provided with 95% confidence intervals, as shown in <ref type="figure">Fig. 3</ref>. In this experiment, we used the LJ speech dataset and set T = 200 and ? = 20. Notably, we dropped their common entropy term R ? (x 0 , x t ) &lt; 0 to mainly compare their KL divergences. This explains those positive lower bound values in the plot. The graph shows that our proposed bound F (t) bddm is always a tighter lower bound than the standard one across all examined t. Moreover, we found that F (t) bddm attained low values with a relatively much lower variance for t ? 50, where F (t) elbo was highly volatile. This implies that F (t) bddm better tackles the difficult training part, i.e., when the score becomes more challenging to estimate as t ? 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>BDDMs parameterize the forward and reverse processes with a schedule network and a score network, of which the former's optimization is tied with the latter by introducing a junctional variable. We derived a new lower bound that leads to the same training loss for the score network as in DDPMs <ref type="bibr" target="#b11">(Ho et al., 2020)</ref>, which thus enables inheriting any pre-trained score networks in DDPMs. We also showed that training the schedule network after a well-optimized score network can be viewed as tightening the lower bound. Followed from the theoretical results, an efficient training algorithm and a noise scheduling algorithm were respectively designed for BDDMs. Finally, in our experiments, BDDMs showed a clear edge over the previous diffusion-based vocoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A THEORETICAL DERIVATIONS FOR BDDMS</head><p>In this section, we provide the theoretical supports for the following:</p><p>? The derivation for upper bounding? n (see Appendix A.1).</p><p>? The score network ? trained with L (t) ddpm (?) for the reverse process p ? (x t?1 |x t ) can be re-used for the reverse process p ? (x n?1 |x n ) (see Appendix A.2).</p><p>? The schedule network ? can be trained with L (n) step (?; ? * ) after the score network ? is optimized. (see Appendix A.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 DERIVING AN UPPER BOUND FOR NOISE SCALE</head><p>Since monotonic noise schedules have been successfully applied to in many prior arts including DPMs <ref type="bibr" target="#b11">(Ho et al., 2020;</ref><ref type="bibr" target="#b17">Kingma et al., 2021)</ref> and score-based methods , we also follow the monotonic assumption and derive an upper bound for? n as below:</p><p>Remark 1. Suppose the noise schedule for sampling is monotonic, i.e., 0 &lt;? 1 &lt; . . . &lt;? N &lt; 1, then, for 1 ? n &lt; N ,? n satisfies the following inequality:</p><formula xml:id="formula_21">0 &lt;? n &lt; min 1 ?? 2 n+1 1 ?? n+1 ,? n+1 .<label>(15)</label></formula><p>Proof. By the general definition of noise schedule, we know that 0 &lt;? 1 , . . . ,? N &lt; 1 (Note: no inequality sign in between). Given that? n = n i=1</p><p>1 ?? i , we also have 0 &lt;? 1 , . . . ,? t &lt; 1.</p><p>First, we show that? n &lt; 1 ?? 2 n+1 1??n+1 :</p><formula xml:id="formula_22">? n?1 =? n 1 ?? n &lt; 1 ??? n &lt; 1 ?? 2 n = 1 ?? 2 n+1 1 ?? n+1 .<label>(16)</label></formula><p>Next, we show that? n &lt; 1 ?? n+1 :</p><formula xml:id="formula_23">? n 1 ?? n =? n 1 ?? n 1 ?? n =? n+1 1 ?? n &lt; 1 ??? n &lt; 1 ?? n+1 .<label>(17)</label></formula><p>Now, we have? n &lt; min 1 ?? 2 n+1</p><formula xml:id="formula_24">1??n+1 , 1 ?? n+1 . When 1 ?? n+1 &lt; 1 ?? 2 n+1 1??n+1 , we can show that? n+1 &lt; 1 ?? n+1 : 1 ?? n+1 &lt; 1 ?? 2 n+1 1 ?? n+1 = 1 ?? 2 n ??? n+1 &gt;? 2 n ??? 2 n+1 ? 2 n &gt;? n+1 (18) ?? 1 ?? 2 n+1 ? 2 n &lt; 1 ?? n+1 ??? n+1 &lt; 1 ?? n+1 .<label>(19)</label></formula><p>By the assumption of monotonic sequence, we also have? n &lt;? n+1 . Knowing that? n+1 &lt; 1?? n+1 is always true, we obtain a tighter bound for? n : 0 &lt;? n &lt; min 1 ?? 2 n+1 1??n+1 ,? n+1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 DERIVING THE TRAINING OBJECTIVE FOR SCORE NETWORK</head><p>First, followed from the data distribution modeling of BDDMs as proposed in Eq. <ref type="formula" target="#formula_11">(8)</ref>:</p><formula xml:id="formula_25">p ? (x 0 ) := Ex n?1?q? (xn?1;xt, n ) Ex 1:n?2?p? (x1:n?2|xn?1) [p ? (x 0 |x 1:n?1 )] ,<label>(20)</label></formula><p>we can derive a new lower bound to the log marginal likelihood as follows:</p><p>Proposition 1. Given x t ? q ? (x t |x 0 ), the following lower bound holds for n ? {2, . . . , N }:</p><formula xml:id="formula_26">log p ? (x 0 ) ? F (n) score (?) := ?L (n) score (?) ? R ? (x 0 , x t ),<label>(21)</label></formula><p>where</p><formula xml:id="formula_27">L (n) score (?) := D KL p ? (x n?1 |x n = x t )||q?(x n?1 ; x t , n ) ,<label>(22)</label></formula><formula xml:id="formula_28">R ? (x 0 , x t ) := ?E p ? (x1|xn=xt) [log p ? (x 0 |x 1 )] .<label>(23)</label></formula><p>Proof.</p><p>log p ? (x 0 ) = log p ? (x 0:n?2 |x n?1 )q?(x n?1 ; x t , n )dx 1:n?1 (24)</p><formula xml:id="formula_29">= log p ? (x 0:n?2 |x n?1 )q?(x n?1 ; x t , n ) p ? (x 1:n?1 |x n = x t ) p ? (x 1:n?1 |x n = x t ) dx 1:n?1 (25) = log E p ? (x1,n?1|xn=xt) p ? (x 0 |x 1 )q?(x n?1 ; x t , n ) p ? (x n?1 |x n = x t ) (26) [Jensen's Inequality] ?E p ? (x1,xn?1|xn=xt) log p ? (x 0 |x 1 )q?(x n?1 ; x t , n ) p ? (x n?1 |x n = x t ) (27) =E p ? (x1|xn=xt) [log p ? (x 0 |x 1 )] ? D KL p ? (x n?1 |x n = x t )||q?(x n?1 ; x t , n ) (28) = ? L (n) score (?) ? R ? (x 0 , x t )<label>(29)</label></formula><p>Next, we show that the score network ? trained with L (t) ddpm (?) can be re-used in BDDMs. We first provide the derivation for Eq. (9-10). We have</p><formula xml:id="formula_30">q?(x n?1 ; x t , n ) := q? x n?1 |x n = x t ,x 0 = x t ? 1 ?? 2 n n ? n (30) = N ? ?? n?1?n 1 ?? 2 n x t ? 1 ?? 2 n n ? n + 1 ?? n (1 ?? 2 n?1 ) 1 ?? 2 n x t , 1 ?? 2 n?1 1 ?? 2 n? n I ? ? (31) = N ? ? ? ?? n?1?n ? n (1 ?? 2 n ) + 1 ?? n (1 ?? 2 n?1 ) 1 ?? 2 n ? ? x t ?? n?1?n ? n 1 ?? 2 n n , 1 ?? 2 n?1 1 ?? 2 n? n I ? ? (32) = N ? ? 1 1 ?? n x t ?? n (1 ?? n )(1 ?? 2 n ) n , 1 ?? 2 n?1 1 ?? 2 n? n I ? ? .<label>(33)</label></formula><p>Proposition 2. Suppose x t ? q ? (x t |x 0 ), then any solution satisfying ? * = argmin ? L (t) ddpm (?), ?t ? {1, ..., T }, also satisfies ? * = argmin ? L (n) score (?), ?n ? {2, ..., N }.</p><p>Proof. By the definition in Eq. (4), we have</p><formula xml:id="formula_31">p ? (x n?1 |x n = x t ) =N ? ? 1 1 ?? n x t ?? n 1 ?? 2 n ? (x t ,? n ) , 1 ?? 2 n?1 1 ?? 2 n? n I ? ? . (34)</formula><p>Here, from the training objective in Eq. (5), since x t = ? t x 0 + 1 ? ? 2 t n , the noise scale argument for the score network is known to be ? t . Therefore, we can use ? (x t , ? t ) instead of ? (x t ,? n ) for expanding L (n) score (?). Since p ? (x n?1 |x n = x t ) and q?(x n?1 ; x t , n ) are two isotropic Gaussians with the same variance, the KL divergence is a scaled 2-norm of their means' difference:</p><formula xml:id="formula_32">L (n) score (?) :=D KL p ? (x n?1 |x n = x t )||q?(x n?1 ; x t , n ) (35) = 1 ?? 2 n 2(1 ?? 2 n?1 )? n 1 1 ?? n x t ?? n 1 ?? 2 n ? (x t , ? t ) (36) ? ? ? 1 1 ?? n x t ?? n (1 ?? n )(1 ?? 2 n ) n ? ? 2 2 (37) = (1 ?? n )(1 ?? 2 n ) 2(1 ?? n ?? 2 n )? n ? n (1 ?? n )(1 ?? 2 n ) ( n ? ? (x t , ? t )) 2 2 (38) = (1 ?? n )(1 ?? 2 n ) 2(1 ?? n ?? 2 n )? n? 2 n (1 ?? 2 n )(1 ?? n ) n ? ? (x t , ? t ) 2 2 (39) =? n 2(1 ?? n ?? 2 n ) n ? ? ? t x 0 + 1 ? ? 2 t n , ? t 2 2 ,<label>(40)</label></formula><p>which is proportional to L (t)</p><formula xml:id="formula_33">ddpm := n ? ? ? t x 0 + 1 ? ? 2 t n , ? t 2 2</formula><p>as defined in Eq. (5). Thus,</p><formula xml:id="formula_34">argmin ? L (t) ddpm (?) ? argmin ? L (n) score (?).<label>(41)</label></formula><p>Next, we can simplify R ? (x 0 , x t ) to a reconstruction loss forx 0 :</p><formula xml:id="formula_35">R ? (x 0 , x t ) := ?E p ? (x1|xn=xt) [log p ? (x 0 |x 1 )]<label>(42)</label></formula><formula xml:id="formula_36">=E p ? (x1|xn=xt) ? ? log N ? ? 1 1 ?? 1 x 1 ?? 1 1 ?? 2 1 ? (x 1 ,? 1 ),? 1 I ? ? ? ? (43) =E p ? (x1|xn=xt) ? ? ? D 2 log 2?? 1 + 1 2? 1 x 0 ? 1 1 ?? 1 ? ?x 1 ?? 1 ? 1 ? (x 1 ,? 1 ) ? ? 2 2 ? ? ? (44) = D 2 log 2?? 1 + 1 2? 1 E p ? (x1|xn=xt) ? ? ? x 0 ? 1 1 ?? 1 x 1 ? ? 1 ? (x 1 ,? 1 ) 2 2 ? ? ? ,<label>(45)</label></formula><p>where p ? (x 1 |x n = x t ) can be efficiently sampled using the reverse process in <ref type="bibr" target="#b37">(Song et al., 2021a</ref>). Yet, in practice, similar to the training in <ref type="bibr" target="#b37">(Song et al., 2021a;</ref><ref type="bibr" target="#b20">Kong et al., 2021)</ref>, we dropped R ? (x 0 , x t ) when training ?. In theory, we know that R ? (x 0 , x t ) achieves its optimal value at ? * = argmin ? ? (x 1 ,? 1 )? 1 2 2 , which shares a similar objective as L (t) ddpm . By minimizing L (t) ddpm , we train a score network ? * that best minimizes ? t := t ? ? * (? t x 0 + 1 ? ? 2 t t , ? t ) 2 2 for all 1 ? t ? T . Since the first diffusion step has the smallest effect on corruptingx 0 (i.e., ? 1 ? 0), it suffices to consider a? 1 = ? 1 ? ? 1 = ? 1 , in which case we can jointly minimize R ? (x 0 , x t ) by minimizing L (1) ddpm . In this sense, during training, given x t ? q ? (x t |x 0 ), we can train the score network with the same training objective as in DDPMs and DDIMs. Practically, it is beneficial for BDDMs as we can re-use the score network ? of any well-trained DDPM or DDIM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 DERIVING THE TRAINING OBJECTIVE FOR SCHEDULE NETWORK</head><p>Given that ? can be trained to maximize the log evidence with the pre-specified noise schedule ? for training, the consequent question of interest in BDDMs is how to find a fast and good enough noise schedule? ? R N for sampling given an optimized ? * . In BDDMs, this problem is reduced to how to effectively learn the network parameters ? .</p><p>Proposition 3. Suppose ? has been optimized and hypothetically converged to the optimal ? * , where by optimal it means that with ? * we have p ? * (x n?1 |x n = x t ) = q?(x n?1 ; x t , n ) given</p><p>x t ? q ? (x t |x 0 ). When? is unknown but we have x 0 =x 0 and? n = ? t , we can minimize the gap between the optimal lower bound F (n) score (? * ) and log p ? * (x 0 ), i.e, log p ? * (x 0 ) ? F (n) score (? * ), by minimizing the following objective with respect to? n :</p><formula xml:id="formula_37">L (n) step (? n ; ? * ) :=D KL p ? * (x n?1 |x n = x t )||q? n (x n?1 |x 0 ; ? t ) (46) = ? t 2(? t ?? n ) n ?? n ? t ? * ? t x 0 + ? t n , ? t 2 2 + C,<label>(47)</label></formula><p>where</p><formula xml:id="formula_38">? t = 1 ? ? 2 t , C = 1 4 log ? t ? n + D 2 ? n ? t ? 1 .<label>(48)</label></formula><p>Proof. Note thatx 0 = x 0 ,? n = ? t , x t = ? t x 0 + 1 ? ? 2 t n and p ? * (x n?1 |x n = x t ) = q?(x n?1 ; x t , n ). When x 0 is given to p ? * , we can express the probability as follows:</p><formula xml:id="formula_39">p ? * (x n?1 |x n = x t (x 0 ),x 0 = x 0 ) (49) = N (z; 0, I)p ? * (x n?1 |x n = ? t x 0 + 1 ? ? 2 t z)dz (50) = N (z; 0, I)q?(x n?1 ; x t = ? t x 0 + 1 ? ? 2 t z, n = z)dz (51) = N (z; 0, I)N ? ?x n?1 ; ? t x 0 + 1 ? ? 2 t z 1 ?? n ?? n (1 ?? n )(1 ?? 2 n ) z, 1 ?? 2 n?1 1 ?? 2 n? n I ? ? dz<label>(52)</label></formula><p>[See Eq. (2) in <ref type="bibr" target="#b7">(Frey, 1999)</ref>]</p><formula xml:id="formula_40">=N ? ? ?xn?1; ? t x 0 1 ?? n , ? ? ? ? ? 1 ? ? 2 t 1 ?? n ?? n (1 ?? n )(1 ? ? 2 t ) ? ? 2 + 1 ? ? 2 t /(1 ?? n ) 1 ? ? 2 t? n ? ? ? I ? ? ? (53) =N ? ?x n?1 ; ? t x 0 1 ?? n , 1 ? ? 2 t ?? n 1 ?? n I ? ? =: q? n (x n?1 ; x 0 , ? t ),<label>(54)</label></formula><p>where, different from p ? * (x n?1 |x n = x t ), from Eq. (49) to Eq. (50), instead of conditioning on a specific x t , when x 0 is given x t can be generated using any z ? N (0, I).</p><p>From this, we can express the gap between log p ? * (x 0 ) and F (n) score (? * ) in the following form:</p><formula xml:id="formula_41">log p ? * (x 0 = x 0 ) ? F (n) score (? * ) (55) = log p ? * (x 0 = x 0 ) ? E p ? * (x1,n?1|xn=xt) log p ? (x 0 |x 1 )q?(x n?1 ; x t , n ) p ? (x n?1 |x n = x t ) (56) = log p ? * (x 0 = x 0 ) ? E p ? * (x1:n?1|xn=xt) log p ? * (x 0:n?1 |x n = x t ) p ? * (x 1:n?1 |x n = x t ) (57) =E p ? * (x1:n?1|xn=xt) log p ? * (x 1:n?1 |x n = x t ) p ? * (x 1:n?1 |x n = x t ,x 0 = x 0 ) (58) =E p ? * (xn?1|xn=xt) log p ? * (x n?1 |x n = x t ) q? n (x n?1 ; x 0 , ? t )<label>(59)</label></formula><formula xml:id="formula_42">=D KL p ? * (x n?1 |x n = x t )||q? n (x n?1 ; x 0 , ? t )<label>(60)</label></formula><p>Next, we evaluate the above KL divergence term. By definition, we have</p><formula xml:id="formula_43">p ? * (x n?1 |x n = x t ) = N ? ? 1 1 ?? n x t ?? n 1 ?? 2 n ? * (x t ,? n ) , 1 ?? 2 n?1 1 ?? 2 n? n I ? ? (61)</formula><p>Together with Eq. (54), we have</p><formula xml:id="formula_44">L (n) step (? n ; ? * ) := D KL p ? * (x n?1 |x n = x t )||q? n (x n?1 ; x 0 , ? t ) (62) = 1 ?? n 2(1 ?? n ? ? 2 t ) ? t 1 ?? n x 0 ? 1 1 ?? n x t ?? n 1 ? ? 2 t ? * (x t , ? t ) 2 2 + C (63) = 1 ?? n 2(1 ?? n ? ? 2 t ) ? t 1 ?? n x 0 ? 1 1 ?? n ? t x 0 + 1 ? ? 2 t n ?? n 1 ? ? 2 t ? * (x t , ? t ) 2 2 + C (64) = 1 ?? n 2(1 ?? n ? ? 2 t ) 1 ? ? 2 t 1 ? ? n n ?? n (1 ? ? n )(1 ? ? 2 t ) ? * (x t , ? t ) 2 2 + C (65) = 1 ? ? 2 t 2(1 ?? n ? ? 2 t ) n ?? n 1 ? ? 2 t ? * (x t , ? t ) 2 2 + C (66) = ? t 2(? t ?? n ) n ?? n ? t ? * ? t x 0 + ? t n , ? t 2 2 + C,<label>(67)</label></formula><p>where</p><formula xml:id="formula_45">? t = 1 ? ? 2 t , C = 1 4 log ? t ? n + D 2 ? n ? t ? 1 .<label>(68)</label></formula><p>As we use a schedule network ? to estimate? n from (? n+1 ,? n+1 ) as defined in Eq. (13), we obtain the final step loss for learning ?:</p><formula xml:id="formula_46">L (n) step (?; ? * ) = ? t 2(? t ?? n (?)) n ?? n (?) ? t ? * (x t , ? t ) 2 2 + 1 4 log ? t ? n (?) + D 2 ? n (?) ? t ? 1 .<label>(69)</label></formula><p>This proposed objective for training the schedule network can be interpreted as to better model the data distribution (i.e., maximizing log p ? (x 0 )) by correcting the gradient scale? n for the next reverse step (fromx n tox n?1 ) given the gradient vector ? * estimated by the score network ? * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B EXPERIMENTAL DETAILS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 CONVENTIONAL GRID SEARCH ALGORITHM FOR DDPMS</head><p>We reproduced the grid search algorithm in , in which a 6-step noise schedule was searched. In our paper, we generalized the grid search algorithm by similarly sweeping the N -step noise schedule over the following possibilities with a bin width M = 9:</p><p>{1, 2, 3, 4, 5, 6, 7, 8, 9} ? {10 ?6?N/N , 10 ?6?(N ?1)/N , ..., 10 ?6?1/N },</p><p>where ? denotes the cartesian product applied on two sets. LS-MSE was used as a metric to select the solution during the search. When N = 6, we resemble the GS algorithm in . Note that above searching method normally does not scale up to N &gt; 6 steps for its exponential computational cost O(9 N ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 HYPERPARAMETER SETTING IN BDDMS</head><p>Algorithm 2 took a skip factor ? to control the stride for training the schedule network. The value of ? would affect the coverage of step sizes when training the schedule network, hence affecting the predicted number of steps N for inference -the higher ? is, the shorter the predicted inference schedule tends to be. We set ? = 66 for training the BDDM vocoders in this paper.</p><p>For initializing Algorithm 3 for noise scheduling, we could take as few as 1 training sample for validation, perform a grid search on the hyperparameters {(? N = 0.1? T i,? N = 0.1j)} for i, j = 1, ..., 9, i.e., 81 possibilities in total, and use the PESQ measure as the selection metric. Then, the predicted noise schedule corresponding to the maximum PESQ was stored and applied to the online inference afterward, as shown in Algorithm 4. Note that this searching has a complexity of only O(M 2 ) (e.g., M = 9 in this case), which is much more efficient than O(M N ) in the conventional grid search algorithm in , as discussed in Section B.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 IMPLEMENTATION DETAILS</head><p>Our proposed BDDMs and the baseline methods were all implemented with the Pytorch library. The score networks for the LJ and VCTK speech datasets were trained from scratch on a single NVIDIA Tesla P40 GPU with batch size 32 for about 1M steps, which took about 3 days.</p><p>For the model architecture, we used the same architecture as in DiffWave <ref type="bibr" target="#b20">(Kong et al., 2021)</ref> for the score network with 128 residual channels; we adopted a lightweight GALR network <ref type="bibr" target="#b25">(Lam et al., 2021)</ref> for the schedule network. GALR was originally proposed for speech enhancement, so we considered it well suited for predicting the noise scales. For the configuration of the GALR network, we used a window length of 8 samples for encoding, a segment size of 64 for segmentation and only two GALR blocks of 128 hidden dimensions, and other settings were inherited from <ref type="bibr" target="#b25">(Lam et al., 2021)</ref>. To make the schedule network output with a proper range and dimension, we applied a sigmoid function to the last block's output of the GALR network. Then the result was averaged over the segments and the feature dimensions to obtain the predicted ratio: ? ? (x) = AvgPool2D(?(GALR(x))), where GALR(?) denotes the GALR network, AvgPool2D(?) denotes the average pooling operation applied to the segments and the feature dimensions, and ?(x) := 1/(1 + e ?x ). The same network architecture was used for the NE approach for estimating ? 2 t and was shown better than the ConvTASNet used in the original paper <ref type="bibr" target="#b34">(San-Roman et al., 2021)</ref>. It is also notable that the computational cost of a schedule network is indeed fractional compared to the cost of a score network, as predicting a noise scalar variable is intrinsically a relatively much easier task. Our GALR-based schedule network, while being able to produce stable and reliable results, was about 3.6 times faster than the score network. The training of schedule networks for BDDMs took only 10k steps to converge, which consumed no more than an hour on a single GPU.  Regarding the image generation task, to demonstrate the generalizability of our method, we directly adopted a score network pre-trained on the CIFAR-10 dataset implemented by a third-party opensource repository. Regarding the schedule network, to demonstrate that it does not have to use specialized architecture, we replaced GALR by the VGG11 <ref type="bibr" target="#b35">(Simonyan &amp; Zisserman, 2014)</ref>, which was also used by as a noise estimator in <ref type="bibr" target="#b34">(San-Roman et al., 2021)</ref>. The output dimension (number of classes) of VGG11 was set to 1. Similar to the setting for GALR in speech synthesis, we added a sigmoid activation to the last layer to ensure a [0, 1] output. Similar to the training in speech domain, we trained the VGG11-based schedule networks while freezing the score networks for 10k steps, which normally can be finished in about two hours.</p><p>Our code for the speech vocoding and the image generation experiments will be uploaded to Github after the final decision of ICLR is released.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 CROWD-SOURCED SUBJECTIVE EVALUATION</head><p>All our Mean Opinion Score (MOS) tests were crowd-sourced. We refer to the MOS scores in <ref type="bibr" target="#b31">(Protasio Ribeiro et al., 2011)</ref>, and the scoring criteria have been included in <ref type="table" target="#tab_3">Table 3</ref> for completeness. The samples were presented and rated one at a time by the testers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C ADDITIONAL EXPERIMENTS</head><p>A demonstration page at https://bilateral-denoising-diffusion-model. github.io shows some samples generated by BDDMs trained on LJ speech and VCTK datasets.  For the unconditional image generation task, we evaluated the proposed BDDMs on the benchmark CIFAR-10 (32 ? 32) dataset. The score functions, including those initially proposed in DDPMs <ref type="bibr" target="#b11">(Ho et al., 2020)</ref> or DDIMs <ref type="bibr" target="#b37">(Song et al., 2021a)</ref> and those pre-trained in the above third-party implementations, are all conditioned on a discrete step-index. We estimated the noise schedule? in continuous space using the VGG11 schedule network and then mapped it to discrete time schedule using the approximation method in <ref type="bibr" target="#b20">(Kong &amp; Ping, 2021)</ref>. <ref type="table" target="#tab_6">Table 6</ref> shows the performances of different sampling methods for DDPMs in CIFAR-10. By setting the maximum number of sampling steps (N ) for noise scheduling, we can fairly compare the improvements achieved by BDDMs against related methods in the literature in terms of FID. Remarkably, BDDMs with 100 sampling steps not only surpassed the 1000-step DDPM baseline, but also produced the SOTA FID performance amongst all generative models using less than or equal to 100 sampling steps. The first row shows the spectrum of a random signal for starting the reverse process. Then, from the top to the bottom, we show the spectrum of the resultant signal after each step of the reverse process performed by the BDDM. We also provide the corresponding WAV files on our demo page.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Different training losses for ? ?Figure 3: Different lower bounds to log p ? (x 0 )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Spectrum plots of the speech samples produced by BDDM within 3 sampling steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc><ref type="bibr" target="#b34">Roman et al., 2021)</ref> 0.966 ? 0.010 3.62 ? 0.18 3.55 ? 0.12 BDDM 0.966 ? 0.011 3.63 ? 0.24 3.64 ? 0.13</figDesc><table><row><cell></cell><cell></cell><cell>.943 ? 0.015</cell><cell>3.42 ? 0.27</cell><cell>3.25 ? 0.13</cell></row><row><cell cols="4">FS (Kong et al., 2021) DDIM (Song et al., 2021a) NE (San-7 NE (San-Roman et al., 2021) 0.978 ? 0.007 0.981 ? 0.006 3.68 ? 0.24 0.974 ? 0.008 3.85 ? 0.12 3.75 ? 0.18</cell><cell>3.70 ? 0.14 3.94 ? 0.12 4.02 ? 0.11</cell></row><row><cell></cell><cell>BDDM</cell><cell cols="3">0.983 ? 0.006 3.96 ? 0.09 4.43 ? 0.11</cell></row><row><cell></cell><cell>DDIM (Song et al., 2021a)</cell><cell>0.979 ? 0.006</cell><cell>3.90 ? 0.10</cell><cell>4.16 ? 0.12</cell></row><row><cell>12</cell><cell cols="2">NE (San-Roman et al., 2021) 0.981 ? 0.007</cell><cell>3.82 ? 0.13</cell><cell>3.98 ? 0.14</cell></row><row><cell></cell><cell>BDDM</cell><cell cols="3">0.987 ? 0.006 3.98 ? 0.12 4.48 ? 0.12</cell></row></table><note>approach based on a user-defined 6-step schedule in DiffWave, the DDIMs (Song et al., 2021a) and a noise estimation (NE) approach (San-Roman et al., 2021). For fair and reproducible comparison with other models and approaches, we used the LJSpeech dataset</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ratings that have been used in evaluation of speech naturalness of synthetic samples.</figDesc><table><row><cell>Rating</cell><cell>Naturalness</cell><cell>Definition</cell></row><row><cell>1</cell><cell>Unsatisfactory</cell><cell>Very annoying, distortion is objectionable.</cell></row><row><cell>2</cell><cell>Poor</cell><cell>Annoying distortion, but not objectionable.</cell></row><row><cell>3</cell><cell>Fair</cell><cell>Perceptible distortion, slightly annoying.</cell></row><row><cell>4</cell><cell>Good</cell><cell>Slight perceptible level of distortion, but not annoying.</cell></row><row><cell>5</cell><cell>Excellent</cell><cell>Imperceptible level of distortion.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Performances of different noise schedules on the multi-speaker VCTK speech dataset, each of which used the same score network ? (?) that was trained on VCTK for about 1M iterations.</figDesc><table><row><cell>Noise schedule</cell><cell cols="4">LS-MSE (?) MCD (?) STOI (?) PESQ (?)</cell><cell>MOS (?)</cell></row><row><cell cols="2">DDPM (Ho et al., 2020; Chen et al., 2020)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>8 steps (Grid Search)</cell><cell>101</cell><cell>2.09</cell><cell>0.787</cell><cell>3.31</cell><cell>4.22 ? 0.04</cell></row><row><cell>1,000 steps (Linear)</cell><cell>85.0</cell><cell>2.02</cell><cell>0.798</cell><cell>3.39</cell><cell>4.40 ? 0.05</cell></row><row><cell cols="2">DDIM (Song et al., 2021a)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>8 steps (Linear)</cell><cell>553</cell><cell>3.20</cell><cell>0.701</cell><cell>2.81</cell><cell>3.83 ? 0.04</cell></row><row><cell>16 steps (Linear)</cell><cell>412</cell><cell>2.90</cell><cell>0.724</cell><cell>3.04</cell><cell>3.88 ? 0.05</cell></row><row><cell>21 steps (Linear)</cell><cell>355</cell><cell>2.79</cell><cell>0.739</cell><cell>3.12</cell><cell>4.12 ? 0.05</cell></row><row><cell>100 steps (Linear)</cell><cell>259</cell><cell>2.58</cell><cell>0.759</cell><cell>3.30</cell><cell>4.27 ? 0.04</cell></row><row><cell cols="2">NE (San-Roman et al., 2021)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>8 steps (Linear)</cell><cell>208</cell><cell>2.54</cell><cell>0.740</cell><cell>3.10</cell><cell>4.18 ? 0.04</cell></row><row><cell>16 steps (Linear)</cell><cell>183</cell><cell>2.53</cell><cell>0.742</cell><cell>3.20</cell><cell>4.26 ? 0.04</cell></row><row><cell>21 steps (Linear)</cell><cell>852</cell><cell>3.57</cell><cell>0.699</cell><cell>2.66</cell><cell>3.70 ? 0.03</cell></row><row><cell>BDDM (? N ,? N )</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>8 steps (0.2, 0.9)</cell><cell>98.4</cell><cell>2.11</cell><cell>0.774</cell><cell>3.18</cell><cell>4.20 ? 0.04</cell></row><row><cell>16 steps (0.5, 0.5)</cell><cell>73.6</cell><cell>1.93</cell><cell>0.813</cell><cell>3.39</cell><cell>4.35 ? 0.05</cell></row><row><cell>21 steps (0.5, 0.1)</cell><cell>76.5</cell><cell>1.83</cell><cell>0.827</cell><cell>3.43</cell><cell>4.48 ? 0.06</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Performances of different reverse processes for BDDMs on the VCTK speech dataset, each of which used the same score network ? (?) and the same noise schedule.</figDesc><table><row><cell>Noise schedule</cell><cell cols="4">LS-MSE (?) MCD (?) STOI (?) PESQ (?)</cell></row><row><cell cols="2">BDDM (DDPM reverse process)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>8 steps (0.3, 0.9, 1e ?5 )</cell><cell>91.3</cell><cell>2.19</cell><cell>0.936</cell><cell>3.22</cell></row><row><cell>16 steps (0.7, 0.1, 1e ?6 )</cell><cell>73.3</cell><cell>1.88</cell><cell>0.949</cell><cell>3.32</cell></row><row><cell>21 steps (0.5, 0.1, 1e ?6 )</cell><cell>72.2</cell><cell>1.91</cell><cell>0.950</cell><cell>3.33</cell></row><row><cell cols="2">BDDM (DDIM reverse process)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>8 steps (0.3, 0.9, 1e ?5 )</cell><cell>91.8</cell><cell>2.19</cell><cell>0.938</cell><cell>3.26</cell></row><row><cell>16 steps (0.7, 0.1, 1e ?6 )</cell><cell>77.7</cell><cell>1.96</cell><cell>0.953</cell><cell>3.37</cell></row><row><cell>21 steps (0.5, 0.1, 1e ?6 )</cell><cell>77.6</cell><cell>1.96</cell><cell>0.954</cell><cell>3.39</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Comparing sampling methods for DDPM with different number of sampling steps in terms of FIDs in CIFAR10.</figDesc><table><row><cell>Sampling method</cell><cell cols="2">Sampling steps FID</cell></row><row><cell>DDPM (baseline) (Ho et al., 2020)</cell><cell>1000</cell><cell>3.17</cell></row><row><cell>DDPM (sub-VP) (Song et al., 2021b)</cell><cell>? 100</cell><cell>3.69</cell></row><row><cell>DDPM (DP + reweighting) (Watson et al., 2021)</cell><cell>128 64</cell><cell>5.24 6.74</cell></row><row><cell>DDIM (quadratic) (Song et al., 2021a)</cell><cell>100 50</cell><cell>4.16 4.67</cell></row><row><cell>FastDPM (approx. STEP) (Kong &amp; Ping, 2021)</cell><cell>100 50</cell><cell>2.86 3.20</cell></row><row><cell>2 Improved DDPM (hybrid) (Nichol &amp; Dhariwal, 2021)</cell><cell>100 50</cell><cell>4.63 5.09</cell></row><row><cell>VDM (augmented) (Kingma et al., 2021)</cell><cell>1000</cell><cell>7.41 3</cell></row><row><cell>Ours BDDM</cell><cell>100 50</cell><cell>2.38 2.93</cell></row><row><cell>C.3 UNCONDITIONAL IMAGE GENERATION</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Here, ? (xt, ?t) is conditioned on the continuous noise scale ?t, as in<ref type="bibr" target="#b41">(Song et al., 2021b;</ref>. Alternatively, the score network can also be conditioned on a discrete time index ? (xt, t), as in<ref type="bibr" target="#b37">(Song et al., 2021a;</ref><ref type="bibr" target="#b11">Ho et al., 2020)</ref>. An approximate mapping of a noise schedule to a time schedule<ref type="bibr" target="#b20">(Kong &amp; Ping, 2021)</ref> exists, therefore we consider conditioning on noise scales as the general case.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Our implementation was based on https://github.com/openai/improved-diffusion 3 The authors of VDM claimed that they tuned the hyperparameters only for minimizing the likelihood and did not pursue further tuning of the model to improve FID.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 MULTI-SPEAKER SPEECH SYNTHESIS</head><p>In addition to the single-speaker speech synthesis, we evaluated BDDMs on the multi-speaker speech synthesis benchmark VCTK <ref type="bibr" target="#b46">(Yamagishi et al., 2019)</ref>. VCTK consists of utterances sampled at 48 KHz by 108 native English speakers with various accents. We split the VCTK dataset for training and testing: 100 speakers were used for training the multi-speaker model and 8 speakers for testing. We trained on a 44257-utterance subset (40 hours) and evaluated on a held-out 100utterance subset. For the score network, we used the Wavegrad architecture  so as to examine whether the superiority of BDDMs remains in a different dataset and with a different score network architecture.</p><p>Results are presented in <ref type="table">Table 4</ref>. For this multi-speaker VCTK dataset, we obtained consistent observations with that for the single-speaker LJ dataset presented in the main paper. Again, the proposed BDDM with only 16 or 21 steps outperformed the DDPM with 1,000 steps. To the best of our knowledge, ours was the first work that reported this degree of superior. When reducing to 8 steps, BDDM obtained performance on par with (except for a worse PESQ) the costly gridsearched 8 steps (which were unscalable to more steps) in DDPM. For NE, we could again observe a degradation from its 16 steps to 21 steps, indicating the instability of NE for the VCTK dataset likewise. In contrast, BDDM gave continuously improved performance while increasing the step number.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 COMPARING DIFFERENT REVERSE PROCESSES FOR BDDMS</head><p>This section demonstrates that BDDMs do not restrict the sampling procedure to a specialized reverse process in Algorithm 4. In particular, we evaluated different reverse processes, including that of DDPMs as shown in Eq. (4) and DDIMs <ref type="bibr" target="#b37">(Song et al., 2021a)</ref>, for BDDMs and compared the objective scores on the generated samples. DDIMs <ref type="bibr" target="#b37">(Song et al., 2021a</ref>) formulate a non-Markovian generative process that accelerates the inference while keeping the same training procedure as DDPMs. The original generative process in Eq. (4) in DDPMs is modified into</p><p>where ? is a sub-sequence of length N of [1, ..., T ] with ? N = T , and? := {1, ..., T } \ ? is defined as its complement; Therefore, only part of the models are used in the sampling process.</p><p>To achieve the above, DDIMs defined a prediction function f (t)</p><p>? (x t ) that depends on ? to predict the observation x 0 given x t directly:</p><p>By leveraging this prediction function, the conditionals in Eq. (71) are formulated as</p><p>where the detailed derivation of ? t and ? can be referred to <ref type="bibr" target="#b37">(Song et al., 2021a)</ref>. In the original DDIMs, the accelerated reverse process produces samples over the subsequence of ? indexed by ?: ? = {? n |n ? ?}. In BDDMs, to apply the DDIM reverse process, we use the? predicted by the schedule network in place of a subsequence of the training schedule ?.</p><p>Finally. the objective scores are given in <ref type="table">Table 5</ref>. Note that the subjective evaluation (MOS) is omitted here since the other assessments above have shown that the MOS scores are highly correlated with the objective measures, including STOI and PESQ. They indicate that applying BDDMs to either DDPM or DDIM reverse process leads to comparable and competitive results. Meanwhile, the results show some subtle differences: BDDMs over a DDPM reverse process gave slightly better samples in terms of signal error and consistency metrics (i.e., LS-MSE and MCD), while BDDM over a DDIM reverse process tended to generate better samples in terms of intelligibility and perceptual metrics (i.e., STOI and PESQ).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">High fidelity speech synthesis with adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miko?aj</forename><surname>Bi?kowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Casagrande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><forename type="middle">C</forename><surname>Cobo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep generative modelling: A comparative review of vaes, gans, normalizing flows, energy-based and autoregressive models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Bond-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Leach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">G</forename><surname>Willcocks</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2103.04922" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Miguel A Carreira-Perpinan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
	<note>On contrastive divergence learning. AISTATS</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Wavegrad: Estimating gradients for waveform generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.00713</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tian Qi Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
		<title level="m">Neural ordinary differential equations. NeurIPs</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6571" to="6583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.05233</idno>
		<title level="m">Diffusion models beat gans on image synthesis</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08803</idno>
		<title level="m">Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Local probability propagation for factor analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="442" to="448" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2661</idno>
		<title level="m">Generative adversarial networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Training products of experts by minimizing contrastive divergence. neural computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1771" to="1800" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Flow++: Improving flowbased generative models with variational dequantization and architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Estimation of non-normalized statistical models by score matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyvarinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The lj speech dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Johnson</surname></persName>
		</author>
		<ptr target="https://keithito.com/LJ-Speech-Dataset/" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient neural audio synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seb</forename><surname>Noury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Casagrande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Stimberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2410" to="2419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhariwal</surname></persName>
		</author>
		<title level="m">Glow: Generative flow with invertible 1x1 convolutions. NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10215" to="10224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.00630</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">Variational diffusion models. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungil</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaekyoung</forename><surname>Bae</surname></persName>
		</author>
		<ptr target="https://github.com/jik876/hifi-gan" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungil</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaekyoung</forename><surname>Bae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">On fast sampling of diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.00132</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Diffwave: A versatile diffusion model for audio synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaji</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kundan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rithesh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Thibault De Boissiere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><forename type="middle">Zhen</forename><surname>Gestin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Teoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sotelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>De Br?bisson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Melgan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06711</idno>
		<title level="m">Generative adversarial networks for conditional waveform synthesis</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kundan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rithesh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Thibault De Boissiere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><forename type="middle">Zhen</forename><surname>Gestin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Teoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sotelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>De Brebisson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Melgan</surname></persName>
		</author>
		<title level="m">Generative adversarial networks for conditional waveform synthesis. NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Official repository for the paper melgan: Generative adversarial networks for conditional waveform synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rithesh</forename><surname>Kumar</surname></persName>
		</author>
		<ptr target="https://github.com/descriptinc/melgan-neurips" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Effective low-cost time-domain audio separation using globally attentive locally recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Max</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.05014</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Biva: A very deep hierarchy of latent variables for generative modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Maal?e</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Fraccaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Li?vin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="6548" to="6558" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Improved denoising diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.09672</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Parallel wavenet: Fast high-fidelity speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Cobo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Stimberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3918" to="3926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papamakarios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><forename type="middle">Jimenez</forename><surname>Rezende</surname></persName>
		</author>
		<title level="m">Shakir Mohamed, and Balaji Lakshminarayanan. Normalizing flows for probabilistic modeling and inference. JMLR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Waveglow: A flow-based generative network for speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3617" to="3621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">CROWDMOS: An approach for crowdsourcing mean opinion score studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinei</forename><surname>Flavio Protasio Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cha</forename><surname>Florencio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seltzer</surname></persName>
		</author>
		<editor>ICASSP. IEEE</editor>
		<imprint>
			<date type="published" when="2011" />
			<publisher>ICASSP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Stochastic backpropagation and approximate inference in deep generative models. ICML</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Perceptual evaluation of speech quality (pesq)-a new method for speech quality assessment of telephone networks and codecs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Antony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">G</forename><surname>Rix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beerends</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andries P</forename><surname>Hollier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hekstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2001 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings (Cat. No. 01CH37221)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="749" to="752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Noise estimation for generative diffusion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>San-Roman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliya</forename><surname>Nachmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.02600</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2256" to="2265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Denoising diffusion implicit models. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Generative modeling by estimating gradients of the data distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Improved techniques for training score-based generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09011</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Sliced score matching: A scalable approach to density and score estimation. UAI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahaj</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="574" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Score-based generative modeling through stochastic differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poole</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A short-time objective intelligibility measure for time-frequency weighted noisy speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hendriks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesper</forename><surname>Heusdens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE international conference on acoustics, speech and signal processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="4214" to="4217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Waveglow: a flow-based generative network for speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Valle</surname></persName>
		</author>
		<ptr target="https://github.com/NVIDIA/waveglow" />
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Wavenet: A generative model for raw audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th ISCA Speech Synthesis Workshop</title>
		<meeting>9th ISCA Speech Synthesis Workshop</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="125" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Learning to efficiently sample from diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03802</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">CSTR VCTK Corpus: English multi-speaker corpus for CSTR voice cloning toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junichi</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Veaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirsten</forename><surname>Macdonald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>version 0.92</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryuichi</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wavenet</surname></persName>
		</author>
		<ptr target="https://github.com/r9y9/wavenet_vocoder" />
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Wavegan: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryuichi</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunwoo</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae-Min</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parallel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Multi-band melgan: Faster waveform generation for high-quality text-to-speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.0510</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
