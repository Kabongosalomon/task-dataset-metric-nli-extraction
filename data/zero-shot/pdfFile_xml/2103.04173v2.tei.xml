<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graphical Abstract LongReMix: Robust Learning with High Confidence Samples in a Noisy Label arXiv:21 Highlights LongReMix: Robust Learning with High Confidence Samples in a Noisy Label LongReMix: Robust Learning with High Confidence Samples in a Noisy Label Environment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filipe</forename><forename type="middle">R</forename><surname>Environment</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ragav</forename><surname>Cordeiro</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Sachdeva</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Belagiannis</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Reid</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carneiro</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filipe</forename><forename type="middle">R</forename><surname>Environment</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ragav</forename><surname>Cordeiro</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Sachdeva</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Belagiannis</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Reid</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carneiro</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filipe</forename><forename type="middle">R</forename><surname>Cordeiro</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="laboratory">Visual Computing Lab</orgName>
								<orgName type="institution">Universidade Federal Rural de Pernambuco</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ragav</forename><surname>Sachdeva</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Engineering Science</orgName>
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Otto-von-Guericke-Universit?t Magdeburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Australian Institute for Machine Learning</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Australian Institute for Machine Learning</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Centre for Vision, Speech and Signal Processing</orgName>
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Graphical Abstract LongReMix: Robust Learning with High Confidence Samples in a Noisy Label arXiv:21 Highlights LongReMix: Robust Learning with High Confidence Samples in a Noisy Label LongReMix: Robust Learning with High Confidence Samples in a Noisy Label Environment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Preprint submitted to Pattern Recognition September 7, 2022</note>
					<note>* Corresponding author in most datasets. The code is available at https://github.com/filipe-research/LongReMix.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>noisy label learning</term>
					<term>deep learning</term>
					<term>empirical vicinal risk</term>
					<term>semi-supervised learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a new two-stage noisy-label learning algorithm, called LongReMix;</p><p>? The first stage finds a highly precise, but potentially small, set of clean samples;</p><p>? The second stage is designed to be robust to small sets of clean samples;</p><p>? LongReMix reaches SOTA performance on the main noisy-label learning benchmarks;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>State-of-the-art noisy-label learning algorithms rely on an unsupervised learning to classify training samples as clean or noisy, followed by a semi-supervised learning (SSL) that minimises the empirical vicinal risk using a labelled set formed by samples classified as clean, and an unlabelled set with samples classified as noisy. The classification accuracy of such noisy-label learning methods depends on the precision of the unsupervised classification of clean and noisy samples, and the robustness of SSL to small clean sets. We address these points with a new noisy-label training algorithm, called LongReMix, which improves the precision of the unsupervised classification of clean and noisy samples and the robustness of SSL to small clean sets with a two-stage learning process. The stage one of LongReMix finds a small but precise high-confidence clean set, and stage two augments this high-confidence clean set with new clean samples and oversamples the clean data to increase the robustness of SSL to small clean sets. We test LongReMix on CIFAR-10 and CIFAR-100 with introduced synthetic noisy labels, and the real-world noisy-label benchmarks CNWL (Red Mini-ImageNet), WebVision, Clothing1M, and Food101-N. The results show that our LongReMix produces significantly better classification accuracy than competing approaches, particularly in high noise rate problems. Furthermore, our approach achieves state-of-the-art performance</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>well on challenging problems such as image classification <ref type="bibr" target="#b0">[1]</ref>. However, the larger the data set, the greater the likelihood for it to be contaminated with noisy labels due to reasons such as low-quality data, human failure, or challenging labelling tasks <ref type="bibr" target="#b1">[2]</ref>. The main issue is that DNNs can overfit noisy-label samples, particularly for large rate of label noise, reducing their accuracies, as shown by Zhang et al. <ref type="bibr" target="#b2">[3]</ref>. Hence the field is focusing on the development of training strategies that reduce the likelihood of DNNs to fit such noisy-label samples.</p><p>In the literature, several methods have been proposed to deal with noisy labels <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>, where one of the most successful methods explores a method formed by an unsupervised learning method to classify training samples as clean or noisy, followed by semi-supervised learning (SSL) to minimise the empirical vicinal risk (EVR) with a labelled set formed by the samples classified as clean, and an unlabelled set with the samples classified as noisy. The unsupervised learning stage generally is based on the small-loss strategy <ref type="bibr" target="#b8">[9]</ref>, where at every epoch, samples with small loss are classified as clean, and large loss as noisy. This strategy can lead to a low classification precision of clean samples, particularly in high noise rate scenarios, because the loss values can be unstable at different training epochs. The SSL stage <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref> is usually based on MixMatch <ref type="bibr" target="#b10">[11]</ref> that minimises the empirical vicinal risk (EVR) <ref type="bibr" target="#b11">[12]</ref>, where a robust estimation of the vicinal distribution is critical for an effective optimisation that generalises well. In turn, such robust estimation depends on a large training set to minimise the EVR <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13]</ref>, but problems with high noise rate usually cause the unsupervised learning stage to build a small training set to be used by this optimisation, affecting the generalisation of the SSL stage.</p><p>In this paper, we hypothesise that the classification accuracy of noisy-label learning  methods depends on the precision of the unsupervised learning stage to classify clean or noisy samples and the robustness of SSL to small clean sets, formed in high label noise rate problems. To validate these two hypotheses, we propose LongReMix, which is a new two-stage noisy-label training algorithm. The first stage forms a high-confidence set of clean samples that is estimated with a new unsupervised learning method that trades off the precision and recall of this clean set, as depicted in <ref type="figure" target="#fig_0">Fig. 1</ref>. The second stage increases the high-confidence clean set from stage 1 with a clean set estimated from the small-loss strategy, which is used to train the proposed LongMix that oversamples this clean set to improve the robustness of SSL to small clean sets, usually formed in high label noise rate problems -see <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><p>The key contributions of LongReMix are:</p><p>? A new two-stage noisy-label learning algorithm based on a highly precise unsu-pervised learning method to classify training samples as clean or noisy, followed by a semi-supervised learning (SSL) approach that is quite robust to small sets of clean samples.</p><p>? The highly precise unsupervised learning in the first stage forms a high-confidence set of clean samples using the losses of many consecutive epochs, instead of one epoch, to differentiate between noisy and clean samples; and the set of clean samples for the second stage is formed by a combination of this high-confidence clean samples (from the first stage) and the clean samples found using the the small-loss strategy (from the second stage), increasing the size and reliability of the clean set;</p><p>? The proposed SSL learning in the second stage estimates the vicinal distribution by oversampling the clean data to increase the training set size and improve the robustness of EVR minimisation to small clean sets, typically formed in large label noise rate scenarios.</p><p>Even though other papers have shown that the detection of clean samples is more precise with the use of loss measures across several epochs <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>, they tend to be unstable during early training stages, and in high-noise rate problems, they form rather small sets of clean samples that can deteriorate training robustness. Hence, in this paper we address these two problems:</p><p>? training instability at the beginning of the training, and</p><p>? robustness to large noise rate problems.</p><p>These two problems are addressed with the high-confidence set (from first stage) that is combined with the small-loss strategy to form large sets of clean samples in the second stage. We show that our high-confidence set of clean samples can be up to 30% more precise than the clean sets formed with the small loss strategy. Furthermore, oversampling the clean set has been explored in SSL <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>, but to the best of our knowledge, we are the first group to use it in noisy-label learning problems.</p><p>We evaluate our approach on the noisy-label learning benchmarks of CIFAR-10 <ref type="bibr" target="#b20">[21]</ref>, CIFAR-100 <ref type="bibr" target="#b20">[21]</ref>, Controlled Noisy Web Labels (CNWL -Red Mini-ImageNet) <ref type="bibr" target="#b21">[22]</ref>, WebVision <ref type="bibr" target="#b22">[23]</ref>, Clothing1M <ref type="bibr" target="#b23">[24]</ref>, and Food101-N <ref type="bibr" target="#b24">[25]</ref>, where LongReMix shows the best performance in the field in almost all of those data sets, particularly in problems with extremely large noise rates. These results are shown to be significantly better than competing SOTA methods using the statistical test in <ref type="bibr" target="#b25">[26]</ref>. We also show that LongReMix finds a set of clean samples with higher precision than the competing methods, and is robust to over-fitting in problems with high label noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Prior Work</head><p>Several methods have been proposed for the noisy-label problem, and they explore different strategies, such as robust loss functions <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b4">5]</ref>, label cleansing <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>, sample weighting <ref type="bibr" target="#b5">[6]</ref>, meta-learning <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>, ensemble learning <ref type="bibr" target="#b32">[33]</ref>, and others <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>Below, we focus on the prior work that is related to our approach and show competitive results on the main benchmarks.</p><p>Sample noise characterisation can be achieved with an auxiliary clean validation set.</p><p>For instance, Ren et al. <ref type="bibr" target="#b5">[6]</ref> use such clean validation set and a meta-learning method to find the noisy samples and re-weight them based on their values and gradient directions.</p><p>Zhang et al. <ref type="bibr" target="#b34">[35]</ref> also use a clean validation set and meta-learning to incorporate pseudo labeling into meta optimisation. Shu et al. <ref type="bibr" target="#b35">[36]</ref> rely on meta-learning and a clean validation set, but they use a multi-layer perceptron to learn a loss-weighting function.</p><p>Even though these methods show competitive results, they need an auxiliary clean validation set that is not always available and can be expensive to acquire. Hence, we disregard these methods in our paper because we argue that they are based on a less general experimental setup.</p><p>The automatic characterisation of sample noise without a clean validation set has also been investigated. Xue et al. <ref type="bibr" target="#b36">[37]</ref> present a probabilistic Local Outlier Factor algorithm (pLOF) to estimate the probability that a sample is an outlier, which is assumed to have a noisy label. The idea explored by pLOF is that the density around a noisy sample is significantly different from the density around its (clean) neighbors. However, in high noise rate problems, the effectiveness of pLOF is reduced because it cannot find significant differences between the densities of noisy and clean samples. Wang et al. <ref type="bibr" target="#b37">[38]</ref> also use pLOF combined with a Siamese network to increase the dissimilarities between clean and noisy samples. Nevertheless, the incorrect classification of clean samples by pLOF can induce the learning of wrong feature representations. Arazo et al. <ref type="bibr" target="#b9">[10]</ref> propose the use of a Beta Mixture Model (BMM) to separate the clean and noise samples during training, based on the classification loss value of each sample.</p><p>Similarly, Li et al. <ref type="bibr" target="#b7">[8]</ref> use Gaussian Mixture Model (GMM) for the same goal. Although the use of BMM and GMM applied on the loss values works well for low noise rate, for higher noise regimes it becomes less precise because these loss values are more unstable over different training epochs. To mitigate this vulnerability, some approaches use stored information across epochs, such as average loss <ref type="bibr" target="#b13">[14]</ref>, difference between logits <ref type="bibr" target="#b14">[15]</ref> and the number of times a sample is forgotten during training <ref type="bibr" target="#b15">[16]</ref>. Even though using information across epochs improves the precision of the identification of clean samples, these models are not effective in the beginning of the training, when such identification is still unstable. Furthermore, in high-noise rate problems, such more constrained identification of clean samples can lead to small labelled training sets, deteriorating training generalisation.</p><p>We address these two problems with our two-stage training process (see <ref type="table" target="#tab_1">Figures 1</ref> and 2), where the first stage finds the high-confidence set of labelled clean samples, and in the second stage we train the model using that high-confidence labelled clean samples.</p><p>More specifically, in the first stage, we classify samples into clean or noisy using their loss values over a range of training epochs. Then in the second stage, we select the largest set of labelled clean samples (from the first stage) to form a high confidence set of clean samples to be used during the rest of the training that also relies on clean samples found by the small-loss strategy. The use of this high confidence set of clean samples addresses the two issues above (i.e., training instability at the beginning of the training and robustness to large noise rate problems). To the best of our knowledge, this approach has not been explored in the field.</p><p>Another technique being studied for noisy-label learning is the use of multiple models to improve the robustness of sample noise characterisation. Han et al. <ref type="bibr" target="#b38">[39]</ref> propose Co-teaching, which trains two models simultaneously, where each model estimates the clean sample set to be used by the other model. However, with an increase in the number of epochs, both networks converge to a consensus and show little difference between their estimated clean sets. Co-teaching+ <ref type="bibr" target="#b8">[9]</ref> relies on small loss samples that disagree on the predictions to select the data for the other model.</p><p>Although this multiple model strategy shows better results for filtering clean samples, noisy samples are usually ignored during training, decreasing the effectiveness of the approach. We also rely on the use of multiple models during training, but we do not ignore the noisy samples during training.</p><p>As mentioned above, after the automatic classification of clean and noisy samples, methods either disregard the noisy samples during training <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b38">39]</ref>, or use both the clean and noisy samples in a semi-supervised learning (SSL) approach <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b40">41]</ref>,</p><p>where SSL-based methods tend to show competitive results on benchmarks. One particularly successful technique that relies on SSL is DivideMix <ref type="bibr" target="#b7">[8]</ref> that relies on</p><p>MixMatch <ref type="bibr" target="#b10">[11]</ref> to linearly combine training samples classified as clean or noisy for the EVR minimisation <ref type="bibr" target="#b11">[12]</ref>. The generalisation of the EVR minimisation has been theoretically shown to depend on a large training set <ref type="bibr" target="#b12">[13]</ref>. However, DivideMix <ref type="bibr" target="#b7">[8]</ref> constrains this training set to be of the same size as the clean set, which tends to be small in large noise rate scenarios, resulting in poor EVR generalisation. To improve the EVR generalisation in noisy-label learning problems, we propose the over-sampling of the clean set to make it as large as the original training set, containing clean-and noisy-label samples. This idea has recently been explored in semi-supervised learning with clean labels <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>, but we are not aware of its extension to noisy-label learning problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem Setup and Hypotheses</head><p>We first define the noisy label learning problem. We summarise all mathematical notation used in this paper in <ref type="table" target="#tab_1">Table 1</ref>. Consider the training set  </p><formula xml:id="formula_0">D = {(x i , y i )} |D| i=1 , where x i ? S ? R W ?H is the i th image (W ?</formula><formula xml:id="formula_1">y i ? p(y|x i , Y,? i ), with p(y(j)|x i , Y,? i (c)) = ? jc (x i ), where the j, c ? {1, ..., Y}</formula><p>are the class indexes, ? jc (x i ) ? [0, 1] the probability of flipping from class c to j, and j?Y ? jc (x i ) = 1. We assume that this noise process can be of three types, namely symmetric <ref type="bibr" target="#b3">[4]</ref>, asymmetric <ref type="bibr" target="#b41">[42]</ref>, and instanced-based <ref type="bibr" target="#b42">[43]</ref>. The symmetric noise, also called uniform noise, refers to a noise type that the hidden label flips to a random class with a fixed probability ?, where the true label is included into the label flipping options, which means that in ? jc (x i ) = ? |Y|?1 , ?j ? Y, such that j = c, and ? cc (x i ) = 1 ? ?, such that ? cc &gt; 1 |Y| . The theoretical upper bound for the symmetric noise defined by ? &lt; 1 |Y| . The asymmetric noise is based on flipping labels between similar classes <ref type="bibr" target="#b41">[42]</ref>, where ? jc (x i ) depends only on the classes j, c ? Y, but not on x i , and ? jc &lt; ? cc when considering flipping labels between two similar classes. For example, using CIFAR-10 data set <ref type="bibr" target="#b20">[21]</ref>, the asymmetric noise maps truck ? automobile, bird ? plane, deer ? horse, as mapped by <ref type="bibr" target="#b43">[44]</ref> and ? jc &lt; 0.5. The instanced-based noise <ref type="bibr" target="#b42">[43]</ref> depends on both the classes j, c ? Y and the image x i .</p><p>Below, we first provide details on state-of-the-art (SOTA) noisy-label learning approaches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46]</ref> that we follow, and then we present our two hypotheses to improve the classification accuracy of these approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">State-of-the-art Noisy-label Learning</head><p>Our algorithm is built upon SOTA noisy-label learning approaches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46]</ref> that are based on: 1) an unsupervised learning classifier that characterises training samples as clean or noisy; and 2) an SSL classifier that assumes that the training samples classified as clean are labelled, and the samples classified as noisy are unlabelled. The SOTA noise-robust classifier <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7]</ref> is formed by an ensemble of two classifiers, each represented by f : S ? ? ? [0, 1] |Y| , where the classifier structure is the same, but their parameters are denoted by ?(1), ?(2) ? ? ? R L . The training for ?(1) influences ?(2) and vice-versa, where this can be achieved by co-training <ref type="bibr" target="#b7">[8]</ref> or student-teacher <ref type="bibr" target="#b6">[7]</ref> approaches. In this paper, we focus on co-training.</p><p>The unsupervised learning classifier predicts the clean and noisy samples based on their loss values <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b21">22]</ref>. Formally, assuming that the training is minimising the empirical risk</p><formula xml:id="formula_2">1 |D| |D| i=1 (f (x i ; ?), y i ),<label>(1)</label></formula><p>the set of clean and noisy samples are respectively defined by</p><formula xml:id="formula_3">X = {(x i , y i ) : p (clean| i , ?) ? ? } , U = {(x i , y * i ) : p (clean| i , ?) &lt; ? } ,<label>(2)</label></formula><p>where</p><formula xml:id="formula_4">y * i = f (x i ; ?), i = (f (x i ; ?), y i )</formula><p>represents a classification loss (e.g., cross entropy), and p (clean| (f (x i ; ?), y i ), ?) is a function that computes the probability that the training sample (x i , y i ) is clean based on its loss i <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b6">7]</ref>, where this function is parameterised by ? (in this paper, this probability function computes the posterior of the smaller-mean component of a bi-modal GMM, where this smaller mean represents the clean GMM component <ref type="bibr" target="#b7">[8]</ref>). To learn ?(1) and ?(2), co-training uses the clean and noisy sets from model ?(1) to train ?(2), and vice-versa. The classification of samples into clean or noisy, defined in <ref type="formula" target="#formula_3">(2)</ref>, is done based on the loss value for each training epoch. For learning problems containing large noise rates, this loss value can be unstable over different training epochs, reducing the precision that samples are classified into clean or noisy, resulting in poorer training convergence and generalisation.</p><p>The SSL based on MixMatch <ref type="bibr" target="#b10">[11]</ref> mixes the elements of X and U to minimise the empirical vicinal risk (EVR) <ref type="bibr" target="#b11">[12]</ref> </p><formula xml:id="formula_5">EV R = 1 |X | (x i ,? i ) ?X (X ) (f (xi; ?),?i) + ? (U ) |U | (x i ,? i ) ?U (U ) (f (xi; ?),?i),<label>(3)</label></formula><p>where ? (U ) weights the noisy set loss, (X ) (.) and (U ) (.) denote the losses in the clean and noisy sets, defined below in <ref type="bibr" target="#b10">(11)</ref>. These sets are respectively defined as</p><formula xml:id="formula_6">X = {(xi,?i) : (xi,?i) ? v(x,?|xi, yi), (xi, yi) ? X } U = {(xi,?i) : (xi,?i) ? v(x,?|xi, yi), (xi, yi) ? U},<label>(4)</label></formula><p>with</p><formula xml:id="formula_7">v(x,?|xi, yi) = 1 |X ? U| (x j ,y j ) ?X ?U E ? [? (x = ?xi + (1 ? ?)xj,? = ?yi + (1 ? ?)yj)] ,<label>(5)</label></formula><p>where ? is a Dirac mass centered at (x,?), ? ? Beta(?, ?), and ? ? (0, ?).</p><p>Note that v(x,?|x i , y i ) in (5) denotes a distribution that measures the probability of finding the pair (x,?) in the vicinity of (x i , y i ). The clean set X from (4) is built by sampling the vicinity distribution of clean samples in X , while the noisy set U is built by sampling the vicinity distribution of noisy samples from U. In <ref type="bibr" target="#b7">[8]</ref>, the noisy set size |U | and clean set size |X | are constrained to be equal to |X |, which means that |X | = |U | = |X | , and therefore |X ? U | &lt; |X ? U|. The main issue with this constraint is that, for problems with large noise rate, |X | tends to be small in comparison with the original dataset size |D|, compromising the generalisation of the EVR minimisation in <ref type="bibr" target="#b2">(3)</ref>.</p><p>We hypothesise that the classification accuracy of these noisy-label learning methods depends on: 1) the precision of the classification of clean samples to be included in X in (2) (Section 3.2), and 2) the size of the clean set denoted by |X | (Section 3.3).</p><p>In particular, a large |X | with a high proportion of clean samples will reduce the bound of the difference between the estimated and vicinal risks <ref type="bibr" target="#b12">[13]</ref>, improving the semi-supervised classification accuracy. We first hypothesise that the precision in the classification of clean samples in X can be improved by classifying as clean the samples that consistently show p(clean| i , ?) ? ? for ? epochs. We conjecture that such improved precision leads to a higher classification accuracy.   nn ) ? (0.0, 0.5)), the classification precision of clean samples in X tends to 1 and recall tends to 0, as ? increases, with U(a, b) denoting the uniform distribution between a and b.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Hypothesis One: Improving the Precision in Differentiating Clean and Noisy Samples</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Assuming that P</head><p>Proof. The precision and recall are calculated with:</p><formula xml:id="formula_8">Precision = P c ? t+? e=t+1 P (e) cc P c ? t+? e=t+1 P (e) cc + P n ? t+? e=t+1 P (e) cn , Recall = P c ? t+? e=t+1 P (e) cc P c ? t+? e=t+1 P (e) cc + P c ? (1 ? t+? e=t+1 P (e) cc ) ,<label>(6)</label></formula><p>where Precision = T P T P +F P and Recall =  nn for e 2 &gt; e 1 . As demonstrated in Lemma 3.1, precision increases to 1 and recall decreases to 0 as we increase ?. Section 5.3 shows empirical evidence that our approach to improve the precision when differentiating between clean and noisy-label samples, increases the clean set precision and classification accuracy, compared with the approach based on the smallloss strategy <ref type="bibr" target="#b7">[8]</ref>. Although the addition of noisy samples in the clean set might help the performance in some noisy-label scenarios, it is hard to measure the amount and to select the noisy samples that can increase classification accuracy. In general, in scenarios where the noisy-label rate is high, the amount of noisy-label samples that are predicted as clean and inserted into the labelled data can also be high, which has a negative impact on classification accuracy, as shown in Section 5.3. Therefore, targeting the formation of a clean set that has the highest possible proportion of clean samples constitutes our main goal when building the clean set, as explained in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Hypothesis Two: Oversampling the Clean Data to Increase the Robustness of EVR</head><p>We also hypothesise that the increase of the clean set |X | leads to a decrease of the bound for vicinal risk minimisation, which improves the semi-supervised classification accuracy, as shown in Theorem 8 of the paper <ref type="bibr" target="#b12">[13]</ref>. More specifically, for low noise rate problems, P cc tends to be large and P cn , small, so even for small values of ?, precision will be close to one with a relatively high Recall, allowing for a large |X | (see blue curves in <ref type="figure" target="#fig_8">Fig. 3</ref> for P n = 0.1). On the other hand, P cc tends to be small and P cn large in high noise rate scenarios, which means that ? needs to increase to push the precision to be close to one, but that can reduce the Recall to very low values, resulting in a potentially small |X |. Therefore, ? is a hyper-parameter that needs to be estimated to enable high precision and large |X |. Nevertheless, even with a careful estimation of ?, pushing the precision to be high in large noise rate scenarios can still result in a small |X |.</p><p>Hence, we propose two solutions to address the small |X | issue. The first solution consists of dividing the training process into two stages, where the first stage finds the largest possible set of high-confidence clean samples X , and the second stage trains the model using X as the minimum set of clean samples that is augmented with clean samples from the small-loss strategy. The second solution consists of sampling X with replacement when mixing up X and U in <ref type="bibr" target="#b3">(4)</ref>, such that |X | = |U | = |D|, which decreases the vicinal risk minimisation bound <ref type="bibr" target="#b12">[13]</ref>, improving the semi-supervised classification accuracy. Differently from standard oversampling approaches, which focus on increasing the minority class X (for high noise scenarios), our focus is to increase the Mixup iterations between X and U rather than just increasing the size of X .</p><p>In Section 5.4, we present an empirical results in <ref type="figure" target="#fig_24">Figure 7</ref> that shows that the test accuracy, as a function of training steps (iterations), for LongMix is better than DivideMix <ref type="bibr" target="#b7">[8]</ref>. In other words, this shows that adding more MixUp iterations per epoch, as in LongMix, is not equivalent to adding more epochs, as in the DivideMix baseline <ref type="bibr" target="#b7">[8]</ref>, so an increase in the number of epochs is not equivalent to adding more MixUp iterations, as we propose for LongMix. Furthermore, by fixing the number of training iterations for LongReMix and DivideMix, we show in <ref type="table" target="#tab_5">Table 3</ref> that our approach produces higher test accuracy, particularly for the high-noise rate scenarios. We also show in <ref type="table" target="#tab_5">Table 3</ref> that LongMix achieves higher accuracy results than the baseline using oversampling of X .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">LongReMix</head><p>In this section, we present the main contribution of this paper, namely the Lon-  <ref type="figure" target="#fig_0">Figures 1 and 2</ref>, where the first stage comprises the High Confidence Training (HCT), which trains the model to find a high confidence set of clean samples with high precision.</p><p>In the second stage, we combine this high confidence set of clean samples with the clean samples from the small-loss strategy to retrain the model. This retraining uses a new way to build the data sets X and U in (4), called LongMix, which enables the number of MixUp operations to be proportional to |D| instead of |X |, as described in Section 3.3. Therefore, the number of additional MixUp operations will be equal to |D| ? |X |. LongMix is the training strategy which oversamples the clean set, whereas LongReMix is the two-stage training, which uses HCT in the first stage and LongMix in the second stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">First Stage: High Confidence Training</head><p>The high confidence training (HCT) stage aims to increase the precision, without reducing too much the recall, of the unsupervised classification of clean and noisy training samples. Following the ideas presented in Sections 3.2 and 3.3, we re-define how to form the sets of clean and noisy samples, originally defined in (2), as follows:</p><formula xml:id="formula_9">X (e) 1 = (xi, yi, wi) : wi = p(clean| (e) i , ?) ? ?, ?e ? E , U (e) 1 = (xi, y * i , wi) : wi = p(clean| (e) i , ?) &lt; ?, ?e ? E ,<label>(7)</label></formula><p>where (e) i represents the loss of sample (x i , y i ) at training epoch e and E denotes the confidence window comprising the current and the previous (? ? 1) epochs -this is represented by the block "filter" that produces the high confidence clean set in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Hence, a sample to be in the clean set X (e) 1 must be classified as clean for ? epochs in a row, resulting in a more consistent, but smaller, set of clean samples, containing fewer noisy samples than the set in (2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Second Stage: Guided Training</head><p>The Guided Training stage depends on the high-confidence set of clean samples estimated from the first training stage with</p><formula xml:id="formula_10">H = arg max X (e) 1 :e?{ E 2 ,...,E} |X (e) 1 |,<label>(8)</label></formula><p>where E is the total number of training epochs for the first stage of training. Therefore, the high confidence set H from <ref type="formula" target="#formula_10">(8)</ref> consists of the largest clean set X (e) 1 obtained from the second half (i.e., e ? { E 2 , ..., E}) of the first stage of training (i.e., the HCT training stage). In the second stage of training, we define the labelled and unlabelled sets as in <ref type="formula" target="#formula_3">(2)</ref>, but we use H to update these sets as follows:</p><formula xml:id="formula_11">X (e) 2 = (x i , y i , w i ) : ? ? ? w i = 1, if (x i , y i ) ? H; or w i = p clean| (e) i , ? ? ?, otherwise , U (e) 2 = (x i , y * i , w i ) : w i = p clean| (e) i , ? &lt; ?, and (x i , y i ) / ? H .<label>(9)</label></formula><p>Hence, to form the clean set X During the second stage of LongReMix, we retrain the model from scratch 1 using the clean and noisy samples defined in <ref type="bibr" target="#b8">(9)</ref>.</p><p>As explained in Sections 3.2 and 3.3, we hypothesise that by sampling the clean set with replacement (i.e. oversampling the clean set), we increase the number of MixUp operations in the EVR loss in <ref type="formula" target="#formula_5">(3)</ref>, resulting in a smaller bound of the difference between estimated and vicinal risks <ref type="bibr" target="#b12">[13]</ref>. Therefore, we propose LongMix that increases the number of MixUp operations to be |D|, instead of the number of predicted clean samples.</p><p>A criticism faced by LongMix is that adding more MixUp iterations per epoch may be equivalent to a simple increase in the number of epochs, but we show in the experiments that this is not true.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Training and Inference</head><p>The training loss for our proposed LongReMix is <ref type="bibr" target="#b7">[8]</ref>:</p><formula xml:id="formula_12">= EV R + ? R R ,<label>(10)</label></formula><p>where EV R denotes the empirical vicinal error from <ref type="formula" target="#formula_5">(3)</ref>, which sums the loss functions from the clean set, represented by (X ) , and from the noisy set, denoted by (U ) , which are defined as</p><formula xml:id="formula_13">(X ) (f (x i ; ?),? i ) = ?? i log(f (x i ; ?)), (U ) (f (x i ; ?),? i ) = ? i ? f (x i ; ?) 2 2 ,<label>(11)</label></formula><p>? R weights the regularisation loss defined as</p><formula xml:id="formula_14">R = KL ? ? ? |Y| 1 |X | + |U | x?(X U ) f (x; ?) ? ? ,<label>(12)</label></formula><p>which regularizes the training by approximating the model output for all samples to a uniform distribution, with ? |Y| denoting a vector of |Y| dimensions with values equal to Hence, LongReMix has a training process that is roughly two times longer than the original noisy-label SSL methods, but both LongReMix and noisy-label SSL methods are asymptotically linear in all parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We compare LongReMix with related approaches on five noisy-label learning benchmarks. We also analyze the performance of LongReMix on a number of ablation studies, where we show empirical evidence of the hypotheses in Sections 3.2 and 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Data Sets</head><p>We conduct our experiments on the data sets CIFAR-10, CIFAR-100 <ref type="bibr" target="#b20">[21]</ref>, CNWL (Red Mini-ImageNet) <ref type="bibr" target="#b21">[22]</ref>, Clothing1M <ref type="bibr" target="#b23">[24]</ref>, WebVision <ref type="bibr" target="#b22">[23]</ref> and Food101-N <ref type="bibr" target="#b24">[25]</ref>.</p><p>These datasets follow the main evaluation protocol for noisy labels used in literature <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b35">36]</ref>, which covers symmetric, asymmetric and real-world instance-dependent noise. 1 Pre-train f (x; ?(1)) and f (x; ?(2)) with cross entropy (CE) loss in D for E W iterations.  for iter=1 to num iters do <ref type="bibr" target="#b14">15</ref> { and CIFAR-100 data sets originally do not contain label noise, a common approach is to add synthetic noise to evaluate the models. For CIFAR-10/CIFAR-100 we investigated three noise types: symmetric, asymmetric and instanced-based, as defined in Section 3.</p><formula xml:id="formula_15">(x b , y b , w b )} B b=1 ? X (e) stage (k) 16 {(u b , y * b , w b )} B b=1 ? U (e) stage (k) 17 for b=1 to B do 18 {x b,m } M m=1 = DataAugment(x b , M ) 19 {? b,m } M m=1 = DataAugment(u b , M ) 20 p b = 1 M M m=1 f (x b,m ; ?(k)) 21 q b = 1 2M M,2 m=1,l=1 f (? b,m ; ?(l))) 22? b = TempShrp(w b y b + (1 ? w b )p b ; T ) 23q b = TempShrp(q b ; T )</formula><p>The symmetric noise is generated using ? ? {0.2, 0.5, 0.8, 0.9}, with ? defined in Section 3.</p><p>The asymmetric noise for CIFAR-10 is produced following the mapping used in <ref type="bibr" target="#b7">[8]</ref>, which maps the classes truck ? automobile, bird ? plane, deer ? horse, with ? jc ? {0.4, 0.49} (note that we study ? jc = 49% because it is close to the theoretical limit of 50% for this type of noise). The asymmetric noise for CIFAR-100 is produced following the mapping used in <ref type="bibr" target="#b41">[42]</ref>, which groups the 100 classes into 20 super-classes The CNWL dataset <ref type="bibr" target="#b21">[22]</ref> forms a benchmark in the study of real-world web label noise. The dataset is built with images and labels being crawled from the web, where the noisy label samples are represented by the matching images. The amount of noise is controlled, with amounts varying from 0% to 80%, where we specifically use noise rates 20%, 60% and 80%, as suggested by <ref type="bibr" target="#b47">[48]</ref>. The Red Mini-ImageNet comprises 100 classes with 50000 training images and 5000 test images, where the original 84?84-pixel images are resized to 32?32 pixels.</p><p>Clothing1M consists of 1 million training images acquired from online shopping websites and it is composed of 14 classes. As the images from the data set vary in size, we resized the images to 256 ? 256 for training, as used in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b48">49]</ref>. The data set is heavily imbalanced and most of the noise is asymmetric <ref type="bibr" target="#b49">[50]</ref>, with noise rate estimated to be around 40% <ref type="bibr" target="#b23">[24]</ref>. The data set provide additional clean sets for training, validation, and test of 50k, 14k and 10k images, respectively. For our experiments we do not use any of the clean training or validation sets, but we use the test set for evaluation.</p><p>WebVision contains 2.4 million images collected from the internet, with the same 1000 classes from ILSVRC12 <ref type="bibr" target="#b50">[51]</ref> and images resized to 256 ? 256 pixels. It provides a clean test set of 50k images, with 50 images per class. We compare our model using the first 50 classes of the Google image subset, as used in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b51">52]</ref>.</p><p>Food101-N <ref type="bibr" target="#b24">[25]</ref> contains 310,009 training images of food recipes classified in 101 classes and 25,000 images for the testing set. The images from this data set were resized to 256 ? 256. This data set is based on the Food101 data set <ref type="bibr" target="#b52">[53]</ref>, but it has more images with noisy labels. The test set is the same provided by the original Food101 <ref type="bibr" target="#b52">[53]</ref>, which is a clean test set of 25K images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation</head><p>The model f (x; ?) is represented by a 18-layer PreAct ResNet18 (PRN18) <ref type="bibr" target="#b53">[54]</ref> for CIFAR-10 and CIFAR-100, InceptionV2 <ref type="bibr" target="#b54">[55]</ref> for WebVision (this is the model used by competing approaches), and ResNet50 <ref type="bibr" target="#b55">[56]</ref> for Clothing1M and Food-101N. The and Food 101N, is a 50-layer Residual Network, with standard residual blocks without pre-activation. InceptionV2 is a CNN composed of inception modules, which consist of wider layers to compute convolutions of different filter sizes that are concatenated and sent to the next inception module. The models for each data set were selected according to the evaluation protocol used in prior works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b23">24]</ref>. The models are trained with stochastic gradient descent with momentum of 0.8, weight decay of 0.0005 and batch size of 64. The learning rate is 0.02 which is reduced to 0.002 in the middle of the training.</p><p>The WarmUp and total number of epochs is defined according to each data set, as defined in <ref type="bibr" target="#b7">[8]</ref>. The Mixup parameter is ? = 4, used to estimate ? ? Beta(?, ?) in <ref type="bibr" target="#b4">(5)</ref>, and the regularisation weight for the loss in <ref type="formula" target="#formula_2">(10)</ref> is ? R = 1 for symmetric noise and ? R = 0 for asymmetric noise-these two parameters are as defined in <ref type="bibr" target="#b7">[8]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Testing Hypothesis One: Improving the Precision in Differentiating Clean and Noisy Samples</head><p>In this section, we show empirically that our approach to differentiate between clean and noisy samples leads to higher precision and lower recall than the method based on the small-loss strategy <ref type="bibr" target="#b7">[8]</ref>, which in turn enables higher classification accuracy. For the  experiments below, we measure the Precision = TP TP+FP and Recall = TP TP+FN results after the first training stage (HCT) and the classification accuracy after the second stage of training (i.e., guided training), where TP refers to the samples correctly predicted as clean, FP denotes the noisy samples incorrectly predicted as clean, and FN denotes the clean samples incorrectly predicted as noisy. We first show in <ref type="figure">Figure 4</ref> that in general, higher accuracy is correlated with a higher precision of the predicted clean set, particularly for high noise rates, where a clean set precision increase is obtained by varying the confidence window ? ? {1, 3, 5, 10}. Increasing the value of ? leads to a higher precision, which is related to a high accuracy, as shown in <ref type="figure">Figure 4</ref> . <ref type="table" target="#tab_3">Table 2</ref> displays the results of LongReMix using different values of ? ? {1, 3, 5, 10}, for CIFAR-10, under symmetric (ranging from 80% to 90%) and asymmetric (ranging from 40% and 49%) noises. Note that in general, precision increases and recall decreases with larger ? values, and classification accuracy reaches a peak at around ? = 5. Hereafter, we fixed the confidence window at ? = 5 in <ref type="bibr" target="#b6">(7)</ref>. <ref type="figure" target="#fig_16">Figure 5 (a)</ref> shows how the training loss can be unstable during training for high levels of noise rate, which harms the precision of the predicted clean set. By increasing ?, we can improve the precision of clean set, as displayed in <ref type="figure" target="#fig_16">Figure 5 (b)</ref>.</p><p>We evaluate the precision and recall of the clean set X   (DivideMix Baseline). and X . We highlight the value of ? = 0.5, which is the default value <ref type="bibr" target="#b7">[8]</ref> that we use to split the clean and noisy samples.</p><p>Notice that in this highly asymmetric noise scenario, the curve from HCT shows a better trade-off than the Baseline. <ref type="figure" target="#fig_23">Figure 6-</ref>(b,c) shows that X (e) 1 from HCT trades off a higher precision for a lower recall, compared with X from the Baseline for several types of noise, As shown below, this has a large influence on the training efficacy of LongReMix.</p><p>The only exception to this pattern in <ref type="figure" target="#fig_23">Figure 6-(b,c)</ref> is the 90% symmetric case for CIFAR-100, where precision is smaller and recall larger for HCT, compared with the baseline. We notice that for this particular case with high-noise rate and large number of classes, the initial clean set has a very low clean sample classification accuracy (around 13%) at the start of the training, which contains a fair amount of noise which can be quickly overfit, damaging the rest of the training. This issue can be fixed by increasing the confidence window ? from 5 to 10, when we notice that the usual pattern of higher precision and lower recall for HCT compared with the baseline is restored. According to the results in <ref type="figure" target="#fig_23">Figure 6</ref>, the use of this high-confidence set can improve precision by as much as 30%, with an average recall reduction of around of 10%. Also, <ref type="table" target="#tab_1">Table 10</ref> shows that the high-confidence clean set from HCT can improve the classification results up to by around 2%, compared with the DivideMix baseline <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Testing Hypothesis Two:</head><p>Oversampling the Clean Data to Increase the Robustness of EVR <ref type="figure" target="#fig_24">Figure 7</ref> shows the test accuracy versus the number of training steps (iterations)</p><p>for LongMix compared to the baseline <ref type="bibr" target="#b7">[8]</ref>, for CIFAR-10 at 90% symmetric noise.</p><p>This figure shows that adding more MixUp iterations per epoch, as in LongMix, is not equivalent to adding more epochs, as in baseline <ref type="bibr" target="#b7">[8]</ref>. This shows evidence for the claim in Section 4.2 that a simple increase in the number of epochs is not equivalent to adding more MixUp iterations, as we propose for LongMix. <ref type="table" target="#tab_5">Table 3</ref> shows further evidence for this claim by comparing LongMix and DivideMix baseline <ref type="bibr" target="#b7">[8]</ref> using the same number of training iterations for different noise rates on CIFAR-10 and CIFAR-100. In <ref type="table" target="#tab_5">Table 3</ref>  oversamples the clean and noisy samples and promotes additional MixUp operations between these two sets. The second approach shows more stable results for the studied noise rates and it is the one selected to be used in the following experiments in this work.</p><p>We can also see in <ref type="table" target="#tab_5">Table 3</ref> that LongMix is in general more accurate than standard DivideMix for most cases, with improvements of up to 9%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Comparison with the State-of-the-Art</head><p>All comparisons in this section are performed with the same network architecture and trained for the same number of epochs as the compared methods. For CIFAR-10 and CIFAR-100, we evaluate our model using different levels of symmetric label noise ranging from 20% to 90%. We also consider asymmetric noise, with noise rates of 40% and 49%. For all results, we train the model three times, using different initialisation, and report the results with the mean ? standard deviation. We report both the best test accuracy across all epochs and the averaged test accuracy over the last 10 epochs of training, similar to <ref type="bibr" target="#b7">[8]</ref>.    <ref type="table">Table 7</ref>: Results for WebVision <ref type="bibr" target="#b22">[23]</ref>. Results from baseline methods are as presented in <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Test Accuracy  <ref type="table">Table 8</ref>: Results for Clothing1M <ref type="bibr" target="#b23">[24]</ref>. Results from baseline methods are as presented in <ref type="bibr" target="#b7">[8]</ref>. The marker ? denotes the model is trained from scratch. <ref type="table" target="#tab_9">Table 5</ref> shows again the superiority of our approach compared to the related work. We also show results on the Red Mini-ImageNet problem from the CNWL dataset <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b47">48]</ref> on <ref type="table" target="#tab_10">Table 6</ref>. Note that in this table, we also train the model three times, using different initialisations, and report the results with the mean ? standard deviation. This table shows again that LongReMix has significantly better accuracy than competing SOTA approaches (at least three standard deviations away from the SOTA).</p><p>Also, we evaluate our method on large-scale data sets. For WebVision, <ref type="table">Table 7</ref> shows  <ref type="table">Table 9</ref>: Results for Food-101N <ref type="bibr" target="#b24">[25]</ref>. Methods marked by * denote re-implementations based on public code. not observe any improvement with pre-trained models, and therefore we trained from scratch with 128k images from Clothing1M. The results in <ref type="table">Table 8</ref> show that our model, trained from scratch and with a reduced training set, obtained comparable results to the competing approaches. Lastly, <ref type="table">Table 9</ref> summarizes the results for Food-101N. For this problem, we evaluate our approach with a pre-trained model and trained from scratch, and LongReMix outperforms all other approaches in both scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Statistical Analysis</head><p>We compare our LongReMix with ELR+ <ref type="bibr" target="#b57">[58]</ref>, DivideMix <ref type="bibr" target="#b7">[8]</ref> and M-correcction <ref type="bibr" target="#b9">[10]</ref>, relying on their best results over multiple data sets, and using the statistical test proposed by Dem?ar <ref type="bibr" target="#b25">[26]</ref>. Specifically, we use the Friedman non-parametric test, with significance level 0.1, where the null hypothesis indicates that all methods perform equally well. For this analysis, we use CIFAR-10, CIFAR-100 and WebVision, comprising a total of 10 different independent experiments (including the different noise rates) for each method. The Friedman test produces a p-value = 3.47 ? 10 ?3 , which rejects the null hypothesis, suggesting that at least one of the methods is statistically different from the others. Thus, to identify the groups of methods that present statistical similarity in multiple comparisons, we apply the post-hoc Nemenyi test to obtain the average ranks and calculate the Critical Difference (CD) value of the results, where the results from two methods are considered to be significantly different if the corresponding average ranks differ by at least the CD value. The test produced a CD value of 1.26, which means that a rank distance above 1.26 represents significantly different methods at 0.1 confidence level. <ref type="figure" target="#fig_25">Figure 8</ref> shows the comparison of these results through the CD   diagram, where we show that LongReMix is significantly better than all other methods, with ELR+ and DivideMix not being statistically different from each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.">Ablation Study</head><p>We analyze the effect of the different components of our proposal in an ablation study, shown in <ref type="table" target="#tab_1">Table 10</ref>. We first evaluate our approach without LongMix -this approach is referred to as "Retrain". Then we evaluate training only with the LongMix, without the second stage of re-training, and the whole model is denoted as LongReMix.</p><p>In general, we can observe that the LongReMix is competitive for all noise scenarios (being best or second best for all cases), but it is generally better for the large-scale data sets. Considering different data sets and noise rates, LongReMix shows the best average rank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We presented LongReMix, a new noisy-label learning algorithm based on an unsupervised learning stage to classify clean and noisy training samples, followed by an SSL stage to minimise the EVR using a labelled set formed by samples classified as clean, and an unlabelled set with samples classified as noisy.</p><p>We showed a thorough analysis of LongReMix, providing evidence that our new unsupervised clean sample classification and novel approach to increase the training set size for MixMatch enable a better classification accuracy for models trained with noisylabel samples. We also showed that LongReMix reaches state-of-the-art performance on the main noisy-label learning benchmarks of the field, showing robustness to overfitting in high label noise problems. Using the statistical test from <ref type="bibr" target="#b25">[26]</ref>, we show that the LongReMix results are statistically significant compared to its main competitors DivideMix <ref type="bibr" target="#b7">[8]</ref>, ELR+ <ref type="bibr" target="#b57">[58]</ref>, and M-Correction <ref type="bibr" target="#b9">[10]</ref>.</p><p>Although our LongReMix showed state-of-the-art results in the main benchmarks of the field, we plan to investigate further how to improve the precision of the unsupervised classification of clean and noisy samples since this seems to be a critical aspect of these two-stage noisy-label learning algorithms. We will also study new ways to improve the SSL learning accuracy by considering state-of-the-art methods <ref type="bibr" target="#b61">[62]</ref>. Another important point worth investigating is the role of learning regularisation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref> in our framework. Finally, the study of new types of noise models (e.g., combined closedand open-set noise <ref type="bibr" target="#b40">[41]</ref>) is also important to consider by future noisy-label learning algorithms.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>LongReMix stage 1 finds a high-confidence set of clean samples using the small loss strategy over a range of epochs, where a larger range implies a larger precision and lower recall in finding clean samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>LongReMix stage 2 increases the high-confidence clean set from stage 1 with a clean set found by the small-loss strategy, which are used to train the proposed LongMix that oversamples this clean set to improve the robustness of the SSL to small clean sets, usually formed in high label noise rate problems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 H</head><label>1</label><figDesc>H denote the width and height of the image) and y i ? {0, 1} |Y| is a one-hot vector representing the noisy label, with Y = {1, ..., |Y|} denoting the set of labels, and c?Y y i (c) = 1. The label y i may differ from the unknown true label? i as a result of a noise process represented by the probability that the label for xi flips from class c to j, (c, j ? {1, ..., |Y|}) f : S ? ? ? [0, 1] |Y| classifier parameterised by ? ? ? ? parameter space of the classifier (f (x; ?), y) classification loss to train the classifier X , U initial sets of clean and noisy samples, respectively p (clean| (f (x; ?), y), ?) probability function, parameterised by ?, that estimates if (x, y) is clean X ,U clean and noisy sets formed by MixMatch [11] from X ,U , respectively (X ) , (U ) loss functions for the clean and noisy sets EV R empirical vicinal risk that sums (X ) and (U ) R regularisation loss that penalizes classifications far from the uniform distribution v(x,?|xi, yi) vicinity distribution for (xi, yi) used by LongMix P (e) cc probability of classifying a clean sample as clean during training epoch e ? {1, ..., E} P (e) nc probability of classifying a clean sample as noisy during training epoch e ? {1, ..., E} P (e) nn probability of classifying a noisy sample as noisy during training epoch e ? {1, ..., E} P (e) cn probability of classifying a noisy sample as clean during training epoch e ? {1, ..., E} Pc, Pn proportion of clean and noisy samples in the training set X sets formed by high confidence training (HCT) in the 1 st training stage ? number of training epochs used by HCT to form X high confidence set of clean samples estimated from X sets formed in the 2 nd training stage</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :(e 2 )</head><label>32</label><figDesc>Precision and recall as a function of the number of training epochs ? (in a range from 1 to 20 in the horizontal axis) for P (e) cc ? U(0.5, 1.0), P (e) nn ? U(0.5, 1.0) and Pn, where the graph on the left assumes that P (e) cc and P (e)nn are independent between epochs indexed by e, while on the right, we assume that P nn for epoch e 2 &gt; e 1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>cc denotes the probability of classifying a clean sample as clean during training epoch e ? {1, ..., E}, P (e) nc = 1 ? P (e) cc the probability of classifying a clean sample as noisy. Similarly, P (e) nn represents the probability of classifying a noisy sample as noisy during training epoch e, P</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(e) cn = 1 ? P (e) nn the probability of classifying a noisy sample as clean. Also, P c and P n denote the proportion of clean and noisy samples in the training set, with P n + P c = 1. The probability of a clean sample being in the clean set X for ? epochs is t+? e=t+1 P (e) cc , and the probability of a noisy sample being in the clean set for ? epochs is</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Lemma 3 . 1 .</head><label>31</label><figDesc>Assuming that P (e) cc ? U(0.5, 1.0) (so P (e) nc = (1 ? P (e) cc ) ? (0.0, 0.5)) and P (e) nn ? U(0.5, 1.0) (so P (e) cn = (1 ? P (e)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>N , with true positives (T P ) being computed by the proportion of clean samples P c multiplied by the proportion of clean samples classified as clean P (e) cc , false positives (F P ) being calculated with the proportion of noisy samples P n times the proportion of noisy samples classified as clean P (e) cn , and false negatives (F N ) being computed as the proportion of clean samples P c times the proportion of clean samples classified as noisy P (e) nc -all terms are computed over ? epochs. Given that P (e) cn ? (0.0, 0.5) and P (e) cc ? (0.5, 1.0) and that lim ??? ( cc ) ? 0, Precision tends to 1, and similarly, given that lim ??? (1 ? cc ) ? ?, Recall tends to 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 3</head><label>3</label><figDesc>shows precision and recall as a function of the number of training epochs ? (in a range from 1 to 20 in the horizontal axis) for several values of P n and assuming that P (e) cc and P (e)nn are independent between epochs (left), while on the right, we assume that P</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>gReMix noisy-label learning algorithm. The two hypothesis in Sections 3.2 and 3.3 are explored for developing LongReMix. A simplified diagram of LongReMix is displayed in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>training stage, samples need to be in the high-confidence set H from (8) or be classified as clean using the small loss strategy (i.e., p(clean| (e) i , ?) ? ? ). To form the noisy set U (e) 2 , samples cannot be in the high-confidence set H and they have to be classified as noisy with p(clean| (e) i , ?) &lt; ? .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>1 /</head><label>1</label><figDesc>|Y|, and KL[a||b] representing the Kullback Leibler divergence between a and b. The pseudocode for the training of LongReMix is shown in Algorithm 1, where the function DataAugment(x, M ) returns M augmented samples from x using simple geometric transformations; and TempShrp(y, T ) sharpens the distribution y with temperature T . The inference for a test sample is calculated based on the average of predictions from both networks. Our model has the same number of hyper-parameters and the same values as SOTA methods [8], except for the addition of the confidence window ?. All hyper-parameters from Algorithm 1 are fixed independently of the data set. Assuming that we have 2 models for training, E is the number of epochs, M denotes the number of data augmentations, |D| represents the training set size, and L is the number of model parameters, the run-time complexity of LongReMix is dominated by the main training iteration of the first (high confidence training) and second (guided training) stages, which is as follows: O(2 ? 2 ? E ? M ? |D| ? L). The original time complexity of noisy-label SSL methods with two models is O(2 ? E ? M ? |D| ? L).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>CIFAR- 10</head><label>10</label><figDesc>and CIFAR-100 have 50000 training and 10000 testing images of size 32?32 pixels, where CIFAR-10 has 10 classes and CIFAR-100 has 100 classes and all training Input: D, batch size B, threshold ? , number of augmentations M , temperature sharpening T , ? (U ) , ? R , Beta parameter ?, confidence window ?, number of epochs for warm-up (E W ) and training (E), stage ? {1, 2}, and H (if stage == 2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>2 while e &lt; E do 3 for 4 Estimatei</head><label>34</label><figDesc>i = {1, ..., |D|} do (k) is the CE loss for (f (xi; ?(k)), yi) stage (k) using<ref type="bibr" target="#b6">(7)</ref> with p(clean|</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>stage (k) and U (e) stage (k) using (9) with p(clean| (e) i (mod(k, 2) + 1), ?) and H 12</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>24 end 25</head><label>2425</label><figDesc>Build sets X ,U using<ref type="bibr" target="#b3">(4)</ref> with X = {(x b,m ,? b )} b?(1,...,B),m?(1,...,M ) U = {(? b,m ,q b )} b?(1,...,B),m?(1,...,M ) Update ?(k) with from (10) LongReMix and testing sets have a perfectly balanced number of images per classes. As CIFAR-10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>containing 5</head><label>5</label><figDesc>original classes (e.g., super-class 'Aquatic Mammals' contains 'Beaver', Dolphin', 'Otter', 'Seal', and 'Whale'), and within each super-class the noise flips each class into the next one, circularly. We also evaluate the instanced-based noise scenario, where we follow the setup from<ref type="bibr" target="#b42">[43]</ref> to generate semantic (or instance-dependent) noisy labels using a trained VGG<ref type="bibr" target="#b46">[47]</ref>, DenseNet (DN), and ResNet (RN) on CIFAR-10 and CIFAR-100. This instance-dependent noise was generated by training the VGG, DN and RN using 5% of the clean-labelled samples from CIFAR-10 and 20% from CIFAR-100 (also clean-labelled samples), and producing the labels of the remaining training samples from the trained model predictions. The instance-dependent label noise dataset for CIFAR-10 and CIFAR-100 are separated by the predictions of each model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>PreAct</head><label></label><figDesc>ResNet18 (PRN18) used for CIFAR-10 and CIFAR-100, is a 18-layer Residual Network (ResNet) containing convolutional and pooling layers, and residual blocks, where skip connections are added to deal with the vanishing gradient issue. It also has a pre-activation variant of residual block, where the ReLU layer is moved from the shortcut connection path to an earlier layer. The ResNet50, used for Clothing-1M</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>For CIFAR-10 and CIFAR-100, PRN18 is based on a WarmUp stage of 30 epochs, with 300 epochs of total training. For WebVision, the InceptionV2 is trained for 100 epochs, with a WarmUp stage of 1 epoch. For Clothing1M, ResNet-50 is trained for 80 epochs with WarmUp stage of 1 epoch. For Food-101N, we also use ResNet-50 and rely on the same training protocol as in [49], consisting of training for 30 epochs, WarmUp stage of 1 epoch and reducing the learning rate by a factor of 10 every 10 epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Best test accuracy versus the precision of predicted clean set for CIFAR-10 for different levels and types of label noise, using LongReMix. The different values of precision are obtained from different values of ? ? {1, 3, 5, 10}. All the runs were performed using 300 epochs. Precision of predicted clean set vs. Epochs Training loss versus number of epochs for CIFAR-100 at 90% symmetric noise rate, using HCT, where the black line and gray error bar denote the mean and standard deviation of the training loss of the last 10 epochs (a), and the precision of the clean set as the first stage training progresses, using ? ? {1, 5} (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 6 :) 1 from ( 7 )</head><label>617</label><figDesc>Number of clean samples vs. Noise rate (a) Precision versus Recall for our proposed X(e(denoted by HCT) and X from (2) (Baseline [8]), for 40% asymmetric noise on CIFAR-10, where ? ? [0, 1] (denoted by th) for p(clean| , ?).(b) Precision and (c) Recall and (d) number of clean samples for different noise rates, for CIFAR-10 and CIFAR-100, using ? = 0.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>(e) 1 from ( 7 )</head><label>17</label><figDesc>in the last epoch of the first stage of training (HCT), compared to the clean set X from (2) that relies on the small loss result from the last epoch (DivideMix Baseline [8]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 6 -</head><label>6</label><figDesc>(a) shows the Precision vs Recall of predicted clean set for CIFAR-10 with 40% asymmetric noise, where results are obtained by varying the threshold ? applied to p(clean| , ?) to form X (e) 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 7 :</head><label>7</label><figDesc>Test accuracy versus number of training steps (or iterations) for CIFAR-10 at 90% symmetric noise for our proposed LongMix (where sizes of X and U in (4) are |D|) and the baseline (with sizes of X and U being |X |<ref type="bibr" target="#b7">[8]</ref>).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 8 :</head><label>8</label><figDesc>Comparison of LongReMix, ELR+<ref type="bibr" target="#b57">[58]</ref>, DivideMix<ref type="bibr" target="#b7">[8]</ref> and M-correcction<ref type="bibr" target="#b9">[10]</ref> using the Friedman-Nemenyi statistical test, which produced a critical distance (CD) value of 1.26. This CD value is used to estimate if two methods are different at level significance level 0.1. Groups of classifiers that are not significantly different are connected with a thick horizontal line (e.g., ELR+ and DivideMix are not significantly different</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>LongReMix: Robust Learning with High Confidence Samples in a Noisy Label Environment in most datasets. The code is available at https://github.com/filipe-research/LongReMix. Keywords: noisy label learning, deep learning, empirical vicinal risk, semi-supervised learning 1. Introduction Training Deep Neural Networks (DNNs) often requires large data sets to perform</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Mathematical notation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results of Precision, Recall and Accuracy (Best value and avg. of Last 10 epochs) of LongReMix for different values of ?, for CIFAR-10 with symmetric and asymmetric noise. We highlight the highest precision, recall, and accuracy (best and last) values for each noisy label problem.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Comparison of the test accuracy between LongMix with additional MixUp operations betweenoversampled clean samples (where size of X is |D|) , LongMix with additional MixUp operations between oversampled clean and noisy samples (where size of X and U in (4) are |D|), and the baseline in<ref type="bibr" target="#b7">[8]</ref> (with sizes of X and U being |X |) , using the same number of iterations on CIFAR-10 and CIFAR-100 under symmetric (ranging from 20% to 90%) and asymmetric (ranging from 40% and 49%) noise.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Results using PRN18 on CIFAR-10 and CIFAR-100 under symmetric (ranging from 20% to 90% and asymmetric (ranging from 40% and 49%) noises. Results from related approaches are as presented in<ref type="bibr" target="#b7">[8]</ref>.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4</head><label>4</label><figDesc>shows that for CIFAR-10 and CIFAR-100 data sets, our method obtains better results for all evaluated noise rates. LongReMix displays a</figDesc><table><row><cell>Data set</cell><cell></cell><cell cols="2">CIFAR-10</cell><cell cols="2">CIFAR-100</cell><cell></cell></row><row><cell>Method/ noise ratio</cell><cell>DN</cell><cell>RN</cell><cell>VGG</cell><cell>DN</cell><cell>RN</cell><cell>VGG</cell></row><row><cell></cell><cell>(32%)</cell><cell>(38%)</cell><cell>(34%)</cell><cell>(34%)</cell><cell>(37%)</cell><cell>(37%)</cell></row><row><cell>CE + RoG</cell><cell cols="6">68.33 64.15 70.04 61.14 53.09 53.64</cell></row><row><cell>Bootstrap + RoG</cell><cell cols="6">68.38 64.03 70.11 54.71 53.30 53.76</cell></row><row><cell>Forward + RoG</cell><cell cols="6">68.20 64.24 70.09 53.91 53.36 53.63</cell></row><row><cell>Backward + RoG</cell><cell cols="6">68.66 63.45 70.18 54.01 53.03 53.50</cell></row><row><cell>D2L + RoG</cell><cell cols="6">68.57 60.25 59.94 31.67 39.92 45.42</cell></row><row><cell>DivideMix*</cell><cell cols="6">84.57 81.61 85.71 68.40 66.28 66.84</cell></row><row><cell>LongReMix [ours]</cell><cell cols="6">85.13 82.51 85.90 69.03 66.70 67.42</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Results for Instanced-based Noise. Results from baseline methods are as presented in<ref type="bibr" target="#b42">[43]</ref>. Methods marked by * denote re-implementations based on public code.</figDesc><table><row><cell>Method/ noise ratio</cell><cell>20%</cell><cell>40%</cell><cell>60%</cell><cell>80%</cell></row><row><cell>Cross-entropy [48]</cell><cell>47.36</cell><cell>42.70</cell><cell>37.30</cell><cell>29.76</cell></row><row><cell>MixUp [12]</cell><cell>49.10</cell><cell>46.40</cell><cell>40.58</cell><cell>33.58</cell></row><row><cell>DivideMix [8]</cell><cell>50.96</cell><cell>46.72</cell><cell>43.14</cell><cell>34.50</cell></row><row><cell>MentorMix [22]</cell><cell>51.02</cell><cell>47.14</cell><cell>43.80</cell><cell>33.46</cell></row><row><cell>FaMUS [48]</cell><cell>51.42</cell><cell>48.06</cell><cell>45.10</cell><cell>35.50</cell></row><row><cell cols="5">LongReMix (Ours) 56.03?0.5 50.69?0.3 46.81?0.3 38.24?0.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Test accuracy (%) for Red Mini-ImageNet<ref type="bibr" target="#b21">[22]</ref>. Results from baseline methods are as presented in<ref type="bibr" target="#b47">[48]</ref>. Top methods are in bold.higher improvement for large symmetric noise and asymmetric noise scenarios, which can be considered as the most challenging cases. We believe that the improvement over higher noise rates is due to the LongMix approach, which runs a large number of MixUp operations proportional to the size of the training set. The retraining with high confidence samples also improves the results for asymmetric noise. Using the mean ? standard deviation results onTable 4, we notice that LongReMix is in general at least 3 standard deviations from the competing methods, which can be considered a significant result (recall that 99.7% of samples following a normal distribution lie within 3 standard deviations of the mean). The results for instanced-based noise<ref type="bibr" target="#b42">[43]</ref> in</figDesc><table><row><cell>Method</cell><cell cols="2">Top 1 Top 5</cell></row><row><cell>Decoupling [59]</cell><cell>62.54</cell><cell>84.74</cell></row><row><cell>D2L [60]</cell><cell>62.68</cell><cell>84.00</cell></row><row><cell>MentorNet [61]</cell><cell>63.00</cell><cell>81.40</cell></row><row><cell>Co-teaching [39]</cell><cell>63.58</cell><cell>85.20</cell></row><row><cell>Iterative-CV [52]</cell><cell>65.24</cell><cell>85.34</cell></row><row><cell>DivideMix [8]</cell><cell>77.32</cell><cell>91.64</cell></row><row><cell>ELR+ [58]</cell><cell>77.78</cell><cell>91.68</cell></row><row><cell>LongReMix [ours]</cell><cell>78.92</cell><cell>92.32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>the Top-1 and Top-5 accuracy, where LongReMix displays better results than competing methods. For the Clothing1M evaluation, the competing methods rely on a pre-trained ImageNet model for training on Clothing1M. In our experiments, we did</figDesc><table><row><cell>Method</cell><cell cols="2">from pre-trained from scratch</cell></row><row><cell>Cross-Entropy</cell><cell>81.44</cell><cell>-</cell></row><row><cell>CleanNet</cell><cell>83.95</cell><cell>-</cell></row><row><cell>DeepSelf</cell><cell>85.10</cell><cell>-</cell></row><row><cell>DivideMix*</cell><cell>86.91</cell><cell>75.53</cell></row><row><cell>LongReMix [ours]</cell><cell>87.39</cell><cell>78.57</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>). 95.01 93.88 81.98 94.64 84.68 77.82 75.59 62.92 33.80 78.92 74.38 87.39 1.46 Last 96.02 94.72 93.37 81.35 94.32 76.08 77.52 75.11 62.34 33.25 78.00 73.00 87.29 1.69 LongMix Best 96.18 95.19 94.09 85.33 93.38 83.23 78.03 75.84 62.24 33.54 78.44 74.05 87.21 1.92 Last 95.98 94.79 93.73 84.71 91.87 77.18 77.56 74.87 61.60 33.00 77.72 73.25 87.12 1.69 Retrain Best 96.23 94.85 92.86 78.47 94.59 85.10 77.20 74.41 60.29 30.61 77.84 74.30 87.16 2.61 Last 95.89 94.60 92.54 77.51 94.31 80.88 76.89 73.89 59.88 30.37 77.84 73.21</figDesc><table><row><cell cols="2">Data set</cell><cell>CIFAR-10</cell><cell></cell><cell>CIFAR-100</cell><cell cols="2">Webv. Cloth.</cell><cell>Food.</cell><cell>Mean Rank</cell></row><row><cell cols="2">Noise type</cell><cell>sym.</cell><cell>asym.</cell><cell>sym.</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Method/ n. ratio</cell><cell>20% 50% 80% 90%</cell><cell>40% 49%</cell><cell>20% 50% 80% 90%</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>LongReMix</cell><cell>Best</cell><cell cols="6">96.25 86.98</cell><cell>2.61</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10 :</head><label>10</label><figDesc>Ablation Study Results. The italic bold, bold, and regular numbers represent respectively the ranking of first, second and third results in accuracy. Last column shows the average rank of each approach (smaller is better).</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We compared if we should fine-tune the model trained from the first stage or train from scratch, and the latter approach showed the best results.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank the support by the Australian Research Council through grants DP180103232, and FT190100525.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A survey on deep learning in medical image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kooi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Bejnordi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A A</forename><surname>Setio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ciompi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghafoorian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Van Der Laak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">I</forename><surname>S?nchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="60" to="88" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Classification in the presence of label noise: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fr?nay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Verleysen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="845" to="869" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Nlnl: Negative learning for noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="101" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Symmetric cross entropy for robust learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="322" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to reweight examples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4334" to="4343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Self: learning to filter noisy labels with self-ensembling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mummadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beggel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dividemix: Learning with noisy labels as semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">How does disagreement help generalization against label corruption?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised label noise modeling and loss correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="312" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5049" to="5059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Mixup: Beyond empirical risk minimization</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.04351</idno>
		<title level="m">Generalization bounds for vicinal risk minimization principle</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Relab: Reliable label bootstrapping for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Identifying mislabeled data using the area under the margin ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Elenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An empirical study of example forgetting during deep neural network learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Toneva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Combes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semi-supervised deep learning with memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="268" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Label propagation for deep semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Iscen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5070" to="5079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pseudo-labeling and confirmation bias in deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<title level="m">Beyond synthetic noise: Deep learning on controlled noisy labels, ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Webvision database: Visual learning and understanding from web data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2691" to="2699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cleannet: Transfer learning for scalable image classifier training with label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5447" to="5456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Statistical comparisons of classifiers over multiple data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dem?ar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Normalized loss functions for deep learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Imae for noise-robust learning: Mean absolute error does not treat examples equally and gradient magnitude&apos;s variance matters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Robertson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12141</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Photometric transformer networks and label adjustment for breast density prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jaehwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Donggeun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hyo-Eun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Iterative cross learning on noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mcmains</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="757" to="765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<title level="m">Pumpout: A meta approach for robustly training deep neural networks with noisy labels</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to rectify for robust learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="page">108467</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Rboost: Label noise-robust boosting algorithm based on a nonconvex loss function and the numerically stable base learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2216" to="2228" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning with biased complementary labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="68" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Distilling effective supervision from severe label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">O</forename><surname>Arik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9294" to="9303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Meta-weight-net: Learning an explicit mapping for sample weighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1919" to="1930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Robust learning at noisy labeled medical images: Applied to skin lesion classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1280" to="1283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Iterative learning with open-set noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8688" to="8696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Co-teaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8527" to="8537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thulasidasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chennupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mohd-Yusof</surname></persName>
		</author>
		<title level="m">Combating label noise in deep learning using abstention</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6234" to="6243" />
		</imprint>
	</monogr>
	<note>International Conference on Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Evidentialmix: Learning with combined open-set and closed-set noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sachdeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Cordeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3607" to="3615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1944" to="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Robust inference via generative classifiers for handling noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8778" to="8788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A semi-supervised two-stage approach to learning from noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1215" to="1224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Recycling: Semisupervised learning with noisy labels in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-J</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="66998" to="67005" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Faster meta update strategy for noise-robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep self-learning from noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5138" to="5147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Probabilistic end-to-end noise correction for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7017" to="7025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>Imagenet: A large-scale hierarchical image database</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Understanding and utilizing deep neural networks trained with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Food-101-mining discriminative components with random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="446" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning to learn from noisy labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5051" to="5059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Early-learning regularization prevents memorization of noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niles-Weed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fernandez-Granda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Decoupling&quot; when to update&quot; from&quot; how to update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="960" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Dimensionality-driven learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wijewickrema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3355" to="3364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2304" to="2313" />
		</imprint>
	</monogr>
	<note>International Conference on Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">In 2015, he received his Ph.D. in computer science from the Federal University of Pernambuco (UFPE). Filipe&apos;s mains contributions are in the area of computer vision, medical image analysis, and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filipe</forename><forename type="middle">R</forename></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Cordeiro is a professor in the Department of Computing at Universidade Federal Rural de Pernambuco (UFRPE)</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Ragav Sachdeva is a Ph.D. student in the Visual Geometry Group at the University of Oxford, supervised by Prof. Andrew Zisserman. He obtained his undergraduate degree in computer science at the University of Adelaide, where he did his honours thesis with Prof</title>
		<imprint>
			<pubPlace>Gustavo Carneiro</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Vasileios Belagiannis is a professor in the Faculty of Computer Science at Otto von Guericke University Magdeburg. His research deals with topics such as representation learning, uncertainty estimation, multi-modal learning, learning with different forms of supervision, learning algorithm for noisy labels, few-shot learning and meta-learning</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">of the School of Computer Science at the University of Adelaide, and the senior researcher at the Australian Institute for Machine Learning. His research interests include robotic and active vision, visual tracking, SLAM, human motion capture and intelligent visual surveillance</title>
		<imprint/>
	</monogr>
	<note>Ian Reid is the Head</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Director of Medical Machine Learning at the Australian Institute of Machine Learning and an Australian Research Council Future Fellow. His main research interests are in computer vision, medical image analysis and machine learning. He is moving to the CVSSP at the University of Surrey</title>
		<imprint>
			<date type="published" when="2022-12" />
		</imprint>
		<respStmt>
			<orgName>School of Computer Science at the University of Adelaide</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
