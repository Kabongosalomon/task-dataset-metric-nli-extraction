<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Face Alignment in Full Pose Range: A 3D Total Solution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Zhen</forename><forename type="middle">Lei</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
						</author>
						<title level="a" type="main">Face Alignment in Full Pose Range: A 3D Total Solution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Face Alignment</term>
					<term>3D Morphable Model</term>
					<term>Convolutional Neural Network</term>
					<term>Cascaded Regression !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Face alignment, which fits a face model to an image and extracts the semantic meanings of facial pixels, has been an important topic in the computer vision community. However, most algorithms are designed for faces in small to medium poses (yaw angle is smaller than 45 ? ), which lack the ability to align faces in large poses up to 90 ? . The challenges are three-fold. Firstly, the commonly used landmark face model assumes that all the landmarks are visible and is therefore not suitable for large poses. Secondly, the face appearance varies more drastically across large poses, from the frontal view to the profile view. Thirdly, labelling landmarks in large poses is extremely challenging since the invisible landmarks have to be guessed. In this paper, we propose to tackle these three challenges in an new alignment framework termed 3D Dense Face Alignment (3DDFA), in which a dense 3D Morphable Model (3DMM) is fitted to the image via Cascaded Convolutional Neural Networks. We also utilize 3D information to synthesize face images in profile views to provide abundant samples for training. Experiments on the challenging AFLW database show that the proposed approach achieves significant improvements over the state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Face alignment is the process of moving and deforming a face model to an image, so as to extract the semantic meanings of facial pixels. It is an essential preprocessing step for many face analysis tasks, e.g. recognition <ref type="bibr" target="#b0">[1]</ref>, animation <ref type="bibr" target="#b1">[2]</ref>, tracking <ref type="bibr" target="#b2">[3]</ref>, attributes classification <ref type="bibr" target="#b3">[4]</ref> and image restoration <ref type="bibr" target="#b4">[5]</ref>. Traditionally, face alignment is approached as a landmark detection problem that aims to locate a sparse set of facial fiducial points, some of which include "eye corner", "nose tip" and "chin center". In the past two decades, a number of effective frameworks have been proposed such as ASM <ref type="bibr" target="#b5">[6]</ref>, AAM <ref type="bibr" target="#b6">[7]</ref> and CLM <ref type="bibr" target="#b7">[8]</ref>. Recently, with the introduction of Cascaded Regression <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[11]</ref> and Convolutional Neural Networks <ref type="bibr" target="#b12">[12]</ref>, <ref type="bibr" target="#b13">[13]</ref>, face alignment has observed significant improvements in accuracy. However, most of the existing methods are designed for medium poses, under the assumptions that the yaw angle is smaller than 45 ? and all the landmarks are visible. When the range of yaw angle is extended up to 90 ? , significant challenges emerge. These challenges can be differentiated in three main ways:</p><p>Modelling: Landmark shape model <ref type="bibr" target="#b5">[6]</ref> implicitly assumes that each landmark can be robustly detected by its distinctive visual patterns. However, when faces deviate from the frontal view, some landmarks become invisible due to self-occlusion <ref type="bibr" target="#b14">[14]</ref>. In medium poses, this problem can be addressed by changing the semantic positions of face contour landmarks to the silhouette, which is termed landmark marching <ref type="bibr" target="#b15">[15]</ref>. However, in large poses where half of face is occluded, some landmarks are inevitably invisible <ref type="figure">Fig. 1</ref>. Fitting results of 3DDFA (the blue/red points indicate visible/invisible landmarks). For each pair of the four results, on the left is the rendering of the fitted 3D face with the mean texture, which is made transparent to demonstrate the fitting accuracy. On the right is the landmarks overlayed on the fitted 3D face model. and show no detectable appearance. In turn, landmarks can lose their semantic meanings, which may cause the shape model to fail.</p><p>Fitting: Another challenge in full-pose face alignment is derived from the dramatic appearance variations from front to profile. Cascaded Linear Regression <ref type="bibr" target="#b11">[11]</ref> and traditional nonlinear models <ref type="bibr" target="#b16">[16]</ref>, <ref type="bibr" target="#b9">[10]</ref> are not flexible enough to cover these complex variations in a unified way. Another framework demonstrates more flexibility by adopting different landmark and fitting models for differing view categories <ref type="bibr" target="#b14">[14]</ref>, <ref type="bibr" target="#b17">[17]</ref>, <ref type="bibr" target="#b18">[18]</ref>. Unfortunately, since the nature of this framework must test every view, computational cost is likely to significantly increase. More recently, Convolutional Neural Network (CNN) based methods have demonstrated improved performance over traditional methods in many applications. For effective large-pose face alignment, CNN should be combined with the Cascaded Regression framework. However, most existing methods adopt a single network to complete fitting <ref type="bibr" target="#b13">[13]</ref>, which limits its performance.</p><p>Training Data: Labelled data is the basis for any supervised learning based algorithms. However, manual labelling of land-marks on large-pose faces is very tedious since the occluded landmarks have to be "guessed" which is impossible for most of people. As a result, almost all the public face alignment databases such as AFW <ref type="bibr" target="#b18">[18]</ref>, LFPW <ref type="bibr" target="#b19">[19]</ref>, HELEN <ref type="bibr" target="#b20">[20]</ref> and IBUG <ref type="bibr" target="#b21">[21]</ref> are collected in medium poses. Few large-pose databases such as AFLW <ref type="bibr" target="#b22">[22]</ref> only contain visible landmarks, which could be ambiguous in invisible landmarks, makes it hard to train a unified face alignment model.</p><p>In this paper, we aim to solve the problem of face alignment in full pose range, where the yaw angle is allowed to vary between ?90 ? . We believe that face alignment is not barely a 2D problem since self-occlusion and large appearance variations are caused by the face rotation in the 3D space, which can be conveniently addressed by incorporating 3D information. More specifically, we improve the face model from 2D sparse landmarks to a dense 3D Morphable Model (3DMM) <ref type="bibr" target="#b23">[23]</ref> and consider face alignment as a 3DMM fitting task. The optimization concept therein will change accordingly from landmark positions to pose (scale, rotation and translation) and morphing (shape and expression) parameters. We call this novel face alignment framework 3D Dense Face Alignment (3DDFA). To realize 3DDFA, we propose to combine two achievements in recent years, namely, Cascaded Regression and the Convolutional Neural Network (CNN). This combination requires the introduction of a new input feature which fulfills the "cascade manner" and "convolution manner" simultaneously (see Sec. <ref type="bibr" target="#b2">3</ref>.2) and a new cost function which can model the priority of 3DMM parameters (see Sec. <ref type="bibr">3.4)</ref>. Besides to provide enough data for training, we find that given a face image and its corresponding 3D model, it is possible to rotate the image out of plane with high fidelity. This rotation enables the synthesis of a large number of training samples in large poses.</p><p>In general, we propose a novel face alignment framework to address the three challenges of modelling, fitting and training data in large poses. The main contributions of the paper are summarized as follows: 1) To address the self-occlusion challenge, we assert that in large poses, fitting a 3DMM is more suitable than detecting 2D landmarks. This paper is an extension of our previous work <ref type="bibr" target="#b24">[24]</ref> the following four aspects: 1) Traditional 3DMM uses Euler angles to represent the 3D rotation, which shows ambiguity when the yaw angle reaches 90 ? . In this paper, quaternions are used instead as the rotation formulation to eliminate the ambiguity. 2) A new input feature called Pose Adaptive Feature (PAF) is utilized to remedy the drawbacks of PNCC to further boost the performance. 3) We improve the cost function in <ref type="bibr" target="#b24">[24]</ref> through the OWPDC which not only formulates the importance but also the priority of 3DMM parameters during training. 4) Additional experiments are conducted to better analyze the motivation behind the design of the input features and the cost function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>Face alignment can be summarized as fitting a face model to an image. As such, there are two basic problems involved with this task: how to model the face shape and how to estimate the model parameters. In this section, we motivate our approach by discussing related works with respect to these two problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Face Model</head><p>Traditionally, face shape is represented by a sparse set of 2D facial fiducial points. Cootes et al. <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> show that shape variations can be modeled with subspace analysis such as Principal Components Analysis (PCA). Although, this 2D-subspace model can only cope with shape variations from a narrow range of face poses, since the non-linear out-of-plane rotation cannot be well represented with the linear subspace. To deal with the pose variations, some modifications like Kernel PCA <ref type="bibr" target="#b25">[25]</ref> and Bayesian Mixture Model <ref type="bibr" target="#b14">[14]</ref> are proposed to introduce non-linearity into the subspace models. Recently, Cao et al. <ref type="bibr" target="#b9">[10]</ref> propose to abandon any explicit shape constraints and directly use landmark coordinates as the shape model, which called 2D Non-Parametric Model (2D-NPM). 2D-NPM considerably improves the flexibility of the shape model at the cost of losing any shape priors and increasing the difficulty of model fitting. Besides 2D shape model, Blanz et al. <ref type="bibr" target="#b26">[26]</ref>, <ref type="bibr" target="#b23">[23]</ref> propose the 3D Morphable Model (3DMM) which applies PCA on a set of 3D face scans. By incorporating 3D information, 3DMM disentangles the non-linear out-of-plane transformation from the PCA subspace. The remaining shape and expression variations have shown high linearity <ref type="bibr" target="#b23">[23]</ref>, <ref type="bibr" target="#b1">[2]</ref>, which can be well modeled with PCA. Compared with 2D models, 3DMM separates rigid (pose) and non-rigid (shape and expression) transformations, enabling it to cover diverse shape variations and keep shape prior at the same time. Additionally, points visibility can be easily estimated by 3DMM <ref type="bibr" target="#b24">[24]</ref>, which can provide important clues to handle self-occlusion in profile views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Model Fitting</head><p>Most fitting methods can be divided into two categories: the template fitting based <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b27">[27]</ref> and regression based <ref type="bibr" target="#b28">[28]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[11]</ref>, <ref type="bibr" target="#b29">[29]</ref>. The template fitting methods always maintain a face appearance model to fit images. For example, Active Appearance Model (AAM) <ref type="bibr" target="#b6">[7]</ref> and Analysis-by-Synthesis 3DMM Fitting <ref type="bibr" target="#b23">[23]</ref> simulate the process of face image generation and achieve alignment by minimizing the difference between the model appearance and the input image. Active Shape Model (ASM) <ref type="bibr" target="#b5">[6]</ref> and Constrained Local Model (CLM) <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b30">[30]</ref> build a template model for each landmark and use a PCA shape model to constrain the fitting results. TSPM <ref type="bibr" target="#b18">[18]</ref> and CDM <ref type="bibr" target="#b17">[17]</ref> employ part based model and DPM-like <ref type="bibr" target="#b31">[31]</ref> method to align faces. Generally, the performance of template fitting methods depends on whether the image patterns reside within the variations described by the face appearance model. Therefore, it shows limited robustness in unconstrained environment where appearance variations are too wide and complicated.</p><p>Regression based methods estimate model parameters by regressing image features. For example, Hou et al. <ref type="bibr" target="#b32">[32]</ref> and Saragih et al. <ref type="bibr" target="#b33">[33]</ref> perform regression between texture residuals and parameter updates to fit AAM. Valstar et al. <ref type="bibr" target="#b34">[34]</ref> locate landmark positions by mapping the landmark related local patches with support vector regression. Recently, Cascaded Regression <ref type="bibr" target="#b8">[9]</ref> has been proposed and becomes most popular in face alignment community <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[11]</ref>, <ref type="bibr" target="#b35">[35]</ref>, <ref type="bibr" target="#b36">[36]</ref>, which can be summarized in Eqn. 1:</p><formula xml:id="formula_0">p k+1 = p k + Reg k (Fea(I, p k )).<label>(1)</label></formula><p>where the shape parameter p k at the kth iteration is updated by conducting regression Reg k on the shape indexed feature Fea, which should depend on both the image I and the current parameter p k . The regression Reg k shows an important "feedback" property that its input feature Fea(I, p) can be updated by its output since after each iteration p is updated. With this property an array of weak regressors can be cascaded to reduce the alignment error progressively. Besides Cascaded Regression, another breakthrough is the introduction of Convolutional Neural Network (CNN), which formulates face alignment as a regression from raw pixels to landmarks positions. For example, Sun et al. <ref type="bibr" target="#b12">[12]</ref> propose to use the CNN to locate landmarks in two stages, first the full set of landmarks are located with a global CNN and then each landmark is refined with a sub-network on its local patch. With one CNN for each landmark, the complexity of the method highly depends on the number of landmarks. Zhang et al. <ref type="bibr" target="#b13">[13]</ref> combine face alignment with attribute analysis through multi-task CNN to boost the performance of both tasks. Wu et al. <ref type="bibr" target="#b37">[37]</ref> cluster face appearances with mid-level CNN features and deal with each cluster with an independent regressor. Jourabloo et al. <ref type="bibr" target="#b38">[38]</ref> arrange the local landmark patches into a large 2D map as the CNN input to regress model parameters. Trigeorgis et al. <ref type="bibr" target="#b29">[29]</ref> convolve the landmark local patch as the shape index feature and conduct linear regression to locate landmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Large Pose Face Alignment</head><p>Despite the great achievements in face alignment, most of the state-of-the-art methods lack the flexibility in large-pose scenarios, since they need to build the challenging relationship between the landmark displacement and landmark related image features, where the latter may be self-occluded. In 2D methods, a common solution is the multi-view framework which uses different landmark configurations for different views. It has been applied in AAM <ref type="bibr" target="#b39">[39]</ref>, DAM <ref type="bibr" target="#b40">[40]</ref> and DPM <ref type="bibr" target="#b18">[18]</ref>, <ref type="bibr" target="#b17">[17]</ref> to align faces with different shape models, among which the one having the highest possibility is chosen as the final result. However, since every view has to be tested, the computational cost is always high. Another method is explicitly estimating the visibility of landmarks and shrink the contribution of occluded features <ref type="bibr" target="#b14">[14]</ref>, <ref type="bibr" target="#b41">[41]</ref>, <ref type="bibr" target="#b42">[42]</ref>. Nevertheless, occlusion estimation is itself a challenging task and handling varying dimensional feature is still an ill-posed problem.</p><p>Different from 2D methods, 3D face alignment <ref type="bibr" target="#b43">[43]</ref> aims to fit a 3DMM [23] to a 2D image. By incorporating 3D information, 3DMM can inherently provide the visibility of each model point without any additional estimation, making it possible to deal with the self-occluded points. The original 3DMM fitting method <ref type="bibr" target="#b23">[23]</ref> fits the 3D model by minimizing the pixel-wise difference between image and rendered face model. Since only the visible model vertices are fitted, it is the first method to cover arbitrary poses <ref type="bibr" target="#b23">[23]</ref>, <ref type="bibr" target="#b44">[44]</ref>, but it suffers from the one-minute-per-image computational cost. Recently, regression based 3DMM fitting, which estimates the model parameters by regressing the features at projected 3D landmarks <ref type="bibr" target="#b17">[17]</ref>, <ref type="bibr" target="#b45">[45]</ref>, <ref type="bibr" target="#b46">[46]</ref>, <ref type="bibr" target="#b47">[47]</ref>, <ref type="bibr" target="#b38">[38]</ref>, <ref type="bibr" target="#b48">[48]</ref>, <ref type="bibr" target="#b49">[49]</ref>, has looked to improve the efficiency. Although these methods face two major challenges. First the projected 3D landmarks may be self-occluded and lose their image patterns, making the features no longer pose invariant. Second, parameters of 3DMM have different priorities during fitting, despite that existing regression based methods treat them equally <ref type="bibr" target="#b9">[10]</ref>. As a result, directly minimizing the parameter error may be sub-optimal, because smaller parameter errors are not necessarily equivalent to smaller alignment errors. This problem will be further discussed in Sec. 3.4. A relevant but distinct task is 3D face reconstruction <ref type="bibr" target="#b50">[50]</ref>, <ref type="bibr" target="#b15">[15]</ref>, <ref type="bibr" target="#b51">[51]</ref>, <ref type="bibr" target="#b52">[52]</ref>, which recovers a 3D face from given 2D landmarks. Interestingly, 2D/3D face alignment results can be mutually transformed, where 3D to 2D is made by sampling landmark vertices and 2D to 3D is made by 3D face reconstruction.</p><p>In this work, we propose a framework to combine three major achievements-3DMM, Cascaded Regression and CNN-to solve the large-pose face alignment problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">3D DENSE FACE ALIGNMENT (3DDFA)</head><p>In this section, we introduce how to combine Cascaded Regression and CNNs to realize 3DDFA. By applying a CNN as the regressor in Eqn. 1, Cascaded CNN can be formulated as:</p><formula xml:id="formula_1">p k+1 = p k + Net k (Fea(I, p k )).<label>(2)</label></formula><p>There are four components in this framework: the regression objective p (Sec. 3.1), the image features Fea (Sec. 3.2), the CNN structure Net (Sec. 3.3) and the cost function to train the framework (Sec. 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">3D Morphable Model</head><p>Blanz et al. <ref type="bibr" target="#b23">[23]</ref> propose the 3D Morphable Model (3DMM) to describe the 3D face space with PCA:</p><formula xml:id="formula_2">S = S + A id ? id + A exp ? exp ,<label>(3)</label></formula><p>where S is a 3D face, S is the mean shape, A id is the principle axes trained on the 3D face scans with neutral expression and ? id is the shape parameter, A exp is the principle axes trained on the offsets between expression scans and neutral scans and ? exp is the expression parameter. In this work, the A id and A exp come from BFM <ref type="bibr" target="#b53">[53]</ref> and FaceWarehouse <ref type="bibr" target="#b54">[54]</ref> respectively. After the 3D face is constructed, it can be projected onto the image plane with scale orthographic projection:</p><formula xml:id="formula_3">V (p) = f * Pr * R * (S + A id ? id + A exp ? exp ) + t 2d ,<label>(4)</label></formula><p>where V (p) is the model construction and projection function, leading to the 2D positions of model vertices, f is the scale factor, Pr is the orthographic projection matrix 1 0 0 0 1 0 , R is the rotation matrix and t 2d is the translation vector. The collection of all the model parameters is p , which is stacked with the input image and sent to the CNN. In the second stream, we get some feature anchors with consistent semantics and conduct Pose Adaptive Convolution (PAC) on them. The outputs of the two streams are merged with an additional fully connected layer to predict the parameter update ?p k .</p><formula xml:id="formula_4">= [f, R, t 2d , ? id , ? exp ] T .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Rotation Formulation</head><p>Face rotation is traditionally formulated with the Euler angles <ref type="bibr" target="#b55">[55]</ref> including pitch, yaw and roll. However, when faces are close to the profile view, there is ambiguity in Euler angles termed gimbal lock <ref type="bibr" target="#b56">[56]</ref>, see <ref type="figure">Fig. 3</ref> as a example. <ref type="figure">Fig. 3</ref>. An example of gimbal lock. We assume the rotation sequence is from pitch to yaw to roll. In the first row, the face is firstly rotated 20 ? around the pitch axis and then 90 ? around the yaw axis, whose Euler angles are <ref type="bibr">[20 ?</ref> , 90 ? , 0 ? ]. In the second row, the face is firstly rotated 90 ? around the yaw axis and then 20 ? around the roll axis, whose Euler angles are [0 ? , 90 ? , 20 ? ]. However the two different Euler angles correspond to the same rotation matrix, generating the profile view of a nodding face.</p><p>The ambiguity in Euler angles will confuse the regressor and affect the fitting performance. Therefore we adopt a four dimensional unit quaternion <ref type="bibr" target="#b56">[56]</ref> [q 0 , q 1 , q 2 , q 3 ] instead of the Euler angles to formulate the rotation. The corresponding rotation matrix is:</p><formula xml:id="formula_5">R = ? ? q 2 0 + q 2 1 ? q 2 2 ? q 2 3 2(q1q2 + q0q3) 2(q1q3 ? q0q2) 2(q1q2 ? q0q3) q 2 0 ? q 2 1 + q 2 2 ? q 2 3 2(q0q1 + q2q3) 2(q0q2 + q1q3) 2(q2q3 ? q0q1) q 2 0 ? q 2 1 ? q 2 2 + q 2 3 ? ?</formula><p>In our implementation, we merge the scale parameter f into [q 0 , q 1 , q 2 , q 3 ] through dividing the quaternion by ? f and do not constrain the quaternion to be unit. As a result, the fitting objective will be p =</p><formula xml:id="formula_6">[q 0 , q 1 , q 2 , q 3 , t 2d , ? id , ? exp ] T .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Feature Design</head><p>As the conjunction point of Cascaded Regression and CNN, the input feature should fulfill the requirements from both frameworks, which can be summarized as the following three aspects: Firstly, the convolvable property requires that the convolution operation on the input feature should make sense. As the CNN input, the feature should be a smooth 2D map reflecting the accuracy of current fitting. Secondly, to enable the cascade manner, the feedback property requires the input feature to depend on the CNN output <ref type="bibr" target="#b8">[9]</ref>. Finally, to guarantee the cascade to converge at the ground truth parameter, the convergence property requires the input feature to be discriminative when the fitting is complete.</p><p>Besides the three requirements, we find that the input features of face alignment can be divided into two categories. The first category is the image-view feature, where the original image is directly sent to the regressor. For example, <ref type="bibr" target="#b12">[12]</ref>, <ref type="bibr" target="#b13">[13]</ref>, <ref type="bibr" target="#b37">[37]</ref> use the input image as the CNN input and <ref type="bibr" target="#b57">[57]</ref>, <ref type="bibr" target="#b58">[58]</ref> stack the image with a landmark response map as the input. These kind of features does not lose any information provided by the image but require the regressor to cover any face appearances. The second category is the model-view feature, where image pixels are rearranged based on the model condition. For example, AAM <ref type="bibr" target="#b6">[7]</ref> warps the face image to the mean shape and SDM <ref type="bibr" target="#b11">[11]</ref> extract SIFT features at landmark locations. This kind of features aligns the face appearance with current fitting, which simplifies the alignment task progressively during optimization. However, they do not cover the pixels beyond the face model, leading to a bad description of context. As such, fitting with model-view features is easily trapped in local minima <ref type="bibr" target="#b36">[36]</ref>. In this paper, we propose a model-view feature called Pose Adaptive Feature (PAF) and a image-view feature called Projected Normalized Coordinate Code (PNCC). We further demonstrate that optimal results can be achieved by combining both features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Pose Adaptive Convolution</head><p>Traditional convolutional layers convolve along a 2D map from pixel to pixel, while we intend to convolve at some semantically consistent locations on the face, called Pose Adaptive Convolution (PAC). Considering human face can be roughly approximated with a cylinder <ref type="bibr" target="#b59">[59]</ref>, we compute the cylindrical coordinate of each vertex and sample 64 ? 64 feature anchors with constant azimuth and height intervals, see <ref type="figure" target="#fig_1">Fig. 4</ref>(a). Given a current model parameter p, we first project 3DMM and sample the feature anchors on the image plane, getting 64 ? 64 ? 2 projected feature anchors V (p) anchor ( <ref type="figure" target="#fig_1">Fig. 4(b)</ref>). Second we crop d ? d (5 in our implementation) patch at each feature anchor and concatenate the patches into a (64 * d) ? (64 * d) patch map according to their cylindrical coordinates ( <ref type="figure" target="#fig_1">Fig. 4(c)</ref>). Finally we conduct d ? d convolutions at the stride of d on the patch map, generating 64 ? 64 response maps ( <ref type="figure" target="#fig_1">Fig. 4(d)</ref>). The convolutional filters are learned with a common convolutional layer, jointly with other CNN layers as described in Sec. 3.3.</p><formula xml:id="formula_7">(a) (b) (c) (d)</formula><p>Note that this process is equivalent to directly conducting d?d convolutions on the projected feature anchors V (p) anchor , which implicitly localize and frontalize the face, making the convolution pose invariant. In order to shrink the features at the occluded region, we consider the vertices whose normal points to minus z as self-occluded and divide the responses at occluded region by two, generating the Pose Adaptive Feature (PAF). We do not eliminate occluded features as <ref type="bibr" target="#b45">[45]</ref> since this information is still valuable prior to perfect fitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Projected Normalized Coordinate Code</head><p>The proposed image-view feature depends on a new type of vertex index, which is introduced as follows: we normalize the 3D mean face to 0 ? 1 in x, y, z axis as Eqn. 5:</p><formula xml:id="formula_8">NCC d = S d ? min(S d ) max(S d ) ? min(S d ) (d = x, y, z),<label>(5)</label></formula><p>where the S is the mean shape of 3DMM. After normalization, the 3D coordinate of each vertex uniquely distributes between [0, 0, 0] and [1, 1, 1], so it can be considered as a vertex index, which we call Normalized Coordinate Code (NCC) ( <ref type="figure" target="#fig_2">Fig. 5(a)</ref>).</p><p>Since NCC has three channels as RGB, we can also show NCC as the face texture. It can be seen as different from the traditional vertex index (from 1 to the number of vertices), NCC is smooth along the face surface.</p><p>In the fitting process, with a model parameter p, we adopt Z-Buffer to render the projected 3D face colored by NCC ( <ref type="figure" target="#fig_2">Fig. 5(b)</ref>) as in Eqn. 6:</p><formula xml:id="formula_9">PNCC = Z-Buffer(V 3d (p), NCC),<label>(6)</label></formula><formula xml:id="formula_10">V 3d (p) = R * (S + A id ? id + A exp ? exp ) + [t 2d , 0] T ,</formula><p>where Z-Buffer(?, ? ) renders the 3D mesh ? colored by ? and V 3d (p) is the projected 3D face. We call the rendered image Projected Normalized Coordinate Code (PNCC). Afterwards, PNCC is stacked with the input image and sent to the CNN. Comparing PAF and PNCC, we can see that PAF is a modelview feature since it implicitly warps the image with feature anchors and PNCC is an image-view feature it sends the original image into a CNN. Regarding the three properties, they fulfill the feedback property since they both depend on p which is updated by the output of the CNN. As for the convolvable property, PAC is the convolution on the continuous locations indicated by the feature anchors and its result PAF is a smooth 2D map. PNCC is also smooth in 2D and the convolution indicates the linear combination of NCCs on a local patch. As for the convergence property, when the CNN detects that in PAF the face is aligned to front and in PNCC each NCC superposes its corresponding image pattern, the cascade will converge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Network Structure</head><p>Unlike existing CNN methods <ref type="bibr" target="#b12">[12]</ref>, <ref type="bibr" target="#b57">[57]</ref> that apply different network structures for different fitting stages, 3DDFA employs a unified network structure across the cascade. In general, at iteration k (k = 0, 1, ..., K), given an initial parameter p k , we construct PNCC and PAF with p k and train a two-stream CNN Net k to conduct fitting. The output features from two streams are merged to predict the parameter update ?p k : ?p k = Net k (PAF(p k , I), PNCC(p k , I)).</p><p>Afterwards, a better intermediate parameter p k+1 = p k + ?p k becomes the input of the next network Net k+1 which has the same structure but different weights with Net k . <ref type="figure" target="#fig_0">Fig. 2</ref> shows the network structure. In the PNCC stream, the input is the The image is processed with the pose adaptive convolution, followed by three pooling layers, three convolutional layers and one fully connected layer. The outputs of the two streams are merged with an additional fully connected layer to predict the 234-dimensional parameter update including 6-dimensional pose parameters [q 0 , q 1 , q 2 , q 3 , t 2dx , t 2dy ], 199-dimensional shape parameters ? id and 29-dimensional expression parameters ? exp .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Cost Function</head><p>Different from the landmark shape model, the parameters in 3DMM contribute to the fitting accuracy with very different impacts, giving parameters different priorities. As a result, regression-based methods suffer from the inequivalence between parameter error and alignment error <ref type="bibr" target="#b9">[10]</ref>. In this section, we will discuss this problem with two baseline cost functions and propose our own ways to model the parameter priority.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Parameter Distance Cost (PDC)</head><p>Take the first iteration as an example. The purpose of the CNN is to predict the parameter update ?p so as to move the initial parameter p 0 closer to the ground truth p g . Intuitively, we can minimize the distance between the ground truth and the current parameter with the Parameter Distance Cost (PDC):</p><formula xml:id="formula_12">E pdc = ?p ? (p g ? p 0 ) 2 .<label>(8)</label></formula><p>PDC has been traditionally used in regression based model fitting <ref type="bibr" target="#b32">[32]</ref>, <ref type="bibr" target="#b33">[33]</ref>, <ref type="bibr" target="#b60">[60]</ref>. However, different dimension in p has different influences on the resultant 3D face. For example, with the same deviation, the yaw angle will bring a larger alignment error than a shape parameter, while PDC optimizes them equally, leading to sub-optimal results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Vertex Distance Cost (VDC)</head><p>Since 3DDFA aims to morph the 3DMM to the ground truth 3D face, we can optimize ?p by minimizing the vertex distances between the current and the ground truth 3D face:</p><formula xml:id="formula_13">E vdc = V (p 0 + ?p) ? V (p g ) 2 ,<label>(9)</label></formula><p>where V (?) is the face construction and projection as Eqn. 4. We call this cost Vertex Distance Cost (VDC). Compared with PDC, VDC better models the fitting error by explicitly considering parameter semantics. However, VDC is not convex itself, the optimization is not guaranteed to converge to the ground truth parameter p g . Furthermore, we observe that VDC exhibits pathological curvature <ref type="bibr" target="#b61">[61]</ref> since the directions of pose parameters always exhibit much higher curvatures than the PCA coefficients. As a result, optimizing VDC with gradient descent converges very slowly due to the "zig-zagging" problem. Second-order optimizations are preferred to handle the pathological curvature but they are expensive and hard to be implemented on GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Weighted Parameter Distance Cost (WPDC)</head><p>In our previous work <ref type="bibr" target="#b24">[24]</ref>, we propose a cost function named Weighted Parameter Distance Cost (WPDC). The motivation is explicitly weighting parameter error by its importance:</p><formula xml:id="formula_14">E wpdc = (?p ? (p g ? p 0 )) T diag(w)(?p ? (p g ? p 0 )) (10)</formula><p>where w is the parameter importance vector, which is defined as follows:</p><formula xml:id="formula_15">w = (w 1 , w 2 , ..., w i , ..., w p ), w i = V (p de,i ) ? V (p g ) /Z, p de,i = (p g 1 , ..., p g i?1 ,(p 0 + ?p) i , p g i+1 , ..., p g p ),<label>(11)</label></formula><p>where p is the number of parameter, p de,i is the i-degraded parameter whose ith element comes from the predicted parameter (p 0 + ?p) and the others come from the ground truth parameter p g , Z is a regular term which is the maximum of w. V (p de (i)) ? V (p g ) models the alignment error brought by miss-predicting the ith model parameter, which is indicative of its importance. In the training process, the CNN firstly concentrates on the parameters with larger V (p de (i)) ? V (p g ) such as rotation and translation. As p de (i) is closer to p g , the weights of these parameters begin to shrink and the CNN will optimize less important parameters while simultaneously keeping the high-priority parameters sufficiently good. Compared with VDC, WPDC makes sure the parameter is optimized toward p g and it remedies the pathological curvature issue at the same time. However, the weight in WPDC only models the "importance" but not the "priority". In fact, parameters become important sequentially. Take <ref type="figure" target="#fig_5">Fig. 6</ref> as an example, when WPDC evaluates a face image with open mouth and large pose, it will assign both expression and rotation high weights. We can observe that attempting to estimate expression makes little sense before the pose is accurate enough, see <ref type="figure" target="#fig_5">Fig. 6(b)</ref>. One step further, if we force the CNN to only concentrate on pose parameters, we obtain a better fitting result, see <ref type="figure" target="#fig_5">Fig. 6(c)</ref>. Consequently for this sample, even though pose and expression are both important, pose has higher priority than expression, but WPDC misses that.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.4">Optimized Weighted Parameter Distance Cost (OW-PDC)</head><p>We can observe that "priority" is a between-parameter relationship which can only be modeled by treating all the parameters as a whole rather than evaluating them separately as WPDC. In this paper, we propose to find the best weights through optimization:</p><formula xml:id="formula_16">E owpdc = (?p ? (p g ? p 0 )) T diag(w * )(?p ? (p g ? p 0 )), w * = arg min w V p c + diag(w) * (p g ? p c ) ? V p g 2 +? diag(w) * (p g ? p c ) 2 ,<label>(12)</label></formula><formula xml:id="formula_17">s.t. 0 w 1,</formula><p>where w is the weights vector, ?p is the CNN output, p c = p 0 + ?p is the current predicted parameter, 0 and 1 are the zeros and ones vectors respectively and is the element-wise less than. In Eqn. 12, by adding a weighted parameter update diag(w)(p g ? p c ) to the current parameter p c , we hope the new face is closer to the ground truth face with limited updating. Note that diag(w) * (p g ? p c ) 2 is the square sum of the gradient of OWPDC, which models how much CNN weights need to be tuned to predict each parameter. We use this penalty term to choose the parameters which are most beneficial to the fitting and are easiest to learn. The range of w is constrained to be [0, 1] to make sure the parameter is optimized to p g . Obviously, when the ? is set to 0, there will be a trivial solution that w = 1 and OWPDC will deteriorate to PDC.</p><p>In the training process, directly optimizing Eqn. 12 for each sample is computationally intensive. We expand V (p c + diag(w)(p g ? p c )) at p g with the Taylor formula and let ?p c = p g ? p c , Eqn. 12 will be:</p><formula xml:id="formula_18">V (p g ) * diag(w ? 1) * ?p c 2 + ? diag(w) * ?p c 2 ,<label>(13)</label></formula><p>where V (p g ) is the Jacobian. Expanding Eqn. 13 and removing the constant terms, we get:</p><formula xml:id="formula_19">w T diag(?p c )V (p g ) T V (p g )diag(?p c ) w ?2 * 1 T diag(?p c )V (p g ) T V (p g )diag(?p c ) w +? * w T diag(?p c . * ?p c )w,<label>(14)</label></formula><p>where . * is the element-wise multiplication. Let H = V (p g )diag(?p c ) which is a 2n ? p matrix where n is the number of vertices and p is the number of parameters, the optimization will be:</p><formula xml:id="formula_20">arg min w w T * (H T * H + ? * diag(?p c . * ?p c )) * w +2 * 1 T * H T * H * w, s.t. 0 w 1,<label>(15)</label></formula><p>which is a standard quadratic programming problem with the unique solution. The most consuming component in Eqn. 15 is the computation of V (p g ). Fortunately, p g is constant during training and V (p g ) can be pre-computed offline. As a result, the computation of w * can be reduced to a p-dimensional quadratic programming which can be efficiently solved. The only parameter in OWPDC is the ?. It directly determines which parameter is valid during training. We set ? = 0.17 * V (p c ) ? V (p g ) 2 in our implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">FACE PROFILING</head><p>All the regression based methods rely on training data, especially for CNNs which have thousands of parameters to learn. Therefore, massive labelled faces in large poses are crucial for 3DDFA. However, few of the released face alignment databases contain large-pose samples <ref type="bibr" target="#b18">[18]</ref>, <ref type="bibr" target="#b19">[19]</ref>, <ref type="bibr" target="#b20">[20]</ref>, <ref type="bibr" target="#b21">[21]</ref> since labelling standardized landmarks on them is very challenging. In this work, we demonstrate that profile faces can be well synthesized from existing training samples with the help of 3D information. Inspired by the recent achievements in face frontalization <ref type="bibr" target="#b15">[15]</ref>, <ref type="bibr" target="#b62">[62]</ref> which generates the frontal view of faces, we propose to invert this process to synthesize the profile view of faces from medium-pose samples, which is called face profiling. Different from the face synthesizing in recognition <ref type="bibr" target="#b63">[63]</ref>, face profiling is not required to keep the identity information but to make the synthesizing results realistic. However, current synthesizing methods do not keep the external face region <ref type="bibr" target="#b64">[64]</ref>, <ref type="bibr" target="#b63">[63]</ref>, which contains important context information for face alignment. In this section, we elucidate a novel face synthesizing method to generate the profile views of face image with out-of-plane rotation, providing abundant realistic training samples for 3DDFA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">3D Image Meshing</head><p>The depth estimation of a face image can be conducted on the face region and the external region respectively, with different requirements of accuracy. On the face region, we fit a 3DMM through the Multi-Features Framework (MFF) <ref type="bibr" target="#b44">[44]</ref> (see <ref type="figure" target="#fig_6">Fig. 7(b)</ref>).</p><p>With the ground truth landmarks as a solid constraint throughout the fitting process, MFF can always get accurate results. Few difficult samples can be easily adjusted manually. On the external region, we follow the 3D meshing method proposed by Zhu et al. <ref type="bibr" target="#b15">[15]</ref> to mark some anchors beyond the face region and simulate their depth, see <ref type="figure" target="#fig_6">Fig. 7(c)</ref>. Afterwards the whole image can be tuned into a 3D object through triangulation (see <ref type="figure" target="#fig_6">Fig. 7(c)7(d)</ref>). </p><formula xml:id="formula_21">(a) (b) (c) (d)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">3D Image Rotation</head><p>The simulated depth information enables the 2D image to rotate out of plane to generate the appearances in larger poses. However, as shown in <ref type="figure" target="#fig_7">Fig. 8(b)</ref>, the 3D rotation squeezes the external face region and loses the background. As a result, we need to further adjust the anchors to keep the background relatively unchanged and preserve the smoothness simultaneously. Inspired by our previous work <ref type="bibr" target="#b15">[15]</ref>, we propose to adjust background anchors by solving an equation list about their relative positions. In the source image as shown in <ref type="figure" target="#fig_7">Fig. 8(a)</ref>, the triangulated anchors build up a graph where the anchors are the vertices and the mesh lines are the edges. In the graph, each edge represents an anchor-to-anchor relationship:</p><p>x a src ? x b src = ?x src , y a src ? y b src = ?y src , <ref type="bibr" target="#b16">(16)</ref> where (x a src , y a src ) and (x b src , y b src ) are two connecting anchors, ?x src and ?y src are the spatial offsets in x, y axes, which should be preserved in synthesizing. After profiling, we keep the face contour anchors (the magenta points in <ref type="figure" target="#fig_7">Fig. 8(b)</ref>) consistent and predicting other anchors with the unchanged anchor offsets:</p><p>x a adj ? x b adj = ?x src , y a adj ? y b adj = ?y src , <ref type="formula" target="#formula_0">(17)</ref> Specifically, if a is a face contour anchor, we set (x a adj , y a adj ) to the positions after profiling (x a pro , y a pro ), otherwise (x a adj , y a adj ) are two unknowns need to be solved. By collecting Eqn. 17 for each graph edge, we form an equation list whose least square solution is the adjusted anchors (as seen in <ref type="figure" target="#fig_7">Fig. 8(c)</ref>).</p><p>In this work, we enlarge the yaw angle of image at the step of 5 ? until 90 ? , see <ref type="figure" target="#fig_8">Fig. 9</ref>. Different from face frontalization, with larger rotation angles the self-occluded region can only be expanded. As a result, we avoid the troubling invisible region filling which may produce large artifacts <ref type="bibr" target="#b15">[15]</ref>. Through face profiling, we not only obtain face samples in large poses but also augment the dataset to a large scale. </p><formula xml:id="formula_22">(a) (b) (c) (d)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">IMPLEMENTATION</head><p>Training Strategy: With a huge number of parameters, the CNN tends to overfit the training set and the deeper cascade might learn nothing with overfitted samples. Therefore we regenerate p k at each iteration using a nearest neighbor strategy. By observing that the fitting error highly depends on the ground truth face posture (FP), we perturb a training sample based on a set of similar-FP validation samples. In this paper, we define the face posture as the rotated 3D face without scaling and translation:</p><formula xml:id="formula_23">FP = R g * (S + A id ? g id + A exp ? g exp ),<label>(18)</label></formula><p>where R g is constructed from the normalized ground truth quaternion, ? g id and ? g exp are the ground truth shape and expression parameters respectively. Before training, we select two folds of samples as the validation set and for each training sample we construct a validation subset {v 1 , ..., v m } whose members share similar FP with the training sample. At iteration k, we regenerate the initial parameter by:</p><formula xml:id="formula_24">p k = p g ? (p g vi ? p k vi ),<label>(19)</label></formula><p>where p k and p g are the initial and ground truth parameter of a training sample, p k vi and p g vi come from a validation sample v i which is randomly chosen from the corresponding validation subset. Note that v i is never used in training. Initialization: Besides the face profiling, we also augment the training data (10 times) by randomly in-plane rotating images (up to 30 degrees) and perturbing bounding boxes. Specifically, the bounding boxes are randomly perturbed by a multivariate normal distribution whose mean vector and covariance matrix are obtained by the difference between ground truth bounding boxes and automated detected face rectangles using FTF <ref type="bibr" target="#b65">[65]</ref>. This augmentation is quite effective in improving the robustness of the model. During testing, to get p 0 we first set ? id , ? exp to zero and the quaternion to [1, 0, 0, 0], getting a frontal 3D mean face. Then we calculate t 2d by moving the mean point of the 3D face to the center of the bounding box. Finally, we scale the 3D face, which is equivalent to scaling the quaternion, to make the bounding box enclose the whole face region. Running Time: During testing, 3DDFA takes 21.3ms for each iteration, among which PAF and PNCC take 11.6ms and 6.8ms respectively on 3.40GHZ CPU and CNN forward propagation takes 2.9ms on GTX TITAN X GPU. In our implementation, 3DDFA has three iterations and takes 63.9ms (15.65fps) for each sample. Note that the efficiency is mainly limited by the input features, which can be further improved by GPU implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Datasets</head><p>Three databases are used in our experiments, i.e. 300W-LP, AFLW <ref type="bibr" target="#b22">[22]</ref> and a specifically constructed AFLW2000-3D.</p><p>300W-LP: 300W <ref type="bibr" target="#b66">[66]</ref> standardises multiple face alignment databases with 68 landmarks, including AFW <ref type="bibr" target="#b18">[18]</ref>, LFPW <ref type="bibr" target="#b67">[67]</ref>, HELEN <ref type="bibr" target="#b68">[68]</ref>, IBUG <ref type="bibr" target="#b66">[66]</ref> and XM2VTS <ref type="bibr" target="#b69">[69]</ref>. With 300W, we adopt the proposed face profiling to generate 61, 225 samples across large poses (1, 786 from IBUG, 5, 207 from AFW, 16, 556 from LFPW and 37, 676 from HELEN, XM2VTS is not used), which is further flipped to 122, 450 samples. We call the synthesized database as 300W Across Large Poses (300W-LP).</p><p>AFLW: AFLW <ref type="bibr" target="#b22">[22]</ref> contains 21, 080 in-the-wild faces with large pose variations (yaw from ?90 ? to 90 ? ). Each image is annotated up to 21 visible landmarks. The database is very suitable for evaluating face alignment performance in large poses.</p><p>AFLW2000-3D: Evaluating 3D face alignment in the wild is difficult due to the lack of pairs of 2D image and 3D scan. Considering the recent achievements in 3D face reconstruction which can construct a 3D face from 2D landmarks <ref type="bibr" target="#b50">[50]</ref>, <ref type="bibr" target="#b15">[15]</ref>, we assume that a 3D model can be accurately fitted if sufficient 2D landmarks are provided. Therefore the evaluation can be degraded to 2D landmark evaluation which also makes it possible to compare 3DDFA with other 2D face alignment methods. While AFLW is not suitable for this task since only visible landmarks may lead to serious ambiguity in 3D shape, as reflected by the fake good alignment phenomenon in <ref type="figure" target="#fig_9">Fig. 10</ref>. In this work, we construct a database called AFLW2000-3D for 3D face alignment evaluation, which contains the ground truth 3D faces and the corresponding 68 landmarks of the first 2,000 AFLW samples. More details about the construction of AFLW2000-3D are given in supplemental material.</p><p>In all the following experiments, we follow <ref type="bibr" target="#b36">[36]</ref> and regard the 300W-LP samples synthesized from the training part of LFPW, HELEN and the whole AFW as the training set (101, 144 images in total). The testing are conducted on three databases: the 300W testing part for general face alignment, the AFLW for large-pose face alignment and the AFLW2000-3D for 3D face alignment. The alignment accuracy is evaluated by the Normalized Mean Error (NME).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Performance with Different Input Features</head><p>As described in Sec. 3.2, the input features of face alignment methods can be divided into two categories, the image-view feature and the model-view feature, which correspond to PNCC and PAF in this paper. To test their effectiveness respectively and evaluate their complementarity, we divide the network in <ref type="figure" target="#fig_0">Fig. 2</ref> into PNCC stream and PAF stream by removing the last fully connected layer and regress the 256-dimensional output of each stream to the parameter update respectively. The combined twostream network is also reported to demonstrate the improvements. As shown in <ref type="figure" target="#fig_10">Fig. 11</ref>, PNCC performs better than PAF when used individually and the improvement is enlarged as the pose becomes larger. Besides, PNCC and PAF achieve better performance when combined, which may infer a complementary relationship. This complementary relationship might be because PNCC covers the whole image and contains rich context information, enabling it to fit large scale facial components like the face contour. While PAF is more adept at fitting facial features due to the implicit frontalizion, which can well assist PNCC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Analysis of Feature Properties</head><p>In Sec. 3.2, we introduce three requirements of the input feature: feedback, convolvable and convergence. Among them, the benefits from convolvable and convergence may not be obvious and are further evaluated here. Corresponding to PNCC and PAF, we propose two alternative input features which miss these two properties respectively. Convovable Property: As the alternative to PNCC, we propose the Projected Index (PIndex) which renders the projected 3D face with the 1-channel vertex index (from 1 to 53, 490 in BFM <ref type="bibr" target="#b53">[53]</ref>) rather than the 3-channel NCC, see <ref type="figure" target="#fig_0">Fig. 12</ref>. Note that even though PIndex provides the semantic meaning of each pixel, it is not smooth and the convolution of vertex indexes on a local patch is hard to be interpreted by the CNN. As a result, PIndex violates the convolvable requirement. Using the PNCC stream as the network, we adopt PNCC and PIndex as the input feature respectively. As shown in <ref type="table" target="#tab_1">Table 1</ref>, by violating the convolvable requirement, the performance drops since the learning task becomes more difficult. Convergence Property: As the alternative to PAF, we propose the Texture Mapping (TM) <ref type="bibr" target="#b50">[50]</ref> which rearranges the pixels on the projected feature anchors to a 64 ? 64 image, see <ref type="figure" target="#fig_12">Fig. 13</ref>. Compared with PAF, the main drawback of TM is the weak description beyond the model region. As shown in <ref type="figure" target="#fig_12">Fig. 13(a)</ref> and <ref type="figure" target="#fig_12">Fig. 13(b)</ref>, TM cannot discriminate whether the projected 3D model occludes the face in the image completely <ref type="bibr" target="#b70">[70]</ref>. As a result, whether the fitting is complete is not discriminative for TM, which means the convergence requirement is not fulfilled. On the contrary, PAF can better describe the context information with the convolution on the face contour vertices. As shown in <ref type="figure" target="#fig_12">Fig. 13</ref>(c) and <ref type="figure" target="#fig_12">Fig. 13(d)</ref>, PAF shows different appearances before and after the face contour is fitted. <ref type="table" target="#tab_1">Table 1</ref> shows the results of PAF and TM which use the PAF stream as the network. We can see that PAF outperforms TM by over 6% which verifies the effectiveness of the convergence property.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Analysis of Cost Function</head><p>Performance with Different Cost: We demonstrate the errors along the cascade with different cost functions including PDC, VDC, WPDC and OWPDC. <ref type="figure" target="#fig_1">Fig. 14</ref>   converges to an unsatisfied result. VDC is better than PDC, but the pathological curvature problem makes it only concentrate on a small set of parameters and limits its performance. WPDC models the importance of each parameter and achieves a better result. Finally OWPDC further models the parameter priority, leading to faster convergence and the best performance. Weights of OWPDC: Since the weights of OWPDC reflect the priority of parameters, how the priority changes along the training process is also an interesting point to investigate. In this experiment, for each mini-batch during training, we record the mean weights of the mini-batch and plot the mini-batch weight in <ref type="figure" target="#fig_2">Fig. 15</ref>. It can be seen that at beginning, the pose parameters (rotation and translation) show much higher priority than morphing parameters (shape and expression). As the training proceeds with error reducing, the pose weights begin to decrease and the CNN deals out its concentration to morphing parameters. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Error Reduction in Cascade</head><p>To analyze the overfitting problem in Cascaded Regression and evaluate the effectiveness of initialization regeneration, we divide 300W-LP into 97, 967 samples for training and 24, 483 samples for testing, without identity overlapping. <ref type="figure" target="#fig_5">Fig. 16</ref> shows the training and testing errors at each iteration, without and with initialization regeneration. As observed, in traditional Cascaded Regression the (a) (b) <ref type="figure" target="#fig_5">Fig. 16</ref>. The training and testing errors without (a) and with (b) initialization regeneration.</p><p>training and testing errors converge fast after two iterations. While with initialization regeneration, the training error is updated at the beginning of each iteration and the testing error continues descending. Considering both effectiveness and efficiency we choose three iterations in 3DDFA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Comparison Experiments</head><p>In this paper, we evaluate the performance of 3DDFA on three different tasks: the large-pose face alignment on AFLW, the 3D face alignment on AFLW2000-3D and the medium-pose face alignment on 300W.  <ref type="bibr" target="#b45">[45]</ref>. The face size is defined as the ? width * height of the bounding box (the rectangle hull of all the 68 landmarks). Besides, we report the standard deviation of NMEs across testing subsets to measure the pose robustness. During training, we use the projected 3D landmarks as the ground truth to train 2D methods. For convenient comparison, the ground truth bounding boxes are used for initialization. Methods: Since little experiment has been conducted on the whole AFLW, we choose some baselines with released training codes, <ref type="figure" target="#fig_6">Fig. 17</ref>. Results of SDM, DCN and our approach on AFLW.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 2</head><p>The NME(%) of face alignment results on AFLW and AFLW2000-3D with the first and the second best results highlighted.  <ref type="table">Table 2</ref> shows the comparison results and <ref type="figure" target="#fig_7">Fig. 18</ref> shows the corresponding CED curves. Each 2D method is trained on 300W and 300W-LP respectively to demonstrate the boost from face profiling. For DCN, 3DDFA and TSPM which depend on large scales of data or large-pose data, we only evaluate the models trained on 300W-LP. Given that RMFA only releases the testing code, we just evaluate it with the provided model. Besides, in large poses TSPM model only detects 10 of the 21 landmarks, we only evaluate the error of the 10 points for TSPM.</p><p>Results: Firstly, the results indicate that all the methods benefit substantially from face profiling when dealing with large poses. The improvements in [60 ? , 90 ? ] exceed 40% for all the methods. This is especially impressive since the alignment models are trained on the synthesized data and tested on real samples, which well demonstrates the fidelity of face profiling. Secondly, in near frontal view, most of methods show very similar performance as shown in <ref type="figure" target="#fig_7">Fig 18(a)</ref>. As the yaw angle increases in <ref type="figure" target="#fig_7">Fig 18(b)</ref> and <ref type="figure" target="#fig_7">Fig 18(c)</ref>, most of 2D methods begin to degrade but 3DDFA could still maintain its performance. Finally, 3DDFA reaches the state of the art above all the 2D methods especially beyond medium poses. The minimum standard deviation also demonstrates its robustness to pose variations.</p><p>In <ref type="figure" target="#fig_6">Fig. 17</ref>, we demonstrate some alignment results of 3DDFA and representative 2D methods. Besides, <ref type="figure" target="#fig_0">Fig. 20</ref> show some typical failure cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6.2">3D Face Alignment in AFLW2000-3D</head><p>As described in Section 6.1, 3D face alignment evaluation can be degraded to full-landmarks evaluation considering both visible and invisible ones. Using AFLW2000-3D as the testing set, this experiment follows the same protocol as AFLW, except all the 68 landmarks are used for evaluation.  <ref type="table">Table 2</ref> and the CED curves are ploted in <ref type="figure" target="#fig_8">Fig. 19</ref>. We do not report the performance of TSPM models since they do not detect invisible landmarks.</p><p>Compared with the results in AFLW, we can see that the standard deviation is dramatically increased, meaning that it is more difficult to keep pose robustness when considering all the landmarks. Besides, the improvement of 3DDFA over the best 2D method DCN is increased from 26.49% in AFLW to 30.33% in AFLW2000-3D, which demonstrates the superiority of 3DDFA in 3D face alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6.3">Medium Pose Face Alignment</head><p>As a face alignment approach to deal with full pose range, 3DDFA also shows competitive performance on the mediumpose 300W database, using the common protocol in <ref type="bibr" target="#b36">[36]</ref>. The alignment accuracy is evaluated by the standard landmark mean error normalized by the inter-pupil distance (NME). For 3DDFA, we sample the 68 landmarks from the fitted 3D face and refine them with SDM to reduce the labelling bias. <ref type="table" target="#tab_6">Table 3</ref> shows that even in medium poses 3DDFA performs competitively, especially on the challenging set. We further plot a mean CED curve (d) with a subset of 12,081 samples whose absolute yaw angles within each yaw iterval are 1/3 each. Only the top 6 methods are shown.  The alignment performance can be greatly affected by the bounding boxes used for initialization. In this experiment, we initialize alignment methods with detected bounding boxes by FTF face detector <ref type="bibr" target="#b65">[65]</ref> rather than the ground truth bounding boxes. We drop the bad boxes whose IOU with ground truth bounding boxes are less than 0.6 and generate the bounding boxes of undetected faces by random perturbation used in training. <ref type="table" target="#tab_7">Table 4</ref> shows the comparison results with the best two competitors DCN and SDM. Firstly, it can be seen that our method still outperforms </p><formula xml:id="formula_25">(a) 0 ? to 30 ? (b) 30 ? to 60 ? (c) 60 ? to 90 ? (d) Mean</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>Most of face alignment methods tend to fail in profile view since the self-occluded landmarks cannot be detected. Instead of the traditional landmark detection framework, this paper fits a dense 3D Morphable Model to achieve pose-free face alignment. By proposing two input features of PNCC and PAF, we cascade a couple of CNNs as a strong regressor to estimate model parameters. A novel OWPDC cost function is also proposed to consider the priority of parameters. To provide abundant samples for training, we propose a face profiling method to synthesize face appearances in profile views. Experiments show the state-of-theart performance on AFLW, AFLW2000-3D and 300W.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">ACKNOWLEDGMENTS</head><p>This work was supported by the National Key Research and Development Plan (Grant No.2016YFC0801002), the Chinese National</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>An overview of the two-stream network in 3DDFA. With an intermediate parameter p k , in the first stream we construct a novel Projected Normalized Coordinate Code (PNCC)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>Pose Adaptive Convolution (PAC): (a) The 64?64 feature anchors on the 3D face model. (b) The projected feature anchors V (p) anchor (the blue/red ones indicate visible/invisible anchors). (c) The feature patch map concatenated by the patches cropped at V (p) anchor . (d) Conducting convolution, whose stride and filter size are the same with the patch size, on the feature patch map and shrinking the responses at invisible points, leading to the Pose Adaptive Feature (PAF).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>The Normalized Coordinate Code (NCC) and the Projected Normalized Coordinate Code (PNCC). (a) The normalized mean face, which is also demonstrated with NCC as its texture (NCCx = R, NCCy = G, NCCz = B). (b) The generation of PNCC, the projected 3D face is rendered by Z-Buffer with NCC as its colormap.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>200 ? 200 ? 3 color image stacked by the 200 ? 200 ? 3 PNCC. The network contains five convolutional layers, four pooling layers and one fully connected layer. In the PAF stream, the input is the 200 ? 200 ? 3 color image and 64 ? 64 feature anchors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>(a) An open-mouth face in near-profile view. (b) The fitting result of WPDC in the first iteration. (c) The fitting result when the CNN is restricted to only regress the 6-dimensional pose parameters. Errors are measured by Normalized Mean Error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>3D Image Meshing. (a) The input image. (b) The fitted 3D face through MFF. (c) The depth image from 3D meshing. (d) A different view of the depth image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>The face profiling and anchor adjustment process. (a) The source image. (b) The profiled face with out of plane rotation. It can be seen that the face locates on the hollow since the background is squeezed. (c) The synthesized image after anchor adjustment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>2D and 3D view of face profiling. (a) The original yaw angle yaw 0 . (b) yaw 0 + 20 ? . (c) yaw 0 + 30 ? . (d) yaw 0 + 40 ? .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 .</head><label>10</label><figDesc>Fake good alignment in AFLW. For each sample, the first shows the visible 21 landmarks and the second shows all the 68 landmarks. The Normalized Mean Error (NME) reflects their accuracy. It can be seen that only evaluating visible landmarks cannot well reflect the accuracy of 3D fitting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 .</head><label>11</label><figDesc>The Normalized Mean Error (%) with different input features, evaluated on AFLW2000-3D with different yaw intervals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 12 .</head><label>12</label><figDesc>The convolvable property of PNCC and PIndex: (a) A local patch of PIndex. The values can only be smooth in the indexing direction (vertical in this figure). (b) A local patch of PNCC. Values are smooth in 2D along each channel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 13 .</head><label>13</label><figDesc>The convergence property of TM and PAF. The first row: the mapped textures from a sub-fitted (a) and a fitted (b) sample. They show very similar appearances. The second row: the feature patch maps of PAF from a sub-fitted (c) and a fitted (d) sample. The convolution on the face contour vertices (the red grid) cover the pixels beyond the face region, enable PAF to exhibit discriminative appearance when the face contour is fitted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 14 .</head><label>14</label><figDesc>The testing errors with different cost functions, evaluated on AFLW2000-3D. The value in the bracket indicates the NME after the third iteration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 15 .</head><label>15</label><figDesc>The mean weights of each mini-batch along the training process in the first iteration. The weights are normalized by w/ w for better representation. The curves indicate the max value among the quaternion (rotation curve), x and y translation (translation curve), PCA shape (shape curve) and expression parameters (expression curve).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>(a) 0 ?Fig. 18 .</head><label>018</label><figDesc>to 30 ? (b) 30 ? to 60 ? (c) 60 ? to 90 ? (d) MeanComparisons of cumulative errors distribution (CED) curves on AFLW with yaw distributing at: (a) [0 ? , 30 ? ], (b) [30 ? , 60 ? ] and (c) [60 ? , 90 ? ].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 19 .Fig. 20 .</head><label>1920</label><figDesc>Comparisons of cumulative errors distribution (CED) curves on AFLW2000-3D with yaw distributing at: (a) [0 ? , 30 ? ], (b) [30 ? , 60 ? ] and (c) [60 ? , 90 ? ]. We further plot a mean CED curve (d) with a subset of 696 samples whose absolute yaw angles within each yaw iterval are 1/3 each. Only the top 6 methods are shown. Typical failure reasons of 3DDFA, including (a) complicated shadow and occlusion, (b) extreme pose and expression, (c) extreme illumination and (d) limited shape variations of 3DMM on nose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 The</head><label>1</label><figDesc></figDesc><table><row><cell cols="5">NME(%) of PAF, PNCC and their corresponding alternative</cell></row><row><cell cols="5">features, evaluated on AFLW2000-3D with different yaw interval.</cell></row><row><cell>Feature</cell><cell>[0, 30]</cell><cell>[30, 60]</cell><cell>[60, 90]</cell><cell>Mean</cell></row><row><cell>PIndex</cell><cell>3.33</cell><cell>3.95</cell><cell>5.60</cell><cell>4.29</cell></row><row><cell>PNCC</cell><cell>3.14</cell><cell>3.81</cell><cell>5.35</cell><cell>4.10</cell></row><row><cell>TM</cell><cell>3.38</cell><cell>4.48</cell><cell>6.76</cell><cell>4.87</cell></row><row><cell>PAF</cell><cell>3.20</cell><cell>4.12</cell><cell>6.36</cell><cell>4.56</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>demonstrates the testing error at each iteration. All the networks are trained until convergence. It is shown that PDC cannot well model the fitting error and</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>6.6.1 Large Pose Face Alignment on AFLW Protocol: In this experiment, we regard the whole AFLW as the testing set and divide it into three subsets according to their absolute yaw angles: [0 ? , 30 ? ], [30 ? , 60 ? ], and [60 ? , 90 ? ] with 11, 596, 5, 457 and 4, 027 samples respectively. The alignment accuracy is evaluated by the Normalized Mean Error (NME), which is the average of landmarks error normalised by face size</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>The brackets show the training sets. RMFA<ref type="bibr" target="#b72">[72]</ref> and TSPM<ref type="bibr" target="#b18">[18]</ref>. Among them RCPR is an occlusion-robust method with the potential to deal with selfocclusion and we train it with landmark visibility computed by 3D information<ref type="bibr" target="#b62">[62]</ref>. ESR, SDM, LBF and CFSS are popular Cascaded Regression based methods, among which SDM<ref type="bibr" target="#b71">[71]</ref> is the winner of ICCV2013 300W face alignment challenge. MDM is a deep learning base method which adopts CNNs to extract image features. TSPM and RMFA adopt the multi-view framework which can deal with large poses. Besides the state-of-the-art methods, we introduce a Deep Convolutional Network (DCN) as a CNN based baseline. DCN directly regresses raw image pixels to the landmark positions with a CNN. The CNN has five convolutional layers, four pooling layers and two fully connected layers (the same as the PNCC stream) to estimate 68 landmarks from a 200 ? 200 ? 3 input image. Besides, we also compare with our previous work<ref type="bibr" target="#b24">[24]</ref> but we do not adopt the SDM based landmark refinement here.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">AFLW Dataset (21 pts)</cell><cell></cell><cell></cell><cell cols="3">AFLW2000-3D Dataset (68 pts)</cell><cell></cell></row><row><cell>Method</cell><cell>[0, 30]</cell><cell>[30, 60]</cell><cell>[60, 90]</cell><cell>Mean</cell><cell>Std</cell><cell>[0, 30]</cell><cell>[30, 60]</cell><cell>[60, 90]</cell><cell>Mean</cell><cell>Std</cell></row><row><cell>LBF(300W)</cell><cell>7.17</cell><cell>17.54</cell><cell>28.45</cell><cell>17.72</cell><cell>10.64</cell><cell>6.17</cell><cell>16.48</cell><cell>25.90</cell><cell>16.19</cell><cell>9.87</cell></row><row><cell>LBF(300W-LP)</cell><cell>8.43</cell><cell>9.54</cell><cell>13.06</cell><cell>10.34</cell><cell>2.42</cell><cell>8.15</cell><cell>9.49</cell><cell>12.91</cell><cell>10.19</cell><cell>2.45</cell></row><row><cell>ESR(300W)</cell><cell>5.58</cell><cell>10.62</cell><cell>20.02</cell><cell>12.07</cell><cell>7.33</cell><cell>4.38</cell><cell>10.47</cell><cell>20.31</cell><cell>11.72</cell><cell>8.04</cell></row><row><cell>ESR(300W-LP)</cell><cell>5.66</cell><cell>7.12</cell><cell>11.94</cell><cell>8.24</cell><cell>3.29</cell><cell>4.60</cell><cell>6.70</cell><cell>12.67</cell><cell>7.99</cell><cell>4.19</cell></row><row><cell>CFSS(300W)</cell><cell>4.68</cell><cell>9.78</cell><cell>23.07</cell><cell>12.51</cell><cell>9.49</cell><cell>3.44</cell><cell>10.90</cell><cell>24.72</cell><cell>13.02</cell><cell>10.08</cell></row><row><cell>CFSS(300W-LP)</cell><cell>5.42</cell><cell>6.73</cell><cell>11.48</cell><cell>7.88</cell><cell>3.19</cell><cell>4.77</cell><cell>6.71</cell><cell>11.79</cell><cell>7.76</cell><cell>3.63</cell></row><row><cell>RCPR(300W)</cell><cell>5.40</cell><cell>9.80</cell><cell>20.61</cell><cell>11.94</cell><cell>7.83</cell><cell>4.16</cell><cell>9.88</cell><cell>22.58</cell><cell>12.21</cell><cell>9.43</cell></row><row><cell>RCPR(300W-LP)</cell><cell>5.43</cell><cell>6.58</cell><cell>11.53</cell><cell>7.85</cell><cell>3.24</cell><cell>4.26</cell><cell>5.96</cell><cell>13.18</cell><cell>7.80</cell><cell>4.74</cell></row><row><cell>MDM(300W)</cell><cell>5.14</cell><cell>10.95</cell><cell>24.11</cell><cell>13.40</cell><cell>9.72</cell><cell>4.64</cell><cell>10.35</cell><cell>24.21</cell><cell>13.07</cell><cell>10.07</cell></row><row><cell>MDM(300W-LP)</cell><cell>5.57</cell><cell>5.99</cell><cell>9.96</cell><cell>7.17</cell><cell>2.43</cell><cell>4.85</cell><cell>5.92</cell><cell>8.47</cell><cell>6.41</cell><cell>1.86</cell></row><row><cell>SDM(300W)</cell><cell>4.67</cell><cell>6.78</cell><cell>16.13</cell><cell>9.19</cell><cell>6.10</cell><cell>3.56</cell><cell>7.08</cell><cell>17.48</cell><cell>9.37</cell><cell>7.23</cell></row><row><cell>SDM(300W-LP)</cell><cell>4.75</cell><cell>5.55</cell><cell>9.34</cell><cell>6.55</cell><cell>2.45</cell><cell>3.67</cell><cell>4.94</cell><cell>9.76</cell><cell>6.12</cell><cell>3.21</cell></row><row><cell>TSPM(300W-LP)</cell><cell>5.91</cell><cell>6.52</cell><cell>7.68</cell><cell>6.70</cell><cell>0.90</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RMFA</cell><cell>5.67</cell><cell>7.77</cell><cell>11.29</cell><cell>8.24</cell><cell>2.84</cell><cell>4.96</cell><cell>8.44</cell><cell>13.92</cell><cell>9.11</cell><cell>4.52</cell></row><row><cell>DCN(300W-LP)</cell><cell>4.99</cell><cell>5.47</cell><cell>8.10</cell><cell>6.19</cell><cell>1.68</cell><cell>3.93</cell><cell>4.67</cell><cell>7.71</cell><cell>5.44</cell><cell>2.00</cell></row><row><cell>3DDFA(Pre) [24]</cell><cell>5.00</cell><cell>5.06</cell><cell>6.74</cell><cell>5.60</cell><cell>0.99</cell><cell>3.78</cell><cell>4.54</cell><cell>7.93</cell><cell>5.42</cell><cell>2.21</cell></row><row><cell>Proposed</cell><cell>4.11</cell><cell>4.38</cell><cell>5.16</cell><cell>4.55</cell><cell>0.54</cell><cell>2.84</cell><cell>3.57</cell><cell>4.96</cell><cell>3.79</cell><cell>1.08</cell></row><row><cell cols="5">including RCPR [42], ESR [10], LBF [35], CFSS [36], SDM [71],</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MDM [29],</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>There are 1, 306 samples in [0 ? , 30 ? ], 462 samples in [30 ? , 60 ? ] and 232 samples in [60 ? , 90 ? ]. The results are demonstrated in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 3</head><label>3</label><figDesc>The NME(%) of face alignment results on 300W, with the first and the second best results highlighted.</figDesc><table><row><cell>Method</cell><cell>Common</cell><cell>Challenging</cell><cell>Full</cell></row><row><cell>TSPM [18]</cell><cell>8.22</cell><cell>18.33</cell><cell>10.20</cell></row><row><cell>ESR [10]</cell><cell>5.28</cell><cell>17.00</cell><cell>7.58</cell></row><row><cell>RCPR [42]</cell><cell>6.18</cell><cell>17.26</cell><cell>8.35</cell></row><row><cell>SDM [11]</cell><cell>5.57</cell><cell>15.40</cell><cell>7.50</cell></row><row><cell>LBF [35]</cell><cell>4.95</cell><cell>11.98</cell><cell>6.32</cell></row><row><cell>CFSS [36]</cell><cell>4.73</cell><cell>9.98</cell><cell>5.76</cell></row><row><cell>TCDCN [73]</cell><cell>4.80</cell><cell>8.60</cell><cell>5.54</cell></row><row><cell>3DDFA(Pre)</cell><cell>5.53</cell><cell>9.56</cell><cell>6.31</cell></row><row><cell>Proposed</cell><cell>5.09</cell><cell>8.07</cell><cell>5.63</cell></row><row><cell cols="3">6.6.4 Robustness to Initialization</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 4</head><label>4</label><figDesc>Alignment performance (NME) initialized by detected bounding boxes. The value in the brackets are the NME difference between results initialized by the detected and the ground truth bounding boxes with face detectors. Besides, by comparing the performance drop brought by replacing bounding boxes, our method demonstrates best robustness to initialization.</figDesc><table><row><cell></cell><cell cols="2">AFLW</cell><cell></cell><cell cols="3">AFLW2000-3D</cell></row><row><cell></cell><cell cols="3">SDM DCN Ours</cell><cell cols="3">SDM DCN Ours</cell></row><row><cell>[0, 30]</cell><cell>5.09 (0.34)</cell><cell>5.31 (0.32)</cell><cell>4.24 (0.13)</cell><cell>4.11 (0.44)</cell><cell>4.34 (0.41)</cell><cell>3.00 (0.16)</cell></row><row><cell>[30, 60]</cell><cell>6.02 (0.47)</cell><cell>5.95 (0.48)</cell><cell>4.59 (0.21)</cell><cell>6.19 (1.25)</cell><cell>5.42 (0.75)</cell><cell>3.89 (0.32)</cell></row><row><cell>[60, 90]</cell><cell>10.13 (0.79)</cell><cell>8.13 (0.03)</cell><cell>5.32 (0.16)</cell><cell>12.03 (2.27)</cell><cell>8.72 (1.01)</cell><cell>5.55 (0.59)</cell></row><row><cell>Mean</cell><cell>7.08 (0.53)</cell><cell>6.47 (0.28)</cell><cell>4.72 (0.17)</cell><cell>7.44 (1.32)</cell><cell>6.16 (0.74)</cell><cell>4.15 (0.36)</cell></row><row><cell cols="2">others when initialized</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Natural Science Foundation Projects #61473291, #61572501, #61502491, #61572536 and AuthenMetric R&amp;D Funds. Zhen Lei is the corresponding author.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">3D shape regression for real-time facial animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">41</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Global supervised descent method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2664" to="2673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Face expression recognition and analysis: The state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bettadapura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Structured face hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1099" to="1106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Active shape models-their training and application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer vision and image understanding</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="38" to="59" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="681" to="685" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
	<note>Active appearance models</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Feature detection and tracking with constrained local models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cristinacce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="929" to="938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cascaded pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Face alignment by explicit shape regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<title level="m">IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep convolutional network cascade for facial point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Facial landmark detection by deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A Bayesian mixture model for multi-view face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">High-fidelity pose and expression normalization for face recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="787" to="796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Coarse-to-fine auto-encoder networks (CFAN) for real-time face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pose-free facial landmark fitting via optimized part mixtures and cascaded deformable shape model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Face detection, pose estimation, and landmark localization in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Guided unsupervised learning of mode specific models for facial point detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jaiswal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Almaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Valstar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Workshops (ICCVW), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Interactive facial feature localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2012</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A semiautomatic methodology for facial landmark annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops (CVPRW), 2013 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Annotated facial landmarks in the wild: A large-scale, real-world database for facial landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>K?stinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Workshops (ICCV Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2144" to="2151" />
		</imprint>
	</monogr>
	<note>2011 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Face recognition based on fitting a 3D morphable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>Pattern Analysis and Machine Intelligence</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Face alignment across large poses: A 3D solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2016 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A multi-view nonlinear active shape model using kernel pca</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Psarrou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in BMVC</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="483" to="492" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A morphable model for the synthesis of 3D faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 26th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<publisher>ACM Press/Addison-Wesley Publishing Co</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="187" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Automatic feature localisation with constrained local models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cristinacce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cootes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3054" to="3067" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A comparative evaluation of active appearance model algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="680" to="689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mnemonic descent method: A recurrent process applied for end-to-end face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Snape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR16)</title>
		<meeting>IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR16)<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deformable model fitting by regularized landmark mean-shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="200" to="215" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Pattern Analysis and Machine Intelligence</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Direct appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings of the</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A nonlinear discriminative approach to aam fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2007 IEEE 11th International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Facial point detection using boosted regression and graph models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Binefa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2729" to="2736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Face alignment at 3000 FPS via regressing local binary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Face alignment by coarse-to-fine shape searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note>3, 4, 8</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Facial landmark detection with tweaked convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Large-pose face alignment via CNNbased dense 3D model fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR16)</title>
		<meeting>IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR16)<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">View-based active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">V</forename><surname>Wheeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">N</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and vision computing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="657" to="664" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multi-view face alignment using direct appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. Fifth IEEE International Conference on</title>
		<meeting>Fifth IEEE International Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="324" to="329" />
		</imprint>
	</monogr>
	<note>Automatic Face and Gesture Recognition</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Active appearance models with occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="593" to="604" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Robust face landmark estimation under occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">P</forename><surname>Burgos-Artizzu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">3D alignment of face in a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1305" to="1312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Estimating 3D shape and texture using pixel intensity, edges, specular highlights, texture constraints and a prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2005 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pose-invariant 3D face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2015 IEEE International Conference on</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Displaced dynamic expression regression for real-time facial tracking and animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">43</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dense 3D face alignment from 2D videos in real-time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Jeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face &amp; Gesture Recognition, 2015. FG&apos;15. 11th IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Pose-invariant face alignment via cnn-based dense 3d model fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Pose-invariant face alignment with a single cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of International Conference on Computer Vision</title>
		<meeting>eeding of International Conference on Computer Vision<address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Inverse rendering of faces with a 3D morphable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Aldrian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Pattern Analysis and Machine Intelligence</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Viewing real-world faces in 3D</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3607" to="3614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Adaptive 3D face reconstruction from unconstrained photo collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR16)</title>
		<meeting>IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR16)<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A 3D face model for pose and illumination invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Paysan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Knothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Amberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Video and Signal Based Surveillance, 2009. AVSS&apos;09. Sixth IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Facewarehouse: a 3D facial expression database for visual computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="413" to="425" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Visualization and Computer Graphics</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Head pose estimation in computer vision: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Murphy-Chutorian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="607" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Monocular model-based 3d tracking of rigid objects: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Computer Graphics and Vision</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="89" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Unconstrained facial landmark localization with backbone-branches fully-convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2016 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Fast and accurate 3D face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Spreeuwers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="389" to="414" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Discriminative 3D morphable model fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th IEEE International Conference and Workshops on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>Automatic Face and Gesture Recognition (FG)</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deep learning via Hessian-free optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning (ICML-10)</title>
		<meeting>the 27th International Conference on Machine Learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="735" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Effective face frontalization in unconstrained images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Enbar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Unconstrained pose-invariant face recognition using 3D generic elastic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1952" to="1961" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Pattern Analysis and Machine Intelligence</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Pose-aware face recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rawls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4838" to="4846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Finding tiny faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">300 faces in-the-wild challenge: The first facial landmark localization challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Workshops (ICCVW), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="397" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Localizing parts of faces using a consensus of exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Extensive facial landmark localization with coarse-to-fine convolutional network cascade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Workshops (ICCVW), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="386" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">XM2VTSDB: The extended M2VTS database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Messer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luettin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Maitre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second international conference on audio and video-based biometric person authentication</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">964</biblScope>
			<biblScope unit="page" from="965" to="966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Automated 3D face reconstruction from multiple images using quality measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Piotraschke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3418" to="3427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Learn to combine multiple hypotheses for accurate face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Workshops (ICCVW), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="392" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Robust multi-view face alignment based on cascaded 2d/3d face shape regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chinese Conference on Biometric Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="40" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Learning deep representation for face alignment with auxiliary attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
