<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chonghao</forename><surname>Sima</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajie</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangwei</forename><surname>Geng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Li</surname></persName>
							<email>lihongyang@pjlab.org.cnyanjunchi@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Shanghai AI Laboratory</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Conghui</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shanghai AI Laboratory</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shanghai AI Laboratory</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Methods for 3D lane detection have been recently proposed to address the issue of inaccurate lane layouts in many autonomous driving scenarios (uphill/downhill, bump, etc.). Previous work struggled in complex cases due to their simple designs of the spatial transformation between front view and bird's eye view (BEV) and the lack of a realistic dataset. Towards these issues, we present PersFormer: an end-toend monocular 3D lane detector with a novel Transformer-based spatial feature transformation module. Our model generates BEV features by attending to related front-view local regions with camera parameters as a reference. PersFormer adopts a unified 2D/3D anchor design and an auxiliary task to detect 2D/3D lanes simultaneously, enhancing the feature consistency and sharing the benefits of multi-task learning. Moreover, we release one of the first large-scale real-world 3D lane datasets: OpenLane, with high-quality annotation and scenario diversity. Open-Lane contains 200,000 frames, over 880,000 instance-level lanes, 14 lane categories, along with scene tags and the closed-in-path object annotations to encourage the development of lane detection and more industrialrelated autonomous driving methods. We show that PersFormer significantly outperforms competitive baselines in the 3D lane detection task on our new OpenLane dataset as well as Apollo 3D Lane Synthetic dataset, and is also on par with state-of-the-art algorithms in the 2D task on OpenLane. The project page is available at https://github.com/OpenP erceptionX/PersFormer 3DLane and OpenLane dataset is provided at https://github.com/OpenPerceptionX/OpenLane.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Autonomous driving is one of the most successful applications for AI algorithms to deploy in recent years. Modern Advanced Driver Assistance Systems (ADAS) for either L2 or L4 routes provide functionalities such as Automated Lane Centering (ALC) and Lane Departure Warning (LDW), where the essential need for perception is a lane detector to generate robust and generalizable lane lines <ref type="bibr">[13]</ref>. With the prosperity of deep learning, lane detection algorithms in the 2D image space has achieved impressive results <ref type="bibr" target="#b53">[55,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b46">48]</ref>, where the task is formulated as a 2D segmentation problem given front view (perspective) image as input <ref type="bibr" target="#b27">[29,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b0">1]</ref>. However, such a framework to perform lane detection in the perspective view is not applicable for industry-level products where complicated scenarios dominate.</p><p>On one side, downstream modules as in planning and control often require the lane location to be in the form of the orthographic bird's eye view (BEV) instead of a front view representation. Representation in BEV is for better task alignment with interactive agents (vehicle, road marker, traffic light, etc.) in the environment and multi-modal compatibility with other sensors such as Li-DAR and Radar. The conventional approaches to address such a demand are either to simply project perspective lanes to ones in the BEV space <ref type="bibr" target="#b58">[61,</ref><ref type="bibr" target="#b37">39]</ref>, or more elegantly to cast perspective features to BEV by aid of camera in/extrinsic matrices <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr">70]</ref>. The latter solution is inspired by the spatial transformer network (STN) <ref type="bibr" target="#b22">[24]</ref> to generate a one-to-one correspondence from the image to BEV feature grids. By doing so, the quality of features in BEV depends solely on the quality of the corresponding feature in the front view. The predictions using these outcome features are not adorable as the blemish of scale variance in the front view, which inherits from the camera's pinhole model, remains.</p><p>On the other side, the height 1 of lane lines has to be considered when we project perspective lanes into BEV space. As illustrated in <ref type="figure">Fig. 1</ref>, the lanes would diverge/converge in case of uphill/downhill if the height is ignored, leading to improper action decisions as in the planning and control module. Previous literature <ref type="bibr" target="#b58">[61,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b51">53]</ref> inevitably hypothesize that lanes in the BEV space lie on a flat ground, i.e., the height of lanes is zero. The planar assumption does not hold true in most autonomous driving scenarios, e.g., uphill/downhill, bump, crush turn, etc. Since the height information is unavailable on public benchmarks or complicated to acquire accurate ground truth, 3D lane detection is ill-posed. There are some attempts to address this issue by creating 3D synthetic benchmarks <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b18">20]</ref>. Their performance still needs improvement in complex, realistic scenarios nonetheless (c.f. (b-c) in <ref type="figure">Fig. 1</ref>). Moreover, the domain adaption between simulation and real data is not well-studied <ref type="bibr" target="#b15">[17]</ref>.</p><p>To address these bottlenecks aforementioned, we propose Perspective Transformer, shortened as PersFormer, which has a spatial feature transformation module to generate better BEV representations for the task. The proposed framework unifies 2D/3D lane detection tasks, and substantiates performance on the proposed large-scale realistic 3D lane dataset, OpenLane.</p><p>First, we model the spatial feature transformation as a learning procedure that has an attention mechanism to capture the interaction both among local region in the front view feature and between two views (front view to BEV), <ref type="figure">Fig. 1</ref>. Motivation of performing lane detection from 2D in (a) to BEV in (b); and the superiority of our method in (c) versus (b). Lanes would diverge/converge in projected BEV on planar assumption, and a 3D solution with height to be considered can accurately predict the parallel topology in this case consequently being able to generate a fine-grained BEV feature representation. Inspired by <ref type="bibr" target="#b57">[60,</ref><ref type="bibr" target="#b7">8]</ref>, we construct a Transformer-based module to realize this, while the deformable attention mechanism [72] is adopted to remarkably reduce the computational memory requirement and dynamically adjust keys through the cross-attention module to capture prominent feature among the local region. Compared with direct 1-1 transformation via Inverse Perspective Mapping (IPM), the resultant features would be more representative and robust as it attends to the surrounding local context and aggregates relevant information. We further aim at unifying 2D and 3D lane detection tasks to benefit from the colearning optimization. Second, we release the first real-world, large-scale 3D lane dataset and corresponding benchmark, OpenLane, to support research into the problem. OpenLane contains 200,000 annotated frames and over 880,000 lanes -each with one of 14 category labels (single white dash, double yellow solid, left/right curbside, etc.), which exceeds all of the existing lane datasets. It also has some distinguishing elements such as scenes, weather, and closed-in-pathobject (CIPO) for other research topics in autonomous driving.</p><p>The main contributions of our work are three-fold: 1) Perspective Transformer, a novel Transformer-based architecture to realize spatial transformation of features; 2) An architecture to simultaneously unify 2D and 3D lane detection, which is feasibly needed in the application. Experiments show that our PersFormer outperforms state-of-the-art 3D lane detection algorithms; 3) The OpenLane dataset, the first large-scale realistic 3D lane dataset with highquality labeling and vast diversity. The dataset, baselines, as well as the whole suite of codebase, is released to facilitate the research in this area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Vision Transformers in Bird's-Eye-View (BEV). Projecting features to BEV and performing downstream tasks in it has become more dominant and ensured better performance recently <ref type="bibr" target="#b34">[36]</ref>. Compared with conventional CNN structure, the cross attention scheme in Vision Transformers <ref type="bibr" target="#b57">[60,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr">72]</ref> is naturally introduced to serve as a learnable transformation of features across different views in an elegant spirit <ref type="bibr" target="#b34">[36]</ref>. Instead of simply projecting features via IPM, the successful application of Transformers in view transformation has demonstrated great success in various domains, including 3D object detection [68, <ref type="bibr" target="#b59">62,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b29">31]</ref>, prediction <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b41">43]</ref>, planning <ref type="bibr" target="#b44">[46,</ref><ref type="bibr" target="#b11">12]</ref>, etc.</p><p>Previous work <ref type="bibr" target="#b14">[16,</ref><ref type="bibr">67,</ref><ref type="bibr" target="#b59">62,</ref><ref type="bibr" target="#b48">50,</ref><ref type="bibr" target="#b6">7]</ref> bring the BEV philosophy into pipeline, and yet they do not consider attention mechanism and/or 3D vision geometry (in this case, camera parameters). For instance, 3D-LaneNet <ref type="bibr" target="#b14">[16]</ref> is set up with camera in/extrinsic matrices; the IPM process generates a virtual BEV representation from front view features. DETR3D <ref type="bibr" target="#b59">[62]</ref> also considers camera geometry and formulates a learnable 3D-to-2D query search with attention scheme. However, there is no explicit BEV modelling for robust feature representation; the aggregated features might not be properly represented in 3D space. To address these shortcomings, our proposed PersFormer takes into account both the effect of camera parameters to generate BEV features and the convenience of crossattention mechanism to model view transformation, achieving better feature representation in the end. Lane Detection Benchmarks. A large-scale, diverse dataset with high-quality annotation is a pivot for lane detection. Along with the progress of lane detection approaches, numerous datasets have been proposed <ref type="bibr" target="#b27">[29,</ref><ref type="bibr" target="#b21">23,</ref><ref type="bibr">69,</ref><ref type="bibr">58,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr">64,</ref><ref type="bibr" target="#b9">10]</ref>. However, they usually fit into one or the other lane detection scenario. Tab. 1 depicts more details of the existing benchmarks and their comparison with our proposed OpenLane dataset. OpenLane is the first large-scale, realistic 3D lane dataset. It equips with a wide span of diversity in both data distribution and task applicability. 3D Lane Detection. As discussed in Section 1, planar assumption does not always reserve in some cases, i.e., uphill/downhill, bump. Several approaches <ref type="bibr" target="#b38">[40,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b2">3]</ref> utilize multi-modal or multi-view sensors, such as a stereo camera or Li-DAR, to get the 3D ground topology. However, these sensors have shortages of high cost in hardware and computation resources, confining their practical applications. Recently, some monocular methods <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" target="#b35">37</ref>] take a single image and employ IPM to predict lanes in 3D space. 3D-LaneNet <ref type="bibr" target="#b14">[16]</ref> is the pioneering work in this domain with one simple end-to-end neural network, which adopts STN <ref type="bibr" target="#b22">[24]</ref> to accomplish the spatial projection of features. Gen-LaneNet <ref type="bibr" target="#b18">[20]</ref> builds on top of 3D-LaneNet and designs a two-stage network for decoupling the segmentation encoder and 3D lane prediction head. These two approaches <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b18">20]</ref> suffer from improper feature transformation and unsatisfying performance in curving or crush turn cases. Confronted with the issues above, we bring in PersFormer to provide better feature representation and optimize anchor design to unify 2D and 3D lane detection simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we propose PersFormer, a unified 2D/3D lane detection framework with Transformer. We first describe the problem formulation, followed by an introduction to the overall structure in Section 3.1. In Section 3.2, we present Our proposed PersFormer pipeline. The core is to learn a spatial feature transformation from front view to BEV space so that the generated BEV features at target point would be more representative by attending local context around reference point. PersFormer consists of the self-attention module to interact with its own BEV queries; the cross-attention module that takes the key-value pair from the IPM-based front view features to generate fine-grained BEV feature Perspective Transformer, an explicit feature transformation module from front view to BEV space by the aid of camera parameters. In Section 3.3, we give details on the anchor design to unify 2D/3D tasks and in Section 3.4 we further elaborate on the auxiliary task and loss function to finalize our training strategy.</p><p>Problem Formulation. Given an input image I org ? R Horg?Worg , the goal of PersFormer is to predict a collection of 3D lanes L 3D = {l 1 , l 2 , . . . , l N 3D } and 2D lanes L 2D = {l 1 , l 2 , . . . , l N 2D }, where N 3D , N 2D are the total number of 3D lanes in the pre-defined BEV range and 2D lanes in the original image space (front view) respectively. Mathematically, each 3D lane l d is represented by an ordered set of 3D coordinates:</p><formula xml:id="formula_0">l d = (x 1 , y 1 , z 1 ), (x 2 , y 2 , z 2 ), . . . , (x N d , y N d , z N d ) ,<label>(1)</label></formula><p>where d is the lane index, and N d is the max number of sample points of this lane. The form of 2D lane is represented similarly with 2D coordinate (u, v) accordingly. Each lane has a categorical attribute c 3D/2D , indicating the type of this lane (e.g., single-white dash line). Also, for each point in a single 2D/3D lane, there exists an attribute property indicating whether the point is visible or not, denoted by vis fv/bev as a vector for the lane.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Approach Overview</head><p>The overall structure, as illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>  <ref type="bibr" target="#b53">[55]</ref> and 3D-LaneNet <ref type="bibr" target="#b14">[16]</ref>, with modification on the structure and anchor design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Proposed Perspective Transformer</head><p>We present Perspective Transformer, a spatial transformation method that combines camera parameters and data-driven learning procedures. The general idea of Perspective Transformer is to use the coordinates transformation matrix from IPM as a reference to generate BEV feature representation, by attending related region (local context) in front view feature. On the assumption that the ground is flat and the camera parameters are given, a classical IPM approach calculates a set of coordinate mapping from front-view to BEV, where the BEV space is defined on the flat ground (see <ref type="bibr" target="#b19">[21]</ref>, Section 8.1.1). Given a point p fv with its coordinate (u, v) in the front-view feature F fv ? R H fv ?W fv ?C , IPM maps the point p fv to the corresponding point p bev in BEV, where (x, y) is the coordinate in the BEV space R H bev ?W bev ?C . The transform is achieved with camera in/extrinsic and can be represented mathematically as:</p><formula xml:id="formula_1">? ? x y 0 ? ? = ? f 2b ? R ? ? K ?1 ? ? ? u v 1 ? ? + ? ? 0 0 ?h ? ? ,<label>(2)</label></formula><p>where ? f 2b implies the scale factor between front-view and BEV, R ? denotes the pitch rotation matrix from extrinsic, K is the intrinsic matrix, and h stands for camera height. Such a transformation in Eqn.</p><p>(2) enframes a strong prior on the attention unit in PerFormer to generate more representative BEV features. The architecture of Perspective Transformer is inspired by popular approaches such as DETR <ref type="bibr" target="#b7">[8]</ref>, and consists of the self-attention module and cross-attention module (see <ref type="figure" target="#fig_0">Fig. 2</ref>). We differentiate from them in that the queries are not implicitly updated. However, instead, they are piloted by an explicit meaning -the physical location to detect objects or lanes in BEV. In the self-attention module, the output Q bev descends from the triplet (key, value, query) input through their interaction. The formulation of such a self-attention can be described as:</p><formula xml:id="formula_2">Q bev = softmax QK ? ? d k V,<label>(3)</label></formula><p>where K, Q, V ? R (H bev ?W bev ?C) are the same query that is pre-defined in BEV, ? d k is the dimensional normalized factor. In the cross-attention module, the input query Q ? bev is the outcome of several additional layers feeding the self-attention output Q bev as input. Note that Q ? bev is an explicit feature representation as to which part in BEV should be ; by learning offsets, the network learns targetreference points mapping from green rectangles to yellow and related blue rectangles as keys to Transformer <ref type="figure">Fig. 4</ref>. Unifying anchor design in 2D and 3D. We first put curated anchors (red) in the BEV space (left), then project them to the front view (right). Offset x i k and u i k (dashed line) are predicted to match ground truth (yellow and green) to anchors. The correspondence is thus built, and features are optimized together paid more attention since the generation of queries is location-sensitive in BEV. This is quite different compared with queries that do not consider view transformation in most Vision Transformers <ref type="bibr" target="#b59">[62,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr">72]</ref>. Furthermore, the intuition behind employing Transformer to map features from front view to BEV is that such an attention mechanism would automatically attend which part of features contribute most towards the target point (query) in the destination view. The direct feature transformation would suffer from camera parameter noise or scale variance issues, as discussed and illustrated in Section 1. Note that the naive Transformer cannot be applied directly since the number of key-value pairs is huge and thus be confined by computational burden. Inspired by Deformable DETR [72], we attend partial key-value pairs around the local region in a learnable manner to save cost and improve efficiency. <ref type="figure" target="#fig_1">Fig. 3</ref> depicts the feature transformation process and the generation of keyvalue pairs in cross-attention. Specifically, given a query point (x, y) in the target BEV map Q ? bev , we project it to the corresponding point (u, v) in the front view via Eqn. <ref type="bibr" target="#b1">(2)</ref>. As does similarly in [72], we learn some offsets based on point (u, v) to generate a set of most related points around it. These learned points, together with (u, v) are defined as reference points. They contribute most to the query point (x, y), defined as target point, in BEV-space. The reference points serve as the surrounding context in the local region that contributes most to the feature representation from perspective view to BEV space. They are the desired keys we try to find, and their features are values for the cross attention module. Note that the initial locations of reference points from IPM are used as preliminary locations for the coordinate mapping; the location are adjusted gradually during the learning procedure, which is the core role of Deformable Attention.</p><p>As a result, the output of the cross-attention module can be formulated as:</p><formula xml:id="formula_3">F bev = DeformAttn(Q ? bev , F fv , p fv2bev ),<label>(4)</label></formula><p>where F bev ? R (H bev ?W bev ?C) is the final desired features for the subsequent 3D head to get lane predictions, Q ? bev denotes the input queries, F fv ? R (H fv ?W fv ?C) indicates the front view features from backbone, and p fv2bev is the IPM-inited coordinate mapping from front view to BEV space. Considering F fv and p fv2bev with the deformable unit, we get the explicit transformed BEV feature F bev .</p><p>To sum up, Perspective Transformer extracts front-view features among the reference points to construct representative BEV features. As demonstrated in Section 5, such a feature transformation in an aggregation spirit via Transformer is proven to perform better than a direct IPM-based projection across views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Simultaneous 2D and 3D Lane Detection</head><p>Although the main focus in this paper lies in 3D detection, we formulate the PersFormer framework to detect 2D and 3D lanes in one shot. On one side, 2D lane detection in the perspective view still draws interest in the community as part of the general high-level vision problems <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b53">55,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b46">48]</ref>; on the other side, unifying 2D and 3D tasks are naturally feasible since the BEV features to predict 3D outputs descend from the counterpart in the 2D branch. An end-to-end unified framework would leverage features and benefit from the co-learning optimization process as proven in most multi-task literature <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b56">59,</ref><ref type="bibr" target="#b26">28]</ref>.</p><p>Unified anchor design. Since our method is anchor-based detection, the core issue to achieve the unified framework is to integrate anchors in both 2D and 3D. Unfortunately, anchors in these two domains usually do not share similar distribution. For example, the popular 2D approach LaneATT <ref type="bibr" target="#b53">[55]</ref> settles too many anchors, spanning different directions in the image; while the recent 3D work Gen-LaneNet <ref type="bibr" target="#b18">[20]</ref> puts too few anchors, which are parallel and sparse in BEV. Based on these observations, we thereby design anchors such that the redesigned anchors could leverage the network to optimize shared features across two domains. We start with several groups of anchors (here, the group number is set to 7) sampled with different incline angles in the BEV space and then projected to the front view. <ref type="figure">Fig. 4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>elaborates on the integration of 2D and 3D anchors. Below we describe how the lane line is modeled via anchors.</head><p>3D anchor design. To match ground truth lanes tightly, the anchors are placed approximately longitudinal along x -axis, with an incline angle ?. As denoted in <ref type="figure">Fig. 4</ref>(left), the initial line (equally spaced) with staring position along x -axis is denoted by X i bev for each anchor i. Similar to anchor regression in object detection, the network predicts the relative offset x i w.r.t. the initial position X i bev ; hence the resultant lane prediction along x -axis is (x i + X i bev ). As indicated in Eqn. <ref type="bibr" target="#b0">(1)</ref>, each lane is represented as a number of N d points. The prediction head generates three vectors related to lane shape as follows:</p><formula xml:id="formula_4">(x i , z i , vis i bev ) = {(x (i,k) , z (i,k) , vis (i,k) bev )} N d k=1<label>(5)</label></formula><p>where z i is the lane height in 3D sense, the binary vis</p><formula xml:id="formula_5">(i,k)</formula><p>bev denotes the visibility of each location k in lane i, which controls the endpoint or length of a lane. Note that the lane position along y-axis does not need to be predicted since each y value of the N d samples in a lane is pre-defined -we predict the x (i,k) value at the corresponding (fixed) y location. To sum up, the description of a lane's location in the world coordinate system is denoted as (x i + X i bev , y, z i ). 2D anchor design. The anchor description and prediction are similar to those defined in 3D view, except that the (u, v) is in 2D space and there is no height (see <ref type="figure">Fig. 4(right)</ref>). We omit the detailed notations for brevity. It is worth mentioning that each 3D anchor X i bev with an incline angle ? corresponds to a specific 2D anchor U i fv with the incline angle ?; the connection is built via the projection in Eqn. <ref type="bibr" target="#b1">(2)</ref>. We achieve the goal of unifying 2D and 3D tasks simultaneously by setting the same set of anchors. Such a design would optimize features together and features being more aligned and representative across views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Prediction Loss</head><p>Binary Segmentation under BEV. As do in many preceding work <ref type="bibr" target="#b60">[63,</ref><ref type="bibr" target="#b40">42,</ref><ref type="bibr" target="#b21">23]</ref>, adding more intermediate supervision into the network training would boost the performance of network. Since lane detection belongs to image segmentation and requires general large resolution, we concatenate a U-Net structure <ref type="bibr" target="#b47">[49]</ref> head on top of the generated BEV features. Such an auxiliary task is to predict lanes in BEV, but instead in a conventional 2D segmentation manner, aiming for better feature representation for the main task. The ground truth S gt is a binary segmentation map projected from 3D lane ground truth to the BEV space. The prediction output is denoted by S pred and owns the same size as S gt .</p><p>Loss function. Equipped with the anchor representation and segmentation head aforementioned, we summarize the overall loss. Given an image input and its ground truth labels, it finally computes a sum of all anchors' loss; the loss is a combination of the 2D lane detection, 3D lane detection and intermediate segmentation with learnable weights (?, ?, ?) accordingly:</p><formula xml:id="formula_6">L = i ?L 2D (c i 2D , u i , vis i fv ) + ?L 3D (c i 3D , x i , z i , vis i bev ) + ?L seg (S pred ),<label>(6)</label></formula><p>where c i (?) is the predicted lane category in 2D and 3D domain respectively. The loss input above shows the prediction part only; we omit the ground truth notation for brevity. The loss of lane category classification for the 2D/3D task is the cross-entropy; the loss of lane shape regression is the l 1 norm; the loss of lane visibility prediction is the binary cross-entropy loss. The loss of the auxiliary task is a binary cross-entropy loss between two segmentation maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">OpenLane: A Large-scale Realistic 3D Lane Benchmark</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Highlights over Previous Benchmarks</head><p>OpenLane is the first real world 3D lane dataset and the largest scale to date compared with existing benchmarks. We construct OpenLane on top of the influential Waymo Open dataset <ref type="bibr" target="#b52">[54]</ref>, following the same data format and evaluation - We annotate all the lanes in each frame, including those in the opposite direction if no curbside exists in the middle. Due to the complicated lane topology, e.g., intersection/roundabout, one frame could contain as many as 24 lanes in OpenLane. Statistically, about 25% frames of OpenLane have more than 6 lanes, which exceeds the maximum number in most lane datasets. 14 lane categories are annotated alongside to cover a wide range of lane types in most scenarios, including road edges. Double yellow solid lanes, single white solid and dash lanes take up almost 90% of total lanes. This is imbalanced, and yet it falls into a longtail distribution problem, which is common in realistic scenarios. In addition to the lane detection task, we also annotate: (a) scene tags, such as weather and locations; (b) the closest-in-path object (CIPO), which is defined as the most concerned target w.r.t. ego vehicle; such a tag is quite pragmatic for subsequent modules as in planning/control, besides a whole set of objects from perception. An annotation example is provided in <ref type="figure" target="#fig_2">Fig. 5(d)</ref>, along with some typical samples in existing 2D lane datasets in <ref type="figure" target="#fig_2">Fig. 5(a-c)</ref>. The detailed statistics, annotation criterion and visualization can be found in Appendix.</p><formula xml:id="formula_7">20K/20K - ? - - 7 Medium OpenDenseLane [10] 1.7K 57K/57K - ? ? - 4 Medium LLAMAS [4] 14 79K/100K - ? ? 4 - Easy ApolloScape [23] 235 115K/115K 16s ? ? - 13 Medium BDD100K [69] 100K 100K/120M 40s ? ? - 11 Medium CULane [44] - 133K/133K - ? - 4 - Medium CurveLanes [64] - 150K/150K - ? - 9 - Medium ONCE-3DLanes [66] - 211K/211K - ? - 8 - Medium OpenLane 1K 200K/200K</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Generation of High-quality Annotation</head><p>Building a real-world 3D lane dataset has challenges mainly in an accurate localization system and occlusions. We compare several popular sensor datasets <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b52">54,</ref><ref type="bibr" target="#b5">6]</ref> by projecting 3D object annotations to image planes and constructing 3D Scene: Suburbs Weather: Clear Hours: scene maps using both learning-based <ref type="bibr" target="#b55">[57]</ref> or SLAM algorithms <ref type="bibr" target="#b49">[51,</ref><ref type="bibr" target="#b50">52]</ref>. The reconstruction precision and scalability of Waymo Open Dataset <ref type="bibr" target="#b52">[54]</ref> outperforms other candidates, leading to employing it as our basis.</p><formula xml:id="formula_8">Dawn/Dusk CIPO R.Curb L.Curb 1-W dash L-Y dash R-Y solid L-Y solid R-Y dash</formula><p>Primarily, we generate the necessary high-quality 2D lane labels. They contain the final annotations of tracking ID, category, and 2D points ground truth. Then for each frame, the point clouds are first filtered with the original 3D object bounding boxes and then projected back into the corresponding image. We further keep those points related to 2D lanes only with a certain threshold. However, the output directly after a static threshold filtering could lead to an unsatisfying ground truth due to the perspective scaling issue. To solve this and keep the slender shape of lanes, we use the filtered point clouds to interpolate the 3D position for each point in 2D annotations. Afterward, with the help of the localization system, 3D lane points in frames within a segment could be spliced into long, high-density lanes. This process could bring some unreasonable parts into the current frame; thus, points in one lane whose 2D projections are higher than the ending position of its 2D annotation are labeled as invisible. A smoothing step is ultimately deployed to filtrate any outliers and generate the 3D labeling results. We omit some technical details, such as how to deal with a large U-turn during smoothing, and we refer the audience to Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We examine PersFormer on two 3D lane benchmarks, the newly proposed realworld OpenLane dataset, and the synthetic Apollo dataset. For both 3D lane datasets, we follow the evaluation metrics designed by Gen-LaneNet <ref type="bibr" target="#b18">[20]</ref>, with additional category accuracy on OpenLane dataset. For the 2D task, the classical metric in CULane <ref type="bibr" target="#b42">[44]</ref> is adopted. We put correlated details in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Results on OpenLane</head><p>We provide 3D and 2D evaluation results on the proposed OpenLane dataset. In order to evaluate the models thoroughly, we report F-Score on the entire validation set and different scenario sets. The scenario sets are selected from the entire validation set based on the scene tags of each frame. In Tab. 2, PersFormer gets the highest F-Score on the entire validation set and every scenario set, surpassing   <ref type="bibr" target="#b53">[55]</ref>, which is our baseline 2D method, by 11%. Detailed comparison with previous 3D SOTAs is presented in Tab. 4. PersFormer outperforms the previous best method in F-Score by 6.4%, realizes satisfying accuracy on the classification of lane type, and presents the first baseline result. Note that Pers-Former is not satisfying on the metric of near error on x-axis. This is probably because the unified anchor design is more suitable in fitting the main body of a lane rather than the starting point. Qualitative results are shown in <ref type="figure" target="#fig_3">Fig. 6</ref>, indicating that PersFormer is good at catching dense and unapparent lanes in usual autonomous driving scenes. Overall, PersFormer reaches the best performance on 3D lane detection and gains remarkable improvement in 2D on OpenLane.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results on Apollo 3D Synthetic</head><p>We evaluate PersFormer on Apollo 3D Lane Synthetic dataset <ref type="bibr" target="#b18">[20]</ref>. In Tab. 5, while limited by the scale of the dataset (10K frames), our PersFormer still  <ref type="bibr" target="#b14">[16]</ref>, and Gen-LaneNet(c) <ref type="bibr" target="#b18">[20]</ref>. Under a straight road scenario, PersFormer can provide lane-type information and even detect subtle curbside while other methods are missing it achieves the best F-Score on every scene set. In terms of X/Z error, our model gets comparable results compared to previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Study</head><p>We present ablation studies on the anchor design, multi-task strategy, transformerbased view transformation, and auxiliary segmentation task. We mainly report the improvement on 3D lane detection and provide related results on 2D task. Anchor design and multi-task. Starting with a pure 3D lane detection framework (similar to 3D-LaneNet <ref type="bibr" target="#b14">[16]</ref>), PersFormer gains 1.7% by adopting multi-task scheme (Exp.2) and 0.98% with new anchor design (Exp.4) respec- <ref type="table">Table 6</ref>. Ablative Study on a 300 segments subset of OpenLane. Exp.1 is the baseline 3D method, growing with anchor design and multi-task learning (Exp. <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref> tively. By jointly using the new anchor and multi-task trick, PersFormer acquires an improvement of 2.5% in 3D task and 2.6% in 2D task (Exp.5). Spatial feature transformation. By using Perspective Transformer with the new anchor design, the improvement increases to 4.9% (Exp.6), almost doubling the previous improvement. Adding auxiliary binary segmentation task further brings an improvement to 6.02% (Exp.7), which is our complete model. These ablations support our assumption that PersFormer indeed generates a fine-grained BEV feature, and the spatial feature transformation does illustrate its importance in 3D lane detection task. Surprisingly, a better BEV feature helps 2D task a lot as well, improving 9.7% (Exp.7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we have proposed Persformer, a novel Transformer-based 2D/3D lane detector, along with OpenLane, a large-scale realistic 3D lane dataset. We demonstrate experimentally that a fine-grained BEV feature with explicit prior and supervision can significantly improve the performance of lane detection. Meanwhile, a large-scale real-world 3D lane dataset effectively align the demand from both the academic and the industrial side. Appendix A More Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Lane Detection Benchmarks</head><p>For example, <ref type="bibr" target="#b27">[29,</ref><ref type="bibr" target="#b21">23,</ref><ref type="bibr">69]</ref> annotate lanes and lane markings in pixel-level so they are best suitable for semantic segmentation task. <ref type="bibr">[58,</ref><ref type="bibr" target="#b3">4]</ref> collect data on highways with light traffic only, which is not challenging and has a large gap between the evaluation and real-world performance for up-to-date algorithms. <ref type="bibr" target="#b42">[44,</ref><ref type="bibr">64]</ref>  , that annotates lane layout in 3D space. The difference between OpenLane and ONCE-3DLanes falls into three aspects. First is the dataset statistics. The number of frames contained is quite the same, where OpenLane has 200K in total and ONCE-3DLanes has 211K. The annotation quality differentiates a lot, as OpenLane has more than 25% of frames with more than 6 lanes, while ONCE-3DLanes only has less than 10% of frames under the same setting. Second is the problem setting. OpenLane provides camera extrinsics as Waymo Open Dataset, while ONCE-3DLanes lacks of this information. Meanwhile, OpenLane provides segments annotation as scene tags, where ONCE-3DLanes doesn't. This could be used in video task and expand the potential usage of OpenLane. Third is the diversity of lane annotation. In OpenLane, the lane annotation not only contains the 3D position of such a lane, but also several attributes and tracking id. In ONCE-3DLanes, only the 3D position information is provided. Due to the difficulty of collecting 3D information for lanes, current 3D lane detection algorithms mainly focus on synthetic data <ref type="bibr" target="#b18">[20]</ref>. It is small-scale and exists the domain gap between simulation and realistic scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 2D Lane Detection</head><p>Early lane detection approaches rely on traditional computer vision techniques, such as filtering <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b30">32]</ref>, clustering <ref type="bibr" target="#b58">[61]</ref>, etc. With the advent of deep learning, CNN-based methods significantly outperform hand-crafted algorithms. A typical way is to treat lane detection as a semantic segmentation problem <ref type="bibr" target="#b27">[29,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b0">1]</ref>. Binary segmentation <ref type="bibr" target="#b39">[41]</ref> needs post-clustering process for lane instance discrimination, while multi-class segmentation <ref type="bibr" target="#b27">[29,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b20">22]</ref> usually limits the maximum detection results in one frame. Moreover, the pixel-wise classification takes large computation resources. To overcome this, several work propose lightweight yet effective grid based <ref type="bibr" target="#b45">[47,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b46">48]</ref> or anchor based <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr">64,</ref><ref type="bibr" target="#b51">53,</ref><ref type="bibr" target="#b53">55]</ref> methods. The grid-based approach detects lanes in a row-wise way, whose resolution is much lower than the segmentation map. The model outputs the probability for each cell if it belongs to a lane, and a vertical post-clustering process is still needed to generate the lane instances. Anchor-based approaches adopt the idea from classical object detection, focusing on optimizing the offsets from predefined line anchors. In this circumstance, how to define anchors is a critical problem. Chen et al. <ref type="bibr" target="#b10">[11]</ref> adopts vertical anchors, which cause great difficulty for curving lane prediction. Some work <ref type="bibr" target="#b28">[30,</ref><ref type="bibr" target="#b53">55,</ref><ref type="bibr" target="#b51">53]</ref> design anchors as a slender tilt shape, while the huge amount of different anchors to improve the detection accuracy would influence the computational efficiency. Nevertheless, considering their incredible performance on public datasets, we adopt the anchor-based formulation and carefully re-design anchors to achieve both high accuracy and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Algorithm</head><p>We summarize the details of PersFormer here. We introduce the backbone, overall structure and the unified anchor design. Later we break down the loss function into pieces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Backbone</head><p>The backbone module is slightly different from previous work <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b18">20]</ref>, as we need to consider 2D/3D branches together. We use EfficientNet <ref type="bibr" target="#b54">[56]</ref> as our backbone, and extract a specific layer as our following module's input. Later we provide two designs, using FPN <ref type="bibr" target="#b32">[34]</ref> or not. After using several convolution layers, the backbone module outputs 4 different scaled front-view feature maps. Their resolutions are 180 ? 240, 90 ? 120, 45 ? 60, 22 ? 30. Each front-view feature map is then transformed to BEV-space feature map with the help of Perspective Transformer, resulting in 4 BEV feature maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Anchor Details</head><p>In this section, we present details of our anchor design, including angles, numbers of anchors and how we associate ground truth lanes with anchors in 2D and 3D. As introduced in the main body of the paper, we first set anchors in BEV space.</p><p>Following Gen-LaneNet <ref type="bibr" target="#b18">[20]</ref>, the starting positions X i bev are evenly placed along x-axis with the spacing of 8 pixels. However, we differentiate it from the incline angle ?. Gen-LaneNet sets straight-forward (parallel to y-axis) only, which makes it hard to predict lanes with large curvatures or perpendicular lanes. Towards this problem, we put 7 anchors at each X i bev with different angles, i.e., ? ? {?/2, arctan (?0.5), arctan (?1), arctan (?2)}. Note that the angles are in terms of grid coordinates, which is not equal to the absolute values when grids are not square. Moreover, we project all the BEV anchors to image space with average camera height and pitch angle of the dataset, leading to corresponding 2D anchors.</p><p>The association between ground truth lanes and anchors is based on the average distance similar to the loss calculation process, instead of assigning the closest anchor at Y ref to ground truths as <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b18">20]</ref>. The Y ref is set very close to ego-vehicle, i.e., 5m in Gen-LaneNet, which makes it better predict lanes in close area while having unsatisfactory performance in the far distance. In our experiments, we assign the anchor with minimum edit distance to ground truth lanes in both 2D and 3D tasks. The distance is calculated at fixed y positions: <ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">15,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b48">50,</ref><ref type="bibr" target="#b57">60,</ref><ref type="bibr">80</ref>, 100) for 3D anchors, and 72 equally sampled heights for 2D anchors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Loss Function</head><p>We give the details of loss function here. As introduced in the main body of the paper, given the pre-defined y value of the N d samples along y-axis, the 3D detection head outputs a set of points for each anchor i as following:</p><formula xml:id="formula_9">(x i , z i , vis i bev ) = {(x (i,k) , z (i,k) , vis (i,k) bev )} N d k=1<label>(7)</label></formula><p>The y values are <ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">15,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b48">50,</ref><ref type="bibr" target="#b57">60,</ref><ref type="bibr">80,</ref><ref type="bibr">100)</ref> in the BEV space, and the size of the BEV space is 20m?100m. Similar to 3D setting, given the pre-defined v value of the N d samples along v -axis in front view, the 2D prediction is:</p><formula xml:id="formula_10">(u i , vis i uv ) = {(u (i,k) , vis (i,k) uv )} N d k=1<label>(8)</label></formula><p>The loss is a combination of the 2D lane detection, 3D lane detection and intermediate segmentation with learnable weights (?, ?, ?) accordingly:</p><formula xml:id="formula_11">L = i ?L 2D (c i 2D , u i , vis i fv ) + ?L 3D (c i 3D , x i , z i , vis i bev ) + ?L seg (S pred ),<label>(9)</label></formula><p>where c i (?) is the predicted lane category in 2D and 3D domain respectively. For L 3D , it consists of classification loss, regression loss and visibility loss. The classification loss is a cross-entropy loss, which is as follow:</p><formula xml:id="formula_12">L 3D-cls = L CE (c i 3D-pred , c i 3D-gt )<label>(10)</label></formula><p>The regression loss is a L 1 loss, which is as follow:</p><formula xml:id="formula_13">L 3D-reg = L L1 ({x i , z i } pred , {x i , z i } gt )<label>(11)</label></formula><p>The visibility loss is a binary cross-entropy loss, which is as follow:</p><formula xml:id="formula_14">L 3D-vis = L BCE (vis i pred , vis i gt )<label>(12)</label></formula><p>The 2D loss functions are similar to the 3D ones, except they are in 2D form:</p><formula xml:id="formula_15">L 2D-cls = L CE (c i 2D-pred , c i 2D-gt ) L 2D-reg = L L1 ({u i } pred , {u i } gt ) L 2D-vis = L BCE (vis i pred , vis i gt )<label>(13)</label></formula><p>The segmentation loss is a binary cross-entropy loss as well, which is as follow: </p><formula xml:id="formula_16">L seg = L BCE (S pred , S gt )<label>(14)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Details on OpenLane Benchmark</head><p>In this section, we present more details on dataset statics, our annotation criterion, visualization examples, algorithms we adopted when generating the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Dataset Statistics</head><p>OpenLane has 1,150 segments with train/validation/test splits of 798/202/150, respectively. Since the test sets are kept for its online leaderboard evaluation, we annotate the other 1,000 segments, i.e., 200K frames at a frequency of 10 FPS, and keep the original train/validation partition for fair comparison with other tasks, such as object detection. We compute the statistics in OpenLane and visualize them. The overall number of segments with different scene tags is given in Tab. 7. It implies great diversity in data collection and raises higher requirements on the robustness of algorithms. The weather distribution is visually presented in <ref type="figure">Fig. 7</ref>. It shows the benchmark covers various weather conditions and well holds the consistency in the train/validation split. The distribution of the number of lanes in each frame is shown in <ref type="figure">Fig. 8</ref>. About 25% frames of OpenLane have more than 6 lanes, which exceeds the maximum number in most lane datasets. <ref type="figure">Fig. 9</ref> shows the distribution of lane categories. Single white solid and dash lanes, double yellow solid lanes take up almost 90% of the total lanes. This is imbalanced and yet it falls into a long-tail distribution problem, which is common in realistic scenarios. <ref type="figure">Fig. 10</ref> presents the distribution of altitude difference per frame. Only around 20% frames are relatively flat with absolute height variation less than 0.5m, whereas the difference is more than 1m in over 50% of OpenLane. This data further demonstrates the necessity of 3D lane detection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Annotation Criterion</head><p>We aim at introducing how we annotate lanes, scene tags and CIPO levels in this section. Details such as data structures, folder hierarchy will be provided in the dataset releasing page in the future. Lanes. Our principle for the 2D lane detection task is to find all visible lanes inside left and right road edges. Following this philosophy, we carefully annotate  All valid 2D ground truths are transformed to 3D annotations by the generation method in Sec. 4.2 of the main body (Generation of High-quality Annotation), except those without LiDAR points scanning through. Thus the criterion above applies to 3D lanes as well.</p><p>Scene tags. We label each segment with 3 scene tags, i.e., weather, scene and hours. We hope these labels can help researchers to investigate the robustness of their models under various scenarios. The statics are shown in Tab. 7. Specifically, the dataset covers 5 different kinds of weather, clear, partly cloud, overcast, rainy and foggy. Note that we classify the video as partly cloud or foggy when there are clouds or fog in the sky respectively, otherwise it will be categorized as overcast. The scene, or the location, includes 5 categories, i.e., residential, urban, suburbs, highway and parking lot. And the hours are divided into 3 parts: daytime, night, dawn/dusk.</p><p>Closest-in-path object (CIPO). CIPO is usually defined as the closest object in ego lane, which refers to a single vehicle only. However, there are cases that vehicles on left/right lanes are intended to cut in which are crucial as well, or there may not be any qualified vehicles in ego lane. To cover the complex scenarios, we categorize objects, mainly including vehicles, pedestrians and cyclists, into 4 different CIPO levels. (1) The most important one, which is closest to ego vehicle within the required reaction distance and has over 50% part of it in the ego lane. Level 1 contains one object at most. (2) Objects are annotated as Level 2 when their bodies interact with the real or virtual lines of ego lane. They are typically in the process of cut-in or cut-out, which hugely influences ego-vehicle decision-making. (3) We consider objects mainly within the reaction distance or drivable area, or those in left/ego/right lanes more specifically. Thus we annotate Level 3 with objects in the above area and having occlusion rate less than 50%. Note that vehicles in the opposite direction can be in this CIPO level as well. (4) The remainings are labeled as Level 4, <ref type="figure">Fig. 11</ref>. Visualization example of lane annotation in OpenLane dataset which means they are almost unlikely to impact the future path at this moment. They are mainly objects in lanes with far distance, objects out of drivable area, or parked vehicles in our dataset. Examples are provided in <ref type="figure" target="#fig_0">Fig. 12</ref>. <ref type="figure" target="#fig_1">Fig. 13</ref> shows the intermediate results of the generation process of 3D lane labels. However, the above process could have a few problems in some cases, especially in the last step, i.e., smoothing and fitting. Multiple filtering and fitting algorithms are adopted to realize it, while all of them require a set of sorted points. Due to the large curvature, the one-to-one mapping probably does not stand either in x or y direction, thus we could not sort the points directly. Towards this problem, for each image with this circumstance, we simply find an angle to rotate the whole points set, do the filtering and fitting process in the temporary coordinate and rotate back in the end. This method is illustrated in <ref type="figure">Fig. 14</ref>  For both 3D lane datasets, we follow the evaluation metric designed by Gen-LaneNet <ref type="bibr" target="#b18">[20]</ref>, with small modifications 2 and additional category accuracy on OpenLane dataset. The matching between prediction and ground truth is built upon edit distance, where one predicted lane is considered to be a true positive only if 75% of its covered y-positions have a point-wise distance less than the max-allowed distance (1.5m). Then, with the percentage of matched groundtruth lanes as recall and the percentage of matched prediction lanes as precision, we use F-score to report the regression performance of such a model. Since OpenLane dataset has category information per lane, we present the accuracy upon the matched lanes to show classification performance. We only report the accuracy of PersFormer on OpenLane dataset, as other 3D methods do not , which a filtering algorithm is not applicable directly; (c) A simple translation and rotation can result in a one-to-one mapping of x and y support classification task. For the 2D task, the classical metric in CULane <ref type="bibr" target="#b42">[44]</ref> is adopted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 3D lane Generation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Implementation Details</head><p>To fairly compare with other methods <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b35">37]</ref>, we retain many model settings of image resolution and BEV scale. We resize the original image to 360 ? 480 as model input, project it to BEV space with a resolution of 208 ? 108. We use PyTorch <ref type="bibr" target="#b43">[45]</ref> to implement the model. The batch size is set to 8; the number of training epochs is set to 100. We re-implement 3D-LaneNet and Gen-LaneNet on OpenLane dataset for a fair comparison. Following previous experience on training vision transformer <ref type="bibr" target="#b7">[8,</ref><ref type="bibr">72,</ref><ref type="bibr" target="#b59">62]</ref>, we use Adam optimizer <ref type="bibr" target="#b25">[27]</ref> with base learning rate of 2?10 ?4 , ? 1 = 0.9, ? 2 = 0.999 and weight decay of 10 ?4 . All of these models are trained on 8 NVIDIA Tesla V100 GPUs. More details about environment setup can be referred to our GitHub repository once accepted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 More Experimental Results</head><p>In this section, we present more experimental results, mainly in 3D comparison on ONCE-3DLanes, additional ablations and more qualitative examples.</p><p>3D Comparisons on ONCE-3DLanes. We provide additional experimental results on ONCE-3DLanes dataset [65], as it's another real-world 3D lane dataset concurrently presented. ONCE-3DLanes also uses F-Score as the evaluation metric, and more details can be found in their repo ONCE-3DLanes. In Tab. 8, PersFormer gets the highest F-Score on the validation set, outperforming its proposed method SALAD [65] over 10%. One thing worth noticing is that ONCE-3DLanes does not provide camera extrinsics, therefore PersFormer predefine a set of extrinsic parameters to fit the model setting. The camera height is set to be 1.5m and pitch to be 0.5. This does not affect the evaluation results since it is just to fit the IPM process in PersFormer.</p><p>Ablations. We provide an additional ablative study on the structure of the feature transformation module on a subset of OpenLane (?300 segments) in Tab. 9. We argue that the IPM-based cross attention is a necessity in PersFormer, as we compare it with two initial designs, naive one-to-one mapping and the learned mapping. The naive one-to-one mapping simply scales every location in the BEV space to the corresponding location in the front view space, not considering camera parameters (Exp.1). A more "aggressive" way to simulate the mapping is directly learning from the front view feature with several fullyconnected layers (Exp.2). Neither of them could catch up with the performance of IPM-based mapping, indicating the importance of such a prior in generating BEV feature. We further attempt to adopt Multi-scale Deformable Attention from [72] to implement a several-for-one feature mapping from multi-scale front view feature to multi-scale BEV feature (Exp.3), just like Deformable DETR. The result slightly falls behind our final design (Exp.5), probably due to the influence of tuning of hyper-parameters and the impact of the small-scale feature on the large-scale feature. Finally, we try to remove the classical self attention Visualization. We provide qualitative results compared with SOTA 3D lane detection methods in different evaluation scenarios on OpenLane dataset in <ref type="figure" target="#fig_2">Fig.  15,16</ref>. Results on Apollo 3D synthetic dataset are shown in <ref type="figure">Fig. 17</ref>. We can observe that PersFormer could achieve higher accuracy and capture more lanes to reconstruct the scenes on both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E License of Assets</head><p>OpenLane dataset is based on the Waymo Open Dataset <ref type="bibr" target="#b52">[54]</ref> and therefore we distribute the data under Creative Commons Attribution-NonCommercial-ShareAlike license and Waymo Dataset License Agreement for Non-Commercial Use (August 2019). You are free to share and adapt the data, but have to give appropriate credit and may not use the work for commercial purposes. All code of PersFormer and OpenLane toolkit is under Apache License 2.0. The pretrained ResNet model weights are under the MIT license. We integrate part of the code of Deformable-DETR [72] and Gen-LaneNet <ref type="bibr" target="#b18">[20]</ref> which are under Apache License 2.0. We also use part of the code of LaneATT <ref type="bibr" target="#b53">[55]</ref> which is under the MIT license.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Outlook</head><p>As OpenLane is built upon Waymo Open Dataset <ref type="bibr" target="#b52">[54]</ref>, a road-object joint detection framework is possible in the future. Moreover, BEV is the necessity in the future of autonomous driving, and how to design a better BEV representation remains to be explored. The proposed PersFormer may also be adapted to new tasks. <ref type="figure" target="#fig_3">Fig. 16</ref>. Qualitative results of PersFormer(a), 3D-LaneNet(b) <ref type="bibr" target="#b14">[16]</ref>, and Gen-LaneNet(c) <ref type="bibr" target="#b18">[20]</ref> on OpenLane. Extreme weather case, Intersection case and Merge&amp;Split case</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Our proposed PersFormer pipeline. The core is to learn a spatial feature transformation from front view to BEV space so that the generated BEV features at target point would be more representative by attending local context around reference point. PersFormer consists of the self-attention module to interact with its own BEV queries; the cross-attention module that takes the key-value pair from the IPM-based front view features to generate fine-grained BEV feature</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Generation of keys in the cross attention. Point (x, y) in BEV space casts the corresponding point (u, v) in front view through intermediate state (x</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Annotation samples of OpenLane compared with other lane datasets. Open-Lane is challenging with more lane categories per frame in average and has rich labels including scene, weather, hours, CIPO</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Qualitative results of PersFormer(a), 3D-LaneNet(b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>64. Xu, H., Wang, S., Cai, X., Zhang, W., Liang, X.,Li, Z.: Curvelane-nas: Unifying lane-sensitive architecture search and adaptive point blending. In: ECCV (2020) 4, 10, 19 65. Yan, F., Nie, M., Cai, X., Han, J., Xu, H., Yang, Z., Ye, C., Fu, Y., Mi, M.B., Zhang, L.: Once-3dlanes: Building monocular 3d lane detection. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 17143-17152 (June 2022) 19, 29 66. Yan, F., Nie, M., Cai, X., Han, J., Xu, H., Yang, Z., Ye, C., Fu, Y., Michael, B.M., Zhang, L.: Once-3dlanes: Building monocular 3d lane detection. In: CVPR (2022) 10 67. Yang, W., Li, Q., Liu, W., Yu, Y., Ma, Y., He, S., Pan, J.: Projecting your view attentively: Monocular road scene layout estimation via cross-view transformation.In: CVPR (2021) 4 68. Yin, J., Shen, J., Guan, C., Zhou, D., Yang, R.: Lidar-based online 3d video object detection with graph-based message passing and spatiotemporal transformer attention. In: CVPR (2020) 4 69. Yu, F., Chen, H., Wang, X., Xian, W., Chen, Y., Liu, F., Madhavan, V., Darrell, T.: Bdd100k: A diverse driving dataset for heterogeneous multitask learning. In: CVPR (2020) 4, 10, 19 70. Yu, Z., Ren, X., Huang, Y., Tian, W., Zhao, J.: Detecting lane and road markings at a distance with perspective transformer layers. In: ITSC (2020) 2 71. Zhang, Y., Zhu, L., Feng, W., Fu, H., Wang, M., Li, Q., Li, C., Wang, S.: Vil-100:A new dataset and a baseline model for video instance lane detection. In: ICCV (2021) 10, 19 72. Zhu, X., Su, W., Lu, L., Li, B., Wang, X., Dai, J.: Deformable DETR: Deformable transformers for end-to-end object detection. In: ICLR (2021) 3, 7, 28, 29, 30</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>t c u r b s i d e R i g h t c u r b s i d e O t h e r s</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 12 .</head><label>12</label><figDesc>Visualization example of CIPO and Scene tags annotation in OpenLane dataset D Experiments D.1 Evaluation Metrics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 13 .Fig. 14 .</head><label>1314</label><figDesc>3D lane generation pipeline. (a) Original point clouds inside a certain threshold of 2D lane annotations are reserved, which is relatively sparse; (b) Positions of points on the 2D annotation are interpolated to get a dense point set; (c) 3D lane points in the same segment are spliced into long, high-density lanes; (d) We remove those too far as they are invisible, while reasonable extensions are desired; (e) A smooth and fitting process is applied to get the final 3D lane annotation Illustration of 3D lane generation problem with large curvatures. (a) The original image and the 2D lane; (b) Unsorted 3D points set of the lane in (a)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparison of OpenLane with existing benchmarks. "Avg. Length" denotes the average time duration of segments. "Inst. Anno." indicates whether lanes are annotated instance-wise (c.f. semantic-wise). "Track. Anno." implies if a lane has a unique tracking ID. Numbers in '#Frames' are the number of annotated frames / total frames respectively. Details of "Scenario" can be found in Appendix</figDesc><table><row><cell>Dataset</cell><cell cols="2">#Segments #Frames</cell><cell>Avg. Length</cell><cell>Inst. Anno.</cell><cell>Track. Anno.</cell><cell>Max #Lanes</cell><cell>Line Category</cell><cell>Scenario</cell></row><row><cell>Caltech Lanes [2]</cell><cell>4</cell><cell>1224/1224</cell><cell>-</cell><cell>?</cell><cell>?</cell><cell>4</cell><cell>-</cell><cell>Easy</cell></row><row><cell>TuSimple [58]</cell><cell>6.4K</cell><cell>6.4K/128K</cell><cell>1s</cell><cell>?</cell><cell>?</cell><cell>5</cell><cell>-</cell><cell>Easy</cell></row><row><cell>3D Synthetic [20]</cell><cell>-</cell><cell>10K/10K</cell><cell>-</cell><cell>?</cell><cell>-</cell><cell>6</cell><cell>-</cell><cell>Easy</cell></row><row><cell>VIL-100 [71]</cell><cell>100</cell><cell>10K/10K</cell><cell>10s</cell><cell>?</cell><cell>?</cell><cell>6</cell><cell>10</cell><cell>Medium</cell></row><row><cell>VPG [29]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .Table 3 .</head><label>23</label><figDesc>Comparison with other open-sourced 3D methods on OpenLane. PersFormer achieves the best F-Score on the entire validation set and every scenario set Comparison with state-of-the-art 2D method on OpenLane. The result from the 2D head of PersFormer also achieves competitive performance</figDesc><table><row><cell>Method</cell><cell>All</cell><cell>Up &amp; Down</cell><cell>Curve</cell><cell>Extreme Weather</cell><cell cols="2">Night Intersection</cell><cell>Merge &amp; Split</cell></row><row><cell>3D-LaneNet [16]</cell><cell>44.1</cell><cell>40.8</cell><cell>46.5</cell><cell>47.5</cell><cell>41.5</cell><cell>32.1</cell><cell>41.7</cell></row><row><cell>Gen-LaneNet [20]</cell><cell>32.3</cell><cell>25.4</cell><cell>33.5</cell><cell>28.1</cell><cell>18.7</cell><cell>21.4</cell><cell>31.0</cell></row><row><cell>PersFormer (ours)</cell><cell>50.5</cell><cell>42.4</cell><cell>55.6</cell><cell>48.6</cell><cell>46.6</cell><cell>40.0</cell><cell>50.7</cell></row><row><cell>Method</cell><cell>All</cell><cell>Up &amp; Down</cell><cell>Curve</cell><cell>Extreme Weather</cell><cell cols="2">Night Intersection</cell><cell>Merge &amp; Split</cell></row><row><cell>LaneATT-S [55]</cell><cell>28.3</cell><cell>25.3</cell><cell>25.8</cell><cell>32.0</cell><cell>27.6</cell><cell>14.0</cell><cell>24.3</cell></row><row><cell>LaneATT-M [55]</cell><cell>31.0</cell><cell>28.3</cell><cell>27.4</cell><cell>34.7</cell><cell>30.2</cell><cell>17.0</cell><cell>26.5</cell></row><row><cell>CondLaneNet-S [35]</cell><cell>52.3</cell><cell>55.3</cell><cell>57.5</cell><cell>45.8</cell><cell>46.6</cell><cell>48.4</cell><cell>45.5</cell></row><row><cell>CondLaneNet-M [35]</cell><cell>55.0</cell><cell>58.5</cell><cell>59.4</cell><cell>49.2</cell><cell>48.6</cell><cell>50.7</cell><cell>47.8</cell></row><row><cell>CondLaneNet-L [35]</cell><cell>59.1</cell><cell>62.1</cell><cell>62.9</cell><cell>54.7</cell><cell>51.0</cell><cell>55.7</cell><cell>52.3</cell></row><row><cell>PersFormer (ours)</cell><cell>42.0</cell><cell>40.7</cell><cell>46.3</cell><cell>43.7</cell><cell>36.1</cell><cell>28.9</cell><cell>41.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table><row><cell>Method</cell><cell>F-Score</cell><cell>Category Accuracy</cell><cell cols="4">X error near X error far Z error near Z error far</cell></row><row><cell>3D-LaneNet [16]</cell><cell>44.1</cell><cell>-</cell><cell>0.479</cell><cell>0.572</cell><cell>0.367</cell><cell>0.443</cell></row><row><cell>Gen-LaneNet [20]</cell><cell>32.3</cell><cell>-</cell><cell>0.591</cell><cell>0.684</cell><cell>0.411</cell><cell>0.521</cell></row><row><cell>Cond-IPM  *</cell><cell>36.6</cell><cell>-</cell><cell>0.563</cell><cell>1.080</cell><cell>0.421</cell><cell>0.892</cell></row><row><cell>PersFormer (ours)</cell><cell>50.5</cell><cell>92.3</cell><cell>0.485</cell><cell>0.553</cell><cell>0.364</cell><cell>0.431</cell></row></table><note>Comprehensive 3D Lane evaluation under different metrics. On the strength of unified anchor design, PersFormer outperforms previous 3D methods on the metrics of far error while retains comparable results on near error (m).* denotes projecting 2D lane results from CondLaneNet [35] to BEV using IPMprevious SOTA methods in varying degrees. In Tab. 3, PersFormer outperforms LaneATT</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table><row><cell>Scene</cell><cell>Method</cell><cell cols="5">F-Score X error near X error far Z error near Z error far</cell></row><row><cell></cell><cell>3D-LaneNet [16]</cell><cell>86.4</cell><cell>0.068</cell><cell>0.477</cell><cell>0.015</cell><cell>0.202</cell></row><row><cell></cell><cell>Gen-LaneNet [20]</cell><cell>88.1</cell><cell>0.061</cell><cell>0.496</cell><cell>0.012</cell><cell>0.214</cell></row><row><cell>Balanced</cell><cell>3D-LaneNet(l/att) [26]</cell><cell>91.0</cell><cell>0.082</cell><cell>0.439</cell><cell>0.011</cell><cell>0.242</cell></row><row><cell>Scenes</cell><cell>Gen-LaneNet(l/att) [26]</cell><cell>90.3</cell><cell>0.080</cell><cell>0.473</cell><cell>0.011</cell><cell>0.247</cell></row><row><cell></cell><cell>CLGo [37]</cell><cell>91.9</cell><cell>0.061</cell><cell>0.361</cell><cell>0.029</cell><cell>0.250</cell></row><row><cell></cell><cell>PersFormer (ours)</cell><cell>92.9</cell><cell>0.054</cell><cell>0.356</cell><cell>0.010</cell><cell>0.234</cell></row><row><cell></cell><cell>3D-LaneNet [16]</cell><cell>72.0</cell><cell>0.166</cell><cell>0.855</cell><cell>0.039</cell><cell>0.521</cell></row><row><cell></cell><cell>Gen-LaneNet [20]</cell><cell>78.0</cell><cell>0.139</cell><cell>0.903</cell><cell>0.030</cell><cell>0.539</cell></row><row><cell>Rarely</cell><cell>3D-LaneNet(l/att) [26]</cell><cell>84.1</cell><cell>0.289</cell><cell>0.925</cell><cell>0.025</cell><cell>0.625</cell></row><row><cell>Observed</cell><cell>Gen-LaneNet(l/att) [26]</cell><cell>81.7</cell><cell>0.283</cell><cell>0.915</cell><cell>0.028</cell><cell>0.653</cell></row><row><cell></cell><cell>CLGo [37]</cell><cell>86.1</cell><cell>0.147</cell><cell>0.735</cell><cell>0.071</cell><cell>0.609</cell></row><row><cell></cell><cell>PersFormer (ours)</cell><cell>87.5</cell><cell>0.107</cell><cell>0.782</cell><cell>0.024</cell><cell>0.602</cell></row><row><cell></cell><cell>3D-LaneNet [16]</cell><cell>72.5</cell><cell>0.115</cell><cell>0.601</cell><cell>0.032</cell><cell>0.230</cell></row><row><cell></cell><cell>Gen-LaneNet [20]</cell><cell>85.3</cell><cell>0.074</cell><cell>0.538</cell><cell>0.015</cell><cell>0.232</cell></row><row><cell>Vivual</cell><cell>3D-LaneNet(l/att) [26]</cell><cell>85.4</cell><cell>0.118</cell><cell>0.559</cell><cell>0.018</cell><cell>0.290</cell></row><row><cell>Variants</cell><cell>Gen-LaneNet(l/att) [26]</cell><cell>86.8</cell><cell>0.104</cell><cell>0.544</cell><cell>0.016</cell><cell>0.294</cell></row><row><cell></cell><cell>CLGo [37]</cell><cell>87.3</cell><cell>0.084</cell><cell>0.464</cell><cell>0.045</cell><cell>0.312</cell></row><row><cell></cell><cell>PersFormer (ours)</cell><cell>89.6</cell><cell>0.074</cell><cell>0.430</cell><cell>0.015</cell><cell>0.266</cell></row></table><note>Comparison with previous 3D methods on Apollo 3D Lane Synthetic. Pers- Former achieves best F-Score on every scene set with comparable X/Z error (m)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 .</head><label>7</label><figDesc>Statics of scenario tags. Scene tags are annotated in terms of segments</figDesc><table><row><cell></cell><cell>Tags</cell><cell>Train</cell><cell>Val.</cell><cell>All</cell></row><row><cell></cell><cell>Clear</cell><cell>515</cell><cell>145</cell><cell>660</cell></row><row><cell></cell><cell>Partly cloud</cell><cell>131</cell><cell>28</cell><cell>159</cell></row><row><cell>Weather</cell><cell>Overcast</cell><cell>33</cell><cell>8</cell><cell>41</cell></row><row><cell></cell><cell>Rainy</cell><cell>107</cell><cell>18</cell><cell>125</cell></row><row><cell></cell><cell>Foggy</cell><cell>12</cell><cell>3</cell><cell>15</cell></row><row><cell></cell><cell>Residential</cell><cell>270</cell><cell>69</cell><cell>339</cell></row><row><cell></cell><cell>Urban</cell><cell>234</cell><cell>56</cell><cell>290</cell></row><row><cell>Scene</cell><cell>Suburbs</cell><cell>259</cell><cell>64</cell><cell>323</cell></row><row><cell></cell><cell>Highway</cell><cell>30</cell><cell>6</cell><cell>36</cell></row><row><cell></cell><cell>Parking lot</cell><cell>5</cell><cell>7</cell><cell>12</cell></row><row><cell></cell><cell>Daytime</cell><cell>653</cell><cell>167</cell><cell>820</cell></row><row><cell>Hours</cell><cell>Night</cell><cell>88</cell><cell>22</cell><cell>110</cell></row><row><cell></cell><cell>Dawn/Dusk</cell><cell>57</cell><cell>13</cell><cell>70</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>The above statistics and examples below demonstrate that OpenLane is the most challenging one compared to existing lane detection datasets.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Val, Foggy, 1%</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Val, Rainy, 9%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Val, Overcast, 4%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Train, Foggy, 2%</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Train, Rainy, 13%</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Val, Partly cloud,</cell><cell></cell><cell cols="3">Train, Overcast, 4%</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>14%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">Train, Partly cloud,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">16%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Train, Clear, 65%</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Val, Clear, 72%</cell></row><row><cell cols="2">Clear</cell><cell cols="2">Partly cloud</cell><cell cols="2">Overcast</cell><cell>Rainy</cell><cell></cell><cell>Foggy</cell></row><row><cell cols="10">Fig. 7. Distribution of weather tags in training and validation sets. The data is collected</cell></row><row><cell cols="10">under different weathers and split into training and validation with great balance</cell></row><row><cell>6, 8.735%</cell><cell cols="2">7, 7.571%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>5, 10.420%</cell><cell></cell><cell cols="3">8, 6.917% 9, 4.382%</cell><cell></cell><cell></cell><cell></cell><cell>15, 0.159%</cell><cell>16, 0.055% 17, 0.055% 18, 0.025%</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">10, 2.915%</cell><cell></cell><cell></cell><cell></cell><cell>19, 0.063%</cell></row><row><cell>4, 9.633%</cell><cell></cell><cell></cell><cell cols="3">11, 1.508% 12, 1.176%</cell><cell></cell><cell></cell><cell></cell><cell>20, 0.022% 21, 0.007% 22, 0.013%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">14, 0.387%</cell><cell></cell><cell></cell><cell>23, 0.015%</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">0, 8.133%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>24, 0.003%</cell></row><row><cell>3, 10.520%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">1, 9.525%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>13, 0.490%</cell></row><row><cell>2, 17.274%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell></row><row><cell>9</cell><cell>10</cell><cell>11</cell><cell>12</cell><cell>13</cell><cell>14</cell><cell>15</cell><cell>16</cell><cell>17</cell></row><row><cell cols="10">Fig. 8. Distribution of lane numbers per frame. The maximum number is 24, and 25%</cell></row><row><cell cols="2">frames have more than 6 lane</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Distribution of the lane category. Here we abbreviate single in 1, double in 2, white in W, yellow in Y, left in L, and right in R. Thus 1-W dash means the category of single white dash lanesFig. 10. Altitude difference per frame. Note the x-axis is approximately in a log scale and its unit is m lanes in each frame. However, due to the complexity of scenarios, there exist some special cases we seek to illustrate here. (1) Lanes are often occluded by objects or invisible because of abrasion but they are still valuable for the real application. Thus we annotate lanes if parts of them are visible, meaning lanes with one side being occluded are extended or lanes with invisible intermediate parts are completed according to the context, as shown in Fig. 11. (2) It is very common that the number of lanes changes, especially when lanes have complex topologies such as fork lanes in merge and split cases. Traditional lane datasets usually omit these scenarios for simplicity, while we keep them all and further choose them out of the whole dataset for evaluation. Fork lanes are annotated as separate lanes with a common starting point (split) or ending point (merge) -two close adjacent lanes are desired for the lane detection methods. (3) We further annotate each lane as one of the 14 lane categories, i.e., single white dash, single white solid, double white dash, double white solid, double white dash solid (left</figDesc><table><row><cell>white dash with right white solid), double white solid dash (left white solid with right white dash), single yellow dash, single yellow solid, double yellow dash, Lane Number double yellow solid, double yellow dash solid (left yellow dash with right yellow</cell></row><row><cell>solid), double yellow solid dash (left yellow solid with right yellow dash), left</cell></row><row><cell>curbside, right curbside. Note that traffic bollards are considered as curbsides</cell></row><row><cell>as well if they are not temporally placed. (4) Different from all the other lane</cell></row><row><cell>datasets, we annotate a tracking ID for each lane which is unique across the</cell></row><row><cell>whole segment. We believe this could be helpful for video lane detection or lane</cell></row><row><cell>Lane category tracking tasks. We also assign a number in 1-4 to the most important 4 lanes</cell></row><row><cell>0 1,000 2,000 3,000 4,000 5,000 6,000 based on their relative position to the ego-vehicle. Basically, the left-left lane is Frame Number 4.89 0.84 2.21 9.81 19.89 Fig. 9. Altitude Difference 1, the left lane is 2, the right lane is 3, and the right-right lane is 4.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 .</head><label>8</label><figDesc>New results on the new benchmark (CVPR22) ONCE-3DLanes [65]. * denotes results from the paper [65]</figDesc><table><row><cell>Method</cell><cell cols="4">F-Score(%) Precision(%) Recall(%) CD error(m)</cell></row><row><cell>3D-LaneNet  *  [16]</cell><cell>44.73</cell><cell>61.46</cell><cell>35.16</cell><cell>0.127</cell></row><row><cell>Gen-LaneNet  *  [20]</cell><cell>45.59</cell><cell>63.95</cell><cell>35.42</cell><cell>0.121</cell></row><row><cell>SALAD  *  [65]</cell><cell>64.07</cell><cell>75.90</cell><cell>55.42</cell><cell>0.098</cell></row><row><cell>PersFormer (ours)</cell><cell>74.33</cell><cell>80.30</cell><cell>69.18</cell><cell>0.074</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 .</head><label>9</label><figDesc>Ablative Study on PersFormer Design. IPM prior plays a vital role in guiding the generation of BEV feature compared to naive one-to-one mapping and learned reference-target mapping. Using MSDeformAttn from Deformable DETR [72] to map multi-scale front-view feature to multi-scale BEV feature is competitive, and the selfattention module of BEV query is important in Transformer-style structure</figDesc><table><row><cell>Exp.</cell><cell>Naive 1-1</cell><cell>Learned</cell><cell>Multi-to-Multi</cell><cell>Self Attn.</cell><cell>IPM Prior</cell><cell>3D F-Score</cell></row><row><cell>1</cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>36.15</cell></row><row><cell>2</cell><cell></cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell>13.45</cell></row><row><cell>3</cell><cell></cell><cell></cell><cell>?</cell><cell></cell><cell></cell><cell>51.35</cell></row><row><cell>4</cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell></cell><cell>47.18</cell></row><row><cell>5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell>52.68</cell></row></table><note>module in ordinary Transformer design (Exp.4), showing that the self attention module is all there for a reason in Transformer-style structure.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We define the height of lane line z to be the relative height concerning the zero point in the ego vehicle coordinate system (x, y, z) in BEV 3D space. The coordinate of the perspective (front view) 2D space in the image plane is referred to as (u, v).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Please see in OpenLane page: https://github.com/OpenPerceptionX/OpenLane.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Fig. 17. Qualitative results of PersFormer(a), 3D-LaneNet(b)<ref type="bibr" target="#b14">[16]</ref>, and Gen-LaneNet(c)<ref type="bibr" target="#b18">[20]</ref> on Apollo. Curve case and Up&amp;Down case</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The project is partially supported by the Shanghai Committee of Science and Technology (Grant No. 21DZ1100100). This work was supported in part by National Key Research and Development Program of China (2020AAA0107600), Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102). We would like to acknowledge the great support from SenseBee labelling team at SenseTime Research, constructive contribution from Zihan Ding at BUAA, and the fruitful discussions and comments for this project from Zhiqi Li, Yuenan Hou, Yu Liu, Jing Shao, Jifeng Dai.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Laneaf: Robust multi-lane detection with affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Abualsaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Situ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rangesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
	<note>RA-L (2021) 2, 8</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Real time detection of lane markers in urban streets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>IV</publisher>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep multi-sensor lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mattyus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Homayounfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Lakshmikanth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Unsupervised labeled lane markers using maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Behrendt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soussan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Stereovision-based 3d lane detection system: a model driven approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Benmansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Labayrade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Aubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Glaser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>ITSC</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">nuscenes: A multimodal dataset for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">E</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Structured bird&apos;s-eye-view traffic scene understanding from onboard images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">B</forename><surname>Can</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liniger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Paudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (2020)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Argoverse: 3d tracking and forecasting with rich maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sangkloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hartnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Opendenselane: a new lidar-based dataset for hd map construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME (2022)</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Pointlanenet: Efficient end-to-end cnns for accurate real-time lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IV</publisher>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Neat: Neural attention fields for end-to-end autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chitta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICLR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Vectornet: Encoding hd maps and agent dynamics from vectorized representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">3d-lanenet: End-to-end 3d multiple lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pe&amp;apos;er</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lahav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Levi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Synthetic-to-real domain adaptation for lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Uziel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Efrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Levi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Densetnt: End-to-end trajectory prediction from dense goal sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">M3detr: Multi-representation, multi-scale, mutual-relation 3d object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>WACV</publisher>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Genlanenet: A generalized and scalable approach for 3d lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Choe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Multiple View Geometry in Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">I</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>second edn.</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning lightweight lane detection cnns by self attention distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The apolloscape open dataset for autonomous driving and its application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spatial transformer networks</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Swiftlane: Towards fast and efficient lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Jayasinghe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anhettigama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hemachandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kariyawasam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jayasekara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICMLA</title>
		<imprint>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Robust monocular 3d lane detection with dual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<editor>Bengio, Y., LeCun, Y.</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Omnidet: Surround view cameras based multi-task visual perception network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yogamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rashed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sitsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Witt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Leang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Milz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>M?der</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>RA-L (2021) 8</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Vpgnet: Vanishing point guided network for lane and road marking detection and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bailo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>So Kweon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Line-cnn: End-to-end traffic line detection with line proposal unit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-ITS</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Bevformer: Learning bird&apos;s-eye-view representation from multi-camera images via spatiotemporal transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.17270</idno>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Road lane detection with gabor filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ISAI</publisher>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Multi-task multi-sensor fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Condlanenet: a top-to-down lane detection framework based on conditional convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2021)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Monocular bev perception with transformers in autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://towardsdatascience.com/monocular-bev-perception-with-transformers-in-autonomous-driving-c41e4a893944" />
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning to predict 3d lane shape and camera pose from a single image via geometry constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI (2022)</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep semantic lane segmentation for mapless driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">O</forename><surname>Salscheider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Orzechowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">lane detection system based on stereovision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nedevschi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Graf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Danescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Frentiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Marita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Oniga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pocol</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>ITSC</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Towards end-to-end lane detection: an instance segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Scene transformer: A unified architecture for predicting future trajectories of multiple agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T L</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Venugopal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICLR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Spatial as deep: Spatial cnn for traffic scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>AAAI</publisher>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">28</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multi-modal fusion transformer for end-to-end autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chitta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2021)</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Ultra fast structure-aware deep lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Focus on local: Detecting lane marker from bottom up via key point</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2021)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>MICCAI</publisher>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Translating images into maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Maldonado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA (2022)</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Lio-sam: Tightly-coupled lidar inertial odometry via smoothing and mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Englot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ratti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Daniela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Lvi-sam: Tightly-coupled lidar-visualinertial odometry via smoothing and mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Englot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ratti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Daniela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Structure guided lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kretzschmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dotiwalla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chouard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Patnaik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caine</surname></persName>
		</author>
		<title level="m">Scalability in perception for autonomous driving: Waymo open dataset. In: CVPR (2020) 9</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Keep your eyes on the lane: Real-time attention-guided lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tabelini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Berriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Paixao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Badue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>De Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Oliveira-Santos</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
	<note>In: CVPR (2021) 2, 6, 8, 12, 19</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ICML</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Multi-task learning for dense prediction tasks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">An approach of lane detection based on inverse perspective mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ITSC</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Detr3d: 3d object detection from multi-view images via 3d-to-2d queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">C</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoRL (2022)</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<title level="m">Convolutional pose machines</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">3D-LaneNet(b) [16], and Gen-LaneNet(c) [20] on OpenLane</title>
	</analytic>
	<monogr>
		<title level="m">Fig. 15. Qualitative results of PersFormer(a)</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
