<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HARMONICITY PLAYS A CRITICAL ROLE IN DNN BASED VERSUS IN BIOLOGICALLY-INSPIRED MONAURAL SPEECH SEGREGATION SYSTEMS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahil</forename><surname>Parikh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Systems Research</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Kavalerov</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Google Inc</orgName>
								<address>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carol</forename><surname>Espy-Wilson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Systems Research</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihab</forename><surname>Shamma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Systems Research</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">HARMONICITY PLAYS A CRITICAL ROLE IN DNN BASED VERSUS IN BIOLOGICALLY-INSPIRED MONAURAL SPEECH SEGREGATION SYSTEMS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-speech segregation</term>
					<term>Conv-TasNet</term>
					<term>har- monic structure</term>
					<term>temporal coherence</term>
					<term>adversarial inputs</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advancements in deep learning have led to drastic improvements in speech segregation models. Despite their success and growing applicability, few efforts have been made to analyze the underlying principles that these networks learn to perform segregation. Here we analyze the role of harmonicity on two state-of-the-art Deep Neural Networks (DNN)-based models-Conv-TasNet and DPT-Net <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. We evaluate their performance with mixtures of natural speech versus slightly manipulated inharmonic speech, where harmonics are slightly frequency jittered. We find that performance deteriorates significantly if one source is even slightly harmonically jittered, e.g., an imperceptible 3% harmonic jitter degrades performance of Conv-TasNet from 15.4 dB to 0.70 dB. Training the model on inharmonic speech does not remedy this sensitivity, instead resulting in worse performance on natural speech mixtures, making inharmonicity a powerful adversarial factor in DNN models. Furthermore, additional analyses reveal that DNN algorithms deviate markedly from biologically inspired algorithms [3] that rely primarily on timing cues and not harmonicity to segregate speech.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The 'cocktail party problem' is the task of attending to a source of interest, usually speech, in a complex acoustic environment with concurrent sounds. Despite the apparent ease with which humans can group acoustic cues from such an environment and organize them to meaningfully perceive them <ref type="bibr" target="#b3">[4]</ref>, the complexity of this problem has inspired generations of neuroscientists, psychologists and engineers. The multi-disciplinary nature of this problem has motivated Computational Auditory Scene Analysis (CASA) algorithms using traditional pitch modulations extractions <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref> and biologically-inspired systems which are based on frameworks such as 'temporal coherence' (TC) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8]</ref>. In contrast to these approaches, there are strictly engineering frameworks such as signal factorization, statistical-speech modelling <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> and recently deep learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref> which ignore biological plausibility. Advances in deep learning have facilitated drastic improvements in speech segregation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref>.</p><p>These models can now be used at scale, to separate individual speech from mixtures, enabling downstream audio processing tasks such as Automatic Speech Recognition (ASR), allowing us to use speech technologies in the real-world. These model architectures have also shown promising results in music segregation <ref type="bibr" target="#b16">[17]</ref> and sound segregation <ref type="bibr" target="#b17">[18]</ref>. Despite their unquestionable success, no work has been done to investigate the underlying principles they learn to perform grouping and consequently segregation.</p><p>We attempt to bridge the gap between our understanding of traditional CASA systems, biologically inspired algorithms and deep-learning systems by investigating the role of harmonicity on the latter. Since harmonicity of natural speech is linked with pitch extraction, it is a crucial cue for grouping in certain CASA systems <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7]</ref>. However, biologically plausible algorithms based on TC depend solely on the temporal correlation of features from the same source and are capable of performing segregation regardless of harmonicity <ref type="bibr" target="#b2">[3]</ref>. While recent findings in <ref type="bibr" target="#b18">[19]</ref> found that human subjects perform worse in segregating inharmonic acoustic mixtures, performance deficits were relatively modest and often attributable to source assignment confusion rather than true segregation difficulty of the overlapping words in the mixtures. In our work, we begin by exploring the importance of harmonicity on two prominent speech segregation models Conv-TasNet <ref type="bibr" target="#b0">[1]</ref> and Dual-Path Transformers (DTP) Net <ref type="bibr" target="#b1">[2]</ref>. We investigate how these models perform on mixtures of harmonic speech, inharmonic speech, and a variety of synthetic simple stimuli that exemplify these properties more clearly. We also extend this analysis to the same models when trained on inharmonic speech. We find that-? DNN based segregation systems trained on natural speech fail to segregate inharmonic speech. The performance of DPT-Net degrades from 20.2 dB to 0.7 dB for imperceptible amounts of harmonic jitter.</p><p>? DNNs find it hard to learn to segregate sources when trained on inharmonic speech, instead becoming worse at segregating natural speech.</p><p>? They differ from biologically inspired systems in heavily relying on harmonic patterns to segregate speech. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Monaural Speech Segregation Models</head><p>Monaural speech segregation involves estimating C sources s 1 (t), ? ? ? , s C (t) for an audio mixture x mix (t) = C i=1 s i . In this work we focus on end-to-end speech segregation models <ref type="bibr" target="#b13">[14]</ref> which share a similar pipeline-the input mixture is encoded using a learnt analysis transform. The segregation network generates C masks for this encoded input. The masked input is then decoded using a learnt synthesis transform to generate a time-domain estimate for each source. These networks typically use Permutation Invariant Training (PIT) <ref type="bibr" target="#b15">[16]</ref> to maximize the Scale Invariant Source-to-Noise Ratio (SI-SNR) <ref type="bibr" target="#b13">[14]</ref>. In Conv-TasNet the segregation network consists of dilated 1-D convolutional networks, whereas DPTNet consists of a cascade of transformers configured to maximize the temporal receptive field over the encoded mixture. In this work we set C = 2 for simplicity and illustrative purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Inharmonic Sound Sources</head><p>Sounds that originate from a periodic process have a harmonic structure in their Fourier representation, consisting usually of a component at the fundamental frequency (F0) and its integer multiples. Inharmonic sounds instead do not have frequency components at exact integer multiples of F0. We use the modified STRAIGHT algorithm <ref type="bibr" target="#b19">[20]</ref> to generate inharmonic speech. This algorithm models voiced excitations as a sum of multiple time varying sinusoids, the frequencies of which are manipulated using 'jittering', wherein a random offset is added to each harmonic. This offset is bounded by some fraction-J of F0. The n th harmonic is thus modified to: f n (t) = nf 0 (t) + J n f 0 (t); ? J ? J n ? J (1) Since other characteristics of the speech remain unchanged, synthetically generated inharmonic speech is very intelligible <ref type="bibr" target="#b0">1</ref> . To generate inharmonic tone complexes, we sample from</p><formula xml:id="formula_0">x tone (t) = N k=1 A k sin(2?f n (t)t) where f n (t) is</formula><p>given by <ref type="bibr" target="#b0">(1)</ref>. Sources are harmonic at J n = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head><p>To demonstrate the importance of harmonicity as a cue for grouping, we first evaluate performance of Conv-TasNet (trained on native (harmonic) WSJ-2-mix) with mixtures of harmonic or inharmonic complex tones. To extend these results to speech, we evaluate Conv-TasNet and DPT-Net with inharmonic versions of WSJ-2-mix, with varying levels of inharmonicity. We also evaluate these models with mixtures where one speaker is natural and the other is inharmonic. Finally, we train a Conv-TasNet on only inharmonic mixtures. 1 http://labrosa.ee.columbia.edu/projects/inharmonic/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset Construction and Evaluation</head><p>We use the native WSJ-2-mix dataset <ref type="bibr" target="#b11">[12]</ref> created using the WSJ0 corpus to train our models on harmonic speech. We create multiple inharmonic versions of the WSJ0-corpus, by varying the harmonic jitter J in (1) and create their corresponding inharmonic WSJ-2-mix datasets. We change J from 0.01 ? 0.30, corresponding to an average offset in the range of ?1.2 ? ?40 Hz for male speakers and ?2.1 ? ?65 Hz for female speakers. A higher value of J corresponds to a higher deviation from the native source. We also generate versions of the WSJ-2-mix datasets wherein one source is harmonic and the other is inharmonic.</p><p>We evaluate source-segregation performance using the metric signal-to-distortion-ratio improvement (SDRi) <ref type="bibr" target="#b20">[21]</ref>. The results are baselined against the model's performance on the harmonic WSJ-2-mix dataset with J = 0. Since each of the speech datasets contain differently jittered versions of the same samples from WSJ-2-mix, we can fairly compare the evaluation performance across these datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Speech Segregation Models Trained on Natural Speech</head><p>We begin by demonstrating that Conv-TasNet trained on the native WSJ-2-mix relies primarily on harmonicty to perform separation. The test stimuli are mixtures of alternating tone complexes (with no overlap) as illustrated in <ref type="figure" target="#fig_0">Fig 1.</ref> The model This phenomenon applies equally when the pair of stimuli are very different as in <ref type="figure" target="#fig_1">Fig.2</ref>, where one is a tone complex and the other is speech. The segregation is perfectly effective when both are harmonic (left panels) while it fails when one of the sources (the tone complex) is inharmonic (right panels) in spite of the speech being harmonic. Harmonicity may also be a transient phenomenon during speech mixtures due to the overlap of harmonic components. To illustrate the immediate sensitivity of the DNN models to the emergence (or absence) of harmonicity, we illustrate in <ref type="figure">Fig.3</ref> two alternating mixtures with 50% overlap. In the left panels, both tone complexes are completely harmonic (F0=100 and 190). On the right, the complexes are inharmonic with frequencies 200, 600 Hz and 100, 300, 500 Hz. During the intervals of overlap, the effective instantaneous spectrum becomes the sum of the two complexes. On the left, the harmonic patterns still exist in the mixture, and hence are extracted out and segregated perfectly. But in the right panels, the tone mixture strongly resemble the harmonics of F0=100 Hz. The DNN model incorrectly groups this harmonic series of tones as one source, while the two individually inharmonic series become the other source. This demonstrates the strong dependence of the model on the harmonicity of the spectrum.</p><p>These tonal stimuli illustrate the vulnerabilities that affect models when harmonicity is disturbed. To extend our analysis to speech mixtures we evaluate the models using mixtures of: two inharmonic speakers (I+I), and one harmonic and one inharmonic speaker (H+I), using datasets discussed in Sec. 3.1. The results in <ref type="figure">Fig.4(a)</ref> illustrate that the models' performance degrades severely, especially on mixtures of two inharmonic speakers-15 dB on natural speech to 0.7 dB for ConvTasnet and 20 dB to 0.7 dB for DPT-Net, for J = 0.03, which reveals a remarkable vulnerability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Training DNN-Models with Inharmonic Speech</head><p>An important question is whether these DNN models can be made more robust to inharmonicity by training them on inharmonic speech. Here, we trained the Conv-TasNet on the WSJ- 2-mix datasets with both inharmonic speakers. We note that the model finds it more challenging to learn to segregate with increasing inharmonicty. Our results from a model trained only on inharmonic data with J = 30% are illustrated in <ref type="figure">Fig.  4(b)</ref>. In spite of being trained on data from the same distribution, the performance of the network drops from 15.3 dB to 10.6 dB. Interestingly, this network now performs poorly on harmonic and mildly inharmonic speech. Furthermore, when the model is trained on a combination of natural and inharmonic mixtures, it still finds it hard to segregate inharmonic mixtures but learns to segregate natural speech. We believe that learning to segregate inharmonic mixtures is a harder problem since the space of inharmonic patterns is very large, while that of harmonic patterns is sparse. This makes it difficult to cover all possibilities during training. These results underscore that the networks rely strongly on the constrained harmonic structure for segregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Discussion</head><p>Our experiments indicate that end-to-end models trained on natural speech are incapable of segregating an inharmonic source. Furthermore, we have shown that the models cannot even be trained to segregate inharmonic mixtures. These results provide clear evidence that DNN-based speech segregation is heavily reliant on learning the harmonic patterns that are available in natural speech, but that are absent or rarely available in inharmonic training. Their sensitivity to harmonicity implies the ability to track the F0, or pitch. But they seem to do so from the full spectral set of harmonic patterns and not just from a few harmonics as humans are readily able to do. For instance, in <ref type="figure">Fig.3</ref>, the network could not treat the sparser pattern of tones as part of a larger harmonic pattern, expecting instead to see the full patterns. To further test this, we generate a mixture of alternating harmonic tone complexes with and without the fundamental component and the second harmonic, as illustrated in <ref type="figure" target="#fig_3">Fig. 5</ref>.</p><p>Since the model fails to segregate the tones, it is likely it has learned to recognize only the full harmonic patterns. They then proceed to track the trajectory of the pitches implied by these harmonics the varied mixtures of speakers and tones, and despite their often proximate values, patterns, and even inter-crossings and gaps and other well-known difficulties. These experiments provide evidence of the important cues involved in the remarkable performance of the DNNs, but they also demonstrate their vulnerabilities. In additional experiments not described here for the sake of brevity, we also observed that the models have difficulties in segregating fricatives and unvoiced segments of speech. Fricatives are plentiful in the training set but are not harmonic in structure, indicating that these vulnerabilities are not a result of testing on 'out-of-distribution' data, but instead stem from the underlying mechanisms used by the DNNs to perform the segregation. Finally, we have also confirmed that these same vulnerabilities are not limited to time domain based models trained in an end-to-end fashion but also extend to spectrogram based segregation models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Divergence of DNN Models from Temporal Coherence</head><p>Our results also indicate another significant divergence of the DNN-based segregation models from biologically plausible and perceptual models that perform speech segregation using TC. While TC partially leverages pitch information and consequently harmonicity, they primarily rely on the coincidence of the onsets and timing information to group the features emanating from a single source. This divergence between DNNs and such algorithms is evident in <ref type="figure" target="#fig_4">Fig. 6</ref>, which illustrates that Conv-TasNet can generally segregate two harmonic complexes that have the same exact onset and offset time. Humans and TC algorithms fail to do so because they would group all these components as coming form a single source. Conversely, unlike Conv-TasNet, TC algorithms would have been able to separate the tones in <ref type="figure">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Inharmonic Sources as Adversarial Inputs</head><p>We have demonstrated that speech segregation networks perform poorly if an inharmonic speech or tone source is present in the mixture. An adversary can thus generate inharmonic speech which sounds similar to natural speech as an adversarial input to hinder source-segregation and its downstreamtasks. Since the presence of an inharmonic source is independent of the model architecture, the adversarial nature of such sources is not limited to Conv-TasNet and DPT-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>We have shown that DNN-based speech segregation models such as Conv-TasNet and DPT-Net rely on the harmonicity of the sources in the mixture to separate them. They, therefore, fail to segregate sources when their harmonic structure is disturbed, and also find it harder to learn to segregate mixtures with such inharmonic sources. This study demonstrates that these networks implicitly learn to estimate pitch by learning the harmonic patterns abundant in the training corpus. They then proceed to track these harmonic complexes to segregate speech. This effort to understand the underlying principles that these networks learn helps us narrow the gap between our understanding of auditory perception and end-toend learning-based segregation models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">ACKNOWLEDGEMENTS</head><p>This work was supported by NSF grant #1764010 and an AFOSR grant. The authors declare no conflict of interests.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Conv-Tasnet Segregates mixtures of harmonic tone complexes (left panels), but fails with inharmonic complexes (right panels). performs well when both complexes are harmonic (J = 0 and F 0 = 110, 210 Hz) as illustrated in the left panels ofFig. 1. It, however, fails completely when the complexes are harmonically-jittered (right panels) with J = 0.20.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Mixtures of natural speech with harmonic (left panels) and inharmonic (right panels) tone complexes are segregated only when both are harmonic.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :Fig. 4 :</head><label>34</label><figDesc>Segregation of overlapped spectra. (Left panels) A mixture of 2 harmonic sources segregate well. (Right panels) Alternating inharmonic spectra become harmonic and incorrectly segregate during overlap. Performance on mixtures of inharmonic speakers with increasing jitter J for models trained on (a) native WSJ-2-mix and (b) inharmonic WSJ-2-mix with J = 0.3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Conv-Tasnet successfully segregates mixtures of harmonic sources (left panels). However, it fails when the fundamentals and some lower harmonics are absent (right panels).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Conv-TasNet segregates two harmonic sources even when synchronous.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Conv-tasnet: Surpassing ideal time-frequency magnitude masking for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">speech, and language processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1256" to="1266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Dualpath transformer network: Direct context-aware modeling for end-to-end monaural speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qirong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.13975</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Segregating complex sound sources through temporal coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lakshmi</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mounya</forename><surname>Elhilali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihab</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">1003985</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Auditory scene analysis: The perceptual organization of sound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Albert S Bregman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An algorithm for speech segregation of co-channel speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikanth</forename><surname>Vishnubhotla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Carol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Espy-Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">109</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Source-filter-based single-channel speech separation using pitch information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wohlmayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><surname>Pernkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="242" to="255" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Separation of speech from interfering sounds based on oscillatory correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deliang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="684" to="697" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A cocktail party with a cortical twist: how cortical mechanisms contribute to sound segregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mounya</forename><surname>Elhilali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shihab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3751" to="3771" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Speech separation using speaker-adapted eigenvoice speech models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel Pw</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="16" to="29" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Monaural speech separation and recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Cooke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven J</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rennie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Joint optimization of masks and deep recurrent neural networks for monaural source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minje</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2136" to="2147" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep clustering: Discriminative embeddings for segmentation and separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>John R Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A deep ensemble learning method for monaural speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deliang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM transactions on audio</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="967" to="977" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>and language processing</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tasnet: time-domain audio separation network for real-time, single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="696" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dualpath rnn: efficient long sequence modeling for timedomain single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Yoshioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="46" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multitalker speech separation with utterancelevel permutation invariant training of deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morten</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Hua</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesper</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1901" to="1913" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Music source separation in the waveform domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>D?fossez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.13254</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Universal sound separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Kavalerov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Patton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="175" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Inharmonic speech reveals the role of harmonicity in the cocktail party problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Popham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dana</forename><surname>Boebinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Dan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><forename type="middle">H</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcdermott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Inharmonic speech: a tool for the study of speech perception and separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><forename type="middle">H</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kawahara</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Performance measurement in blind audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?dric</forename><surname>F?votte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on audio, speech, and language processing</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1462" to="1469" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
