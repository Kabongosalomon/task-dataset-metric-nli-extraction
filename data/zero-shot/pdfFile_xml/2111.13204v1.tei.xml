<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BaLeNAS: Differentiable Architecture Search via the Bayesian Learning Rule</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Aalborg University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Aalborg University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Su</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">UTS</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Monash University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">RMIT University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Aalborg University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Monash University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">BaLeNAS: Differentiable Architecture Search via the Bayesian Learning Rule</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Differentiable Architecture Search (DARTS) has received massive attention in recent years, mainly because it significantly reduces the computational cost through weight sharing and continuous relaxation. However, more recent works find that existing differentiable NAS techniques struggle to outperform naive baselines, yielding deteriorative architectures as the search proceeds. Rather than directly optimizing the architecture parameters, this paper formulates the neural architecture search as a distribution learning problem through relaxing the architecture weights into Gaussian distributions. By leveraging the natural-gradient variational inference (NGVI), the architecture distribution can be easily optimized based on existing codebases without incurring more memory and computational consumption. We demonstrate how the differentiable NAS benefits from Bayesian principles, enhancing exploration and improving stability. The experimental results on NAS-Bench-201 and NAS-Bench-1shot1 benchmark datasets confirm the significant improvements the proposed framework can make. In addition, instead of simply applying the argmax on the learned parameters, we further leverage the recentlyproposed training-free proxies in NAS to select the optimal architecture from a group architectures drawn from the optimized distribution, where we achieve state-of-the-art results on the NAS-Bench-201 and NAS-Bench-1shot1 benchmarks. Our best architecture in the DARTS search space also obtains competitive test errors with 2.37%, 15.72%, and 24.2% on CIFAR-10, CIFAR-100, and ImageNet datasets, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Neural Architecture Search (NAS) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b37">38]</ref> is attaining increasing attention in the deep learning community by automating the labor-intensive and time-consuming neural network design process. More recently, NAS has achieved the state-of-the-art results on various deep learning applications, including image classification <ref type="bibr" target="#b40">[41]</ref>, object detection <ref type="bibr" target="#b10">[11]</ref>, stereo matching <ref type="bibr" target="#b12">[13]</ref>. Although NAS has the potential to find high-performing architectures without human intervention, the early NAS methods have extremelyhigh computational requirements <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b54">55]</ref>. For example, in <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b54">55]</ref>, NAS costs thousands of GPU days to obtain a promising architecture through reinforcement learning (RL) or evolutionary algorithm (EA). This high computational requirement in NAS is unaffordable for most researchers and practitioners. Since then, more researchers shift to improve the efficiency of NAS methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b35">36]</ref>. Weight sharing NAS, also called One-Shot NAS <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b35">36]</ref>, defines the search space as a supernet, and only the supernet is trained for once during the architecture search. The architecture evaluation is based on inheriting weights from the supernet without retraining, thus significantly reducing the computational cost. Differentiable architecture search (DARTS) <ref type="bibr" target="#b30">[31]</ref>, which is one of the most representative works, further relaxes the discrete search space into continuous space and jointly optimize supernet weights and architecture parameters with gradient descent, to further improve efficiency. Through employing two techniques, weight sharing <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b35">36]</ref> and continuous relaxation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b44">45]</ref>, DARTS reformulates the discrete operation selection problem in NAS as a continuous magnitude optimization problem, which reduces the computational cost significantly and completes the architecture search process within several hours on a single GPU.</p><p>Despite notable benefits on computational efficiency from differentiable NAS, more recent works find it is still unreliable <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b48">49]</ref> to directly optimize the architecture magnitudes. For example, DARTS is unable to stably obtain excellent solutions and yields deteriorative architectures during the search proceeds, performing even worse than random search in some cases <ref type="bibr" target="#b47">[48]</ref>. This critical weakness is termed as instability in differentiable NAS <ref type="bibr" target="#b48">[49]</ref>. Zela et al. <ref type="bibr" target="#b48">[49]</ref> empirically point out that the instability of DARTS is highly correlated with the dominant eigenvalue of the Hessian of the validation loss with respect to the architectural parameters, while this dominant eigenvalue increases during the architecture search. Accordingly, they proposed a simple early-stopping criterion based on this dominant eigenvalue to robustify DARTS. In addition, Wang et al. <ref type="bibr" target="#b43">[44]</ref> observe that the instability in DARTS's final discretization process of architecture selection, where the optimized magnitude could hardly indicate the importance of operations. On the other hand, several works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b51">52]</ref> state that directly optimizing the architecture parameters without exploration easily entails the rich-gets-richer problem, leading to those architectures that converge faster at the beginning while achieve poor performance at the end of training, e.g. architectures with intensive skip-connections <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>Unlike most existing works that directly optimize the architecture parameters, we investigate differentiable NAS from a distribution learning perspective, and introduce the Bayesian Learning rule <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35]</ref> to the architecture optimization in differentiable NAS with considering naturalgradient variational inference (NGVI) methods to optimize the architecture distribution, which we call BaLeNAS. We theoretically demonstrate how the framework naturally enhance the exploration for differentiable NAS and improves the stability, and the experimental results confirm that our framework enhances the performance for differentiable NAS. Rather than simply applying argmax on the mean to get a discrete architecture, we for the first time leverage the training free proxies <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b31">32]</ref> to select a more competitive architecture from the optimized distribution, without incurring any additional training costs. Specifically, our approach achieves state-of-the-art performance on NAS-Bench-201 <ref type="bibr" target="#b16">[17]</ref> and improves the performance on NAS-Bench-1shot1 <ref type="bibr" target="#b49">[50]</ref> by large margins, and obtains competitive results on CIFAR-10, CIFAR-100, and ImageNet datasets in the DARTS <ref type="bibr" target="#b30">[31]</ref> search space, with test error 2.37%, 15.72%, and 24.2%, respectively. Our contributions are summarized as follows.</p><p>? Firstly, this paper formulates the neural architecture search as a distribution learning problem and builds a generalized Bayesian framework for architecture optimization in differentiable NAS. We demonstrate that the proposed Bayesian framework is a practical solution to enhance exploration for differentiable NAS and improve stability as a by-product via implicitly regularizing the Hessian norm.</p><p>? Secondly, instead of directly applying the argmax on the learned parameters to get architectures, we for the first time leverage zero-cost proxies to select competitive architectures from the optimized distributions. As these proxies are calculated without any training, the architecture selection phase can be finished extremely efficiently.</p><p>? Thirdly, the proposed framework is built based on DARTS and is also comfortable to be extended to other differentiable NAS methods with minimal modifications through leveraging the natural-gradient variational inference (NGVI). Experiments show that our framework consistently improves the baselines with obtaining more competitive architectures in various search spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Differentiable Architecture Search</head><p>Differentiable architecture search (DARTS) is built on weight-sharing NAS <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b35">36]</ref>, where the supernet is trained for once per the architecture search cycle. Rather than using the heuristic methods <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b51">52]</ref> to search for the promising architecture in the discrete architecture space A, DARTS <ref type="bibr" target="#b30">[31]</ref> proposes the differentiable NAS framework by applying a continuous relaxation (usually a softmax) to the discrete architecture space and enabling gradient descent for architecture optimization. Therefore, architecture parameters ? ? and supernet weights w could be jointly optimized during the supernet training, and the promising architecture parameters ? * ? are searched from the continuous search space A ? once the supernet is trained. The bilevel optimization formulation is usually adopted to alternatively learn ? ? and w:</p><formula xml:id="formula_0">min ? ? ?A ? L val argmin w L train (w(? ? ), ? ? ) ,<label>(1)</label></formula><p>and the best discrete architecture ? * is obtained after applying argmax on ? * ? . Despite notable benefits on computational efficiency from DARTS, more recent works find it is still unreliable <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b48">49]</ref> that directly optimizes the architecture magnitudes, where DARTS usually observes a performance collapses with search progresses. This phenomenon is also called the instability of differentiable NAS <ref type="bibr" target="#b7">[8]</ref>. Zela et al. <ref type="bibr" target="#b48">[49]</ref> observed that the there is a strong correlation between the dominant eigenvalue of the Hessian of the validation loss and the architecture's generalization error in DARTS, and keeping the the Hessian matrix's norm in a low level plays a key role in robustifying the performance of differentiable NAS <ref type="bibr" target="#b7">[8]</ref>. In addition, as described above, the differentiable NAS first relaxes the discrete architectures into continuous representations to enable the gradient descent optimization, and projects the continuous architecture representation ? ? into discrete architecture ? after the differentiable architecture optimization. However, more recent works <ref type="bibr" target="#b43">[44]</ref> cast doubts on the robustness of this discretization process in DARTS that the magnitude of architecture parameter ? * ? could hardly indicate the importance of operations with argmax. Taking the DARTS as example, the searched architecture parameters ? ? are continuous, while ? is represented with {0, 1} after argmax. DARTS assumes that the L val (w * , ? * ? ) is a good indicator to the validation performance of ?, L val (w * , ? * ). However, when we conduct the Taylor expansion on the local optimal ? * ? <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, we have:</p><formula xml:id="formula_1">L val (w * , ? * ) = L val (w * , ? * ? ) + ?? L val (w * , ? * ? ) T (? * ? ? * ? ) + 1 2 (? * ? ? * ? ) T H(? * ? ? * ? ) = L val (w * , ? * ? ) + 1 2 (? * ? ? * ? ) T H(? * ? ? * ? )<label>(2)</label></formula><p>where ? ? L val = 0 due to the local optimality condition, and H is the Hessian matrix of L val (w * , ? ? ). We can see that the incongruence of the final continuous architecture representation and the final discrete architecture relates to the Hessian matrix's norm. However, as demonstrated by the empirical results in <ref type="bibr" target="#b48">[49]</ref>, the eigenvalue of this Hessian matrix increases during the architecture search, incurring more incongruence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Bayesian Deep Learning</head><p>Given a dataset D = {D 1 , D 1 , ..., D N } and a deep neural network with parameters ?, the most popular method to learn ? with D is Empricial Risk Minimization (ERM):</p><formula xml:id="formula_2">min? (?) := N i=1 i (?) + ?R(?),<label>(3)</label></formula><p>where i is a loss function, e.g., i = ?log p(D i | ?) for classification and R is the regularization term.</p><p>In contrast, the Bayesian deep learning estimate the posterior distribution of ?, p(? | D) := p(D | ?)p(?)/p(D), where p(?) is the prior distribution. However, the normalization constant p(D) = p(D | ?)p(?)d? is difficult to compute for large DNNs. The variational inference (VI) <ref type="bibr" target="#b17">[18]</ref> resolves this issue in Bayesian deep learning by approximating p(? | D) with a new distribution q(?), and minimizes the Kullback-Leibler (KL) divergence between p(? | D) and q(?), argmin ? KL(q(?) p(? | D)).</p><p>When considering both p(?) and q(?) as Gaussian distributions with diagonal covariances:</p><formula xml:id="formula_4">p(?) := N (? | 0, I/?), q(?) := N (? | ?, diag(? 2 )),<label>(5)</label></formula><p>where ? is a known precision parameter with ? &gt; 0, the mean ? and deviation ? 2 of q can be estimated by minimizing the negative of evidence lower bound (ELBO) <ref type="bibr" target="#b2">[3]</ref>:</p><formula xml:id="formula_5">L(?, ?) : = ? N i=1 E q [log p(D i | ?)] + KL(q(?) p(?)) = ?E q N i=1 log p(D i | ?) + E q log q(?) p(?)<label>(6)</label></formula><p>A straightforward approach is using the stochastic gradient descent to learn ? and ? 2 along with minimizing L, called as the Bayes by Backprob (BBB) <ref type="bibr" target="#b3">[4]</ref>:</p><formula xml:id="formula_6">? t+1 = ? t ? ? t?? L t , ? t+1 = ? t ? ? t?? L t ,<label>(7)</label></formula><p>where ? t and ? t are the learning rates, and? ? L t and? ? L t are the unbiased stochastic gradient estimates of L at ? t and ? t . However, VI remains to be impractical for learning large deep networks. The obvious issue is that VI introduces more parameters to learn, as it needs to replace all neural networks weights with random variables and simultaneously optimize two vectors ? and ? to estimate the distribution of ?, so the memory requirement is also doubled, leading a lot of modifications when fitting existing differentiable NAS codebases with the variational inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Training Free Proxies for NAS</head><p>Training Free NAS tries to identify promising architectures at initialization without incurring training. Mellor et al. <ref type="bibr" target="#b31">[32]</ref> empirically find that the correlation between samplewise input-output Jacobian can indicate the architecture's test performance, and propose using the Jacobian to score a set of randomly sampled models with randomly initialized weights, which greedily chooses the model with the highest score. TE-NAS <ref type="bibr" target="#b6">[7]</ref> utilizes the spectrum of NTKs and the number of linear regions to analyzing the trainability and expressivity of architectures. Rather than evaluating the whole architecture, TE-NAS uses the perturbation-based architecture selection as <ref type="bibr" target="#b43">[44]</ref>, to measure the importance of each operation for the supernet prune.</p><p>Zero-cost NAS <ref type="bibr" target="#b0">[1]</ref> extends the saliency metrics in the network pruning at initialization to score an architecture, through summing scores of all parameters ? in the architecture. There are three popular saliency metrics, SNIP <ref type="bibr" target="#b23">[24]</ref>, GraSP <ref type="bibr" target="#b42">[43]</ref>, and Synflow <ref type="bibr" target="#b41">[42]</ref>:</p><formula xml:id="formula_7">S snip (?) = ?L ?? ? , S grasp (??) = ?(H ?L ?? ) ?, S SF (?) = ?R SF ?? ?,<label>(8)</label></formula><p>where L is the common loss based on initialized weights, H is the Hessian matrix, and R SF is defined as</p><formula xml:id="formula_8">R SF = 1 T L l=1 ? [l]</formula><p>1 that makes SynFlow data-agnostic. Since these scores can be obtained without any training, zero-cost NAS utilizes these zero-cost proxies to assist NAS by warmup different search algorithms, e.g., initializing population or controller for aging evolution NAS and RL based NAS, respectively. Different from zero-cost NAS that leverages proxies before the search, we utilize these zero-cost proxies for the architecture selection after search, to select more competitive architectures from the optimized distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Proposed Method: BaLeNAS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Formulating NAS as Distribution Learning</head><p>Differentiable NAS normally considers the architecture parameters ? ? as learnable parameters and directly conducts optimization in this space. Most previous differentiable NAS methods first optimize the architecture parameters based on the gradient of the performance, then update the supernet weights based on the updated architecture parameters. Since architectures with updated supernet weights are supposed to have higher performance, architectures with better performance in the early stage have a higher probability of being selected for the supernet training. The supernet training again improves these architectures' performance. This is to say, directly optimizing ? ? without exploration easily entails the rich-get-richer problem <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b51">52]</ref>, leading to suboptimal paths in the search space that converges faster at the beginning but plateaued quickly <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b38">39]</ref>. In contrast, formulating the differentiable NAS as a distribution learning problem by relaxing architecture parameters can naturally introduce stochasticity and encourage exploration to resolve this problem <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>In this paper, we formulate the architecture search as a distribution learning problem, that for the first time consider the more general Gaussian distributions for the architecture parameters to optimize the posterior distribution p(? ? | D) rather than ? ? . Considering both p(?) and q(?) as Gaussian distributions as Eq. <ref type="formula" target="#formula_4">(5)</ref>, the bilevel optimization problem in Eq.(1) could be reformulated as the distribution learning based NAS:</p><formula xml:id="formula_9">min ?,? E q(? ? |?,?) L val (w * (? ? ), ? ? ), s.t. w * (? ? ) = argmin w L train (w(? ? ), ? ? ),<label>(9)</label></formula><p>where ? and ? are the two learnable parameters for the dis-</p><formula xml:id="formula_10">tribution q(? ? | ?, ?) := N (? ? | ?, diag(? 2 )).</formula><p>Considering the variational inference and Bayesian deep learning, based on Eq.(4)-(6), the loss function for the outer-loop architecture distribution optimization problem could be defined as:</p><formula xml:id="formula_11">E q [L val ] := ?E q N i=1 log p(D i | ? ? ) + E q log q(? ? ) p(? ? ) .</formula><p>(10) Since the architecture parameters ? ? are random variables sampled from the Gaussian distribution q(? ? | ?, ?), the distribution learning-based method naturally encourages exploration during the architecture search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Natural-Gradient VI for NAS</head><p>As describe in Sec.2.2, the traditional variational inference has double memory requirement and needs to re-design the object function, making it difficult to fit with the differentiable NAS. Thus, this paper considers natural-gradient variational inference (NGVI) methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b34">35]</ref> to optimize the architecture distribution p(? ? | D) in a natural parameter space, which requires the same number of parameters as the traditional learning method. By leveraging NGVI, the architecture parameter distribution could be learned by only updating a natural parameter ? during the search.</p><p>NGVI parameterizes the distribution q(? ? ) with a natural parameter ?, considering q(? ? | ?) in a class of minimal exponential family with natural parameter ? <ref type="bibr" target="#b20">[21]</ref>:</p><formula xml:id="formula_12">q(? ? | ?) := h(? ? )exp ? T ?(? ? ) ? A(?) ,<label>(11)</label></formula><p>where h(? ? ) is the base measure, ?(? ? ) is a vector containing sufficient statistics, and A(?) is the log-partition function.</p><p>When h(? ? ) ? 1, the distribution q(? ? | ?) could be learned by only updating ? during the training <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>, and ? could be learned in the natural-parameter space by:</p><formula xml:id="formula_13">? t+1 = (1 ? ? t )? t ? ? t ? ? E qt ? (? ? ) ,<label>(12)</label></formula><p>where ? t is the learning rate,? is in the form of Eq. <ref type="formula" target="#formula_2">(3)</ref>, and</p><formula xml:id="formula_14">the derivative ? ? E qt(? ? ) ? (? ? ) is taken at ? = ? t which is the expectation parameter with Markov Chain Monte Carlo (MCMC) sampling. And q t is the q(? ? | ?) parameterized by ? t , ? = ?(?) is the expectation parameter of q(? ? | ?)</formula><p>. This is also called as the Bayesian learning rule <ref type="bibr" target="#b22">[23]</ref>. When we consider Gaussin mean-field VI that p(? ? ) and q(? ? ) are in the form of Eq.(5), the Variational Online-Newton (VON) method proposed by Khan et. al. <ref type="bibr" target="#b21">[22]</ref> shows that the NGVI update could be written with the following update:</p><formula xml:id="formula_15">? t+1 = ? t ? ? t (?(? t ) +?? t )/(s t+1 +?),<label>(13)</label></formula><formula xml:id="formula_16">s t+1 = (1 ? ? t )s t + ? t diag[? 2? (? t )],<label>(14)</label></formula><p>where ? t is the learning rate,</p><formula xml:id="formula_17">? t ? N (? ? | ? t , ? 2 t ) with ? 2 t = 1/[N (s t +?)] and? = ?/N .? is the stochastic estimate with respect to q through MCMC sampling that, g(? t ) = 1 M i?M ? ? ?? i (? ? )</formula><p>, and the minibatch M contains M samples. More details are in <ref type="bibr" target="#b21">[22]</ref>. Variational RMSprop (Vprop) <ref type="bibr" target="#b21">[22]</ref> further uses gradient magnitude (GM) <ref type="bibr" target="#b4">[5]</ref> approximation to reformulate Eq.(14) as: <ref type="bibr" target="#b4">[5]</ref>. The most important benefit of VON and Vprop is that they only need to calculate one parameter's gradient to update posterior distribution. In this way, this learning paradigm requires the same number of parameters as traditional learning methods and easy to fit with existing codebases.</p><formula xml:id="formula_18">s t+1 = (1 ? ? t )s t + ? t [?(? t ) ??(? t )],<label>(15)</label></formula><formula xml:id="formula_19">with? 2 j,j? (? t ) ? 1 M i?Mt g i (? j ? ) 2 = [?(? j t )] 2</formula><p>We implement the proposed BaLeNAS based on the DARTS <ref type="bibr" target="#b30">[31]</ref> framework, the most popular differentiable NAS baseline. Similar to DARTS, BaLeNAS also considers an Adam-like optimizer for the architecture optimization, updating the natural parameter ? of p(? | D) as:</p><formula xml:id="formula_20">? t+1 = ? t ? ? t ? ? L t + ? t (? t ? ? t?1 ),<label>(16)</label></formula><p>where the last term is the momentum. Based on the Vprop in Eq. <ref type="formula" target="#formula_0">(13)</ref> and <ref type="formula" target="#formula_0">(15)</ref>, the update of ? and ? for the Adamlike optimizer with NGVI, also called as Variational Adam (VAdam), could be defined as following: Obtain discrete architecture ? * through argmax on ?; or sample a set of ? ? from q(? * ? | ?, ? 2 ), and utilize the training free proxies for selection.</p><formula xml:id="formula_21">? t+1 =? t ? ? t (?(? t ) +?? t ) ? 1 (s t+1 +?) + ? t s t +? s t+1 +? ? (? t ? ? t?1 ),<label>(17)</label></formula><formula xml:id="formula_22">s t+1 = (1 ? ? t )s t + ? t [?(? t ) ??(? t )].<label>(18)</label></formula><p>where "?" stands for element-wise product,</p><formula xml:id="formula_23">? t ? N (? ? | ? t , ? 2 t ) with ? 2 t = 1/[N (s t +?)].</formula><p>As pointed out in Sec. 2.2 and shown in Eq. <ref type="formula" target="#formula_0">(17)</ref> and Eq. <ref type="formula" target="#formula_0">(18)</ref>, the distribution q(? ? ) = N (? ? | ?, ? 2 ) is now optimized, needing to calculate the gradient of only one parameter.</p><p>Implicit Regularization from MCMC Sampling: Several recent works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b48">49]</ref> empirically and theoretically show that the performance of differentiable NAS is highly related to the norm of H, the Hessian matrix of L val (w * , ? ? ), and keeping this norm in a low level plays a key role in robustifying differentiable NAS. As described before, we know the loss E qt(? ? ) ? (? ? ) of architecture optimization in BaLeNAS is calculated based on MCMC sampling, showing the naturality of enhancing exploration. Besides, E qt(? ? ) ? (? ? ) also has the naturality to enhance the stability in differentiable NAS as SDARTS <ref type="bibr" target="#b7">[8]</ref>. When conducting the Taylor expansion, the loss function for the architecture parameters update E qt(? ? ) ? (? ? ) could be described as:</p><formula xml:id="formula_24">E qt(??) ? (? ? ) =E q(??|?,?) L val (w, ? ? ) = E ?N (0,? 2 ) L val (w, ? + ) =E ?N (0,? 2 ) [L val (w, ?) + ? L val (w, ?) T + 1 2 T H ] =E ?N (0,? 2 ) L val (w, ?) + 1 2 T H =L val (w, ?) + ? 2 2 Tr {H} ,<label>(19)</label></formula><p>where the line 4 in Eq.(19) is obtained since</p><formula xml:id="formula_25">E ?N (0,? 2 ) [ ? L val (w, ? ? ) T ] = E ?N (0,? 2 ) [ ] * ? L val (w, ? ? ) = 0, as ? N (0, ? 2 )</formula><p>is a Gaussian distribution with zero mean, and E( 2 ) = ? 2 . ? is the expectation parameter of q(? ? | ?, ? 2 ), and H is the Hessian matrix of L val (w, ?). We can find the loss function that could implicitly control the trace norm of H similar as <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, helping stabilizing differentiable NAS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Architecture Selecting from the Distribution</head><p>After the optimization of BaLeNAS, we learns an optimized Gaussian distribution for the architecture parameters q(? * ? | ?, ? 2 ), which is used to get the optimal architecture ? * . In this paper, we consider two methods to get the discrete architecture ? * . The first one is a simple and direct method, which utilizes the expectation of ? * ? to select the best operation for each edge through the argmax as DARTS, where the expectation term is simply the mean ? <ref type="bibr" target="#b8">[9]</ref>. However, as we described in Sec. 2.1, this method may result in instability and incongruence. The second one is more general, which samples a set of ? from the distribution q(? * ? | ?, ? 2 ) for architecture selection. However, in the neural architecture search, evaluating a set of architectures will incur unaffordable computational costs. In this paper, instead of utilizing training-free proxies to assist NAS by warmup before search as <ref type="bibr" target="#b0">[1]</ref>, we leverage these proxies, including SNIP <ref type="bibr" target="#b23">[24]</ref>, GraSP <ref type="bibr" target="#b42">[43]</ref>, and Synflow <ref type="bibr" target="#b41">[42]</ref>, to score the sampled architectures for selection after search.</p><p>Algorithm 1 gives a simple implementation of BaLeNAS, where only the red part is different from DARTS. As shown, in our BaLeNAS, only architecture parameter optimization is different from DARTS which uses the VAdam optimizer, making it easy to be implemented. Furthermore, as most existing differentiable NAS methods are built based on DARTS codebase, our BaLeNAS is also comfortable to be adapted to them with minimal modifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Results</head><p>In this section, we consider three different search spaces to analyze the proposed BaLeNAS framework. The first two are NAS benchmark datasets, NAS-Bench-201 <ref type="bibr" target="#b16">[17]</ref> and NAS-Bench-1shot1 <ref type="bibr" target="#b49">[50]</ref>. The ground-truth for all candidate architectures in the two benchmark datasets is known. The NAS methods could be evaluated without retraining the searched architectures based on these benchmark datasets, thus greatly relieving the computational burden. The third one is the commonly-used CNN search space in DARTS <ref type="bibr" target="#b30">[31]</ref>. We first analyze our proposed BaLeNAS in the two benchmark datasets, then compare BaLeNAS with state-of-the-art NAS methods in the DARTS search space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiments on Benchmark Datasets</head><p>The NAS-Bench-201 <ref type="bibr" target="#b16">[17]</ref> has a unified cell-based search space, where the cell structure is densely-connected, containing four nodes with five candidate operations applied on each node, resulting in 15,625 architectures. NAS-Bench-201 reports the CIFAR-10, CIFAR-100, and Imagenet performance for all architecture in this search space. The NAS-Bench-1shot1 <ref type="bibr" target="#b49">[50]</ref> is built from the NAS-Bench-101 benchmark dataset <ref type="bibr" target="#b46">[47]</ref>, through dividing all architectures in NAS-Bench-101 into 3 different unified cell-based search spaces,   <ref type="table" target="#tab_1">Table 1</ref> summarizes the performance of BaleNAS on NAS-Bench-201 compared with differentiable NAS baselines, where the statistical results are obtained from 4 independent search experiments with four different random seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Reproducible Comparison on NAS Benchmarks</head><p>In our BaLeNAS, we consider the expectation of ? ? with argmax to get the valid architecture, while BaLeNAS-TF consider the training-free proxies for the architecture selection, with the sample size is set as 100. As shown in <ref type="table" target="#tab_1">Table 1</ref>, BaLeNAS achieves the best results on the NAS-Bench-201 benchmark and greatly outperforms other baselines on all three datasets. As described in Sec. 3, BaLeNAS is built based on the DARTS framework, with only modeling the architecture parameters into distributions and introducing Bayesian learning rule for optimization. As shown in Table 1, BaLeNAS with first and second-order approximations We also conduct a comparison study on the NAS-Bench-1shot1 dataset to further verify the effectiveness of our BaLe-NAS which reformulates architecture search as a distribution learning problem. We have compared BaLeNAS with the baseline DARTS on the three search spaces of NAS-Bench-1shot1 with tracking the validation and test performance of the search architectures in every iteration. As shown in <ref type="figure" target="#fig_0">Fig. 1, our</ref> BaLeNAS, without training-free proxies based architecture selection, generally outperforms DARTS during the architecture search in terms of validation and test error in the most complicated search space 3, both with first and second-order approximation. More specifically, our BaLe-NAS significantly outperforms the baseline in the early stage, demonstrating our BaLeNAS could quickly find the superior architectures and is more stable. The results on both NAS-Bench-201 and NAS-Bench-1shot1 verify that, by formulating the architecture search as a distribution learning </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Ablation Study on the Architecture Selection</head><p>As described, our BaLeNAS-TF samples several architectures from the optimized distribution and leverages the training-free proxies for architecture selection, rather than simply applying argmax on the mean. In this subsection, we conduct ablation study to investigate the benefits of our training-free based architecture selection. We considered 3 different training-free proxies as described in Sec. 2.3, including SNIP, GraSP, and Synflow. We find that Synflow is the most reliable proxies in the architecture selection, as it achieves better performance than the remaining two proxies for both zero-cost NAS and BaLeNAS, and also consistently enhances the performance with the increase of sample size. More detailed comparison can be found in the Appendix. Zero-cost NAS <ref type="bibr" target="#b0">[1]</ref> randomly generates samples and calculates the scores based on the proxies for architecture selection, while our BaLeNAS-TF generates samples based on the optimized distribution (? * ? | ?, ? 2 ). <ref type="table" target="#tab_2">Table 2</ref> compared zero-cost NAS and BaLeNAS-TF with different sample sizes in the architecture selection. As shown, the Synflow proxy can assist NAS as zero-cost NAS with different sample sizes achieve much better results than the Random baseline in <ref type="table" target="#tab_1">Table 1</ref>, and these proxies also enhance our BaLeNAS, where our BaLeNAS-TF achieve higher accuracy. These results again verified that the architecture selection with train-free proxies can further improve the performance for distribution learning based NAS. More interesting, <ref type="table" target="#tab_2">Table  2</ref> also showed that our BaLeNAS-TF outperformed zero-cost NAS by a large margin, suggesting that our BaLeNAS can converge to a competitive distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiments on DARTS Search Space</head><p>To compare with the state-of-the-art differentiable NAS methods, we applied BaLeNAS to the typical DARTS search space <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31]</ref> for convolutional architecture search, where all experiment settings are following DARTS <ref type="bibr" target="#b30">[31]</ref> for fair comparisons as the same as the most recent works. Our BaLeNAS-TF also considers the Synflow proxy in this experiment. The architecture search in DARTS space generally contains three stages: The differentiable NAS first searches for micro-cell structures on CIFAR-10, and then stack more cells to form the full structure for the architecture evaluation. The best-found cell on CIFAR-10 is finally transferred to larger datasets to evaluate its transferability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Search Results on CIFAR-10</head><p>The comparison results with the state-of-the-art NAS methods are presented in <ref type="table" target="#tab_3">Table 3</ref>. The best architecture searched by our BaLeNAS-TF achieves a 2.37% test error on CIFAR-10, which outperforms state-of-the-art NAS methods. We can also see that both BaLeNAS-TF and BaLeNAS outperform DARTS by a large margin, demonstrating the effectiveness of the proposed method. Besides, although BaLeNAS introduced MCMC during architecture optimization, it is still efficient in the sense that the whole architecture search phase in BaLeNAS (2nd) only took 0.6 GPU days.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Transferability Results Analysis</head><p>Following DARTS experimental setting, the best-searched architectures on CIFAR-10 are then transferred to CIFAR-100 and ImageNet to evaluate the transferability. The comparison results with state-of-the-art differentiable NAS approaches on CIFAR-100 and ImageNet are demonstrated in <ref type="table" target="#tab_3">Table 3</ref>. As shown in Table2, BaLeNAS-TF achieves a 15.72% test error on the CIFAR-100 dataset, which is a state-of-the-art performance and outperforms peer algorithms by a large margin. On the ImageNet dataset, the best-discovered architecture by our BaLeNAS-TF also achieved a competitive result with 24.2 / 7.3 % top1 / top5 test error, outperforming or on par with all peer algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Analysis on the Effect of Exploration</head><p>Several recent works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b51">52]</ref> point out that directly optimizing architecture parameters without exploration easily entails the rich-gets-richer problem, leading to those architectures that converge faster at the beginning while achieve poor performance at the end of training, e.g. architectures with intensive skip-connections <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b29">30]</ref>. However, when the number of skip-connections is larger than 3, the architecture's retraining accuracy is usually extremely low <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b48">49]</ref>.</p><p>To relieve this issue, BaLeNAS formulates the differentiable neural architecture search as a distribution learning problem, and this experiment verifies how the proposed formulation naturally enhance the exploration to relieve this issue. <ref type="figure" target="#fig_1">Fig. 2</ref> plots the ratio of skip-connection in the searched normal cell for BaLeNAS and DARTS (the total number of operations in a cell is 8). As shown, DARTS is likely to select more than 3 skip-connection in the normal cell during the search. In contrast, in the proposed BaLeNAS, the number of skipconnections is generally less than 2 in the normal cell during the search for BaLeNAS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Tracking of the Hessian norm</head><p>As described in Section 2.1, a large Hessian norm deteriorate the robustness of DARTS, and the incongruence between L val (w * , ? * ? ) and L val (w * , ? * ) is not negligible if we could  not maintain the maintains the Hessian norm at a low level. The analysis in Sec. 3.2 and Eq. <ref type="bibr" target="#b18">(19)</ref> shows that the loss function of the proposed BaLeNAS implicitly controls the trace norm of H similar as <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, helping stabilizing differentiable NAS. We plot the trajectory of the Hessian norm of BaLeNAS compared with the vanilla DARTS in <ref type="figure" target="#fig_2">Fig. 3</ref>. As show, the Hessian norm in our BaLeNAS is always kept in a low level. Although the Hessian norm of BaLeNAS also increases with the supernet training similar as DARTS, BaLe-NAS's largest Hessian norm is still smaller than DARTS in the early stage, showing the effectiveness of implicit regularization of our BaLeNAS as described in Sec. 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have formulated the architecture optimization in the differentiable NAS as a distribution learning problem and introduced a Bayesian learning rule to optimize the architecture parameters posterior distributions. We have theoretically demonstrated that the proposed framework can enhance the exploration for differentiable NAS and implicitly impose regularization on the Hessian norm to improve the stability. The above properties show that reformulating differentiable NAS as distribution learning is a promising direction. In addition, with leveraging the training-free proxies, our BaLeNAS can select more competitive architectures from the optimized distributions instead of applying argmax on the mean to get the the discrete architecture, so that alleviate the discretization instability and enhance the performance. We operationalize the framework based on the common differentiable NAS baseline, DARTS, and experimental results on NAS benchmark datasets and the common DARTS search space have verified the proposed framework's effectiveness.</p><p>Although BaLeNAS improves the differentiable NAS baseline by large margins, it computational consumption and memory consumption are similar with DARTS where our BaLeNAS is built on. Further questions include how to further decrease the computational and memory cost and also eliminate the depth gap existing between architecture search and evaluation in differentiable NAS <ref type="bibr" target="#b9">[10]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX:</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Search Spaces and Experimental Setting</head><p>In our experiments, we consider two scenarios, NAS benchmark datasets and the common DARTS space, to analyze the proposed framework FreeDARTS. The high computational cost in evaluation is a major obstacle when analyzing and reproducing differentiable NAS methods. To alleviate this issue, several benchmark datasets have been recently published <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b49">50]</ref>, where the ground-truth for all candidate architectures in the benchmark datasets is known. The NAS-Bench-201 dataset <ref type="bibr" target="#b16">[17]</ref> is a popular NAS benchmark dataset to analyze differentiable NAS methods. The search space in NAS-Bench-201 contains four nodes with five associated operations, resulting in 15,625 cell candidates, where the performance of CIFAR-100, CIFAR-100, and ImageNet for all architectures in this search space are reported. The NAS-Bench-101 <ref type="bibr" target="#b46">[47]</ref> is another famous NAS benchmark dataset, which is much larger than NAS-Bench-201 while only the CIFAR-10 performance for all architectures are reported. More important, the architectures in NAS-Bench-101 contain different number of nodes, which makes it impossible to build a generalized supernet for one-shot nor differential NAS methods. To leverage the NAS-Bench-101 for analyzing the differentiable NAS methods, NAS-Bench-1shot1 <ref type="bibr" target="#b49">[50]</ref> builds from the NAS-Bench-101 benchmark dataset by dividing all architectures in NAS-Bench-101 into 3 different unified cell-based search spaces, which contain 6240, 29160, and 363648 architectures, respectively. The architectures in each search space have the same number of nodes and connections, making the differentiable NAS could be directly applied to each search space. We choose the third search space in NAS-Bench-1shot1 since it is much more complicated than the remaining two search spaces.</p><p>As to the most common search space in NAS, DARTS needs to search for two types of cells: a normal cell ? normal and a reduction cell ? reduce . Cell structures are repeatedly stacked to form the final CNN structure. There are only two reduction cells in the final CNN structure, located in the 1/3 and 2/3 depths of the network. There are seven nodes in each cell: two input nodes, four operation nodes, and one output node. Each operation node selects two of the previous nodes' output as input nodes in this search space. Each input node will select one operation from |O| = 8 candidate operations. <ref type="figure" target="#fig_3">Fig. 4</ref> describes a unified convolutional search space in DARTS. The common practice in DARTS is to search on CIFAR-10, and the best searched cell structures are directly transferred to CIFAR-100 and ImageNet. The experimental settings on DARTS space in this paper are following the common DARTS setting. We conduct the architecture search with 5 different random seeds, and the best one is selected after the evaluation on CIFAR-10, which is then transferred to CIFAR-100 and ImageNet. The architecture evaluation for CIFAR-10 and CIFAR-100 are on a single GPU with batch size 96, while for ImageNet is performed on 2 GPUs. Since the sizes of searched architectures are in a range, we adjust the number of filter in the evaluation to make the model sizes similar for fair comparison. We use a linear learning rate scheduler with following PDART <ref type="bibr" target="#b9">[10]</ref> and PCDARTS <ref type="bibr" target="#b45">[46]</ref> to use a smaller slope in the last five epochs for the architecture evaluation on ImageNet 1 . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ablation Study on the Saliency Metrics</head><p>In our BaLeNAS-TF, we utilize three train-free saliency metrics, SNIP, GraSP, and Synflow, as proxies for the architecture selection from the optimized distribution. In <ref type="table" target="#tab_5">Table  4</ref>, we considered different number of sample size for our BaLeNAS-TF when combined with the three saliency metrics. As shown in <ref type="table" target="#tab_5">Table 4</ref>, combined with different train-free proxies, our BaLeNAS-TF achieve higher performance than the original BaLeNAS when the sample size is 10. However, when increasing the sample size, we can see a sharp drop for BaLeNAS-TF with SNIP and GraSP, showing the two metrics are not appropriate metrics to for the architecture selection. On the contrary, the SynFlow, also adopted by our BaLeNAS-TF, shows a clear improvement with the sample size from 10 to 100, implying that this proxies is more reliable for the architecture selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Study of MCMC on NAS-Bench-201</head><p>As we described in Section 3.2, one key additional hyperparameter in BaLeNAS is the sampling number M in MCMC, and this subsection investigates how this hyperparameter affects the performance of BaLeNAS. <ref type="table" target="#tab_6">Table 5</ref> summarizes the performance our BaLeNAS (2nd) with different number of MCMC sampling. As shown, our BaLeNAS is very robust to the number of MCMC sampling, where BaLe-NAS achieves excellent results under different scenarios, outperforming most existing NAS baselines. An interesting observation is that the performance of BaLeNAS increase with multiple samplings when M &lt; 4 in MCMC, and M = 3 achieves the best performance. Theorem 1 in <ref type="bibr" target="#b21">[22]</ref> points out that VAdam with M &gt; 1 will converge fast while might result in slightly less exploration. The exploration and exploitation can be balanced by the MCMC sample size. A detailed explanation can be found in the Section 3.4 of <ref type="bibr" target="#b21">[22]</ref>. is also observed by several existing works <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b50">51]</ref>. As we know the shallow architectures are easier to train and usually perform excellently in the small dataset, implying that the differentiable NAS methods prefer those "shallow" architectures if we only utilize the validation accuracy as the indicator. However, the performance of those "shallow" architectures on the large dataset is not as competitive as on the small dataset, indicating poor transferability. These results suggest the importance of introducing other indicator to differentiable NAS for architecture search, especially in the complicated real-world search space, to help finding more robust architectures. In contrast, as shown in <ref type="figure">Fig.5</ref>, our BaLeNAS-TF can found "deeper" architectures as it does not only rely on the validation accuracy for the architecture, but also another saliency metric. We can find a similar phenomenon in the NAS-Bench-201 search space that, even though DrNAS achieves near-optimal results on CIFAR-10, while our BaLeNAS-TF outperform it on the larger dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Searched Architectures Visualization</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Validation and test error of BaLeNAS and DARTS on the search space 3 of NAS-Bench-1shot1. containing 6,240, 29,160, and 363,648 architectures, respectively, and the CIFAR-10 performance for all architectures are reported. The architectures in each search space have the same number of nodes and connections, making the differentiable NAS could be directly applied to each space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The ratio of skip-connection the searched normal cells during the architecture search in the DARTS space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Trajectory of the Hessian norm in DARTS space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Description of DARTS search space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 Figure 5 .</head><label>55</label><figDesc>plots the searched architectures on DARTS space by BaLeNAS and BaLeNAS-TF. We could observe that, our BaLeNAS tends to obtain "shallow" architectures, which Examples of searched cells by BaLeNAS and BaLeNAS without regularizatons (BaLeNAS w/o).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 BaLeNAS Initialize a supernet with supernet weights w and architecture parameters ? ? while not converged do 2:Update ? and ? 2 for q(? ? | ?, ? 2 ) based on Eq.<ref type="bibr" target="#b16">(17)</ref> and Eq.(18), with VAdam optimizer. Update supernet weights w based on cross-entropy loss with the common SGD optimizer.</figDesc><table /><note>4: end while</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparison results with state-of-the-art NAS approaches on NAS-Bench-201. 20?13.28 86.61?13.46 60.70?12.55 60.83?12.58 33.34?9.39 33.13?9.66</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Method</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="8">CIFAR-10 Valid(%) Test(%)</cell><cell></cell><cell>CIFAR-100 Valid(%) Test(%)</cell><cell>ImageNet-16-120 Valid(%) Test(%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="10">Random baseline 83.ENAS [36] 37.51?3.19</cell><cell></cell><cell cols="4">53.89?0.58</cell><cell>13.37?2.35</cell><cell>13.96?2.33</cell><cell>15.06?1.95 14.84?2.10</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">RandomNAS [28]</cell><cell></cell><cell cols="4">85.63?0.44</cell><cell></cell><cell cols="4">88.58?0.21</cell><cell>60.99?2.79</cell><cell>61.45?2.24</cell><cell>31.63?2.15 31.37?2.51</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">SETN [15]</cell><cell></cell><cell></cell><cell></cell><cell cols="4">84.04?0.28</cell><cell></cell><cell cols="4">87.64?0.00</cell><cell>58.86?0.06</cell><cell>59.05?0.24</cell><cell>33.06?0.02 32.52?0.21</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">GDAS [16]</cell><cell></cell><cell></cell><cell cols="4">90.00?0.21</cell><cell></cell><cell cols="4">93.51?0.13</cell><cell>71.14?0.27</cell><cell>70.61?0.26</cell><cell>41.70?1.26 41.84?0.90</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">DrNAS [9]</cell><cell></cell><cell></cell><cell></cell><cell cols="4">91.55?0.00</cell><cell></cell><cell cols="4">94.36?0.00</cell><cell>73.49?0.00</cell><cell>73.51?0.00</cell><cell>46.37?0.00 46.34?0.00</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">DARTS (1st) [31]</cell><cell></cell><cell cols="4">39.77?0.00</cell><cell></cell><cell cols="4">54.30?0.00</cell><cell>15.03?0.00</cell><cell>15.61?0.00</cell><cell>16.43?0.00 16.32?0.00</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">DARTS (2nd) [31]</cell><cell cols="4">39.77?0.00</cell><cell></cell><cell cols="4">54.30?0.00</cell><cell>15.03?0.00</cell><cell>15.61?0.00</cell><cell>16.43?0.00 16.32?0.00</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">Zero-cost NAS [1]</cell><cell cols="4">90.19?0.66</cell><cell></cell><cell cols="4">93.45?0.28</cell><cell>70.55?1.61</cell><cell>70.73?1.36</cell><cell>43.24?2.52 43.64?2.42</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">BaLeNAS (1st)</cell><cell></cell><cell cols="4">91.03?0.15</cell><cell></cell><cell cols="4">93.62?0.12</cell><cell>70.88?0.60</cell><cell>70.98?0.41</cell><cell>45.19?0.75 45.25?0.86</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">BaLeNAS (2nd)</cell><cell></cell><cell cols="4">91.32?0.09</cell><cell></cell><cell cols="4">94.02?0.14</cell><cell>71.53?0.08</cell><cell>71.93?0.27</cell><cell>45.39?0.17 45.48?0.39</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">BaLeNAS-TF</cell><cell></cell><cell></cell><cell cols="4">91.52?0.04</cell><cell></cell><cell cols="4">94.33?0.03</cell><cell>72.67?0.41</cell><cell>72.95?0.28</cell><cell>46.14?0.23 46.54?0.36</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">optimal</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">91.61</cell><cell></cell><cell></cell><cell></cell><cell cols="2">94.37</cell><cell></cell><cell>74.49</cell><cell>73.51</cell><cell>46.77</cell><cell>47.31</cell></row><row><cell></cell><cell>0.12</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.12</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">DARTS(1st) Valid</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">DARTS(2nd) Valid</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">BaLeNAS(1st) Valid</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">BaLeNAS(2nd) Valid</cell></row><row><cell></cell><cell>0.11</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">DARTS(1st) Test</cell><cell></cell><cell>0.11</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">DARTS(2nd) Test</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">BaLeNAS(1st) Test</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">BaLeNAS(2nd) Test</cell></row><row><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Error</cell><cell>0.09</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Error</cell><cell>0.09</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.08</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.08</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.07</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.07</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.06</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.06</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.05</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.05</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>20</cell><cell>25</cell><cell>30</cell><cell>35</cell><cell>40</cell><cell>0</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>20</cell><cell>25</cell><cell>30</cell><cell>35</cell><cell>40</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Epoch</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Epoch</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="8">(a) First order approximation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>The best single run of BaLeNAS-TF achieves 94.37%, 73.22%, and 46.71% test accuracy on three datasets, respectively. Our BaLeNAS-TF considers the Synflow based proxy for architecture selection in this experiment.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Ablation study on the sample size. BaLeNAS-TF can achieve better results, showing that apart from warmup, these proxies could also assist differentiable NAS at architecture selection. The best single run of our BaLeNAS-TF achieves 94.37%, 73.22%, and 46.71% test accuracy on three datasets, respectively, which are state-of-the-art on this benchmark dataset.</figDesc><table><row><cell>Method (size)</cell><cell>CIFAR-10</cell><cell>Test Accuracy CIFAR-100</cell><cell>ImageNet</cell></row><row><cell>Zero-cost NAS(10)</cell><cell>92.12?1.25</cell><cell>68.1?2.49</cell><cell>40.07?1.86</cell></row><row><cell>Zero-cost NAS(50)</cell><cell cols="3">92.52?0.05 70.27?0.25 42.92?0.95</cell></row><row><cell cols="4">Zero-cost NAS(100) 93.45?0.16 69.87?0.35 44.43?0.75</cell></row><row><cell>BaLeNAS-TF(10)</cell><cell cols="3">94.08?0.13 72.55?0.42 45.82?0.30</cell></row><row><cell>BaLeNAS-TF(50)</cell><cell cols="3">94.33?0.03 72.95?0.28 46.54?0.36</cell></row><row><cell>BaLeNAS-TF(100)</cell><cell cols="3">94.33?0.03 72.95?0.28 46.54?0.36</cell></row><row><cell cols="4">both outperform DARTS by large margins, verifying the</cell></row><row><cell cols="4">effectiveness of our method. More interesting, combining</cell></row><row><cell cols="2">with the training-free proxies,</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Comparison results with state-of-the-art weight-sharing NAS approaches.</figDesc><table><row><cell>Method</cell><cell cols="3">Test Error (%) CIFAR-10 CIFAR-100 ImageNet</cell><cell cols="4">Param FLOPs Search Architecture (M) (M) Cost Optimization</cell></row><row><cell cols="2">RandomNAS [28] 2.85?0.08</cell><cell>17.63</cell><cell>27.1</cell><cell>4.3</cell><cell>595</cell><cell>2.7</cell><cell>random</cell></row><row><cell>SNAS [45]</cell><cell>2.85?0.02</cell><cell>20.09</cell><cell>27.3 / 9.2</cell><cell>2.8</cell><cell>467</cell><cell>1.5</cell><cell>gradient</cell></row><row><cell>BayesNAS [54]</cell><cell>2.81?0.04</cell><cell>-</cell><cell>26.5 / 8.9</cell><cell>3.40</cell><cell>-</cell><cell>0.2</cell><cell>gradient</cell></row><row><cell>MdeNAS [53]</cell><cell>2.55</cell><cell>17.61</cell><cell>25.5 / 7.9</cell><cell>3.61</cell><cell>500</cell><cell>0.16</cell><cell>gradient</cell></row><row><cell>GDAS [16]</cell><cell>2.93</cell><cell>18.38</cell><cell>26.0 / 8.5</cell><cell>3.4</cell><cell>538</cell><cell>0.21</cell><cell>gradient</cell></row><row><cell>XNAS [34]</cell><cell>2.57?0.09</cell><cell>16.34</cell><cell>24.7 / 7.5</cell><cell>3.7</cell><cell>590</cell><cell>0.3</cell><cell>gradient</cell></row><row><cell>PDARTS [10]</cell><cell>2.50</cell><cell>16.63</cell><cell>24.4 / 7.4</cell><cell>3.4</cell><cell>543</cell><cell>0.3</cell><cell>gradient</cell></row><row><cell>PC-DARTS [46]</cell><cell>2.57?0.07</cell><cell>17.11</cell><cell>25.1 / 7.8</cell><cell>3.6</cell><cell>571</cell><cell>0.3</cell><cell>gradient</cell></row><row><cell>DrNAS [9]</cell><cell>2.54?0.03</cell><cell>16.30</cell><cell>24.2 / 7.3</cell><cell>4.0</cell><cell>644</cell><cell>0.4</cell><cell>gradient</cell></row><row><cell>DARTS+ [30]</cell><cell>2.50?0.11</cell><cell>16.28</cell><cell>-</cell><cell>3.7</cell><cell>-</cell><cell>0.4</cell><cell>gradient</cell></row><row><cell>DARTS (1st) [31]</cell><cell>2.94</cell><cell>-</cell><cell>-</cell><cell>2.9</cell><cell>505</cell><cell>1.5</cell><cell>gradient</cell></row><row><cell cols="2">DARTS (2nd) [31] 2.76?0.09</cell><cell>17.54</cell><cell>26.9 / 8.7</cell><cell>3.4</cell><cell>530</cell><cell>4</cell><cell>gradient</cell></row><row><cell>BaLeNAS</cell><cell>2.50?0.07</cell><cell>16.84</cell><cell>25.0 / 7.7</cell><cell>3.82</cell><cell>593</cell><cell>0.6</cell><cell>gradient</cell></row><row><cell>BaLeNAS-TF</cell><cell>2.43?0.08</cell><cell>15.72</cell><cell>24.2 / 7.3</cell><cell>3.86</cell><cell>597</cell><cell>0.6</cell><cell>gradient</cell></row><row><cell cols="4">problem and introducing the Bayesian learning rule to op-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">timize the posterior distribution, BaLeNAS can relieve the</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">instability and naturally enhance exploration to avoid local</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">optimum for differentiable NAS.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Zero-cost NAS and FreeDARTS with different saliency metrics on NAS-Bench-201. 32?0.09 94.02?0.14 71.53?0.08 71.93?0.27 45.39?0.17 45.48?0.39 BaLeNAS with SNIP 10 90.95?0.39 93.85?0.22 71.37?0.35 71.48?0.52 46.04?0.47 46.03?0.41 50 88.23?2.18 92.56?1.18 68.26?2.74 64.58?3.18 27.13?9.20 35.23?10.3 100 86.04?0.00 91.37?0.00 65.52?0.00 67.77?0.00 36.33?0.00 24.97?0.00 BaLeNAS with Grasp 10 91.10?0.23 93.94?0.05 72.03?0.53 72.00?0.06 45.26?0.56 44.67?1.54 50 90.56?0.76 93.72?0.16 71.52?1.03 70.62?1.43 45.01?0.81 44.92?1.64 100 89.01?0.78 92.32?1.25 67.86?2.61 67.32?1.85 40.29?3.91 39.84?3.43 52?0.04 94.08?0.13 72.37?0.53 72.55?0.42 45.34?0.23 45.82?0.30 50 91.52?0.04 94.33?0.03 72.67?0.41 72.95?0.28 46.14?0.23 46.54?0.36 100 91.52?0.04 94.33?0.03 72.67?0.41 72.95?0.28 46.14?0.23 46.54?0.36</figDesc><table><row><cell>Method</cell><cell>Sample Size</cell><cell>CIFAR-10 Valid(%) Test(%)</cell><cell>CIFAR-100 Valid(%) Test(%)</cell><cell>ImageNet-16-120 Valid(%) Test(%)</cell></row><row><cell cols="3">BaLeNAS 91.BaLeNAS with SynFlow -10 91.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Ablation study on the MCMC sampling size on NAS-Bench-201. 20?1.86 43.09?2.93 43.21?2.88 Random baseline 83.20?13.28 86.61?13.46 60.70?12.55 60.83?12.58 33.34?9.39 33.</figDesc><table><row><cell>MCMC number</cell><cell cols="2">CIFAR-10 Valid(%) Test(%)</cell><cell cols="2">CIFAR-100 Valid(%) Test(%)</cell><cell cols="2">ImageNet-16-120 Valid(%) Test(%)</cell></row><row><cell>M = 1</cell><cell>90.52?0.09</cell><cell>93.33?0.04</cell><cell>70.67?0.08</cell><cell cols="3">70.95?0.27 44.39?0.47 44.32?0.39</cell></row><row><cell>M = 2</cell><cell>90.71?0.12</cell><cell>93.75?0.87</cell><cell>71.25?0.92</cell><cell cols="3">71.43?0.45 44.63?0.55 45.05?0.95</cell></row><row><cell>M = 3</cell><cell>91.32?0.09</cell><cell>94.02?0.14</cell><cell>71.53?0.08</cell><cell cols="3">71.93?0.27 45.39?0.17 45.48?0.39</cell></row><row><cell>M = 4</cell><cell>90.03?0.96</cell><cell>93.04?1.09</cell><cell>68.80?1.46</cell><cell cols="3">69.13?9.66</cell></row><row><cell>DARTS (2nd)</cell><cell>37.51?3.19</cell><cell>53.89?0.58</cell><cell>13.37?2.35</cell><cell cols="3">13.96?2.33 15.06?1.95 14.84?2.10</cell></row><row><cell>optimal</cell><cell>91.61</cell><cell>94.37</cell><cell>74.49</cell><cell>73.51</cell><cell>46.77</cell><cell>47.31</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The reproducible codes could be found in the supplementary material.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Zero-cost proxies for lightweight nas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Mohamed S Abdelfattah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas D</forename><surname>Dudziak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Understanding and simplifying one-shot architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="549" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Variational inference: A review for statisticians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">D</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcauliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American statistical Association</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">518</biblScope>
			<biblScope unit="page" from="859" to="877" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weight uncertainty in neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Cornebise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1613" to="1622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Optimization methods for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Curtis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nocedal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Siam Review</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="223" to="311" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<title level="m">Proxylessnas: Direct neural architecture search on target task and hardware. ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural architecture search on imagenet in four gpu hours: A theoretically inspired perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wuyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Stabilizing differentiable architecture search via perturbation-based regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangning</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Drnas: Dirichlet neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangning</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruochen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Progressive differentiable architecture search: Bridging the depth gap between search and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Detnas: Backbone search for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaofeng</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6642" to="6652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hierarchical Neural Architecture Search for Deep Stereo Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrtash</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Drummond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongyuan</forename><surname>Ge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hierarchical neural architecture search for deep stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrtash</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Drummond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongyuan</forename><surname>Ge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Fairnas: Rethinking evaluation fairness of weight sharing neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jixiang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.01845</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">One-shot neural architecture search via self-evaluated template network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3681" to="3690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Searching for a robust neural architecture in four gpu hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Nas-bench-201: Extending the scope of reproducible neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Practical variational inference for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2348" to="2356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Irlas: Inverse reinforcement learning for architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Single path one-shot neural architecture search with uniform sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyuan</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Conjugate-computation variational inference: Converting variational inference in nonconjugate models to inferences in conjugate models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Lin</surname></persName>
		</author>
		<idno>PMLR, 2017. 4</idno>
	</analytic>
	<monogr>
		<title level="j">In Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="page" from="878" to="887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fast and scalable bayesian deep learning by weight-perturbation in adam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didrik</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Voot</forename><surname>Tangkaratt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akash</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haavard</forename><surname>Mohammad Emtiyaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10778</idno>
		<title level="m">Learningalgorithms from bayesian principles</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Snip: Single-shot network pruning based on connection sensitivity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namhoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thalaiyasingam</forename><surname>Ajanthan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Block-wisely supervised neural architecture search with knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changlin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuchun</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1986" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Bossnas: Exploring hybrid cnn-transformers with block-wisely selfsupervised neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changlin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<idno>abs/2103.12424, 2021. 1</idno>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Computer Vision Foundation / IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changlin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<idno>2021. 1</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8607" to="8617" />
		</imprint>
	</monogr>
	<note>Dynamic slimmable network</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Random search and reproducibility for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improving one-shot nas by suppressing the posterior fading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingqiu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kechen</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.06035</idno>
		<title level="m">Darts+: Improved differentiable architecture search with early stopping</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Darts: Differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Mellor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elliot J</forename><surname>Crowley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04647</idno>
		<title level="m">Neural architecture search without training</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangming</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Bachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Emtiyaz</forename><surname>Khan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10778</idno>
		<title level="m">Training binary neural networks using the bayesian learning rule</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Xnas: Neural architecture search with expert advice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niv</forename><surname>Nayman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itamar</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihi</forename><surname>Zelnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1975" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Practical deep learning with bayesian principles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuki</forename><surname>Osawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Swaroop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Emtiyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runa</forename><surname>Eschenhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rio</forename><surname>Yokota</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melody</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="4092" to="4101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengzhen</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Yao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.02903</idno>
		<title level="m">A comprehensive survey of neural architecture search: Challenges and solutions</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Understanding architectures learnt by cell-based neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaofeng</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Nas-bench-301 and the case for surrogate benchmarks for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Siems</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arber</forename><surname>Zela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jovita</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.09777</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pruning neural networks without any data by iteratively conserving synaptic flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hidenori</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kunin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Picking winning tickets before training by preserving gradient flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Rethinking architecture selection in differentiable nas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruochen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangning</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2021. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Snas: stochastic neural architecture search. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sirui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hehui</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pc-darts: Partial channel connections for memory-efficient architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2020</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Nas-bench-101: Towards reproducible neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Christiansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Evaluating the search phase of neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaicheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Sciuto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudiu</forename><surname>Musat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Understanding and robustifying differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arber</forename><surname>Zela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yassine</forename><surname>Marrakchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Nas-bench-1shot1: Benchmarking and dissecting one-shot neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arber</forename><surname>Zela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Siems</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2020</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">One-shot neural architecture search: Maximising diversity to overcome catastrophic forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongyuan</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven W</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">One-shot neural architecture search via novelty driven sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taoping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Multinomial distribution learning for effective neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiawu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Bayesnas: A bayesian approach for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongpeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7603" to="7613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
