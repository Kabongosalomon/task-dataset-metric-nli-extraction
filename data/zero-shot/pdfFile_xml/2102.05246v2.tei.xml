<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Memory-Associated Differential Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<postCode>611731</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aiguo</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<postCode>611731</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Hui</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information and Software Engineering</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<postCode>611731</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yan</surname></persName>
							<email>kyan@uestc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<postCode>611731</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Memory-Associated Differential Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conventional Supervised Learning approaches focus on the mapping from input features to output labels. After training, the learnt models alone are adapted onto testing features to predict testing labels in isolation, with training data wasted and their associations ignored. To take full advantage of the vast number of training data and their associations, we propose a novel learning paradigm called Memory-Associated Differential (MAD) Learning. We first introduce an additional component called Memory to memorize all the training data. Then we learn the differences of labels as well as the associations of features in the combination of a differential equation and some sampling methods. Finally, in the evaluating phase, we predict unknown labels by inferencing from the memorized facts plus the learnt differences and associations in a geometrically meaningfull manner. We gently build this theory in unary situations and apply it on Image Recognition, then extend it into Link Prediction as a binary situation, in which our method outperforms strong state-of-the-art baselines on ogbl-ddi dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this work, we develop MAD Learning, Memory-Associated Differential Learning, to inference from the memorized facts that we already know to predict what we want to know. Different from conventional Supervised Learning approaches which learn the mapping from input features to output labels, our method focuses on the relationship between features and features, labels and labels. When predicting on testing features, MAD Learning inferences from similar memory and make some difference upon it to get testing labels. We illustrate this difference in <ref type="figure">Figure 1</ref>.</p><p>In Section 3.1, we gently build this theory in the unary situation. To illustrate that the existence of memory and differences are equally important, we conduct experiments with either or none of these two parts disabled in Section 4.1. <ref type="figure">Figure 1</ref>: The difference between Conventional Supervised Learning and MAD Learning. The former learns the mapping from features to labels in training data and apply this mapping on testing data, while the latter learns the differences and associations among data and inferences testing labels from memorized training data.</p><p>To have a better approximation, we investigate several techniques of sampling such as multi-heads <ref type="bibr" target="#b4">[Vaswani et al., 2017]</ref> and propose a mechanism called Soft Sentinel to softly filter out unreliable estimations. In Section 4.1 we examine the effectiveness of these techniques and mechanism.</p><p>We then extend MAD Learning into binary situations in Section 3.5 where Link Prediction <ref type="bibr" target="#b4">[Sun et al., 2011]</ref> is taken as an example. In Section 4.3 we experiment on dataset ogbl-ddi <ref type="bibr" target="#b4">[Wishart et al., 2018]</ref> from Open Graph Benchmark (OGB) <ref type="bibr">[Weihua Hu, 2020]</ref>. On ogbl-ddi, MAD Learning outperforms state-of-the-art (SotA) baselines.</p><p>Since the differential equation in MAD Learning has the form of the first-order Taylor series approximation, it gains clear interpretability in geometry. In Section 3.5, we visualize a social network Zachary's Karate Club <ref type="bibr" target="#b4">[Zachary, 1977]</ref> to reveal the meaning of learnt encodings.</p><p>Finally, we extend MAD Learning to ternary and multi situations to construct more complex applications such as relation predictors for Knowledge Graphs. However, due to the huge space occupation of this method, this extension remains as a theory that we place it into Section 5 as the discussion.</p><p>As a novel learning paradigm, MAD Learning opens the door for many research directions. We raise some of them in Section 6 as conclusions.  <ref type="bibr" target="#b2">[Mikolov et al., 2010]</ref> had so insufficient to accurately remember all the facts occurred in history, that they proposed Memory Networks <ref type="bibr" target="#b4">[Weston et al., 2015]</ref> to take advantage of historical facts by writing to and reading from External Memory. A similar idea is adopted by us but in a different way. Instead of treating External Memory as a way to add more learnable parameters to store uninterpretable hidden states, we try to memorize the facts as they are, and then learn the differences and associations between them.</p><p>Most of the experiments in this article are designed to solve Link Prediction problem that we predict whether a pair of nodes in a graph are likely to be connected, how much the weight their edge bares, or what attributes their edge should have. In such a field, two of the most popular methods are Graph Convolution Networks (GCN) <ref type="bibr" target="#b0">[Kipf and Welling, 2016]</ref> and Matrix Factorization <ref type="bibr">[Koren et al., 2009]</ref>.</p><p>The idea of GCN is that the hidden representation of a node can be aggregated from the states of itself and of its neighbours, this usually implies that connected nodes should have similar representations. However, this assumption results in over-smoothing <ref type="bibr" target="#b1">[Li et al., 2019]</ref> that the representations of nodes become nearly identical after multiple layers that they can hardly be distinguished. MAD Learning on graphs does not suffer from this issue. It has a loose constraint that the neighbours of the same node ought to have similar representations while the connected pairs do not have to. This matters when edges are directional.</p><p>Matrix Factorization is a classical algorithm used in Recommender Systems. It decomposes the adjacency matrix into the product of two matrices. Each works as a group of embeddings for nodes. Although our method is derived from a different perspective of view, we point out that Matrix Factorization can be seen as a simplification of MAD Learning with no memory and no sampling.</p><p>3 Proposed Approach</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Memory-Associated Differential Learning</head><p>We assume that the output label y of an instance of input features x is a differentiable function y = y(x). And besides bare features x with its label y unknown, we also have another x 0 called reference with its output label y 0 already known.</p><p>By applying Mean Value Theorem for Definite Integrals <ref type="bibr" target="#b0">[Comenetz, 2002]</ref>, we can estimate the unknown y with known y 0 if x 0 is close enough to x:</p><formula xml:id="formula_0">y = y(x) = y 0 + x x0 dy ? y 0 + (x ? x 0 ) ? y (x) ?|y 0</formula><p>In such way, we connect the current prediction tasks y to the past fact y 0 , which can be stored in external memory, and convert the learning of our target function y(x) to the learning of a differential function y (x), which in general is more accessible than the former.</p><p>In <ref type="figure" target="#fig_0">Figure 2</ref>(a), we depict how MAD Learning predict the distribution of labels by incorporating memory and the learnable differential function. The importance of these two parts is verified in Section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Inferencing from Multiple References</head><p>To get a steady and accurate estimation of y, we can sample n references x 1 , x 2 , ? ? ? , x n to get n estimation? y|y 1 ,?|y 2 , ? ? ? ,?|y n and combine them with an aggregator such as mean:?</p><formula xml:id="formula_1">= 1 n ? n i=1? |y i</formula><p>Since the closer the reference x i is, the more accurate the estimation?|y i it gives, we can attach a weight on each estimation inversely proportional to the distance between the reference x i and x. Here we adopt a function Softmin derived from Softmax which rescales the inputted d-dimentional array v so that every element of v lies in the range [0, 1] and all of them sum to 1:</p><formula xml:id="formula_2">Softmin(v i ) = Softmax(?v i ) = e ?vi d j=1 e ?vj</formula><p>By applying Softmin we get the aggregated estimation:</p><formula xml:id="formula_3">? ? ? ? ? ? ?? = 1 Z ? n i=1? |y i ? e ?||xi?x|| Z = n i=1 e ?||xi?x||</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Soft Sentinels and Uncertainty</head><p>With Softmin, inaccurate estimations given by distant references can hardly distort the final result, if but only if nearby references exist. Otherwise, a group of distant references which gives less reliable estimations also has a summed weight of 1, the same as a group of close references. To rectify this issue, we introduce a mechanism on top of Softmin named Soft Sentinel. A Soft Sentinel is a dummy element mixed into the array of estimations with no information (e.g. the logit is 0) but a set distance (e.g. 0).</p><p>The estimation after k Soft Sentinels distant at 1 added is</p><formula xml:id="formula_4">? ? ? ? ? ? ?? = 1 Z ? n i=1? |y i ? e ?||xi?x|| Z = ke ?1 + n i=1 e ?||xi?x||</formula><p>When Soft Sentinels involved, only estimations given by close-enough references can have most of their impacts on the final result that unreliable estimations are supressed.</p><p>Furthermore, The weight of a single Soft Sentinel distant at 0 can be viewed as a measure of uncertainty when predicting: the further the references are, the more the uncertainty is.</p><p>In Section 4.1, along with the comparison between mean and Softmin, we also compare the effect of Soft Sentinels.  <ref type="bibr">(u,v)</ref> is simplified to be ?r ?u |v since it is the change of r after slightly moving u to u0 with v fixed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Other Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adaptors of Position and Memory</head><p>For the sake of flexibility and performance, we usually do not use inputted features x directly, but to first convert x into position f (x). Besides, the training labels are not always consistent with the models' output. For example, sometimes labels stand for discrete possibilities while the model outputs logits. To adapt to this situation, we generally wrap the memory with an adaptor function m such as a one-layer MLP, gettin?</p><formula xml:id="formula_5">y|y 0 = m(y 0 ) + (f (x) ? f (x 0 )) ? g(x)</formula><p>where g(x) stands for gradient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Choice of References</head><p>We investigate four modes to choose references:</p><p>1. Fixed. When the inputs are rich-featured (different inputs are distinguishable simply by features), we can precompute the feature distances among data and find K nearest neighbours for each input as its fixed references.</p><p>2. Random. References are sampled arbitrarily.</p><p>3. Dynamic NN. K nearest neighbours according to the distance of position f (x) (not x as in Fixed Mode) are selected to be references. Since f (x) are dynamically changed following the updating of f , this mode may require heavy computations.</p><p>When the encodings of nodes are dynamic and no features are provided, we usually adopt Random Mode in the training phase for efficiency and adopt Dynamic NN Mode in the evaluation phase for performance.</p><p>In experiments of Section 4.1 that we carry on dataset ogblddi, we record both the scores in Random Mode and Dynamic NN Mode in the evaluating phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multiple Heads</head><p>Multi-heads can be a solution when it is hard to boost performance by adding more parameters in a single structure. It applies a model in separate instances. Each instance has the potential to learn embeddings from different subspaces. So it can also be regarded as an approach of Sampling.</p><p>We implement multi-heads in MAD Learning by combining the results from separate instances with mean function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Binary MAD Learning Link Prediction</head><p>We model the relationship between a pair of nodes in a graph by extending MAD Learning into binary situations.</p><p>Like what we do in the previous section, we first assume that the relation r between node u and node v is a differentiable function: r = r <ref type="bibr">(u, v)</ref>. And besides the to-predict pair (u, v), we also have another pair of nodes (u 0 , v 0 ) called a reference, with their relation r 0 already known.</p><p>We apply Total Derivative and Mean Value Theorem for Definite Integrals, getting:</p><formula xml:id="formula_6">r = r(u, v) = r 0 + (u,v) (u0,v0) dr = r 0 + (u,v) (u0,v0) ( ?r ?u du + ?r ?v dv) ? r 0 + (u ? u 0 ) ? ?r ?u | (u,v) + (v ? v 0 ) ? ?r ?v | (u,v) r|r 0</formula><p>To simplify the above model and assure the reference (u 0 , v 0 ) as close to the to-predict pair (u, v) as possible, we set u 0 = u or v 0 = v, meaning (u, v) always shares with a common node with r(u 0 , v 0 ).</p><p>When v = v 0 holds, the partial differential ?r ?u | (u,v) can be regarded as the change of r(u, v) after slightly moving the node u to u 0 but with node v fixed, as depicted in <ref type="figure" target="#fig_0">Figure 2(b)</ref>.</p><p>Therefore, we may further assume ?r ?u | <ref type="bibr">(u,v)</ref>  v 0 and ?r ?v | (u,v) = g 2 (u) if u = u 0 , making</p><formula xml:id="formula_7">= g 1 (v) if v =</formula><formula xml:id="formula_8">r|r 0 = g 1 (v) ? (u ? u 0 ) + r 0 , if v = v 0 r|r 0 = g 2 (u) ? (v ? v 0 ) + r 0 , if u = u 0</formula><p>Here g 1 (?) is destination differential function and g 2 (?) is source differential function. If the edge is undirected, these two functions can be shared. A direct application in this binary situation is to predict whether a pair of nodes in a graph are likely to be connected. We test our method on dataset ogbl-ddi from OGB and MAD Learning outperforms SotA baselines by a large margin but with fewer parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Geometric Meaning of MAD Learning</head><p>To reveal the meaning of the positions and gradients within MAD Learning, in <ref type="figure" target="#fig_1">Figure 3</ref>, we visualize the 2-dimensional encodings learnt from only connections in Zachary's Karate Club, a social network representing the interaction among club members from two communities. In both plots, each member as a node is placed at her 2-dimensional position and coloured according to which group she belongs.</p><p>In the left plot, the nodes are positioned geometrically into two clusters. This implies that the positions may have enough information for downstream tasks such as Node Classification <ref type="bibr" target="#b2">[Perozzi et al., 2014]</ref>. And as we discussed earlier when comparing with GCN in Section 2, connected nodes have not to be close to each other, while neighbours of the same node tend to stay in nearby positions.</p><p>In the right plot, we see each node is attached with its gradient as a vector, pointing regularly opposite to the centre. More precisely, a node's gradient is the direction along with which other nodes become more and more 'connectable'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>If not mentioned, the following experiments 1 in this work use Adam <ref type="bibr" target="#b0">[Kingma and Ba, 2014]</ref> as optimizers with their learning rate set to 0.005, set K = 8, run in Random Mode in the training phase and in Dynamic NN Mode in the evaluating phase, encode positions and gradients into 32-dimensional vectors, disable multi-heads, and mix Softmin with 8 Soft Sentinels distant at 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">On Hyperparameters</head><p>In this section, we evaluate our method on the dataset ogblddi from Open Graph Benchmark (OGB) to analyse hyperparameters of MAD Learning. The metric is Hits@20, the rate of true connections that are ranked higher than the 20 top-ranked but false ones.</p><p>In the training phase, we sample arbitrary pairs of nodes to construct negative samples <ref type="bibr" target="#b0">[Grover and Leskovec, 2016]</ref> and compare the scores between connected pairs and negative samples with Cross-Entropy as the loss function:</p><formula xml:id="formula_9">L = ? y i=1 log(p y (i)) ? n i=1 log(1 ? p n (i))</formula><p>where y is the number of positive samples and n of negative samples, p y (i) is the predicted probability of the i-th positive sample and p n (i) of the i-th negative sample.</p><p>In the evaluating phase, we record the scores not only in Dynamic NN Mode but also in Random Mode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Memory and Differential Functions</head><p>To measure how important the memory and the differential functions are, we experiment with different parts of MAD Learning disabled:</p><p>1. mad. Complete MAD Learning.</p><p>2. nograd. MAD with gradients g 1 (v) = g 2 (u) = 0. Only memory and the Softmin weights are involved.</p><p>3. nomem. MAD with memory r 0 = 0.</p><p>The results are depicted in <ref type="figure" target="#fig_2">Figure 4(a)</ref>. The performance of the complete MAD Learning in Dynamic NN Mode significantly surpasses all others, proving that both memory and inference are indispensable.</p><p>Besides, we notice that in Random Mode, having no memory still works, because in such way the referenced nodes u 0 and v 0 can be regarded as some other pseudo nodes located at the same positions but with 0 logits to connect to v or u.</p><p>Furthermore, Matrix Factorization can be reduced to MAD Learning with no memory but only one fixed reference of v and a pseudo node located at the origin point, a?</p><formula xml:id="formula_10">r = g 1 (v) ? (u ? 0) + 0 = g 1 (v) ? u</formula><p>Another discovery is that the performance without memory in Dynamic NN Mode is far worth than in Random Mode. This is because references in Dynamic NN Mode are too close to contribute enough differences to reach the scale of predictions without memory. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Aggregators and Soft Sentinel</head><p>We have these three experimental settings to examine the contribution of Softmin and Soft Sentinels:</p><p>1. mean. Estimations are aggregated by mean function.</p><p>2. softmin. Estimations given by different references are summed up weighted by the results of Softmin applied to the distances.</p><p>3. sentinel. Estimations of softmin with 8 Soft Sentinels at distance 1 added.</p><p>As is shown in <ref type="figure" target="#fig_2">Figure 4(b)</ref>, it is no much difference between mean and Softmin. But when mixed with Soft Sentinels, MAD Learning performs better and converges faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Image Recognition</head><p>We conduct experiments on Image Recognition as an application of unary MAD Learning.</p><p>The datasets we use are <ref type="bibr">MNIST [LeCun et al., 1998]</ref>, <ref type="bibr">KMNIST [Clanuwat et al., 2018]</ref>, CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b0">[Krizhevsky et al., 2009]</ref>. The baselines are a twolayered convolutional neural network notated as ConvNet and ResNet18 <ref type="bibr" target="#b0">[He et al., 2015]</ref>. In MAD Learning we separately use the above ConvNet or ResNet18 to extract image features before mapping them into positions and gradients. The two variances of MAD Learning with different features extractors are notated as MAD-conv and MAD-18.</p><p>We train these models for 50 epochs and record their best accuracy scores every 5 epochs, which are summarized in Table 1. As we can see, MAD Learning has no advantage in this application. We suggest that MAD Learning is better at complex tasks involving both memory and inference. Since Image Recognition is a intuitive task as "You know it when </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Link Prediction</head><p>We compare the performance of MAD Learning, implemented with 12 heads, 12-dimensional positions and 12dimensional gradients, against SotA baselines from the top of the leaderboard on OGB, including GCN, Graph-SAGE <ref type="bibr" target="#b0">[Hamilton et al., 2017]</ref>, JKNet <ref type="bibr" target="#b4">[Xu et al., 2018]</ref>, and LRGA <ref type="bibr" target="#b3">[Puny et al., 2020]</ref>.</p><p>Results in <ref type="table" target="#tab_3">Table 2</ref> show that MAD Learning can achieve a higher Hits@20 score with fewer parameters, thus producing the new SotA.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Most experiments in this work are conducted on predicting links where the relation r represents a logit, but it is not difficult to explain it as edge weights. And by extending it from a scalar to a vector, MAD Learning can be used for graphs with featured edges. We also point out that MAD Learning can learn relations in heterogeneous graphs where nodes belong to different types (usually represented by encodings in different lengths). The only requirement is that positions of the source nodes should match with gradients of the destination nodes and vice versa.</p><p>For example, in Recommender Systems, we can encode positions of users and gradients of items in 8-dimensional vectors, and encode positions of items and gradients of users in vectors with different dimensions, say 16.</p><p>For ternary relations such as head-relation-tail triplets f = f (h, r, t) in Knowledge Graphs, we may also extend the binary MAD Learning into: ? ? ? ? ?f |f 0 = g 1 (h, r) ? (t ? t 0 ) + f 0 , if h = h 0 , r = r 0 f |f 0 = g 2 (r, t) ? (h ? h 0 ) + f 0 , if r = r 0 , t = t 0 f |f 0 = g 3 (t, h) ? (r ? r 0 ) + f 0 , if t = t 0 , h = h 0</p><p>The same extension can be made in multi situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we explore a novel learning paradigm which is flexible, effective and interpretable. The outstanding results, especially on Link Prediction, open the door for several research directions:</p><p>1. The most important part of MAD Learning is memory.</p><p>However, MAD Learning have to index the whole training data for random access. In Link Prediction, we implement memory as a dense adjacency matrix which results in huge occupation of space. The way to shrink memory and improve the utilization of space should be investigated in the future.</p><p>2. Based on memory as the ground-truth, MAD Learning appends some difference as the second part. We implement this difference simply as the product of distance and differential function, but we believe there exist different ways to model it.</p><p>3. The third part of MAD Learning is the similarity, which is used to assign weights to estimations given by different references. We reuse distance to compute the similarity, but decoupling it by some other embeddings and some other measurements such as inner product should also be worthy to explore.</p><p>4. In this work, we do deliberately not combine direct information to focus only on MAD Learning. Since MAD Learning takes another parallel route to predict, we believe integrating MAD Learning and Conventional Supervised Learning is also a promising direction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>(a) Memory-Associated Differential Learning inferences labels from memorized ones following the first-order Taylor series approximation: y ? y0 + ?x ? y (x). (b) In binary MAD Learning, when v = v0 holds, ?r ?u |</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>2-dimensional encodings learnt from connections in Zachary's Karate Club. Nodes are placed with their 2-dimensional positions as coordinates and coloured according to their groups. In the left plot, nodes are connected if corresponding club members interact. In the right plot, 2-dimensional gradients are visualized as vectors attached to nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Hits@20 on ogbl-ddi in 25 epochs. R in brackets stands for Random Mode in the evaluating phase and N for Dynamic NN Mode</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:2102.05246v2 [cs.LG] 15 Jun 2021 2 Related Works Known facts to Machine Learning models are just memorized experience to human. Early in 2014, researchers in Natural Language Processing believed the internal memory a Recurrent Neural Network</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Link Prediction on ogbl-ddi</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code: https://github.com/cf020031308/mad-learning</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Clanuwat</surname></persName>
		</author>
		<idno>abs/1609.02907</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<editor>Koren et al., 2009] Yehuda Koren, Robert Bell, and Chris Volinsky</editor>
		<meeting><address><addrLine>Ba</addrLine></address></meeting>
		<imprint>
			<publisher>Grover and Leskovec</publisher>
			<date type="published" when="1998" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
	<note>Yann LeCun. Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns</title>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019</title>
		<meeting><address><addrLine>Seoul, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association</title>
		<editor>Takao Kobayashi, Keikichi Hirose, and Satoshi Nakamura</editor>
		<meeting><address><addrLine>Makuhari, Chiba, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010-09-26" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
	<note>Online learning of social representations</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">From graph low-rank global attention to 2-fwl approximation</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Co-author relationship prediction in heterogeneous bibliographic networks</title>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<idno>arXiv:1806.03536</idno>
	</analytic>
	<monogr>
		<title level="m">Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks</title>
		<editor>Marinka Zitnik Yuxiao Dong Hongyu Ren Bowen Liu Michele Catasta Jure Leskovec Weihua Hu, Matthias Fey</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1977" />
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="452" to="473" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Nucleic acids research</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
