<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GraphMLP: A Graph MLP-Like Architecture for 3D Human Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Guo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runwei</forename><surname>Ding</surname></persName>
						</author>
						<title level="a" type="main">GraphMLP: A Graph MLP-Like Architecture for 3D Human Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-3D Human Pose Estimation</term>
					<term>Multi-Layer Per- ceptron</term>
					<term>Graph Convolutional Network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modern multi-layer perceptron (MLP) models have shown competitive results in learning visual representations without self-attention. However, existing MLP models are not good at capturing local details and lack prior knowledge of human body configurations, which limits their modeling power for skeletal representation learning. To address these issues, we propose a simple yet effective graph-reinforced MLP-Like architecture, named GraphMLP, that combines MLPs and graph convolutional networks (GCNs) in a global-local-graphical unified architecture for 3D human pose estimation. GraphMLP incorporates the graph structure of human bodies into an MLP model to meet the domain-specific demand of the 3D human pose, while allowing for both local and global spatial interactions. Furthermore, we propose to flexibly and efficiently extend the GraphMLP to the video domain and show that complex temporal dynamics can be effectively modeled in a simple way with negligible computational cost gains in the sequence length. To the best of our knowledge, this is the first MLP-Like architecture for 3D human pose estimation in a single frame and a video sequence. Extensive experiments show that the proposed GraphMLP achieves stateof-the-art performance on two datasets, i.e., Human3.6M and MPI-INF-3DHP. Our source code will be open-sourced.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>that the skeleton of a human body can be naturally represented as the graph structure and utilize graph convolutional networks (GCNs) for this task <ref type="bibr" target="#b10">[10]</ref>- <ref type="bibr" target="#b12">[12]</ref>. Although GCN-based methods are effective at aggregating neighboring nodes for extracting local features, they suffer from limited receptive fields to obtain stronger representation power. Recently, Transformers <ref type="bibr" target="#b13">[13]</ref> with a global receptive field have shown promising advances on many computer vision tasks <ref type="bibr" target="#b14">[14]</ref>- <ref type="bibr" target="#b17">[17]</ref>. Nevertheless, the strong performance of Transformers comes at a relatively high computational cost in the selfattention block whose complexity increases quadratically with the number of input sequences. Considering this, recent progress on modern multi-layer perceptron (MLP) models, in particular MLP-Mixer <ref type="bibr" target="#b7">[7]</ref>, provides new network architectural designs in vision. The MLP model stacks only fully-connected layers without self-attention and consists of two types of blocks. The spatial MLP aggregates global information among tokens, and the channel MLP focuses on extracting features for each token. By stacking these two MLP blocks, it can be simply built with less inductive bias and achieves impressive performance in learning visual representations. This motivates us to explore an MLP-Like architecture for skeleton-based representation learning.</p><p>However, there remain two critical challenges in adapting the MLP models from vision to skeleton: (i) Despite their successes in vision, existing MLPs are less effective in modeling graphstructured data due to their simple connections among all nodes. Different from RGB images represented by highly dense pixels, skeleton inputs are inherently sparse and graphstructured data (see <ref type="figure">Fig. 2 (left)</ref>). Without incorporating the prior knowledge of human body configurations, the model is prone to learning spurious dependencies, which can lead Overview of the proposed GraphMLP architecture. The left illustrates the skeletal structure of the human body. The 2D joint inputs detected by a 2D pose estimator are sparse and graph-structured data. GraphMLP treats each 2D keypoint as an input token, linearly embeds each of them through the skeleton embedding, feeds the embedded tokens to GraphMLP layers, and finally performs regression on resulting features to predict the 3D pose via the prediction head. Each GraphMLP layer contains one spatial graph MLP (SG-MLP) and one channel graph MLP (CG-MLP). For easy illustration, we show the architecture using a single image as input.</p><p>to physically implausible poses. (ii) Such models are able to capture global interactions via their spatial MLPs but may not be good at capturing fine-grained local details. Yet, local information is important for 3D human pose estimation <ref type="bibr" target="#b16">[16]</ref>, <ref type="bibr" target="#b18">[18]</ref>, <ref type="bibr" target="#b19">[19]</ref>, since a body joint is highly related to adjacent joints, e.g., the position of a missing joint can be approximately predicted from neighboring joints according to the semantics of human actions and structures of human bodies.</p><p>To overcome both limitations, we present GraphMLP, a graph-reinforced MLP-Like architecture for 3D human pose estimation, as depicted in <ref type="figure">Fig. 2</ref>. Our GraphMLP is conceptually simple yet effective: it builds a strong collaboration between modern MLPs and GCNs to construct a global-local-graphical unified architecture for learning better skeletal representations. Specifically, the GraphMLP mainly contains a stack of novel GraphMLP layers. Each layer consists of two Graph-MLP blocks where the spatial graph MLP (SG-MLP) and the channel graph MLP (CG-MLP) blocks are built by injecting GCNs into the spatial MLP and channel MLP, respectively. By combining MLPs and GCNs in a unified architecture, our GraphMLP is able to obtain the prior knowledge of human configurations encoded by the graph's connectivity and capture both local and global spatial interactions among body joints, hence yielding better performance.</p><p>In addition, we extend our GraphMLP from a single frame to a video sequence. Most existing video-based methods <ref type="bibr" target="#b20">[20]</ref>- <ref type="bibr" target="#b22">[22]</ref> independently model temporal information by treating each frame as a token or regarding the time axis as a separated dimension, but too computationally expensive to process long videos (e.g., 243 frames), limiting their real-world application scenarios. To tackle this issue, we propose to utilize a simple and efficient video representation of pose sequences to capture complex temporal dynamics, which is also a unified and flexible representation to be compatible with arbitrary-length sequences (i.e., a single frame and variable-length videos). This representation uniquely mixes temporal information in the feature channels and maintains treating each joint as a token, offering negligible computational cost gains in the sequence length.</p><p>The proposed GraphMLP is evaluated on two challenging datasets, Human3.6M <ref type="bibr" target="#b23">[23]</ref> and MPI-INF-3DHP <ref type="bibr" target="#b24">[24]</ref>. Extensive experiments show the effectiveness and generalization ability of our approach, which advances the state-of-the-art performance for estimating 3D human poses from a single image. Its performance surpasses the previous best result <ref type="bibr" target="#b25">[25]</ref> by 1.4 mm in mean per joint position error (MPJPE) on the Human3.6M dataset. Furthermore, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, it brings clear 6.6 mm and 6.9 mm improvements in MPJPE for the MLP model <ref type="bibr" target="#b7">[7]</ref> and the GCN model <ref type="bibr" target="#b8">[8]</ref> on the MPI-INF-3DHP dataset, respectively. Surprisingly, compared to video pose Transformer (e.g., PoseFormer <ref type="bibr" target="#b21">[21]</ref>), even with 5? fewer computational costs, our MLP-Like architecture achieves better performance.</p><p>Overall, our main contributions are summarized as follows: </p><formula xml:id="formula_0">? We present,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>3D Human Pose Estimation. There are mainly two categories to estimate 3D human poses. The first category of methods directly regresses 3D human joints from RGB images <ref type="bibr" target="#b26">[26]</ref>- <ref type="bibr" target="#b30">[30]</ref>. The second category is the 2D-to-3D pose lifting method <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b20">[20]</ref>, <ref type="bibr" target="#b31">[31]</ref>, which employs an off-the-shelf 2D pose detection as the front end and designs a 2D-to-3D lifting network using detected 2D poses as input. This lifting method can achieve state-of-the-art performance and has become the mainstream method due to its efficiency and effectiveness. For example, FCN <ref type="bibr" target="#b2">[3]</ref> shows that 3D poses can be regressed simply and effectively from 2D keypoints with fully-connected networks. TCN <ref type="bibr" target="#b20">[20]</ref> extends the FCN to video by utilizing temporal convolutional networks to exploit temporal information from 2D pose sequences.</p><p>Since the physical skeleton topology can form a graph structure, recent progress has focused on employing graph convolutional networks (GCNs) to address the 2D-to-3D lifting problem. LCN <ref type="bibr" target="#b6">[6]</ref> introduces a locally connected network to improve the representation capability of GCN. SemGCN <ref type="bibr" target="#b10">[10]</ref> allows the model to learn the semantic relationships among the human joints. MGCN <ref type="bibr" target="#b25">[25]</ref> improves SemGCN by introducing a weight modulation and an affinity modulation. Transformers in Vision. Recently, Transformer-based methods achieve excellent results on various computer vision tasks, such as image classification <ref type="bibr" target="#b14">[14]</ref>, <ref type="bibr" target="#b15">[15]</ref>, <ref type="bibr" target="#b32">[32]</ref>, object detection <ref type="bibr" target="#b33">[33]</ref>- <ref type="bibr" target="#b35">[35]</ref>, and pose estimation <ref type="bibr" target="#b16">[16]</ref>, <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b22">[22]</ref>, <ref type="bibr" target="#b36">[36]</ref>. The seminal work of ViT <ref type="bibr" target="#b14">[14]</ref> divides an image into 16?16 patches and uses a pure Transformer encoder to extract visual features. Strided Transformer <ref type="bibr" target="#b16">[16]</ref> incorporates strided convolutions into Transformers to aggregate information from local contexts for video-based 3D human pose estimation. PoseFormer <ref type="bibr" target="#b21">[21]</ref> utilizes a pure Transformer-based architecture to model spatial and temporal relationships from videos. MHFormer <ref type="bibr" target="#b22">[22]</ref> proposes a Transformer-based framework with multiple plausible pose hypotheses. Mesh Graphormer <ref type="bibr" target="#b36">[36]</ref> combines GCNs and attention layers in a serial order to capture local and global dependencies for human mesh reconstruction.</p><p>Unlike <ref type="bibr" target="#b36">[36]</ref>, we mainly investigate how to combine more efficient architectures (i.e., modern MLPs) and GCNs to construct a stronger architecture for 3D human pose estimation and adopt a parallel manner to make it possible to model local and global information at the same time. Moreover, different from previous video-based methods <ref type="bibr" target="#b16">[16]</ref>, <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b22">[22]</ref> that treat each frame as a token for temporal modeling, we mix features across all frames and maintain each joint as a token, which makes the network to be economical and easy to train. MLPs in Vision. Modern MLP models are proposed to reduce the inductive bias and computational cost by replacing the complex self-attentions of Transformers with spatial-wise linear layers <ref type="bibr" target="#b37">[37]</ref>- <ref type="bibr" target="#b39">[39]</ref>. MLP-Mixer <ref type="bibr" target="#b7">[7]</ref> firstly proposes an MLP-Like model, which is a simple network architecture containing only pure MLP layers. Compared with FCN <ref type="bibr" target="#b2">[3]</ref> (i.e., conventional MLPs), this architecture introduces some modern designs, e.g., layer normalization (LN) <ref type="bibr" target="#b40">[40]</ref>, GELU <ref type="bibr" target="#b41">[41]</ref>, mixing spatial information. Moreover, ResMLP <ref type="bibr" target="#b42">[42]</ref> proposes a purely MLP architecture with the Affine transformation. CycleMLP <ref type="bibr" target="#b43">[43]</ref> proposes a cycle fully-connected layer to aggregate spatial context information and deal with variable input image scales. However, these modern MLP models have not yet been applied to 3D human pose estimation. Inspired by their successes in vision, we first attempt to explore how MLP-Like architectures can be used for 3D human pose estimation in non-Euclidean skeleton data. The difference between our approach and the existing MLPs is that we introduce the inductive bias of the physical skeleton topology by combining MLP models with GCNs, providing more physically plausible and accurate estimations. We further investigate applying MLP-Like architecture in video and design an efficient video representation, which is seldom studied. <ref type="figure">Fig. 2</ref> illustrates the overall architecture of the proposed GraphMLP. Our approach takes 2D joint locations P ? R N ?2 estimated by an off-the-shelf 2D pose detector as input and outputs predicted 3D poses X ? R N ?3 , where N is the number of joints. The proposed GraphMLP architecture consists of a skeleton embedding module, a stack of L identical GraphMLP layers, and a prediction head module. The core operation of GraphMLP architecture is the GraphMLP layer, each of which has two parts: a spatial graph MLP (SG-MLP) block and a channel graph MLP (CG-MLP) block. Our GraphMLP has a similar architecture to the original MLP-Mixer <ref type="bibr" target="#b7">[7]</ref>, but we incorporate graph convolutional networks (GCNs) into the model to meet the domain-specific requirement of the 3D human pose estimation and learn the local and global interactions of human body joints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED GRAPHMLP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Preliminary</head><p>In this subsection, we briefly introduce the preliminaries in GCNs and modern MLPs.</p><p>1) Graph Convolutional Networks: Let G = (V, A) denotes a graph where V is a set of N nodes and A ? {0, 1} N ?N is the adjacency matrix encoding the edges between the nodes. Given a ( ? 1) th layer feature X ?1 ? R N ?C with C dimensions, a generic GCN <ref type="bibr" target="#b44">[44]</ref> layer, which is used to aggregate features from neighboring nodes, can be formulated as:</p><formula xml:id="formula_1">X = D ? 1 2 A D ? 1 2 X ?1 W,<label>(1)</label></formula><p>where W ? R C?C is the learnable weight matrix and A = A + I is the adjacency matrix with added self-connections. I is the identity matrix, D is a diagonal matrix, and D ii = j? ij . The design principle of the GCN block is similar to <ref type="bibr" target="#b8">[8]</ref>, but we adopt a simplified single-frame version that contains only one graph layer. For the GCN blocks of our GraphMLP, the nodes in the graph denote the joint locations of the human body in <ref type="figure" target="#fig_2">Fig. 3 (a)</ref>, and the adjacency matrix represents the bone connections between every two joints for node information passing in <ref type="figure" target="#fig_2">Fig. 3 (b)</ref>, e.g., the rectangle of 1st row and 2nd column denotes the connection between joint 0 and joint 1. In addition, the different types of bone connections use different kernel weights following <ref type="bibr" target="#b8">[8]</ref>.</p><p>2) Modern MLPs: MLP-Mixer is the first modern MLP model proposed in <ref type="bibr" target="#b7">[7]</ref>. It is a simple and attention-free architecture that mainly consists of a spatial MLP and a channel MLP, as illustrated in <ref type="figure" target="#fig_3">Fig. 4 (a)</ref>. The spatial MLP aims to transpose the spatial axis and channel axis of tokens to mix spatial information. Then the channel MLP processes tokens in the channel dimension to mix channel information. Let X ?1 ? R N ?C be an input feature, an MLP-Mixer layer can be calculated as:</p><formula xml:id="formula_2">X = X ?1 + Spatial-MLP(LN(X ?1 )) ,<label>(2)</label></formula><formula xml:id="formula_3">X = X + Channel-MLP(LN(X )),<label>(3)</label></formula><p>where LN(?) is layer normalization (LN) <ref type="bibr" target="#b40">[40]</ref> and is the matrix transposition. Both spatial and channel MLPs contain two linear layers and a GELU <ref type="bibr" target="#b41">[41]</ref> non-linearity in between.</p><p>We adopt this MLP model as our baseline, which is similar to <ref type="bibr" target="#b7">[7]</ref>, but we transpose the tokens before LN in the spatial MLP (i.e., normalize tokens along the spatial dimension). However, such a simple MLP model neglects to extract fine-grained local details and lacks prior knowledge about the human configurations, which are perhaps the bottlenecks restricting the representation ability of MLP-Like architectures for learning skeletal representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Network Architecture</head><p>In this work, we present a novel GraphMLP built upon the MLP-Mixer described in <ref type="bibr" target="#b7">[7]</ref> to overcome the aforementioned limitations of existing MLP models. Below we elaborate on each module used in GraphMLP and provide its detailed implementations.</p><p>1) Skeleton Embedding: Raw input data are mapped to latent space via the skeleton embedding module. Given the input 2D pose P ? R N ?2 with N body joints, we treat each joint as an input token. These tokens are projected to the high-dimension token feature X 0 ? R N ?C by a linear layer, where C is the hidden size. 2) GraphMLP Layer: The existing MLP simply connects all nodes but does not take advantage of graph structures, making it less effective in handling graph-structured data. To tackle this issue, we introduce the GraphMLP layer that unifies globality, locality, and connectivity in a single layer. Compared with the original MLP-Mixer in <ref type="figure" target="#fig_3">Fig. 4</ref> (a), the main difference of our GraphMLP layer ( <ref type="figure" target="#fig_3">Fig. 4 (b)</ref>) is that we utilize GCNs for local feature communication. This modification retains the domain-specific knowledge of human body configurations, which induces an inductive bias enabling the GraphMLP to perform very well in skeletal representation learning.</p><p>Specifically, our GraphMLP layer is composed of an SG-MLP and a CG-MLP. The SG-MLP and CG-MLP are built by injecting GCNs into the spatial MLP and channel MLP (mentioned in Sec. III-A), respectively. To be more specific, the SG-MLP contains a spatial MLP block and a GCN block. These blocks process token features in parallel, where the spatial MLP extracts features among tokens with a global receptive field, and the GCN block focuses on aggregating local information between neighboring joints. The CG-MLP has a similar architecture to SG-MLP but replaces the spatial MLP with the channel MLP and has no matrix transposition. Based on the above description, the MLP layers in Eq. (2) and Eq. (3) are modified to process tokens as:</p><formula xml:id="formula_4">X = X ?1 + Spatial-MLP(LN(X ?1 )) + GCN(LN(X ?1 ) ),<label>(4)</label></formula><formula xml:id="formula_5">X = X + Channel-MLP(LN(X )) + GCN(LN(X )),<label>(5)</label></formula><p>where GCN(?) denotes the GCN block, ? [1, . . . , L] is the index of GraphMLP layers. Here X and X are the output features of the SG-MLP and the CG-MLP for block , respectively.</p><p>3) Prediction Head: Different from <ref type="bibr" target="#b7">[7]</ref>, <ref type="bibr" target="#b14">[14]</ref> that use a classifier head to do classification, our prediction head performs regression with a linear layer. It is applied on the extracted features X L ? R N ?C of the last GraphMLP layer to predict the final 3D pose X ? R N ?3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Loss Function:</head><p>To train our GraphMLP, we apply an L 2 -norm loss to calculate the difference between prediction and ground truth. The model is trained in an end-to-end fashion, and the L 2 -norm loss is defined as follows:</p><formula xml:id="formula_6">L = N n=1 J n ? X n 2 ,<label>(6)</label></formula><p>where X n and J n are the predicted and ground truth 3D coordinates of joint n, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Extension in the Video Domain</head><p>To extend our GraphMLP for capturing temporal information, we introduce a simple video representation that changes the skeleton embedding module in the original architecture. Specifically, given a 2D pose sequence P ? R T ?N ?2 with T frames and N joints, we first concatenate features between the coordinates of x-y axis and all frames for each joint into P ? R N ?(2?T ) , and then fed it into a linear layer W ? R (2?T )?C to map the tokens and get X 0 ? R N ?C . Subsequently, each joint is treated as an input token and fed into the GraphMLP layers and prediction head module to output the 3D pose of the center frame X ? R N ?3 . These processes of GraphMLP in the video domain are illustrated in <ref type="figure">Figure 5</ref>. This is also a unified and flexible representation strategy that can process arbitrary-length sequences, e.g., a single frame with T = 1 and variable-length videos with T = 27, 81, 243. Interestingly, we find that using a single linear layer to encode temporal information can achieve competitive performance without explicitly temporal modeling by treating each frame as a token. More importantly, this representation is efficient since the amount of N is small (e.g., 17 joints). The increased computational costs from single frame to video sequence are only in one linear layer that weights are W ? R (2?T )?C with linear computational complexity to sequence length, which can be neglected. Meanwhile, the computational overhead and the number of parameters in the GraphMLP layers and the prediction head module are the same for different input sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Implementation Details</head><p>In our implementation, the proposed GraphMLP is stacked by L = 3 GraphMLP layers with hidden size C = 512, the MLP dimension of CG-MLP, i.e., D C = 1024, and the MLP dimension of SG-MLP, i.e., D S = 256. The whole framework is trained in an end-to-end fashion from scratch on a single NVIDIA RTX 2080 Ti GPU. The learning rate starts from 0.001 with a decay factor of 0.95 utilized each epoch and 0.5 utilized per 5 epochs. We follow the generic data augmentation (horizontal flip augmentation) in <ref type="bibr" target="#b8">[8]</ref>, <ref type="bibr" target="#b20">[20]</ref>, <ref type="bibr" target="#b25">[25]</ref>. Following <ref type="bibr" target="#b9">[9]</ref>, <ref type="bibr" target="#b19">[19]</ref>, <ref type="bibr" target="#b25">[25]</ref>, we use 2D joints detected by cascaded pyramid network (CPN) <ref type="bibr" target="#b45">[45]</ref> for Human3.6M and 2D joints provided by the dataset for MPI-INF-3DHP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we first introduce experimental settings for evaluation. Then, we compare the proposed GraphMLP with state-of-the-art methods. We also conduct detailed ablation studies on the importance of designs in our proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets and Evaluation Metrics</head><p>Human3.6M. Human3.6M <ref type="bibr" target="#b23">[23]</ref> is the largest benchmark for 3D human pose estimation. It contains 3.6 million video frames captured by a motion capture system in an indoor environment, where 11 professional actors perform 15 actions such as greeting, phoning, and sitting. Following previous works <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b9">[9]</ref>, <ref type="bibr" target="#b19">[19]</ref>, our model is trained on five subjects (S1, S5, S6, S7, S8) and tested on two subjects (S9, S11). We report our performance using two evaluation metrics. One is the mean per joint position error (MPJPE), referred to as Protocol #1, which calculates the mean Euclidean distance in millimeters between the predicted and the ground truth joint coordinates. The other is the PA-MPJPE which measures the MPJPE after Procrustes analysis (PA) <ref type="bibr" target="#b50">[50]</ref> and is referred to as Protocol #2. MPI-INF-3DHP. MPI-INF-3DHP <ref type="bibr" target="#b24">[24]</ref> is a large-scale 3D human pose dataset containing both indoor and outdoor scenes. Its test set consists of three different scenes: studio with green screen (GS), studio without green screen (noGS), and outdoor scene (Outdoor). Following previous works <ref type="bibr" target="#b6">[6]</ref>, <ref type="bibr" target="#b9">[9]</ref>, <ref type="bibr" target="#b25">[25]</ref>, we use Percentage of Correct Keypoint (PCK) with the threshold of 150 mm and Area Under Curve (AUC) for a range of PCK thresholds as evaluation metrics. To verify the generalization ability of our approach, we directly apply the model trained on Human3.6M to the test set of this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison with State-of-the-Art Methods</head><p>Comparison with Single-frame Methods. <ref type="table" target="#tab_1">Table I</ref> reports the performance comparison between our GraphMLP and previous state-of-the-art methods on Human3.6M. It can be seen that our approach reaches 49.2 mm in MPJPE and 38.6 mm in PA-MPJPE, which outperforms all previous methods. Note that some works <ref type="bibr" target="#b8">[8]</ref>, <ref type="bibr" target="#b25">[25]</ref> adopt a pose refinement module to boost the performance further. Compared with them, GraphMLP achieves lower MPJPE even though we do not use the refinement module. Moreover, with this module, it achieves Due to the uncertainty of 2D detections, we also report results using ground truth 2D keypoints as input to explore the upper bound of the proposed approach. As shown in <ref type="table" target="#tab_1">Table II</ref>, our GraphMLP obtains substantially better performance when given precise 2D joint information and attains state-of-the-art performance, which indicates its effectiveness. <ref type="table" target="#tab_1">Table III</ref> further compares our GraphMLP against previous state-of-the-art methods on cross-dataset scenarios. We only train our model on the Human3.6M dataset and test it on the MPI-INF-3DHP dataset. The results show that our approach obtains the best results in all scenes and all metrics, consistently surpassing other methods. This verifies the strong generalization ability of our approach to unseen scenarios. Comparison with Video-based Methods. As shown in Table IV, our method achieves outstanding performance against video-based methods in both CPN and GT inputs. The proposed method surpass our baseline model (i.e., MLP-Mixer <ref type="bibr" target="#b7">[7]</ref>) by a large margin of 4.6 mm (10% improvement) with CPN inputs and 4.9 mm (14% improvement) with GT inputs. Compared with the most related work, Poseformer <ref type="bibr" target="#b21">[21]</ref>, a self-attention based architecture, our GraphMLP, such a self-attention free architecture, improves the results from 31.3 mm to 30.3 mm with GT inputs (3% improvement).  <ref type="bibr" target="#b21">[21]</ref> in different input frames. Surprisingly, our method requires only 20% FLOPs (356M vs. 1625M) while achieving better performance. Note that <ref type="bibr" target="#b21">[21]</ref> does not report the result of the 243-frame model since the training is difficult and time-consuming, where it requires 4874M FLOPs. In contrast, our method requires only 14? fewer FLOPs and offers negligible parameters and FLOPs gains in the sequence length, which allows it easy to deploy in real-time applications and has great potential for better results with a large input frame. These results show that our GraphMLP in video reaches competitive performance with fewer computational costs and can serve as a strong baseline for video-based 3D human pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Study</head><p>The large-scale ablation studies with 2D detected inputs on the Human3.6M dataset are conducted to investigate the effectiveness of our model (in MPJPE). Model Configurations. We start our ablation studies by exploring the GraphMLP on different hyper-parameters. The results are shown in <ref type="table" target="#tab_1">Table VI</ref>. It can be observed that using the expand ratio of 2 (C = 512, D C = 2C = 1024) works better than the ratio value of 4 which is common in vision Transformers and MLPs. Increasing or reducing the number of GraphMLP layers L hurts performance, while using L = 3 performs best. The hidden size C is important to determine the modeling ability. While increasing the C from 128 to 512 (keeping the same MLP ratios), the MPJPE decreases from 50.2 mm to 49.2 mm. Meanwhile, the number of parameters increases from 0.60M to 9.49M. The performance saturates when C surpasses 512. Therefore, the optimal hyper-parameters for our model are L = 3, C=512, D C = 1024, and D S = 256, which are different from the original setting of MLP-Mixer <ref type="bibr" target="#b7">[7]</ref>. The reason for this may be the gap between vision and skeleton data, where the Human3.6M dataset is not diverse enough to train a large GraphMLP model. Input 2D Detectors. A high-performance 2D detector is vital in achieving accurate 3D pose estimation. <ref type="table" target="#tab_1">Table VII</ref> reports the performance of our model with ground truth 2D joints, and detected 2D joints from Stack Hourglass (SH) <ref type="bibr" target="#b52">[52]</ref>, Detectron <ref type="bibr" target="#b20">[20]</ref>, and CPN <ref type="bibr" target="#b45">[45]</ref>. We can observe that our  improvement), indicating that the proposed components are mutually reinforced to produce more accurate 3D poses. These results validate the effectiveness of our motivation: combining modern MLPs and GCNs in a unified architecture for better 3D human pose estimation.</p><p>V. QUALITATIVE RESULTS Qualitative comparison with MLP-Mixer <ref type="bibr" target="#b7">[7]</ref> and GCN <ref type="bibr" target="#b8">[8]</ref> on Human3.6M dataset is shown in <ref type="figure">Fig. 6</ref>. In addition, we provide qualitative results on challenging in-the-wild images in <ref type="figure" target="#fig_5">Fig. 7</ref>. Note that these actions from in-the-wild images are rare or absent from the training set of Human3.6M. GraphMLP, benefiting from its globality, locality, and connectivity, performs better and is able to predict accurate and plausible 3D poses. It indicates both the effectiveness and generalization ability of our approach. However, there are still some failure cases of our approach due to large 2D detection error, half body, rare poses, and heavy occlusion, as shown in <ref type="figure">Fig. 8</ref>. More qualitative results can be found in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we propose a simple yet effective graphreinforced MLP-Like architecture, termed GraphMLP, which represents the first use of modern MLP dedicated to 3D human pose estimation. Our GraphMLP inherits the advantages of both MLPs and GCNs, making it a global-local-graphical unified architecture without self-attention. Extensive experiments show that the proposed GraphMLP achieves state-of-the-art performance and can serve as a strong baseline for 3D human pose estimation in both single frame and video sequence. We also show that complex temporal dynamics can be effectively modeled in a simple way without explicitly temporal modeling, and the proposed GraphMLP in video reaches competitive results with fewer computational costs.</p><p>As the MLPs and GCNs in our GraphMLP are straightforward, we look forward to combining more powerful MLPs or GCNs to further improve the performance. In addition, we hope this work could inspire further study on exploring the potential of MLP-Like architecture in more skeleton-based tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. APPENDIX</head><p>This supplementary material contains the following details: (1) Detailed description of multi-layer perceptrons (see Sec. VIII). (2) Additional implementation details (see Sec. IX). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. MULTI-LAYER PERCEPTRONS</head><p>In Sec. III-A of our main manuscript, we give a brief description of the MLP-Mixer layer <ref type="bibr" target="#b7">[7]</ref> which is defined as below:</p><formula xml:id="formula_7">X = X ?1 + Spatial-MLP(LN(X ?1 )) ,<label>(7)</label></formula><formula xml:id="formula_8">X = X + Channel-MLP(LN(X )).<label>(8)</label></formula><p>If considering more details about the spatial and channel MLPs, Eq. (7) and Eq. (8) can be further defined as:</p><formula xml:id="formula_9">X = X ?1 + W 2 ? W 1 (LN(X ?1 )) ,<label>(9)</label></formula><formula xml:id="formula_10">X = X + W 4 ? (W 3 (LN(X ?1 )))),<label>(10)</label></formula><p>where ? is the GELU activation function <ref type="bibr" target="#b41">[41]</ref>. W 1 ? R N ?D S and W 2 ? R D S ?N are the weights of two linear layers in the Spatial-MLP(?). W 3 ? R C?D C and W 2 ? R D C ?C are the weights of two linear layers in the Channel-MLP(?). For GraphMLP in video, LN is additionally applied after every fully-connected layer in Spatial-MLP(?), Eq. (9) is modified to:</p><formula xml:id="formula_11">X = X ?1 + LN W 2 ? LN W 1 (LN(X ?1 ))</formula><p>.</p><p>IX. ADDITIONAL IMPLEMENTATION DETAILS In Sec. IV-C of our main manuscript, we conduct extensive ablation studies on different network architectures and model variants of our GraphMLP. Here, we provide implementation details of these models. Network Architectures. In <ref type="table" target="#tab_1">Table VIII</ref> of our main paper, we report the results of various network architectures. Here, we provide illustrations in <ref type="figure">Fig. 9</ref> and implementation details as follows: (a) FCN: FCN <ref type="bibr" target="#b2">[3]</ref> is a conventional MLP whose building block contains a linear layer, followed by batch normalization, dropout, and a ReLU activation. We follow their original implementation and use their code [53] to report the performance. (b) GCN: We remove the spatial MLP and the channel MLP in our SG-MLP and CG-MLP, respectively. (c) Transformer: We build it by using a standard Transformer encoder which is the same as ViT <ref type="bibr" target="#b14">[14]</ref>. (d) MLP-Mixer: It is our baseline model that has the same architecture as <ref type="bibr" target="#b7">[7]</ref>. We build it by replacing multi-head attention blocks with spatial MLPs and removing the position embedding module in the Transformer. (e) Mesh Graphormer: Mesh Graphormer <ref type="bibr" target="#b36">[36]</ref> is the most relevant study to our approach that focuses on combining self-attentions and GCNs in a Transformer model for mesh reconstruction. Instead, our GraphMLP focuses on combining modern MLPs and GCNs to construct a stronger architecture for 3D human pose estimation. We follow their design to construct a model by adding a GCN block after the multi-head attention in the Transformer. Network Design Options. In <ref type="figure" target="#fig_0">Fig. 10</ref>, we graphically illustrate five different design options of the GraphMLP layer as mentioned in <ref type="table" target="#tab_1">Table IX</ref> of our main paper. <ref type="figure" target="#fig_0">Fig. 10</ref> (e) shows that we adopt the design of GCN and MLP in parallel but use a spatial GCN block in SG-MLP. The spatial GCN block processes tokens in the spatial dimension, which can be calculated as: X = X ?1 + Spatial-MLP(LN(X ?1 )) + Spatial-GCN(LN(X ?1 )) .</p><p>Model Components. In <ref type="table">Table X</ref> of our main paper, we investigate the effectiveness of each component in our design.</p><p>Here, we provide the illustrations of these model variants in <ref type="figure" target="#fig_0">Fig. 11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X. ADDITIONAL ABLATION STUDIES</head><p>Transposition Design Options. As mentioned in Sec. III-A of our main manuscript, we transpose the tokens before LN in the spatial MLP, and therefore the LN normalizes tokens along the spatial dimension. Here, we investigate the influence of transposition design options in <ref type="table" target="#tab_1">Table XI</ref>. The 'Transposition Before LN' can be formulated as: X = X ?1 + Spatial-MLP(LN(X ?1 )) + GCN(LN(X ?1 ) ).</p><p>The 'Transposition After LN' can be written as: X = X ?1 + Spatial-MLP(LN(X ?1 ) ) + GCN(LN(X ?1 )).</p><p>From <ref type="table" target="#tab_1">Table XI</ref>, the results show that performing transposition before LN brings more benefits in both MLP-Mixer and our GraphMLP models. Note that it is different from the original implementation of MLP-Mixer, which uses transposition after LN.</p><p>XI. ADDITIONAL QUALITATIVE RESULTS <ref type="figure" target="#fig_0">Fig. 12</ref> show qualitative results of the proposed GraphMLP on Human3.6M and MPI-INF-3DHP datasets. The Human3.6M is an indoor dataset (top three rows), and the test set of MPI-INF-3DHP contains three different scenes: studio with green screen (GS, fourth row), studio without green screen (noGS, fifth row), and outdoor scene (Outdoor, sixth row). Moreover, <ref type="figure" target="#fig_0">Fig. 13</ref> shows qualitative comparisons with MLP-Mixer <ref type="bibr" target="#b7">[7]</ref> (i.e., baseline), GCN <ref type="bibr" target="#b8">[8]</ref>, and MGCN <ref type="bibr" target="#b25">[25]</ref> on more challenging in-the-wild images. We can observe that our approach is able to predict more reliable and plausible 3D poses on these challenging cases.  <ref type="figure" target="#fig_0">Fig. 11</ref>. The model components we have studied for building our proposed GraphMLP.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Performance comparison with MLP-Mixer [7] and GCN [8] on Human3.6M (a) and MPI-INF-3DHP (b) datasets. The proposed GraphMLP absorbs the advantages of modern MLPs and GCNs to effectively learn skeletal representations, consistently outperforming each of them. The evaluation metric is MPJPE (the lower the better).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Fig. 2. Overview of the proposed GraphMLP architecture. The left illustrates the skeletal structure of the human body. The 2D joint inputs detected by a 2D pose estimator are sparse and graph-structured data. GraphMLP treats each 2D keypoint as an input token, linearly embeds each of them through the skeleton embedding, feeds the embedded tokens to GraphMLP layers, and finally performs regression on resulting features to predict the 3D pose via the prediction head. Each GraphMLP layer contains one spatial graph MLP (SG-MLP) and one channel graph MLP (CG-MLP). For easy illustration, we show the architecture using a single image as input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>(a) The human skeleton graph in physical and symmetrical connections. (b) The adjacency matrix used in the GCN blocks of GraphMLP. Different colors denote the different types of bone connections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Comparison of MLP Layers. (a) MLP-Mixer Layer [7]. (b) Our GraphMLP Layer. Compared with MLP-Mixer, our GraphMLP incorporates graph structural priors into the MLP model via GCN blocks. The MLPs and GCNs are in a paralleled design to model both local and global interactions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>3 &lt;Fig. 5 .</head><label>35</label><figDesc>l a t e x i t s h a 1 _ b a s e 6 4 = " H 1 n n C e 4 p e w p P r 6 5 Y U M e M 0 v B P f P s = " &gt; A A A B 8 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m 0 o M e C F 0 9 S w X 5 g G 8 p m u 2 m X b j Z h d y K U 0 H / h x Y M i X v 0 3 3 v w 3 b t s c t P X B w O O 9 G W b m B Y k U B l 3 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j l o l T z X i T x T L W n Y A a L o X i T R Q o e S f R n E a B 5 O 1 g f D P z 2 0 9 c G x G r B 5 w k 3 I / o U I l Q M I p W e r w j P R Q R N + S y X 6 6 4 V X c O s k q 8 n F Q g R 6 N f / u o N Y p Z G X C G T 1 J i u 5 y b o Z 1 S j Y J J P S 7 3 U 8 I S y M R 3 y r q W K 2 j V + N r 9 4 S s 6 s M i B h r G 0 p J H P 1 9 0 R G I 2 M m U W A 7 I 4 o j s + z N x P + 8 b o r h t Z 8 J l a T I F V s s C l N J M C a z 9 8 l A a M 5 Q T i y h T A t 7 K 2 E j q i l D G 1 L J h u A t v 7 x K W h d V z 6 1 6 9 7 V K 3 c 3 j K M I J n M I 5 e H A F d b i F B j S B g Y J n e I U 3 x z g v z r v z s W g t O P n M M f y B 8 / k D c g G Q C w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H 1 n n C e 4 p e w p P r 6 5 Y U M e M 0 v B P f P s = " &gt; A A A B 8 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m 0 o M e C F 0 9 S w X 5 g G 8 p m u 2 m X b j Z h d y K U 0 H / h x Y M i X v 0 3 3 v w 3 b t s c t P X B w O O 9 G W b m B Y k U B l 3 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j l o l T z X i T x T L W n Y A a L o X i T R Q o e S f R n E a B 5 O 1 g f D P z 2 0 9 c G x G r B 5 w k 3 I / o U I l Q M I p W e r w j P R Q R N + S y X 6 6 4 V X c O s k q 8 n F Q g R 6 N f / u o N Y p Z G X C G T 1 J i u 5 y b o Z 1 S j Y J J P S 7 3 U 8 I S y M R 3 y r q W K 2 j V + N r 9 4 S s 6 s M i B h r G 0 p J H P 1 9 0 R G I 2 M m U W A 7 I 4 o j s + z N x P + 8 b o r h t Z 8 J l a T I F V s s C l N J M C a z 9 8 l A a M 5 Q T i y h T A t 7 K 2 E j q i l D G 1 L J h u A t v 7 x K W h d V z 6 1 6 9 7 V K 3 c 3 j K M I J n M I 5 e H A F d b i F B j S B g Y J n e I U 3 x z g v z r v z s W g t O P n M M f y B 8 / k D c g G Q C w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H 1 n n C e 4 p e w p P r 6 5 Y U M e M 0 v B P f P s = " &gt; A A A B 8 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m 0 o M e C F 0 9 S w X 5 g G 8 p m u 2 m X b j Z h d y K U 0 H / h x Y M i X v 0 3 3 v w 3 b t s c t P X B w O O 9 G W b m B Y k U B l 3 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j l o l T z X i T x T L W n Y A a L o X i T R Q o e S f R n E a B 5 O 1 g f D P z 2 0 9 c G x G r B 5 w k 3 I / o U I l Q M I p W e r w j P R Q R N + S y X 6 6 4 V X c O s k q 8 n F Q g R 6 N f / u o N Y p Z G X C G T 1 J i u 5 y b o Z 1 S j Y J J P S 7 3 U 8 I S y M R 3 y r q W K 2 j V + N r 9 4 S s 6 s M i B h r G 0 p J H P 1 9 0 R G I 2 M m U W A 7 I 4 o j s + z N x P + 8 b o r h t Z 8 J l a T I F V s s C l N J M C a z 9 8 l A a M 5 Q T i y h T A t 7 K 2 E j q i l D G 1 L J h u A t v 7 x K W h d V z 6 1 6 9 7 V K 3 c 3 j K M I J n M I 5 e H A F d b i F B j S B g Y J n e I U 3 x z g v z r v z s W g t O P n M M f y B 8 / k D c g G Q C w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H 1 n n C e 4 p e w p P r 6 5 Y U M e M 0 v B P f P s = " &gt; A A A B 8 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m 0 o M e C F 0 9 S w X 5 g G 8 p m u 2 m X b j Z h d y K U 0 H / h x Y M i X v 0 3 3 v w 3 b t s c t P X B w O O 9 G W b m B Y k U B l 3 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j l o l T z X i T x T L W n Y A a L o X i T R Q o e S f R n E a B 5 O 1 g f D P z 2 0 9 c G x G r B 5 w k 3 I / o U I l Q M I p W e r w j P R Q R N + S y X 6 6 4 V X c O s k q 8 n F Q g R 6 N f / u o N Y p Z G X C G T 1 J i u 5 y b o Z 1 S j Y J J P S 7 3 U 8 I S y M R 3 y r q W K 2 j V + N r 9 4 S s 6 s M i B h r G 0 p J H P 1 9 0 R G I 2 M m U W A 7 I 4 o j s + z N x P + 8 b o r h t Z 8 J l a T I F V s s C l N J M C a z 9 8 l A a M 5 Q T i y h T A t 7 K 2 E j q i l D G 1 L J h u A t v 7 x K W h d V z 6 1 6 9 7 V K 3 c 3 j K M I J n M I 5 e H A F d b i F B j S B g Y J n e I U 3 x z g v z r v z s W g t O P n M M f y B 8 / k D c g G Q C w = = &lt; / l a t e x i t &gt; N ? C &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G 0 R k z e F E r L M j S W K g j M 8 l W q q 2 n t Q = " &gt; A A A B 8 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M d C L 5 6 k g v 3 A N p T N d t M u 3 W z C 7 k Q o o f / C i w d F v P p v v P l v 3 L Y 5 a O u D g c d 7 M 8 z M C x I p D L r u t 1 P Y 2 N z a 3 i n u l v b 2 D w 6 P y s c n b R O n m v E W i 2 W s u w E 1 X A r F W y h Q 8 m 6 i O Y 0 C y T v B p D H 3 O 0 9 c G x G r B 5 w m 3 I / o S I l Q M I p W e r w j f R Q R N 6 Q x K F f c q r s A W S d e T i q Q o z k o f / W H M U s j r p B J a k z P c x P 0 M 6 p R M M l n p X 5 q e E L Z h I 5 4 z 1 J F 7 R o / W 1 w 8 I x d W G Z I w 1 r Y U k o X 6 e y K j k T H T K L C d E c W x W f X m 4 n 9 e L 8 X w x s + E S l L k i i 0 X h a k k G J P 5 + 2 Q o N G c o p 5 Z Q p o W 9 l b A x 1 Z S h D a l k Q / B W X 1 4 n 7 a u q 5 1 a 9 + + t K 3 c 3 j K M I Z n M M l e F C D O t x C E 1 r A Q M E z v M K b Y 5 w X 5 9 3 5 W L Y W n H z m F P 7 A + f w B i k G Q G w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G 0 R k z e F E r L M j S W K g j M 8 l W q q 2 n t Q = " &gt; A A A B 8 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M d C L 5 6 k g v 3 A N p T N d t M u 3 W z C 7 k Q o o f / C i w d F v P p v v P l v 3 L Y 5 a O u D g c d 7 M 8 z M C x I p D L r u t 1 P Y 2 N z a 3 i n u l v b 2 D w 6 P y s c n b R O n m v E W i 2 W s u w E 1 X A r F W y h Q 8 m 6 i O Y 0 C y T v B p D H 3 O 0 9 c G x G r B 5 w m 3 I / o S I l Q M I p W e r w j f R Q R N 6 Q x K F f c q r s A W S d e T i q Q o z k o f / W H M U s j r p B J a k z P c x P 0 M 6 p R M M l n p X 5 q e E L Z h I 5 4 z 1 J F 7 R o / W 1 w 8 I x d W G Z I w 1 r Y U k o X 6 e y K j k T H T K L C d E c W x W f X m 4 n 9 e L 8 X w x s + E S l L k i i 0 X h a k k G J P 5 + 2 Q o N G c o p 5 Z Q p o W 9 l b A x 1 Z S h D a l k Q / B W X 1 4 n 7 a u q 5 1 a 9 + + t K 3 c 3 j K M I Z n M M l e F C D O t x C E 1 r A Q M E z v M K b Y 5 w X 5 9 3 5 W L Y W n H z m F P 7 A + f w B i k G Q G w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G 0 R k z e F E r L M j S W K g j M 8 l W q q 2 n t Q = " &gt; A A A B 8 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M d C L 5 6 k g v 3 A N p T N d t M u 3 W z C 7 k Q o o f / C i w d F v P p v v P l v 3 L Y 5 a O u D g c d 7 M 8 z M C x I p D L r u t 1 P Y 2 N z a 3 i n u l v b 2 D w 6 P y s c n b R O n m v E W i 2 W s u w E 1 X A r F W y h Q 8 m 6 i O Y 0 C y T v B p D H 3 O 0 9 c G x G r B 5 w m 3 I / o S I l Q M I p W e r w j f R Q R N 6 Q x K F f c q r s A W S d e T i q Q o z k o f / W H M U s j r p B J a k z P c x P 0 M 6 p R M M l n p X 5 q e E L Z h I 5 4 z 1 J F 7 R o / W 1 w 8 I x d W G Z I w 1 r Y U k o X 6 e y K j k T H T K L C d E c W x W f X m 4 n 9 e L 8 X w x s + E S l L k i i 0 X h a k k G J P 5 + 2 Q o N G c o p 5 Z Q p o W 9 l b A x 1 Z S h D a l k Q / B W X 1 4 n 7 a u q 5 1 a 9 + + t K 3 c 3 j K M I Z n M M l e F C D O t x C E 1 r A Q M E z v M K b Y 5 w X 5 9 3 5 W L Y W n H z m F P 7 A + f w B i k G Q G w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G 0 R k z e F E r L M j S W K g j M 8 l W q q 2 n t Q = " &gt; A A A B 8 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M d C L 5 6 k g v 3 A N p T N d t M u 3 W z C 7 k Q o o f / C i w d F v P p v v P l v 3 L Y 5 a O u D g c d 7 M 8 z M C x I p D L r u t 1 P Y 2 N z a 3 i n u l v b 2 D w 6 P y s c n b R O n m v E W i 2 W s u w E 1 X A r F W y h Q 8 m 6 i O Y 0 C y T v B p D H 3 O 0 9 c G x G r B 5 w m 3 I / o S I l Q M I p W e r w j f R Q R N 6 Q x K F f c q r s A W S d e T i q Q o z k o f / W H M U s j r p B J a k z P c x P 0 M 6 p R M M l n p X 5 q e E L Z h I 5 4 z 1 J F 7 R o / W 1 w 8 I x d W G Z I w 1 r Y U k o X 6 e y K j k T H T K L C d E c W x W f X m 4 n 9 e L 8 X w x s + E S l L k i i 0 X h a k k G J P 5 + 2 Q o N G c o p 5 Z Q p o W 9 l b A x 1 Z S h D a l k Q / B W X 1 4 n 7 a u q 5 1 a 9 + + t K 3 c 3 j K M I Z n M M l e F C D O t x C E 1 r A Q M E z v M K b Y 5 w X 5 9 3 5 W L Y W n H z m F P 7 A + f w B i k G Q G w = = &lt; / l a t e x i t &gt; P &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M 6 b Q 2 2 f + T M V H d J f 0 d c 3 Q m k L i 5 G c = " &gt; A A A B 6 H i c b V D L S g M x F L 3 j s 9 Z X 1 a W b Y B F c l Y w I u i y 4 c d m C f U A 7 S C a 9 0 8 Z m M k O S E c r Q L 3 D j Q h G 3 f p I 7 / 8 a 0 n Y W 2 H g g c z j m X 3 H v C V A p j K f 3 2 1 t Y 3 N r e 2 S z v l 3 b 3 9 g 8 P K 0 X H b J J n m 2 O K J T H Q 3 Z A a l U N i y w k r s p h p Z H E r s h O P b m d 9 5 Q m 1 E o u 7 t J M U g Z k M l I s G Z d V K z 8 V C p 0 h q d g 6 w S v y B V K O D y X / 1 B w r M Y l e W S G d P z a W q D n G k r u M R p u Z 8 Z T B k f s y H 2 H F U s R h P k 8 0 W n 5 N w p A x I l 2 j 1 l y V z 9 P Z G z 2 J h J H L p k z O z I L H s z 8 T + v l 9 n o J s i F S j O L i i 8 + i j J J b E J m V 5 O B 0 M i t n D j C u B Z u V 8 J H T D N u X T d l V 4 K / f P I q a V / W f F r z m 1 f V O i 3 q K M E p n M E F + H A N d b i D B r S A A 8 I z v M K b 9 + i 9 e O / e x y K 6 5 h U z J / A H 3 u c P p C + M w g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M 6 b Q 2 2 f + T M V H d J f 0 d c 3 Q m k L i 5 G c = " &gt; A A A B 6 H i c b V D L S g M x F L 3 j s 9 Z X 1 a W b Y B F c l Y w I u i y 4 c d m C f U A 7 S C a 9 0 8 Z m M k O S E c r Q L 3 D j Q h G 3 f p I 7 / 8 a 0 n Y W 2 H g g c z j m X 3 H v C V A p j K f 3 2 1 t Y 3 N r e 2 S z v l 3 b 3 9 g 8 P K 0 X H b J J n m 2 O K J T H Q 3 Z A a l U N i y w k r s p h p Z H E r s h O P b m d 9 5 Q m 1 E o u 7 t J M U g Z k M l I s G Z d V K z 8 V C p 0 h q d g 6 w S v y B V K O D y X / 1 B w r M Y l e W S G d P z a W q D n G k r u M R p u Z 8 Z T B k f s y H 2 H F U s R h P k 8 0 W n 5 N w p A x I l 2 j 1 l y V z 9 P Z G z 2 J h J H L p k z O z I L H s z 8 T + v l 9 n o J s i F S j O L i i 8 + i j J J b E J m V 5 O B 0 M i t n D j C u B Z u V 8 J H T D N u X T d l V 4 K / f P I q a V / W f F r z m 1 f V O i 3 q K M E p n M E F + H A N d b i D B r S A A 8 I z v M K b 9 + i 9 e O / e x y K 6 5 h U z J / A H 3 u c P p C + M w g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M 6 b Q 2 2 f + T M V H d J f 0 d c 3 Q m k L i 5 G c = " &gt; A A A B 6 H i c b V D L S g M x F L 3 j s 9 Z X 1 a W b Y B F c l Y w I u i y 4 c d m C f U A 7 S C a 9 0 8 Z m M k O S E c r Q L 3 D j Q h G 3 f p I 7 / 8 a 0 n Y W 2 H g g c z j m X 3 H v C V A p j K f 3 2 1 t Y 3 N r e 2 S z v l 3 b 3 9 g 8 P K 0 X H b J J n m 2 O K J T H Q 3 Z A a l U N i y w k r s p h p Z H E r s h O P b m d 9 5 Q m 1 E o u 7 t J M U g Z k M l I s G Z d V K z 8 V C p 0 h q d g 6 w S v y B V K O D y X / 1 B w r M Y l e W S G d P z a W q D n G k r u M R p u Z 8 Z T B k f s y H 2 H F U s R h P k 8 0 W n 5 N w p A x I l 2 j 1 l y V z 9 P Z G z 2 J h J H L p k z O z I L H s z 8 T + v l 9 n o J s i F S j O L i i 8 + i j J J b E J m V 5 O B 0 M i t n D j C u B Z u V 8 J H T D N u X T d l V 4 K / f P I q a V / W f F r z m 1 f V O i 3 q K M E p n M E F + H A N d b i D B r S A A 8 I z v M K b 9 + i 9 e O / e x y K 6 5 h U z J / A H 3 u c P p C + M w g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M 6 b Q 2 2 f + T M V H d J f 0 d c 3 Q m k L i 5 G c = " &gt; A A A B 6 H i c b V D L S g M x F L 3 j s 9 Z X 1 a W b Y B F c l Y w I u i y 4 c d m C f U A 7 S C a 9 0 8 Z m M k O S E c r Q L 3 D j Q h G 3 f p I 7 / 8 a 0 n Y W 2 H g g c z j m X 3 H v C V A p j K f 3 2 1 t Y 3 N r e 2 S z v l 3 b 3 9 g 8 P K 0 X H b J J n m 2 O K J T H Q 3 Z A a l U N i y w k r s p h p Z H E r s h O P b m d 9 5 Q m 1 E o u 7 t J M U g Z k M l I s G Z d V K z 8 V C p 0 h q d g 6 w S v y B V K O D y X / 1 B w r M Y l e W S G d P z a W q D n G k r u M R p u Z 8 Z T B k f s y H 2 H F U s R h P k 8 0 W n 5 N w p A x I l 2 j 1 l y V z 9 P Z G z 2 J h J H L p k z O z I L H s z 8 T + v l 9 n o J s i F S j O L i i 8 + i j J J b E J m V 5 O B 0 M i t n D j C u B Z u V 8 J H T D N u X T d l V 4 K / f P I q a V / W f F r z m 1 f V O i 3 q K M E p n M E F + H A N d b i D B r S A A 8 I z v M K b 9 + i 9 e O / e x y K 6 5 h U z J / A H 3 u c P p C + M w g = = &lt; / l a t e x i t &gt; e X &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I T 4 9 d R r M z E i x r z D v t K n u l H b M s N A = " &gt; A A A B 9 H i c b V B N S 8 N A E N 3 4 W e t X 1 a O X x S J 4 K o k I e i x 4 8 V j B f k A b y m Y z a Z d u N n F 3 U i m h v 8 O L B 0 W 8 + m O 8 + W / c t j l o 6 4 O B x 3 s z z M w L U i k M u u 6 3 s 7 a + s b m 1 X d o p 7 + 7 t H x x W j o 5 b J s k 0 h y Z P Z K I 7 A T M g h Y I m C p T Q S T W w O J D Q D k a 3 M 7 8 9 B m 1 E o h 5 w k o I f s 4 E S k e A M r e T 3 n k Q I K G Q I e W f a r 1 T d m j s H X S V e Q a q k Q K N f + e q F C c 9 i U M g l M 6 b r u S n 6 O d M o u I R p u Z c Z S B k f s Q F 0 L V U s B u P n 8 6 O n 9 N w q I Y 0 S b U s h n a u / J 3 I W G z O J A 9 s Z M x y a Z W 8 m / u d 1 M 4 x u / F y o N E N Q f L E o y i T F h M 4 S o K H Q w F F O L G F c C 3 s r 5 U O m G U e b U 9 m G 4 C 2 / v E p a l z X P r X n 3 V 9 W 6 W 8 R R I q f k j F w Q j 1 y T O r k j D d I k n D y S Z / J K 3 p y x 8 + K 8 O x + L 1 j W n m D k h f + B 8 / g A 2 0 5 J R &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I T 4 9 d R r M z E i x r z D v t K n u l H b M s N A = " &gt; A A A B 9 H i c b V B N S 8 N A E N 3 4 W e t X 1 a O X x S J 4 K o k I e i x 4 8 V j B f k A b y m Y z a Z d u N n F 3 U i m h v 8 O L B 0 W 8 + m O 8 + W / c t j l o 6 4 O B x 3 s z z M w L U i k M u u 6 3 s 7 a + s b m 1 X d o p 7 + 7 t H x x W j o 5 b J s k 0 h y Z P Z K I 7 A T M g h Y I m C p T Q S T W w O J D Q D k a 3 M 7 8 9 B m 1 E o h 5 w k o I f s 4 E S k e A M r e T 3 n k Q I K G Q I e W f a r 1 T d m j s H X S V e Q a q k Q K N f + e q F C c 9 i U M g l M 6 b r u S n 6 O d M o u I R p u Z c Z S B k f s Q F 0 L V U s B u P n 8 6 O n 9 N w q I Y 0 S b U s h n a u / J 3 I W G z O J A 9 s Z M x y a Z W 8 m / u d 1 M 4 x u / F y o N E N Q f L E o y i T F h M 4 S o K H Q w F F O L G F c C 3 s r 5 U O m G U e b U 9 m G 4 C 2 / v E p a l z X P r X n 3 V 9 W 6 W 8 R R I q f k j F w Q j 1 y T O r k j D d I k n D y S Z / J K 3 p y x 8 + K 8 O x + L 1 j W n m D k h f + B 8 / g A 2 0 5 J R &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I T 4 9 d R r M z E i x r z D v t K n u l H b M s N A = " &gt; A A A B 9 H i c b V B N S 8 N A E N 3 4 W e t X 1 a O X x S J 4 K o k I e i x 4 8 V j B f k A b y m Y z a Z d u N n F 3 U i m h v 8 O L B 0 W 8 + m O 8 + W / c t j l o 6 4 O B x 3 s z z M w L U i k M u u 6 3 s 7 a + s b m 1 X d o p 7 + 7 t H x x W j o 5 b J s k 0 h y Z P Z K I 7 A T M g h Y I m C p T Q S T W w O J D Q D k a 3 M 7 8 9 B m 1 E o h 5 w k o I f s 4 E S k e A M r e T 3 n k Q I K G Q I e W f a r 1 T d m j s H X S V e Q a q k Q K N f + e q F C c 9 i U M g l M 6 b r u S n 6 O d M o u I R p u Z c Z S B k f s Q F 0 L V U s B u P n 8 6 O n 9 N w q I Y 0 S b U s h n a u / J 3 I W G z O J A 9 s Z M x y a Z W 8 m / u d 1 M 4x u / F y o N E N Q f L E o y i T F h M 4 S o K H Q w F F O L G F c C 3 s r 5 U O m G U e b U 9 m G 4 C 2 / v E p a l z X P r X n 3 V 9 W 6 W 8 R R I q f k j F w Q j 1 y T O r k j D d I k n D y S Z / J K 3 p y x 8 + K 8 O x + L 1 j W n m D k h f + B 8 / g A 2 0 5 J R &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I T 4 9 d R r M z E i x r z D v t K n u l H b M s N A = " &gt; A A A B 9 H i c b V B N S 8 N A E N 3 4 W e t X 1 a O X x S J 4 K o k I e i x 4 8 V j B f k A b y m Y z a Z d u N n F 3 U i m h v 8 O L B 0 W 8 + m O 8 + W / c t j l o 6 4 O B x 3 s z z M w L U i k M u u 6 3 s 7 a + s b m 1 X d o p 7 + 7 t H x x W j o 5 b J s k 0 h y Z P Z K I 7 A T M g h Y I m C p T Q S T W w O J D Q D k a 3 M 7 8 9 B m 1 E o h 5 w k o I f s 4 E S k e A M r e T 3 n k Q I K G Q I e W f a r 1 T d m j s H X S V e Q a q k Q KN f + e q F C c 9 i U M g l M 6 b r u S n 6 O d M o u I R p u Z c Z S B k f s Q F 0 L V U s B u P n 8 6 O n 9 N w q I Y 0 S b U s h n a u / J 3 I W G z O J A 9 s Z M x y a Z W 8 m / u d 1 M 4 x u / F y o N E N Q f L E o y i T F h M 4 S o K H Q w F F O L G F c C 3 s r 5 U O m G U e b U 9 m G 4 C 2 / v E p a l z X P r X n 3 V 9 W 6 W 8 R R I q f k j F w Q j 1 y T O r k j D d I k n D y S Z / J K 3 p y x 8 + K 8 O x + L 1 j W n m D k h f + B 8 / g A 2 0 5 J R &lt; / l a t e x i t &gt; N ? C &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G 0 R k z e F E r L M j S W K g j M 8 l W q q 2 n t Q = " &gt; A A A B 8 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M d C L 5 6 k g v 3 A N p T N d t M u 3 W z C 7 k Q o o f / C i w d F v P p v v P l v 3 L Y 5 a O u D g c d 7 M 8 z M C x I p D L r u t 1 P Y 2 N z a 3 i n u l v b 2 D w 6 P y s c n b R O n m v E W i 2 W s u w E 1 X A r F W y h Q 8 m 6 i O Y 0 C y T v B p D H 3 O 0 9 c G x G r B 5 w m 3 I / o S I l Q M I p W e r w j f R Q R N 6 Q x K F f c q r s A W S d e T i q Q o z k o f / W H M U s j r p B J a k z P c x P 0 M 6 p R M M l n p X 5 q e E L Z h I 5 4 z 1 J F 7 R o / W 1 w 8 I x d W G Z I w 1 r Y U k o X 6 e y K j k T H T K L C d E c W x W f X m 4 n 9 e L 8 X w x s + E S l L k i i 0 X h a k k G J P 5 + 2 Q o N G c o p 5 Z Q p o W 9 l b A x 1 Z S h D a l k Q / B W X 1 4 n 7 a u q 5 1 a 9 + + t K 3 c 3 j K M I Z n M M l e F C D O t x C E 1 r A Q M E z v M K b Y 5 w X 5 9 3 5 W L Y W n H z m F P 7 A + f w B i k G Q G w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G 0 R k z e F E r L M j S W K g j M 8 l W q q 2 n t Q = " &gt; A A A B 8 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M d C L 5 6 k g v 3 A N p T N d t M u 3 W z C 7 k Q o o f / C i w d F v P p v v P l v 3 L Y 5 a O u D g c d 7 M 8 z M C x I p D L r u t 1 P Y 2 N z a 3 i n u l v b 2 D w 6 P y s c n b R O n m v E W i 2 W s u w E 1 X A r F W y h Q 8 m 6 i O Y 0 C y T v B p D H 3 O 0 9 c G x G r B 5 w m 3 I / o S I l Q M I p W e r w j f R Q R N 6 Q x K F f c q r s A W S d e T i q Q o z k o f / W H M U s j r p B J a k z P c x P 0 M 6 p R M M l n p X 5 q e E L Z h I 5 4 z 1 J F 7 R o / W 1 w 8 I x d W G Z I w 1 r Y U k o X 6 e y K j k T H T K L C d E c W x W f X m 4 n 9 e L 8 X w x s + E S l L k i i 0 X h a k k G J P 5 + 2 Q o N G c o p 5 Z Q p o W 9 l b A x 1 Z S h D a l k Q / B W X 1 4 n 7 a u q 5 1 a 9 + + t K 3 c 3 j K M I Z n M M l e F C D O t x C E 1 r A Q M E z v M K b Y 5 w X 5 9 3 5 W L Y W n H z m F P 7 A + f w B i k G Q G w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G 0 R k z e F E r L M j S W K g j M 8 l W q q 2 n t Q = " &gt; A A A B 8 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M d C L 5 6 k g v 3 A N p T N d t M u 3 W z C 7 k Q o o f / C i w d F v P p v v P l v 3 L Y 5 a O u D g c d 7 M 8 z M C x I p D L r u t 1 P Y 2 N z a 3 i n u l v b 2 D w 6 P y s c n b R O n m v E W i 2 W s u w E 1 X A r F W y h Q 8 m 6 i O Y 0 C y T v B p D H 3 O 0 9 c G x G r B 5 w m 3 I / o S I l Q M I p W e r w j f R Q R N 6 Q x K F f c q r s A W S d e T i q Q o z k o f / W H M U s j r p B J a k z P c x P 0 M 6 p R M M l n p X 5 q e E L Z h I 5 4 z 1 J F 7 R o / W 1 w 8 I x d W G Z I w 1 r Y U k o X 6 e y K j k T H T K L C d E c W x W f X m 4 n 9 e L 8 X w x s + E S l L k i i 0 X h a k k G J P 5 + 2 Q o N G c o p 5 Z Q p o W 9 l b A x 1 Z S h D a l k Q / B W X 1 4 n 7 a u q 5 1 a 9 + + t K 3 c 3 j K M I Z n M M l e F C D O t x C E 1 r A Q M E z v M K b Y 5 w X 5 9 3 5 W L Y W n H z m F P 7 A + f w B i k G Q G w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G 0 R k z e F E r L M j S W K g j M 8 l W q q 2 n t Q = " &gt; A A A B 8 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M d C L 5 6 k g v 3 A N p T N d t M u 3 W z C 7 k Q o o f / C i w d F v P p v v P l v 3 L Y 5 a O u D g c d 7 M 8 z M C x I p D L r u t 1 P Y 2 N z a 3 i n u l v b 2 D w 6 P y s c n b R O n m v E W i 2 W s u w E 1 X A r F W y h Q 8 m 6 i O Y 0 C y T v B p D H 3 O 0 9 c G x G r B 5 w m 3 I / o S I l Q M I p W e r w j f R Q R N 6 Q x K F f c q r s A W S d e T i q Q o z k o f / W H M U s j r p B J a k z P c x P 0 M 6 p R M M l n p X 5 q e E L Z h I 5 4 z 1 J F 7 R o / W 1 w 8 I x d W G Z I w 1 r Y U k o X 6 e y K j k T H T K L C d E c W x W f X m 4 n 9 e L 8 X w x s + E S l L k i i 0 X h a k k G J P 5 + 2 Q o N G c o p 5 Z Q p o W 9 l b A x 1 Z S h D a l k Q / B W X 1 4 n 7 a u q 5 1 a 9 + + t K 3 c 3 j K M I Z n M M l e F C D O t x C E 1 r A Q M E z v M K b Y 5 w X 5 9 3 5 W L Y W n H z m F P 7 A + f w B i k G Q G w = = &lt; / l a t e x i t &gt; N ? (2 ? T ) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " + j 8 Y g Q p a 1 P Z Q R v 8 6 s X Y e p Q 6 O N A s = " &gt; A A A B / n i c b V B N S 8 N A E N 3 4 W e N X V D x 5 W S x C v Z S k C H o s e P E k F f o F T S i b z b Z d u t m E 3 Y l Q Q s G / 4 s W D I l 7 9 H d 7 8 N 2 7 b H L T 1 w c D j v R l m 5 o W p 4 B p c 9 9 t a W 9 / Y 3 N o u 7 d i 7 e / s H h 8 7 R c V s n m a K s R R O R q G 5 I N B N c s h Z w E K y b K k b i U L B O O L 6 d + Z 1 H p j R P Z B M m K Q t i M p R 8 w C k B I / W d U / s e + 8 B j p n G l h n 0 a J Y C b l 3 2 n 7 F b d O f A q 8 Q p S R g U a f e f L j x K a x U w C F U T r n u e m E O R E A a e C T W 0 / 0 y w l d E y G r G e o J G Z h k M / P n + I L o 0 R 4 k C h T E v B c / T 2 R k 1 j r S R y a z p j A S C 9 7 M / E / r 5 f B 4 C b I u U w z Y J I u F g 0 y g S H B s y x w x B W j I C a G E K q 4 u R X T E V G E g k n M N i F 4 y y + v k n a t 6 r l V 7 + G q X H e L O E r o D J 2 j C v L Q N a q j O 9 R A L U R R j p 7 R K 3 q z n q w X 6 9 3 6 W L S u W c X M C f o D 6 / M H K b G T n g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " + j 8 Y g Q p a 1 P Z Q R v 8 6 s X Y e p Q 6 O N A s = " &gt; A A A B / n i c b V B N S 8 N A E N 3 4 W e N X V D x 5 W S x C v Z S k C H o s e P E k F f o F T S i b z b Z d u t m E 3 Y l Q Q s G / 4 s W D I l 7 9 H d 7 8 N 2 7 b H L T 1 w c D j v R l m 5 o W p 4 B p c 9 9 t a W 9 / Y 3 N o u 7 d i 7 e / s H h 8 7 R c V s n m a K s R R O R q G 5 I N B N c s h Z w E K y b K k b i U L B O O L 6 d + Z 1 H p j R P Z B M m K Q t i M p R 8 w C k B I / W d U / s e + 8 B j p n G l h n 0 a J Y C b l 3 2 n 7 F b d O f A q 8 Q p S R g U a f e f L j x K a x U w C F U T r n u e m E O R E A a e C T W 0 / 0 y w l d E y G r G e o J G Z h k M / P n + I L o 0 R 4 k C h T E v B c / T 2 R k 1 j r S R y a z p j A S C 9 7 M / E / r 5 f B 4 C b I u U w z Y J I u F g 0 y g S H B s y x w x B W j I C a G E K q 4 u R X T E V G E g k n M N i F 4 y y + v k n a t 6 r l V 7 + G q X H e L O E r o D J 2 j C v L Q N a q j O 9 R A L U R R j p 7 R K 3 q z n q w X 6 9 3 6 W L S u W c X M C f o D 6 / M H K b G T n g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " + j 8 Y g Q p a 1 P Z Q R v 8 6 s X Y e p Q 6 O N A s = " &gt; A A A B / n i c b V B N S 8 N A E N 3 4 W e N X V D x 5 W S x C v Z S k C H o s e P E k F f o F T S i b z b Z d u t m E 3 Y l Q Q s G / 4 s W D I l 7 9 H d 7 8 N 2 7 b H L T 1 w c D j v R l m 5 o W p 4 B p c 9 9 t a W 9 / Y 3 N o u 7 d i 7 e / s H h 8 7 R c V s n m a K s R R O R q G 5 I N B N c s h Z w E K y b K k b i U L B O O L 6 d + Z 1 H p j R P Z B M m K Q t i M p R 8 w C k B I / W d U / s e + 8 B j p n G l h n 0 a J Y C b l 3 2 n 7 F b d O f A q 8 Q p S R g U a f e f L j x K a x U w C F U T r n u e m E O R E A a e C T W 0 / 0 y w l d E y G r G e o J G Z h k M / P n + I L o 0 R 4 k C h T E v B c / T 2 R k 1 j r S R y a z p j A S C 9 7 M / E / r 5 f B 4 C b I u U w z Y J I u F g 0 y g S H B s y x w x B W j I C a G E K q 4 u R X T E V G E g k n M N i F 4 y y + v k n a t 6 r l V 7 + G q X H e L O E r o D J 2 j C v L Q N a q j O 9 R A L U R R j p 7 R K 3 q z n q w X 6 9 3 6 W L S u W c X M C f o D 6 / M H K b G T n g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " + j 8 Y g Q p a 1 P Z Q R v 8 6 s X Y e p Q 6 O N A s = " &gt; A A A B / n i c b V B N S 8 N A E N 3 4 W e N X V D x 5 W S x C v Z S k C H o s e P E k F f o F T S i b z b Z d u t m E 3 Y l Q Q s G / 4 s W D I l 7 9 H d 7 8 N 2 7 b H L T 1 w c D j v R l m 5 o W p 4 B p c 9 9 t a W 9 / Y 3 N o u 7 d i 7 e / s H h 8 7 R c V s n m a K s R R O R q G 5 I N B N c s h Z w E K y b K k b i U L B O O L 6 d + Z 1 H p j R P Z B M m K Q t i M p R 8 w C k B I / W d U / s e + 8 B j p n G l h n 0 a J Y C b l 3 2 n 7 F b d O f A q 8 Q p S R g U a f e f L j x K a x U w C F U T r n u e m E O R E A a e C T W 0 / 0 y w l d E y G r G e o J G Z h k M / P n + I L o 0 R 4 k C h T E v B c / T 2 R k 1 j r S R y a z p j A S C 9 7 M / E / r 5 f B 4 C b I u U w z Y J I u F g 0 y g S H B s y x w x B W j I C a G E K q 4 u R X T E V G E g k n M N i F 4 y y + v k n a t 6 r l V 7 + G q X H e L O E r o D J 2 j C v L Q N a q j O 9 R A L U R R j p 7 R K 3 q z n q w X 6 9 3 6 W L S u W c X M C f o D 6 / M H K b G T n g = = &lt; / l a t e x i t &gt; T ? N ? 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " p n Z y / k 7 u L J w T I L l C j G z / W m 2 H H k Y = " &gt; A A A B / H i c b Z D L S g N B E E V r 4 i v G 1 2 i W b h q D 4 C r M B E G X A T e u J E J e k A y h p 9 N J m v Q 8 6 K 4 R h i H + i h s X i r j 1 Q 9 z 5 N 3 a S E T T x Q s P h V h V V f f 1 Y C o 2 O 8 2 U V N j a 3 t n e K u 6 W 9 / Y P D I / v 4 p K 2 j R D H e Y p G M V N e n m k s R 8 h Y K l L w b K 0 4 D X / K O P 7 2 Z 1 z s P X G k R h U 1 M Y + 4 F d B y K k W A U j T W w y 0 3 S R x F w T e 5 + o D a w K 0 7 V W Y i s g 5 t D B X I 1 B v Z n f x i x J O A h M k m 1 7 r l O j F 5 G F Q o m + a z U T z S P K Z v S M e 8 Z D K l Z 4 2 W L 4 2 f k 3 D h D M o q U e S G S h f t 7 I q O B 1 m n g m 8 6 A 4 k S v 1 u b m f 7 V e g q N r L x N h n C A P 2 X L R K J E E I z J P g g y F 4 g x l a o A y J c y t h E 2 o o g x N X i U T g r v 6 5 X V o 1 6 q u U 3 X v L y t 1 J 4 + j C K d w B h f g w h X U 4 R Y a 0 A I G K T z B C 7 x a j 9 a z 9 W a 9 L 1 s L V j 5 T h j + y P r 4 B C B W T p w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " p n Z y / k 7 u L J w T I L l C j G z / W m 2 H H k Y = " &gt; A A A B / H i c b Z D L S g N B E E V r 4 i v G 1 2 i W b h q D 4 C r M B E G X A T e u J E J e k A y h p 9 N J m v Q 8 6 K 4 R h i H + i h s X i r j 1 Q 9 z 5 N 3 a S E T T x Q s P h V h V V f f 1 Y C o 2 O 8 2 U V N j a 3 t n e K u 6 W 9 / Y P D I / v 4 p K 2 j R D H e Y p G M V N e n m k s R 8 h Y K l L w b K 0 4 D X / K O P 7 2 Z 1 z s P X G k R h U 1 M Y + 4 F d B y K k W A U j T W w y 0 3 S R x F w T e 5 + o D a w K 0 7 V W Y i s g 5 t D B X I 1 B v Z n f x i x J O A h M k m 1 7 r l O j F 5 G F Q o m + a z U T z S P K Z v S M e 8 Z D K l Z 4 2 W L 4 2 f k 3 D h D M o q U e S G S h f t 7 I q O B 1 m n g m 8 6 A 4 k S v 1 u b m f 7 V e g q N r L x N h n C A P 2 X L R K J E E I z J P g g y F 4 g x l a o A y J c y t h E 2 o o g x N X i U T g r v 6 5 X V o 1 6 q u U 3 X v L y t 1 J 4 + j C K d w B h f g w h X U 4 R Y a 0 A I G K T z B C 7 x a j 9 a z 9 W a 9 L 1 s L V j 5 T h j + y P r 4 B C B W T p w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " p n Z y / k 7 u L J w T I L l C j G z / W m 2 H H k Y = " &gt; A A A B / H i c b Z D L S g N B E E V r 4 i v G 1 2 i W b h q D 4 C r M B E G X A T e u J E J e k A y h p 9 N J m v Q 8 6 K 4 R h i H + i h s X i r j 1 Q 9 z 5 N 3 a S E T T x Q s P h V h V V f f 1 Y C o 2 O 8 2 U V N j a 3 t n e K u 6 W 9 / Y P D I / v 4 p K 2 j R D H e Y p G M V N e n m k s R 8 h Y K l L w b K 0 4 D X / K O P 7 2 Z 1 z s P X G k R h U 1 M Y + 4 F d B y K k W A U j T W w y 0 3 S R x F w T e 5 + o D a w K 0 7 V W Y i s g 5 t D B X I 1 B v Z n f x i x J O A h M k m 1 7 r l O j F 5 G F Q o m + a z U T z S P K Z v S M e 8 Z D K l Z 4 2 W L 4 2 f k 3 D h D M o q U e S G S h f t 7 I q O B 1 m n g m 8 6 A 4 k S v 1 u b m f 7 V e g q N r L x N h n C A P 2 X L R K J E E I z J P g g y F 4 g x l a o A y J c y t h E 2 o o g x N X i U T g r v 6 5 X V o 1 6 q u U 3 X v L y t 1 J 4 + j C K d w B h f g w h X U 4 R Y a 0 A I G K T z B C 7 x a j 9 a z 9 W a 9 L 1 s L V j 5 T h j + y P r 4 B C B W T p w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " p n Z y / k 7 u L J w T I L l C j G z / W m 2 H H k Y = " &gt; A A A B / H i c b Z D L S g N B E E V r 4 i v G 1 2 i W b h q D 4 C r M B E G X A T e u J E J e k A y h p 9 N J m v Q 8 6 K 4 R h i H + i h s X i r j 1 Q 9 z 5 N 3 a S E T T x Q s P h V h V V f f 1 Y C o 2 O 8 2 U V N j a 3 t n e K u 6 W 9 / Y P D I / v 4 p K 2 j R D H e Y p G M V N e n m k s R 8 h Y K l L w b K 0 4 D X / K O P 7 2 Z 1 z s P X G k R h U 1 M Y + 4 F d B y K k W A U j T W w y 0 3 S R x F w T e 5 + o D a w K 0 7 V W Y i s g 5 t D B X I 1 B v Z n f x i x J O A h M k m 1 7 r l O j F 5 G F Q o m + a z U T z S P K Z v S M e 8 Z D K l Z 4 2 W L 4 2 f k 3 D h D M o q U e S G S h f t 7 I q O B 1 m n g m 8 6 A 4 k S v 1 u b m f 7 V e g q N r L x N h n C A P 2 X L R K J E E I z J P g g y F 4 g x l a o A y J c y t h E 2 o o g x N X i U T g r v 6 5 X V o 1 6 q u U 3 X v L y t 1 J 4 + j C K d w B h f g w h X U 4 R Y a 0 A I G K T z B C 7 x a j 9 a z 9 W a 9 L 1 s L V j 5 T h j + y P r 4 B C B W T p w = = &lt; / l a t e x i t &gt; Illustration of the process of GraphMLP in the video domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Visualization results of the proposed GraphMLP for reconstructing 3D human poses on challenging in-the-wild images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>( 3 )</head><label>3</label><figDesc>Additional ablation studies (see Sec. X). (4) Additional qualitative results (see Sec. XI). (5) Code in the attached file ('./Code' directory) will be open-sourced.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(f) Graph-Mixer: We replace linear layers in MLP-Mixer with GCN layers. (g) Transformer-GCN: We replace spatial MLPs in GraphMLP with multi-head self-attention blocks and add a position embedding module before Transformer-GCN layers. (h) GraphMLP: It is our proposed approach. Please refer to Fig. 1 of our main paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .Fig. 10 .</head><label>910</label><figDesc>Different network architectures. Different design options of the GraphMLP layer. GraphMLP (CG-MLP)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 12 .</head><label>12</label><figDesc>Visualization results of our approach for reconstructing 3D human poses on Human3.6M dataset (top three rows) and MPI-INF-3DHP dataset (bottom three rows).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 13 .</head><label>13</label><figDesc>Qualitative comparison with MLP-Mixer<ref type="bibr" target="#b7">[7]</ref>, GCN<ref type="bibr" target="#b8">[8]</ref>, and MGCN<ref type="bibr" target="#b25">[25]</ref> for reconstructing 3D human poses on challenging in-the-wild images. Red arrows highlight wrong estimations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>to the best of our knowledge, the first MLP-Like architecture called GraphMLP for 3D human pose estimation. It combines the advantages of modern MLPs and GCNs, including globality, locality, and connectivity. Extensive experiments demonstrate the effectiveness and generalization ability of the proposed GraphMLP, and show new state-of-the-art results on two challenging datasets, i.e., Human3.6M<ref type="bibr" target="#b23">[23]</ref> and MPI-INF-3DHP<ref type="bibr" target="#b24">[24]</ref>.</figDesc><table /><note>? The novel SG-MLP and CG-MLP blocks are proposed to encode the graph structure of human bodies within MLPs to obtain domain-specific knowledge about the human body while enabling the model to capture both local and global interactions.? A simple and efficient video representation is further proposed to extend our GraphMLP to the video domain flexibly. This representation enables the model to effec- tively process arbitrary-length sequences with negligible computational cost gains.?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I QUANTITATIVE</head><label>I</label><figDesc>COMPARISON ON HUMAN3.6M UNDER PROTOCOL #1 AND PROTOCOL #2. DETECTED 2D KEYPOINTS ARE USED AS INPUT. ? -ADOPTS THE SAME REFINEMENT MODULE AS<ref type="bibr" target="#b8">[8]</ref>,<ref type="bibr" target="#b25">[25]</ref>.TABLE II QUANTITATIVE COMPARISON ON HUMAN3.6M UNDER PROTOCOL #1. GROUND TRUTH 2D KEYPOINTS ARE USED AS INPUT.</figDesc><table><row><cell>Protocol #1</cell><cell>Dir.</cell><cell>Disc</cell><cell>Eat</cell><cell>Greet</cell><cell cols="4">Phone Photo Pose Purch.</cell><cell>Sit</cell><cell cols="6">SitD. Smoke Wait WalkD. Walk WalkT.</cell><cell>Avg.</cell></row><row><cell>Martinez et al. [3]</cell><cell>51.8</cell><cell>56.2</cell><cell>58.1</cell><cell>59.0</cell><cell>69.5</cell><cell>78.4</cell><cell>55.2</cell><cell>58.1</cell><cell>74.0</cell><cell>94.6</cell><cell>62.3</cell><cell>59.1</cell><cell>65.1</cell><cell>49.5</cell><cell>52.4</cell><cell>62.9</cell></row><row><cell>Fang et al. [46]</cell><cell>50.1</cell><cell>54.3</cell><cell>57.0</cell><cell>57.1</cell><cell>66.6</cell><cell>73.3</cell><cell>53.4</cell><cell>55.7</cell><cell>72.8</cell><cell>88.6</cell><cell>60.3</cell><cell>57.7</cell><cell>62.7</cell><cell>47.5</cell><cell>50.6</cell><cell>60.4</cell></row><row><cell>Yang et al. [47]</cell><cell>51.5</cell><cell>58.9</cell><cell>50.4</cell><cell>57.0</cell><cell>62.1</cell><cell>65.4</cell><cell>49.8</cell><cell>52.7</cell><cell>69.2</cell><cell>85.2</cell><cell>57.4</cell><cell>58.4</cell><cell>43.6</cell><cell>60.1</cell><cell>47.7</cell><cell>58.6</cell></row><row><cell>Ci et al. [6]</cell><cell>46.8</cell><cell>52.3</cell><cell>44.7</cell><cell>50.4</cell><cell>52.9</cell><cell>68.9</cell><cell>49.6</cell><cell>46.4</cell><cell>60.2</cell><cell>78.9</cell><cell>51.2</cell><cell>50.0</cell><cell>54.8</cell><cell>40.4</cell><cell>43.3</cell><cell>52.7</cell></row><row><cell>Liu et al. [18]</cell><cell>46.3</cell><cell>52.2</cell><cell>47.3</cell><cell>50.7</cell><cell>55.5</cell><cell>67.1</cell><cell>49.2</cell><cell>46.0</cell><cell>60.4</cell><cell>71.1</cell><cell>51.5</cell><cell>50.1</cell><cell>54.5</cell><cell>40.3</cell><cell>43.7</cell><cell>52.4</cell></row><row><cell>Xu et al. [19]</cell><cell>45.2</cell><cell>49.9</cell><cell>47.5</cell><cell>50.9</cell><cell>54.9</cell><cell>66.1</cell><cell>48.5</cell><cell>46.3</cell><cell>59.7</cell><cell>71.5</cell><cell>51.4</cell><cell>48.6</cell><cell>53.9</cell><cell>39.9</cell><cell>44.1</cell><cell>51.9</cell></row><row><cell>Zhao et al. [48]</cell><cell>45.2</cell><cell>50.8</cell><cell>48.0</cell><cell>50.0</cell><cell>54.9</cell><cell>65.0</cell><cell>48.2</cell><cell>47.1</cell><cell>60.2</cell><cell>70.0</cell><cell>51.6</cell><cell>48.7</cell><cell>54.1</cell><cell>39.7</cell><cell>43.1</cell><cell>51.8</cell></row><row><cell>Pavllo et al. [20]</cell><cell>47.1</cell><cell>50.6</cell><cell>49.0</cell><cell>51.8</cell><cell>53.6</cell><cell>61.4</cell><cell>49.4</cell><cell>47.4</cell><cell>59.3</cell><cell>67.4</cell><cell>52.4</cell><cell>49.5</cell><cell>55.3</cell><cell>39.5</cell><cell>42.7</cell><cell>51.8</cell></row><row><cell>Cai et al. [8] ?</cell><cell>46.5</cell><cell>48.8</cell><cell>47.6</cell><cell>50.9</cell><cell>52.9</cell><cell>61.3</cell><cell>48.3</cell><cell>45.8</cell><cell>59.2</cell><cell>64.4</cell><cell>51.2</cell><cell>48.4</cell><cell>53.5</cell><cell>39.2</cell><cell>41.2</cell><cell>50.6</cell></row><row><cell>Zeng et al. [49]</cell><cell>44.5</cell><cell>48.2</cell><cell>47.1</cell><cell>47.8</cell><cell>51.2</cell><cell>56.8</cell><cell>50.1</cell><cell>45.6</cell><cell>59.9</cell><cell>66.4</cell><cell>52.1</cell><cell>45.3</cell><cell>54.2</cell><cell>39.1</cell><cell>40.3</cell><cell>49.9</cell></row><row><cell>Zou et al. [25] ?</cell><cell>45.4</cell><cell>49.2</cell><cell>45.7</cell><cell>49.4</cell><cell>50.4</cell><cell>58.2</cell><cell>47.9</cell><cell>46.0</cell><cell>57.5</cell><cell>63.0</cell><cell>49.7</cell><cell>46.6</cell><cell>52.2</cell><cell>38.9</cell><cell>40.8</cell><cell>49.4</cell></row><row><cell>GraphMLP (Ours)</cell><cell>45.4</cell><cell>50.2</cell><cell>45.8</cell><cell>49.2</cell><cell>51.6</cell><cell>57.9</cell><cell>47.3</cell><cell>44.9</cell><cell>56.9</cell><cell>61.0</cell><cell>49.5</cell><cell>46.9</cell><cell>53.2</cell><cell>37.8</cell><cell>39.9</cell><cell>49.2</cell></row><row><cell>GraphMLP (Ours) ?</cell><cell>43.7</cell><cell>49.3</cell><cell>45.5</cell><cell>47.8</cell><cell>50.5</cell><cell>56.0</cell><cell>46.3</cell><cell>44.1</cell><cell>55.9</cell><cell>59.0</cell><cell>48.4</cell><cell>45.7</cell><cell>51.2</cell><cell>37.1</cell><cell>39.1</cell><cell>48.0</cell></row><row><cell>Protocol #2</cell><cell>Dir.</cell><cell>Disc</cell><cell>Eat</cell><cell>Greet</cell><cell cols="4">Phone Photo Pose Purch.</cell><cell>Sit</cell><cell cols="6">SitD. Smoke Wait WalkD. Walk WalkT.</cell><cell>Avg.</cell></row><row><cell>Martinez et al. [3]</cell><cell>39.5</cell><cell>43.2</cell><cell>46.4</cell><cell>47.0</cell><cell>51.0</cell><cell>56.0</cell><cell>41.4</cell><cell>40.6</cell><cell>56.5</cell><cell>69.4</cell><cell>49.2</cell><cell>45.0</cell><cell>49.5</cell><cell>38.0</cell><cell>43.1</cell><cell>47.7</cell></row><row><cell>Fang et al. [46]</cell><cell>38.2</cell><cell>41.7</cell><cell>43.7</cell><cell>44.9</cell><cell>48.5</cell><cell>55.3</cell><cell>40.2</cell><cell>38.2</cell><cell>54.5</cell><cell>64.4</cell><cell>47.2</cell><cell>44.3</cell><cell>47.3</cell><cell>36.7</cell><cell>41.7</cell><cell>45.7</cell></row><row><cell>Ci et al. [6]</cell><cell>36.9</cell><cell>41.6</cell><cell>38.0</cell><cell>41.0</cell><cell>41.9</cell><cell>51.1</cell><cell>38.2</cell><cell>37.6</cell><cell>49.1</cell><cell>62.1</cell><cell>43.1</cell><cell>39.9</cell><cell>43.5</cell><cell>32.2</cell><cell>37.0</cell><cell>42.2</cell></row><row><cell>Liu et al. [18]</cell><cell>35.9</cell><cell>40.0</cell><cell>38.0</cell><cell>41.5</cell><cell>42.5</cell><cell>51.4</cell><cell>37.8</cell><cell>36.0</cell><cell>48.6</cell><cell>56.6</cell><cell>41.8</cell><cell>38.3</cell><cell>42.7</cell><cell>31.7</cell><cell>36.2</cell><cell>41.2</cell></row><row><cell>Cai et al. [8] ?</cell><cell>36.8</cell><cell>38.7</cell><cell>38.2</cell><cell>41.7</cell><cell>40.7</cell><cell>46.8</cell><cell>37.9</cell><cell>35.6</cell><cell>47.6</cell><cell>51.7</cell><cell>41.3</cell><cell>36.8</cell><cell>42.7</cell><cell>31.0</cell><cell>34.7</cell><cell>40.2</cell></row><row><cell>Pavllo et al. [20]</cell><cell>36.0</cell><cell>38.7</cell><cell>38.0</cell><cell>41.7</cell><cell>40.1</cell><cell>45.9</cell><cell>37.1</cell><cell>35.4</cell><cell>46.8</cell><cell>53.4</cell><cell>41.4</cell><cell>36.9</cell><cell>43.1</cell><cell>30.3</cell><cell>34.8</cell><cell>40.0</cell></row><row><cell>Zeng et al. [49]</cell><cell>35.8</cell><cell>39.2</cell><cell>36.6</cell><cell>36.9</cell><cell>39.8</cell><cell>45.1</cell><cell>38.4</cell><cell>36.9</cell><cell>47.7</cell><cell>54.4</cell><cell>38.6</cell><cell>36.3</cell><cell>39.4</cell><cell>30.3</cell><cell>35.4</cell><cell>39.4</cell></row><row><cell>Zou et al. [25] ?</cell><cell>35.7</cell><cell>38.6</cell><cell>36.3</cell><cell>40.5</cell><cell>39.2</cell><cell>44.5</cell><cell>37.0</cell><cell>35.4</cell><cell>46.4</cell><cell>51.2</cell><cell>40.5</cell><cell>35.6</cell><cell>41.7</cell><cell>30.7</cell><cell>33.9</cell><cell>39.1</cell></row><row><cell>GraphMLP (Ours)</cell><cell>35.0</cell><cell>38.4</cell><cell>36.6</cell><cell>39.7</cell><cell>40.1</cell><cell>43.9</cell><cell>35.9</cell><cell>34.1</cell><cell>45.9</cell><cell>48.6</cell><cell>40.0</cell><cell>35.3</cell><cell>41.6</cell><cell>30.0</cell><cell>33.3</cell><cell>38.6</cell></row><row><cell>GraphMLP (Ours) ?</cell><cell>35.1</cell><cell>38.2</cell><cell>36.5</cell><cell>39.8</cell><cell>39.8</cell><cell>43.5</cell><cell>35.7</cell><cell>34.0</cell><cell>45.6</cell><cell>47.6</cell><cell>39.8</cell><cell>35.1</cell><cell>41.1</cell><cell>30.0</cell><cell>33.4</cell><cell>38.4</cell></row><row><cell>Protocol #1</cell><cell>Dir.</cell><cell>Disc</cell><cell>Eat</cell><cell cols="5">Greet Phone Photo Pose Purch.</cell><cell>Sit</cell><cell cols="5">SitD. Smoke Wait WalkD. Walk</cell><cell>WalkT.</cell><cell>Avg.</cell></row><row><cell>Martinez et al. [3]</cell><cell>37.7</cell><cell>44.4</cell><cell>40.3</cell><cell>42.1</cell><cell>48.2</cell><cell>54.9</cell><cell>44.4</cell><cell>42.1</cell><cell>54.6</cell><cell>58.0</cell><cell>45.1</cell><cell>46.4</cell><cell>47.6</cell><cell>36.4</cell><cell>40.4</cell><cell>45.5</cell></row><row><cell>Zhao et al. [10]</cell><cell>37.8</cell><cell>49.4</cell><cell>37.6</cell><cell>40.9</cell><cell>45.1</cell><cell>41.4</cell><cell>40.1</cell><cell>48.3</cell><cell>50.1</cell><cell>42.2</cell><cell>53.5</cell><cell>44.3</cell><cell>40.5</cell><cell>47.3</cell><cell>39.0</cell><cell>43.8</cell></row><row><cell>Cai et al. [8]</cell><cell>33.4</cell><cell>39.0</cell><cell>33.8</cell><cell>37.0</cell><cell>38.1</cell><cell>47.3</cell><cell>39.5</cell><cell>37.3</cell><cell>43.2</cell><cell>46.2</cell><cell>37.7</cell><cell>38.0</cell><cell>38.6</cell><cell>30.4</cell><cell>32.1</cell><cell>38.1</cell></row><row><cell>Liu et al. [18]</cell><cell>36.8</cell><cell>40.3</cell><cell>33.0</cell><cell>36.3</cell><cell>37.5</cell><cell>45.0</cell><cell>39.7</cell><cell>34.9</cell><cell>40.3</cell><cell>47.7</cell><cell>37.4</cell><cell>38.5</cell><cell>38.6</cell><cell>29.6</cell><cell>32.0</cell><cell>37.8</cell></row><row><cell>Zou et al. [25]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>37.4</cell></row><row><cell>Zeng et al. [49]</cell><cell>35.9</cell><cell>36.7</cell><cell>29.3</cell><cell>34.5</cell><cell>36.0</cell><cell>42.8</cell><cell>37.7</cell><cell>31.7</cell><cell>40.1</cell><cell>44.3</cell><cell>35.8</cell><cell>37.2</cell><cell>36.2</cell><cell>33.7</cell><cell>34.0</cell><cell>36.4</cell></row><row><cell>Ci et al. [6]</cell><cell>36.3</cell><cell>38.8</cell><cell>29.7</cell><cell>37.8</cell><cell>34.6</cell><cell>42.5</cell><cell>39.8</cell><cell>32.5</cell><cell>36.2</cell><cell>39.5</cell><cell>34.4</cell><cell>38.4</cell><cell>38.2</cell><cell>31.3</cell><cell>34.2</cell><cell>36.3</cell></row><row><cell>Xu et al. [19]</cell><cell>35.8</cell><cell>38.1</cell><cell>31.0</cell><cell>35.3</cell><cell>35.8</cell><cell>43.2</cell><cell>37.3</cell><cell>31.7</cell><cell>38.4</cell><cell>45.5</cell><cell>35.4</cell><cell>36.7</cell><cell>36.8</cell><cell>27.9</cell><cell>30.7</cell><cell>35.8</cell></row><row><cell>Zhao et al. [48]</cell><cell>32.0</cell><cell>38.0</cell><cell>30.4</cell><cell>34.4</cell><cell>34.7</cell><cell>43.3</cell><cell>35.2</cell><cell>31.4</cell><cell>38.0</cell><cell>46.2</cell><cell>34.2</cell><cell>35.7</cell><cell>36.1</cell><cell>27.4</cell><cell>30.6</cell><cell>35.2</cell></row><row><cell>GraphMLP (Ours)</cell><cell>33.1</cell><cell>38.7</cell><cell>29.3</cell><cell>34.4</cell><cell>34.1</cell><cell>38.0</cell><cell>39.0</cell><cell>32.1</cell><cell>38.4</cell><cell>39.4</cell><cell>34.2</cell><cell>37.5</cell><cell>34.5</cell><cell>28.0</cell><cell>28.8</cell><cell>34.6</cell></row><row><cell cols="8">48.0 mm in MPJPE, surpassing MGCN [25] by a large margin</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">of 1.4 mm error reduction (relative 3% improvement).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table V</head><label>V</label><figDesc></figDesc><table><row><cell>further reports the comparison of computational</cell></row><row><cell>costs with Poseformer</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III PERFORMANCE</head><label>III</label><figDesc>COMPARISON WITH STATE-OF-THE-ARTS ON MPI-INF-3DHP. Method GS ? noGS ? Outdoor ? All PCK ? All AUC ? 6M UNDER MPJPE. CPN AND GT DENOTE THE INPUTS OF 2D POSES DETECTED BY CPN AND GROUND TRUTH 2D POSES, RESPECTIVELY.</figDesc><table><row><cell cols="2">Martinez et al. [3] 49.8</cell><cell>42.5</cell><cell>31.2</cell><cell>42.5</cell><cell>17.0</cell></row><row><cell>Mehta et al. [24]</cell><cell>70.8</cell><cell>62.3</cell><cell>58.5</cell><cell>64.7</cell><cell>31.7</cell></row><row><cell>Ci et al. [6]</cell><cell>74.8</cell><cell>70.8</cell><cell>77.3</cell><cell>74.0</cell><cell>36.7</cell></row><row><cell>Zhao et al. [48]</cell><cell>80.1</cell><cell>77.9</cell><cell>74.1</cell><cell>79.0</cell><cell>43.8</cell></row><row><cell>Liu et al. [18]</cell><cell>77.6</cell><cell>80.5</cell><cell>80.1</cell><cell>79.3</cell><cell>47.6</cell></row><row><cell>Xu et al. [19]</cell><cell>81.5</cell><cell>81.7</cell><cell>75.2</cell><cell>80.1</cell><cell>45.8</cell></row><row><cell>Zeng et al. [9]</cell><cell>-</cell><cell>-</cell><cell>84.6</cell><cell>82.1</cell><cell>46.2</cell></row><row><cell>Zou et al. [25]</cell><cell>86.4</cell><cell>86.0</cell><cell>85.7</cell><cell>86.1</cell><cell>53.7</cell></row><row><cell cols="2">GraphMLP (Ours) 87.3</cell><cell>87.1</cell><cell>86.3</cell><cell>87.0</cell><cell>54.3</cell></row><row><cell></cell><cell></cell><cell cols="2">TABLE IV</cell><cell></cell><cell></cell></row><row><cell cols="6">QUANTITATIVE COMPARISONS WITH VIDEO-BASED METHODS ON</cell></row><row><cell>HUMAN3.Method</cell><cell></cell><cell></cell><cell>CPN</cell><cell></cell><cell>GT</cell></row><row><cell cols="2">ST-GCN [8] (T =9)</cell><cell></cell><cell>48.8</cell><cell></cell><cell>37.2</cell></row><row><cell cols="3">VideoPose3D [20] (T =243)</cell><cell>46.8</cell><cell></cell><cell>37.8</cell></row><row><cell cols="3">Liu et al. [51] (T =243)</cell><cell>45.1</cell><cell></cell><cell>34.7</cell></row><row><cell cols="2">SRNet [49] (T =243)</cell><cell></cell><cell>44.8</cell><cell></cell><cell>32.0</cell></row><row><cell cols="3">Poseformer [21] (T =81)</cell><cell>44.3</cell><cell></cell><cell>31.3</cell></row><row><cell cols="3">Anatomy3D [49] (T =243)</cell><cell>44.1</cell><cell></cell><cell>32.3</cell></row><row><cell cols="2">Baseline (T =243)</cell><cell></cell><cell>48.4</cell><cell></cell><cell>35.2</cell></row><row><cell cols="3">GraphMLP (Ours, T =243)</cell><cell>43.8</cell><cell></cell><cell>30.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V COMPARISON</head><label>V</label><figDesc>OF PARAMETERS, FLOPS, AND MPJPE WITH POSEFORMER IN DIFFERENT INPUT FRAMES ON HUMAN3.6M.TABLE VI ABLATION STUDY ON VARIOUS CONFIGURATIONS OF OUR APPROACH. L IS THE NUMBER OF GRAPHMLP LAYERS, C IS THE HIDDEN SIZE, D C IS THE MLP DIMENSION OF CG-MLP, AND D S IS THE MLP DIMENSION OF SG-MLP.</figDesc><table><row><cell></cell><cell>Model</cell><cell cols="4">T Param (M) FLOPs (M) MPJPE (mm)</cell></row><row><cell></cell><cell cols="2">PoseFormer [21] 27</cell><cell>9.57</cell><cell>541</cell><cell>47.0</cell></row><row><cell></cell><cell cols="2">PoseFormer [21] 81</cell><cell>9.60</cell><cell>1625</cell><cell>44.3</cell></row><row><cell></cell><cell cols="2">PoseFormer [21] 243</cell><cell>9.69</cell><cell>4874</cell><cell>-</cell></row><row><cell></cell><cell cols="2">GraphMLP (Ours) 1</cell><cell>9.49</cell><cell>348</cell><cell>49.2</cell></row><row><cell></cell><cell cols="2">GraphMLP (Ours) 27</cell><cell>9.51</cell><cell>349</cell><cell>45.5</cell></row><row><cell></cell><cell cols="2">GraphMLP (Ours) 81</cell><cell>9.57</cell><cell>351</cell><cell>44.5</cell></row><row><cell></cell><cell cols="2">GraphMLP (Ours) 243</cell><cell>9.73</cell><cell>356</cell><cell>43.8</cell></row><row><cell cols="6">Fig. 6. Qualitative comparison with MLP-Mixer [7] and GCN [8] for reconstructing 3D human poses on Human3.6M dataset. Red arrows highlight wrong</cell></row><row><cell>estimations.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>L</cell><cell>C</cell><cell>D C</cell><cell>D S</cell><cell>Params (M)</cell><cell>MPJPE (mm) ?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VII ABLATION</head><label>VII</label><figDesc>STUDY ON DIFFERENT 2D DETECTORS.an optimal architecture. As shown inTable IX, the location matters of GCN are studied by five different designs: (i) The GCN is placed before spatial MLP. (ii) The GCN is placed after spatial MLP. (iii) The GCN is placed after channel MLP. (iv) The GCN and MLP are in parallel but using a spatial GCN block (process tokens in spatial dimension) in SG-MLP. (v) The GCN and MLP are in parallel. The results show that all these designs can help the model produce more accurate 3D poses, and using GCN and MLP in parallel achieves the best estimation accuracy. The illustrations of these design options can be found in the supplementary. Model Components. We also investigate the effectiveness of each component in our design. InTable X, the first row corresponds to the baseline model (i.e., MLP-Mixer<ref type="bibr" target="#b7">[7]</ref>) that does not use any GCNs in the model. The MPJPE is 52.0 mm. The rest of the rows show the results of replacing its spatial MLP with SG-MLP or channel MLP with CG-MLP by adding a GCN block into the baseline. It can be found that using our SG-MLP or CG-MLP can improve performance (50.6 mm and 50.5 mm respectively). When enabling both SG-MLP and CG-MLP, GraphMLP improves the performance over the baseline by a clear margin of 2.8 mm (relatively 5.4%TABLE IX ABLATION STUDY ON DIFFERENT DESIGN OPTIONS OF COMBINING MLPS AND GCNS. * MEANS USING SPATIAL GCN BLOCK IN SG-MLP.</figDesc><table><row><cell>Method</cell><cell>2D Mean Error</cell><cell>MPJPE (mm) ?</cell><cell></cell><cell></cell></row><row><cell>SH [52]</cell><cell>9.03</cell><cell>56.9</cell><cell>Method</cell><cell></cell><cell>MPJPE (mm) ?</cell></row><row><cell>Detectron [20]</cell><cell>7.77</cell><cell>54.5</cell><cell>Baseline</cell><cell></cell><cell>52.0</cell></row><row><cell>CPN [3] 2D Ground Truth</cell><cell>6.67 0</cell><cell>49.2 34.6</cell><cell cols="2">GCN Before Spatial MLP GCN After Spatial MLP</cell><cell>51.6 50.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">GCN After Channel MLP</cell><cell>50.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">GCN and MLP in Parallel  *</cell><cell>50.2</cell></row><row><cell></cell><cell>TABLE VIII</cell><cell></cell><cell cols="2">GCN and MLP in Parallel</cell><cell>49.2</cell></row><row><cell cols="3">ABLATION STUDY ON DIFFERENT NETWORK ARCHITECTURES.</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell>MPJPE (mm) ?</cell><cell cols="3">TABLE X ABLATION STUDY ON DIFFERENT COMPONENTS IN GRAPHMLP.</cell></row><row><cell>FCN [3]</cell><cell></cell><cell>53.5</cell><cell></cell><cell></cell></row><row><cell>GCN [8] Transformer [13]</cell><cell></cell><cell>51.3 52.7</cell><cell>Method</cell><cell cols="2">SG-MLP CG-MLP MPJPE (mm) ?</cell></row><row><cell>MLP-Mixer [7]</cell><cell></cell><cell>52.0</cell><cell>Baseline</cell><cell></cell><cell>52.0</cell></row><row><cell cols="2">Mesh Graphormer [36]</cell><cell>51.3</cell><cell>GraphMLP (SG-MLP)</cell><cell></cell><cell>50.6</cell></row><row><cell>Graph-Mixer</cell><cell></cell><cell>50.4</cell><cell>GraphMLP (CG-MLP)</cell><cell></cell><cell>50.5</cell></row><row><cell>Transformer-GCN</cell><cell></cell><cell>51.5</cell><cell>GraphMLP</cell><cell></cell><cell>49.2</cell></row><row><cell>GraphMLP</cell><cell></cell><cell>49.2</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE XI ABLATION</head><label>XI</label><figDesc>STUDY ON DIFFERENT DESIGN OPTIONS OF TRANSPOSITION.</figDesc><table><row><cell>Method</cell><cell>MPJPE (mm) ?</cell></row><row><cell>MLP-Mixer, Transposition After LN</cell><cell>52.6</cell></row><row><cell>MLP-Mixer, Transposition Before LN</cell><cell>52.0</cell></row><row><cell>GraphMLP, Transposition After LN</cell><cell>49.7</cell></row><row><cell>GraphMLP, Transposition Before LN</cell><cell>49.2</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>approach can produce more accurate results with a better 2D detector and is effective on different 2D estimators. Network Architectures. To clearly demonstrate the advantage of the proposed GraphMLP, we compare our approach with various baseline architectures. The 'Graph-Mixer' is constructed by replacing the linear layers in MLP-Mixer with GCN layers, and the 'Transformer-GCN' is built by replacing the spatial MLPs in GraphMLP with multi-head self-attention blocks and adding a position embedding module. To ensure a consistent and fair evaluation, the parameters of these architectures (e.g., the number of layers, hidden size) keep the same. As shown in <ref type="table">Table VIII</ref>, our proposed approach consistently surpasses all other architectures. For example, our approach can improve the GCN-based model <ref type="bibr" target="#b8">[8]</ref>  This may be because a small number of tokens (e.g., 17 joints) are less effective for self-attention in learning longrange dependencies. Overall, these results confirm that the GraphMLP can serve as a new and strong baseline for 3D human pose estimation. The implementation details of these network architectures can be found in the supplementary. Network Design Options. Our approach allows the model to learn strong structural priors of human joints by injecting GCNs into the baseline model (i.e., MLP-Mixer <ref type="bibr" target="#b7">[7]</ref>). We also explore the different combinations of MLPs and GCNs to find</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Boosting monocular 3D human pose estimation with part aware attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<idno>2022. 1</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Limb pose aware networks for monocular 3D pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="906" to="917" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A simple yet effective baseline for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weakly-supervised 3D human pose estimation with cross-view U-shaped graph convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<idno>2022. 1</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE TMM</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Robust estimation of 3D human poses from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2361" to="2368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Challenging scenarios where GraphMLP fails to produce accurate 3D human poses</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Optimizing network structure for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">MLP-Mixer: An all-MLP architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">O</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Exploiting spatial-temporal relationships for 3D pose estimation via graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning skeletal graph neural networks for hard 3D pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semantic graph convolutional networks for 3D human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Motion guided 3D pose estimation from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="764" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Conditional directed graph convolution for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="602" to="611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note>in ICLR, 2021. 1, 3, 4</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Swin Transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno>pp. 10 012-10 022</idno>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploiting temporal contexts with strided transformer for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMM</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Transformer-based attention networks for continuous pixel-wise prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">279</biblScope>
			<biblScope unit="page" from="16" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A comprehensive study of weight sharing in graph networks for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Graph stacked hourglass networks for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Takano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">3D human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="7753" to="7762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">3D human pose estimation with spatial and temporal transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mendieta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">MHFormer: Multihypothesis transformer for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2022</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Human3.6M: Large scale datasets and predictive methods for 3D human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Monocular 3D human pose estimation in the wild using improved CNN supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Modulated graph convolutional network for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021</title>
		<imprint>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">3D human pose estimation from monocular images with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ACCV</publisher>
			<biblScope unit="page" from="332" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3D human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7025" to="7034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Towards 3D human pose estimation in the wild: A weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="398" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A gaussian process guided particle filter for tracking 3D human pose in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sedai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Q</forename><surname>Huynh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4286" to="4300" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A joint relationship aware neural network for single-image 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="4747" to="4758" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Generating multiple hypotheses for 3D human pose estimation with mixture density network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9887" to="9895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="568" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Deformable DETR: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<idno>2021. 3</idno>
		<editor>ICLR</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Tracking objects as pixel-wise distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Mesh Graphormer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">AS-MLP: An axial shifted MLP architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pay attention to MLPs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="9204" to="9215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Cascaded cross MLP-Mixer GANs for cross-view image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<idno>2021. 3</idno>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Gaussian error linear units (GELUs)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">ResMLP: Feedforward networks for image classification with data-efficient training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03404</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">CycleMLP: A MLP-like architecture for dense prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning pose grammar to encode human body configuration for 3D pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">3D human pose estimation in the wild by adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="5255" to="5264" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Graformer: Graph-oriented transformer for 3D pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2022</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">SRNet: Improving generalization in 3D human pose estimation with a split-and-recombine approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Generalized procrustes analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Gower</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="51" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Attention mechanism exploits temporal contexts: Real-time 3D human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Asari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5064" to="5073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Code for A Simple Yet Effective Baseline for 3D Human Pose Estimation</title>
		<ptr target="https://github.com/weigq/3dposebaselinepytorch/.11" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
