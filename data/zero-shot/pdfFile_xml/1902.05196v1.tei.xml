<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Categorical Metadata Representation for Customized Text Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihyeok</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinald</forename><forename type="middle">Kim</forename><surname>Amplayo</surname></persName>
							<email>reinald.kim@ed.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungjae</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sua</forename><surname>Sung</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minji</forename><surname>Seo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seung-Won</forename><surname>Hwang</surname></persName>
							<email>seungwonh@yonsei.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Categorical Metadata Representation for Customized Text Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>(* equal contribution)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The performance of text classification has improved tremendously using intelligently engineered neural-based models, especially those injecting categorical metadata as additional information, e.g., using user/product information for sentiment classification. These information have been used to modify parts of the model (e.g., word embeddings, attention mechanisms) such that results can be customized according to the metadata. We observe that current representation methods for categorical metadata, which are devised for human consumption, are not as effective as claimed in popular classification methods, outperformed even by simple concatenation of categorical features in the final layer of the sentence encoder. We conjecture that categorical features are harder to represent for machine use, as available context only indirectly describes the category, and even such context is often scarce (for tail category). To this end, we propose to use basis vectors to effectively incorporate categorical metadata on various parts of a neural-based model. This additionally decreases the number of parameters dramatically, especially when the number of categorical features is large. Extensive experiments on various datasets with different properties are performed and show that through our method, we can represent categorical metadata more effectively to customize parts of the model, including unexplored ones, and increase the performance of the model greatly.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text classification is the backbone of most NLP tasks: review classification in sentiment analysis <ref type="bibr" target="#b19">(Pang et al., 2002)</ref>, paper classification in scientific data discovery <ref type="bibr" target="#b25">(Sebastiani, 2002)</ref>, and question classification in question answering <ref type="bibr">(Li and</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Political Message Type Classification</head><p>Not much to say it is just boring. And I have watched Incredibles 1 so many times since when I was 10 years old, never got bored. This movie was boring and had too many weak characters. The villain twist was very obvious and the cringey sjw moments ruined the feel of the movie.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text (Review)</head><p>Categories User</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class: Negative</head><p>Today, i joined with my republican and democrat colleagues in asking the leaders of both parties to come together to do what's best for America. too many families in the central valley and across our country are hurting. congress must end this government shutdown now.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text (Abstract)</head><p>Categories Politician (Writer)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class: Policy</head><p>Political Bias Product Customized Text Classification <ref type="figure">Figure 1</ref>: A high-level framework of models for the Customized Text Classification Task that inputs a text with n tokens (e.g. review) and m categories (e.g. users, products, etc.) and outputs a class (e.g. positive/negative). Example tasks are shown in the left of the figure. <ref type="bibr" target="#b13">Roth, 2002)</ref>, to name a few. While prior methods require intensive feature engineering, recent methods enjoy automatic extraction of features from text using neural-based models <ref type="bibr" target="#b26">(Socher et al., 2011)</ref> by encoding texts into low-dimensional dense feature vectors.</p><p>This paper studies customized text classification, generalized from personalized text classification <ref type="bibr" target="#b3">(Baruzzo et al., 2009)</ref>, where we customize classifiers based on possibly multiple different known categorical metadata information (e.g., user/product information for sentiment classification) instead of just the user information. As shown in <ref type="figure">Figure 1</ref>, in addition to the text, a customizable text classifier is given a list of categories specific to the text to predict its class. Existing works applied metadata information to improve the performance of a model, such as user and product <ref type="bibr" target="#b28">(Tang et al., 2015)</ref> information in sentiment classification, and author <ref type="bibr" target="#b24">(Rosen-Zvi et al., 2004)</ref> and publication <ref type="bibr" target="#b9">(Joorabchi and Mahdi, 2011)</ref> information in paper classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:1902.05196v1 [cs.CL] 14 Feb 2019</head><p>Towards the goal, we are inspired by the advancement in neural-based models, incorporating categorical information "as is" and injecting them on various parts of the model such as in the word embeddings <ref type="bibr" target="#b28">(Tang et al., 2015)</ref>, attention mechanism <ref type="bibr" target="#b4">(Chen et al., 2016;</ref><ref type="bibr" target="#b0">Amplayo et al., 2018a)</ref> and memory networks <ref type="bibr" target="#b6">(Dou, 2017)</ref>. Indeed, these methods theoretically make use of combined features from both textual and categorical features which make them more powerful than disconnected features. However, metadata is generated for human understanding, and thus we claim that these categories need to be carefully represented for machine use to improve the performance of the text classifier effectively.</p><p>First, we empirically invalidate the results from previous studies by showing in our experiments on multiple datasets that popular methods using metadata categories "as is" perform worse than a simple concatenation of textual and categorical feature vectors. We argue that this is because of the difficulties of the model in learning optimized dense vector representation of the categorical features to be used by the classification model. The reasons are two-fold: (a) categorical features do not have direct context and thus rely solely on classification labels when training the feature vectors, and (b) there are categorical information that are sparse and thus cannot effectively learn optimal feature vectors.</p><p>Second, we suggest an alternative representation, using low-dimensional basis vectors to mitigate the optimization problems of categorical feature vectors. Basis vectors have nice properties that can solve the issues presented above because they (a) transform multiple categories into useful combinations, which serve as mutual context to all categories, and (b) intelligently initialize vectors, especially of sparse categorical information, to a suboptimal location to efficiently train them further. Furthermore, our method reduces the number of trainable parameters and thus is flexible for any kinds and any number of available categories.</p><p>We experiment on multiple classification tasks with different properties and kinds of categories available. Our experiments show that while customization methods using categorical information "as is" do not perform as well as the naive concatenation method, applying our proposed basiscustomization method makes them much more effective than the naive method. Our method also enables the use of categorical metadata to customize other parts of the model, such as the encoder weights, that are previously unexplored due to their high space complexity and weak performance. We show that these unexplored use of customization outperform popular and conventional methods such as attention mechanism when our proposed basis-customization method is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem: Customized text classification</head><p>The original text classification task is defined as follows: Given a text W = {w 1 , w 2 , ..., w n }, we are tasked to train a mapping function f (W ) to predict a correct class y ? {y 1 , y 2 , ..., y p } among the p classes. The customized text classification task makes use of the categorical metadata information attached on the text to customize the mapping function. In this paper, we define categorical metadata as non-continuous information that describes the text 1 . An example task is review sentiment classification with user and product information as categorical metadata.</p><p>Formally, given a text t = {W, C}, where W = {w 1 , w 2 , ..., w n }, C = {c 1 , c 2 , ..., c m } and w x is the xth of the n tokens in the text, and c z is the category label of the text on the zth category of the m available categories, the goal of customized text classification is to optimize a function f C (W ) to predict a label y, where f C (W ) is the classifier dependent with C. In the example task above, W is the review text, and we have m = 2 categories where c 1 and c 2 are the user and product information. This is an interesting problem because of the vast opportunities it brings. First, we are motivated to use categorical metadata because existing works have shown that non-textual additional information, such as POS tags <ref type="bibr" target="#b7">(Go et al., 2009)</ref> and latent topics <ref type="bibr" target="#b33">(Zhao et al., 2017)</ref>, can be used as strong supplementary supervision to improve the performance of text classification. Second, while previously used additional information are found to be helpful signals, they are either domain-dependent or very noisy <ref type="bibr" target="#b1">(Amplayo et al., 2018b)</ref>. On the other hand, categorical metadata are usually factual and valid information that are either inherent (e.g., user/product information) or humanlabeled (e.g., research area). Finally, the customized text classification task generalizes the personalization problem <ref type="bibr" target="#b3">(Baruzzo et al., 2009)</ref>, where instead of personalizing based on single user information, we customize based on possibly multiple categories, which may or may not include user information. This consequently creates an opportunity to develop customizable virtual assistants <ref type="bibr" target="#b20">(Papacharissi, 2002)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Base classifier: BiLSTM</head><p>We use a Bidirectional Long Short Term Memory (BiLSTM) network <ref type="bibr" target="#b8">(Hochreiter and Schmidhuber, 1997)</ref> as our base text classifier as it is proven to work well on classifying text sequences <ref type="bibr" target="#b34">(Zhou et al., 2016)</ref>. Although the methods that are described here apply to other effective classifiers as well, such as CNNs <ref type="bibr" target="#b10">(Kim, 2014)</ref> and hierarchical models <ref type="bibr" target="#b31">(Yang et al., 2016)</ref>, we limit our experiments to BiLSTM to cover more important findings.</p><p>Our BiLSTM classifier starts by encoding the word embeddings using a forward and a backward LSTM. The resulting pairs of vectors are concatenated to get the final encoded word vectors, as shown below.</p><formula xml:id="formula_0">w i ? W (1) ? ? h i = LST M f (w i , ? ? h i?1 ) (2) ? ? h i = LST M b (w i , ? ? h i+1 )<label>(3)</label></formula><formula xml:id="formula_1">h i = [ ? ? h i ; ? ? h i ]<label>(4)</label></formula><p>Next, we pool the encoded word vectors h i into a text vector d using attention mechanism <ref type="bibr" target="#b2">(Bahdanau et al., 2015;</ref><ref type="bibr" target="#b16">Luong et al., 2015)</ref>, which calculates importance scores using a latent context vector x for all words, normalizes the scores using softmax, and use them to do weighted sum on encoded word vectors, as shown below.</p><formula xml:id="formula_2">e i = x h i (5) a i = exp(e i ) j exp(e j )<label>(6)</label></formula><formula xml:id="formula_3">d = i h i * a i<label>(7)</label></formula><p>Finally, we use a logistic regression classifier to classify labels using learned weight matrix W <ref type="bibr">(c)</ref> and bias vector b (c) , as shown below.</p><formula xml:id="formula_4">y = W (c) d + b (c)<label>(8)</label></formula><p>We can then train our classifier using any gradient descent algorithm by minimizing the negative log likelihood of the log softmax of predicted labels y with respect to the actual labels y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Baseline 1: Concatenated BiLSTM</head><p>To incorporate the categories into the classifier, a simple and naive method is to concatenate the categorical features with the text vector d. To do this, we create embedding spaces for the different categories and get the category vectors c 1 , c 2 , ..., c m based on the category labels of text d. We then use the concatenated vector as features for the logistic regression classifier:</p><formula xml:id="formula_5">y = W (c) [d; c 1 ; c 2 ; ...; c m ] + b (c)<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Baseline 2: Customized BiLSTM</head><p>While the Concatenated BiLSTM easily makes use of the categories as additional features for the classifier, it is not able to leverage on the possible low-level dependencies between textual and categorical features.</p><p>There are different levels of dependencies between texts and categories. For example, when predicting the sentiment of a review "The food is very sweet." given the user who wrote the review, the classifier should give a positive label if the user likes sweet foods and a negative label otherwise. In this case, the dependency between the review and the user is on the higher level, where we look at relationships between the full text and the categories. Another example is when predicting the acceptance of a research paper given that the research area is NLP, the classifier should focus more on NLP words (e.g., language, text) rather than less related words (e.g., biology, chemistry). In this case, the dependency between the research paper and the research area is on the lower level, where we look at relationships between segments of text and the categories.</p><p>We present five levels of Customized BiLSTM, which differ on the location where we inject the categorical features, listed below from the highest level to the lowest level of dependencies between text and categories. The main idea is to impose category-specific weights, instead of a single weight at each level of the model:</p><p>1. Customize on the bias vector: At this level of customization, we look at the general biases the categories have towards the problem. As a concrete example, when classifying the type of message a politician wrote, he/she can be biased towards writing personal messages than policy messages. Instead of using a single bias vector b (c) in the logistic regression classifier (Equation 8), we use additional multiple bias vectors for each category, as shown below. In fact, this is in spirit essentially equivalent to Concatenated BiL-STM (Equation 9), where the derivation is:</p><formula xml:id="formula_6">y = W d d + b c 1 + ... + b cm + b (c) = W d d + W c 1 c 1 + ... + W cm c m + b (c) = W (c) [d; c 1 ; c 2 ; ...; c m ] + b (c)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Customize on the linear transformation:</head><p>At this level of customization, we look at the text-level semantic biases the categories have. As a concrete example, in the sentiment classification task, the review "The food is very sweet" can have a negative sentiment if the user who wrote the review does not like sweets. Instead of using a single weight matrix W (c) in the logistic regression classifier (Equation 8), we use different weight matrices for each category:</p><formula xml:id="formula_7">y = W (c) c 1 d + W (c) c 2 d + ... + W (c) cm d + b (c)</formula><p>3. Customize on the attention pooling: At this level of customization, we look at the word importance biases the categories have. A concrete example is, when classifying a research paper, NLP words should be focus more when the research area is NLP. Instead of using a single context vector x when calculating the attention scores e (Equation 5), we use different context vectors for each category:</p><formula xml:id="formula_8">e i = x c 1 h i + x c 2 h i + ... + x cm h i a = sof tmax(e) d = i h i * a i</formula><p>4. Customize on the encoder weights: At this level of customization, we look at the word contextualization biases the categories need. A concrete example is, given the text "deep learning for political message classification", when encoding the word classification, the BiLSTM should retain the semantics of words political message more and forget the semantics of other words more when the research area is about politics. Instead of using a single set of input, forget, output, and memory cell weights for each LSTM (Equations 2 and 3), we use multiple sets of the weights, one for each category:</p><formula xml:id="formula_9">? ? ? ? g t i t f t o t ? ? ? ? = ? ? ? ? tanh ? ? ? ? ? ? ? ? ? 0&lt;k?m W (e) c k [w t ; h t?1 ] + b ? ? 5.</formula><p>Customize on the word embeddings: At this level of customization, we look at the word preference biases the categories have. For example, a user can prefer the use of word "terribly" as a positive adverb rather than the more common usage of the word with negative sentiment. Instead of directly using the word vectors from the embedding space W (Equation 1), we add a residual vector calculated based on a nonlinear transformation of the word vector using categoryspecific weights:</p><formula xml:id="formula_10">r = tanh(W (w) c 1 w i + ... + W (w) cm w i ) (10) w i = w i + r</formula><p>Previous works have proposed customization on bias vectors and word embeddings <ref type="bibr" target="#b28">(Tang et al., 2015)</ref>, and on attention pooling <ref type="bibr" target="#b4">(Chen et al., 2016)</ref>. We are the first to introduce customization on the linear transformation matrix and the encoders. Moreover, we are the first to use residual perturbations as word meaning modification for customizing word embeddings, in which we saw better performance than using a naive affine transformation, proposed in <ref type="bibr" target="#b28">(Tang et al., 2015)</ref>, in our prior experiments. optimize these weights is very hard because of two reasons.</p><p>First, categorical information has unique properties that make it nontrivial to train. One property is that unlike texts which naturally use neighboring words/sentences as context <ref type="bibr" target="#b14">(Lin et al., 2015;</ref><ref type="bibr" target="#b22">Peters et al., 2018)</ref>, categorical information stands alone and thus does not have information aside from itself. This forces the learning algorithm to rely solely on the classification labels y to find the optimal category-specific weights. Another property is that some categories may contain labels that are sparse or do not have enough instances. For example, a user can be cold-start <ref type="bibr" target="#b11">(Lam et al., 2008)</ref> or does not have enough reviews. In this case, the problem expands to few-shot learning <ref type="bibr" target="#b12">(Li et al., 2006)</ref>. Thus weights are hard to optimize using gradient-based techniques <ref type="bibr" target="#b23">(Ravi and Larochelle, 2016)</ref>.</p><p>Second, the number of weights is multiplied by the number of categories m and the number of category labels each category has, which enlarges the number of parameters needed to be trained as m increases. This magnifies the problems of context absence and information sparsity described above, since optimizing large parameters with limited inductive bias is very difficult. Moreover, because of the large parameters, some methods may not fit in commercially-available machines and thus may not be practically trainable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Basis Customization</head><p>We propose to solve all the problems above by using basis vectors to produce basis-customized weights, as shown visually in <ref type="figure" target="#fig_1">Figure 2</ref></p><formula xml:id="formula_11">. Specif- ically, we use a trainable set of d dim basis vectors B = {b 1 , b 2 , ..., b d },</formula><p>where dim is the dimension of the original weights. Let V c be the vector search space that contains all the optimal customized weight vectors v c , such that B is the basis of V c . Basis vectors follow the spanning property, thus we can represent all vectors in v ? V c as a linear combination of B, i.e. v c = i ? i * b i , where the ?s are the coefficients. Moreover, since we set d to a small number, we constrain the search space to a smaller vector space. Hence we can find the optimal weights in a constrained search space much faster.</p><p>To determine the ? coefficients, we first set the concatenated category vectors of the text q = [c 1 ; c 2 ; ...; c m ] as the query vector, and use a trainable set of key vectors K = {k 1 , k 2 , ..., k d }. We then calculate the dot product between the query and key vectors, and finally use softmax to create ? coefficients that sum to one:</p><formula xml:id="formula_12">z i = q k i ? i = exp(z i ) j exp(z j )</formula><p>We can then use the ? coefficients to basiscustomize a specific weight v, i.e. v c = i ? i * b i . In our BiLSTM classifier, we can basis-customize</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Splits</head><p>Categories Properties Yelp 2013 62,522 / 7,773 / 8,671 ? users (1.6k)</p><p>? products (1.6k)</p><p>Categories can be sparse, i.e. there may not be enough reviews for each user/product). AAPR 33,464 / 2,000 / 2,000 ? authors (48k)</p><p>? research area <ref type="formula" target="#formula_1">(144)</ref> Authors are sparse and have many category labels. Categories can have multiple labels (e.g. multiple authors, multidisciplinary fields). PolMed 4,500 / 0 / 500 ? politician <ref type="formula">(505)</ref> ? media source <ref type="formula">(2)</ref> ? audience <ref type="formula">(2)</ref> ? political bias <ref type="formula">(2)</ref> The dataset has more categories. Categories with binary labels may not be diverse enough to be useful. one of the following weights: (1) the bias vector v = b (c) and <ref type="formula">(2)</ref>  Basis-customizing weights help solve the problems of customizing BiLSTM in three ways. First, the basis vectors serve as fuzzy clusters of all the categories, that is, we can say that two sets of category labels are similar if they have similar ? coefficients. This information can serve as mutual context information that helps the learning algorithm find optimal weights. Second, since the search space V c is constrained, the model is forced to initialize the category vectors and look for the optimal vectors inside the constrained space. This smart initialization contributes to situate vectors of sparse categorical information to a suboptimal location and efficiently trains them further, despite the lack of instances. Finally, since we only use a very small set of basis vectors, we reduce the number of weights dramatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We experiment on three datasets for different tasks: (1) Yelp 2013 dataset 2 <ref type="bibr" target="#b28">(Tang et al., 2015)</ref> for Review Sentiment Classification, (2) AAPR dataset 3 <ref type="bibr" target="#b30">(Yang et al., 2018)</ref>  General experimental settings are as follows. The dimensions of the word vectors are set to 300. We use pre-trained GloVe embeddings <ref type="bibr" target="#b21">(Pennington et al., 2014)</ref> to initialize our word vectors. We create UNK tokens by transforming tokens with frequency less than five into UNK. We handle unknown category labels by setting their corresponding vectors to zero. We tune the number of basis vectors d using a development set, first by sweeping across 2 to 30 with large intervals, and then by searching through the neighbors of the best configuration during the first sweep. Interestingly, d tends to be very small between values 2 to 4. We set the batch size to 32. We use stochastic gradient descent over shuffled mini-batches with the Adadelta update rule (Zeiler, 2012) with l 2 constraint of 3. We do early stopping using the accuracy of the development set. We perform 10fold cross-validation on the training set when the development set is not available. Dataset-specific settings are described in their corresponding sections.</p><p>We compare the performance of the following competing models: the base classifier BiLSTM with no customization, the five versions (i.e., bias, linear, attention, encoder, embedding) of Customized BiLSTM, and our proposed basiscustomized versions. We report the accuracy and the number of parameters of all models, and addi-  <ref type="table">Table 2</ref>: Accuracy, RMSE, and parameter values of competing models for all datasets. An asterisk (*) indicates customization methods first introduced in this paper. A dash (-) indicates the model is too big to be trained in an NVIDIA 1080 Ti GPU. Bold-face indicates that the performance of basis-customization is significantly better (p &lt; 0.05) than that of a simple customization. Values colored red are performance weaker than that of the BiLSTM model, thus customization hurts the performance in those cases.</p><p>tionally report the RMSE values for the sentiment classification task. We also compare with results from previous papers whenever available. Results are shown in <ref type="table">Table 2</ref>, and further discussions are reported in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Review sentiment classification</head><p>Review sentiment classification is a task of predicting the sentiment label (e.g., 1 to 5 stars) of a review text <ref type="bibr" target="#b19">(Pang et al., 2002)</ref>. We use users and products as categorical metadata. One main characteristic of the categorical information here is that both user and product can be cold-start entities (Amplayo et al., 2018a). Thus issues on sparseness may aggravate. We use 256 dimensions for the hidden states in the BiLSTM encoder and the context vector in the attention mechanism, and 64 dimensions for each of the user and product category vectors. The results in <ref type="table">Table 2</ref> show that when using Customized BiLSTM, customizing on the bias vector (i.e., Concatenated BiLSTM) performs the best compared to customizing on other parts of the model with lower dependencies, which is counterintuitive and contrary to previously reported results. Moreover, the performances of customizing on the linear transformation matrix and word embedding are weaker than that of the base BiL-STM model, while customizing on the encoder weights makes the model too big to be trained in our GPU. When using our proposed basis-customization method, we obtain a significant increase in performance on all levels of customization in almost all performance metrics. Overall, a BiLSTM basis-customized on the linear transformation matrix, the bias vector, and the encoder weights perform the best among the models. Finally, we reduce the number of parameters dramatically by at least half compared to the Customized BiLSTM, which enables the training of Basis-Customized BiLSTM on encoder weights.</p><p>In addition to the competing models above, we also report results from previous state-of-the-art sentiment classification models that use user and product information: (a) UPNN <ref type="bibr" target="#b28">(Tang et al., 2015)</ref> uses a CNN encoder and customizes on bias vectors and word embeddings; (b) UPDMN <ref type="bibr" target="#b6">(Dou, 2017)</ref> uses an LSTM encoder and customizes on memory vectors; (c) NSC <ref type="bibr" target="#b4">(Chen et al., 2016)</ref> uses a hierarchical LSTM encoder and customizes on attention mechanism; (d) HCSC (Amplayo et al., 2018a) uses a BiLSTM and a CNN as encoders and customizes on a cold-start aware attention mechanism (CSAA); (e) PMA <ref type="bibr" target="#b35">(Zhu and Yang, 2017)</ref> uses a hierarchical LSTM encoder and customizes on PMA, an attention mechanism guided by external features; (f) DUPMN <ref type="bibr" target="#b15">(Long et al., 2018)</ref> uses a hierarchical LSTM encoder and customizes on memory vectors; and (g) CMA <ref type="bibr" target="#b17">(Ma et al., 2017)</ref> uses a hierarchical attention-based encoder and customizes on user-and productspecific attention mechanism (CMA). The com-Models Acc RMSE UPNN <ref type="bibr" target="#b28">(Tang et al., 2015)</ref> CNN + word-cust + bias-cust 59.6 0.784 UPDMN <ref type="bibr" target="#b6">(Dou, 2017)</ref> LSTM + memory-cust 63.9 0.662 NSC <ref type="bibr" target="#b4">(Chen et al., 2016)</ref> LSTM + attention-cust 65.0 0.692 HCSC (Amplayo et al., 2018a) BiLSTM + CNN + attention-cust (CSAA) 65.7 0.660 PMA <ref type="bibr" target="#b35">(Zhu and Yang, 2017)</ref> HierLSTM + attention-cust (PMA) 65.8 0.668 DUPMN <ref type="bibr" target="#b15">(Long et al., 2018)</ref> HierLSTM + memory-cust 66.2 0.667 CMA <ref type="bibr" target="#b17">(Ma et al., 2017)</ref> HierAttention + attention-cust (CMA) 66.4 0.677</p><p>Our best models BiLSTM + encoder-basis-cust 66.1 0.665 BiLSTM + bias-basis-cust 66.9 0.654 BiLSTM + linear-basis-cust 67.1 0.662   <ref type="table" target="#tab_3">Table 3</ref> shows that our methods outperform previous models, even though (1) we only use a single BiLSTM encoder rather than more complicated ones (UPDMN and DUPMN use deep memory networks, NSC, PMA, and CMA use hierarchical encoders) and <ref type="formula">(2)</ref> we only customize on one part of the model rather than on multiple parts (UPNN customizes on bias vectors and word embeddings).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Paper acceptance classification</head><p>Paper acceptance classification is a task of predicting whether the paper in question is accepted or rejected <ref type="bibr" target="#b30">(Yang et al., 2018)</ref>. We use the authors 5 and the research area of the papers as categorical metadata. Both authors and research field information accept multiple labels per instance (e.g., multiple authors, multidisciplinary field), hence learning the category vector space properly is crucial to perform vector operations <ref type="bibr" target="#b18">(Mikolov et al., 2013)</ref>. We use 128 dimensions for both the hidden states in the BiLSTM encoder and the context vector in the attention mechanism and 32 dimensions for each of the categorical information. We use the paper abstract as the text. To handle multiple labels, we find that averaging the category vectors works well. The results in <ref type="table">Table 2</ref> show similar trends from the sentiment classification results. First, we obtain better performance when using Concatenated BiLSTM compared to when using Customized BiLSTM. Second, incorporating metadata information on the attention mechanism does not perform as well as previously reported. Third, when customizing on encoder weights and word embedding, the model parameters are too big to be trained on a commercial GPU. Finally, we see significant improvements in all levels of customization when using our proposed basis-customization method, except on the bias vectors where we obtain comparable results. Overall, a BiLSTM basiscustomized on the encoder weights, the attention pooling, and the word embedding perform the best among all the models. We also see at least 3.7x reduction of parameters when comparing Customized BiLSTM and Basis-Customized BiLSTM.</p><p>We also compare our results from previous literature <ref type="bibr" target="#b30">(Yang et al., 2018)</ref>, where they proposed papers to read. a modular and hierarchical CNN-based encoder (MHCNN), and used the full text (i.e., from the title and authors up to the conclusion section), instead of just the abstract, the author and the research area information. Results are reported in <ref type="table" target="#tab_4">Table 4</ref>, although full text and abstract results are not directly comparable since the original authors did not release the train/dev/test splits of their experiments. We instead re-run MHCNN using our settings and compare with our models. The results show that using either full text or abstract as input to LSTM produces similar results, thus using just the abstract can give us similar predictive bias when using the full text, at least in this dataset. Moreover, our best models (1) perform significantly better (p &lt; 0.5) than MHCNN when restricted to our settings, and (2) are competitive with the state-of-the-art, even though we use a simple BiLSTM encoder and only have access to the abstract, authors, and research area information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Political message type classification</head><p>Political message type classification is a task of predicting the type of information a message written by a politician, is conveying, among the following nine types: attack, constituency, information, media, mobilization, personal, policy, support, and others. Two characteristics of this dataset different from others are (a) that it has four kinds of categorical information: the audience (national or constituency), bias (neutral or partisan), politician, and the source (Twitter or Facebook) information, and (b) that the category types of three categories are not diverse as they only have binary category labels. Since all of these categories may not give useful information biases to the classifier, models should be able to select which categories are informative or not. We use 64 dimensions for the hidden states in the BiLSTM encoder and the context vector in the attention mechanism, and 16 dimensions for the category vectors of each of the categorical information.</p><p>The results in <ref type="table">Table 2</ref> also show similar trends from the previous task, but since the dataset is smaller, we can compare the performance of the model when customizing on encoder weights. We show that while Customized BiLSTM on linear transformation matrix and encoder weights show weaker performance than the base BiLSTM model, Basis-Customized BiLSTM on the same  levels show significantly improved performance, where Basis-Customized BiLSTM on linear transformation matrix performs the best among the competing models. The parameters also decreased dramatically, especially on encoder weights and on word embedding where we see at least 100x difference in parameter size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Semantics of basis attention vectors</head><p>We investigate at how basis vectors understand word-level semantics through the lens of the attention vectors they create. Previous models either combine user/product information into a single attention vector <ref type="bibr" target="#b4">(Chen et al., 2016)</ref> or entirely separate them into distinct user and product attention vectors <ref type="bibr" target="#b0">(Amplayo et al., 2018a)</ref>. On the other hand, our model creates a single attention vector, but through the k basis attention vectors, which are vectors containing fuzzy semantics among users and products. <ref type="figure" target="#fig_3">Figure 3</ref> shows two examples of six attention vectors regarding a single text using the following: (1) the original user, product pair (u, p), (2-3) a sampled user/product paired with the original product/user (u , p) and (u, p ), and (4-6) the basis vectors, in the Yelp 2013 dataset.</p><p>We can see in the first example that the first basis vector focuses on "cheap" while the third basis vector focuses on "delicious". An interesting output is by user u, such that it wants cheaper food in product p yet cares more about the taste in product p .</p><p>Abstract Several tasks in argumentation mining and debating, question-answering, and natural language inference involve classifying a sequence in the context of another sequence (referred as bi-sequence classification). For several single sequence classification tasks, the current state-of-the-art approaches are based on recurrent and convolutional neural networks. On the other hand, for bi-sequence classification problems, there is not much understanding as to the best deep learning architecture. In this paper, we attempt to get an understanding of this category of problems by extensive empirical evaluation of 19 different deep learning architectures (specifically on different ways of handling context) for various problems originating in natural language processing like debating, textual entailment and question-answering. Following the empirical evaluation, we offer our insights and conclusions regarding the architectures we have considered. We also establish the first deep learning baselines for three argumentation mining tasks.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Document-level customized dependencies</head><p>Previous literature only focused on the analysis <ref type="bibr" target="#b0">(Amplayo et al., 2018a)</ref> and case studies <ref type="bibr" target="#b4">(Chen et al., 2016)</ref> of word-level customized dependencies, usually through attention vectors. In this paper, we additionally investigate the documentlevel customized dependencies, i.e., how our basis-customization changes the document-level semantics when a category is different. <ref type="table" target="#tab_7">Table 5</ref> shows two examples, one from the AAPR dataset and one from the Political Media dataset, with a variable category research area and political bias, respectively. In the first example, the abstract refers to a study on bi-sequence classification problem, a task mainly studied in the natural language processing domain, and thus gets classified as accepted when the research area category is cs.CL. The model also classifies the paper as accepted when the research area is cs.IR because the two areas are related. However, when the research area is changed to an unrelated area like cs.CR, the paper gets rejected. In the second example, the classifier predicts that when a politician with a neutral bias posts a Christmas greeting and mentions people who work on holidays, he is conveying a personal message. However, when the politician is biased towards a political party, the classifier thinks that the message is to offer sup-port to those workers who are unable to be with their families.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Learning strategy of basis-customized vectors</head><p>We argue that since the basis vectors B limit the search space into a constrained vector space V c , then finding the optimal values of the basiscustomized vectors is faster. We show in <ref type="figure" target="#fig_4">Figure 4</ref> the difference between the category vector space of Customized BiLSTM and of Basis-Customized BiLSTM. We see that the vector space of Customized BiLSTM looks random, with very few noticeable clusters, even when we iterate with four epochs. On the other hand, the basis-customized vector space starts as a cluster of one continuous spiral line, then starts to break down into smaller clusters. Multiple clusters of vectors in the vector space are clearly seen when epoch is 4. Therefore, using the basis vectors makes optimization more efficient by following the above learning strategy of starting from one cluster and dividing into smaller coherent clusters. This can also be shown in the visualization of the ? coefficients also shown in the figure, where the coefficient values that are clumped together gradually spread out to their optimal values. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Performance on sparse conditions</head><p>We look at the performance of three models, BiLSTM, Customized BiLSTM, and Basis-Customized BiLSTM, per review frequency of user or product. <ref type="figure" target="#fig_5">Figure 5</ref> shows plots of the accuracy of the models over different user review frequency and product review frequency on the Yelp 2013 dataset. We observe that naive customization drops the performance of the BiLSTM model as the frequency of user/product review decreases. This means that the model is heavily reliant on large amounts of data for optimization. On the other hand, since basis customization can learn the optimal weights of category vectors more smartly, it improves the performance of the model across all ranges of review frequency.</p><p>We finally examine the performance of our models when data contains cold-start entities (i.e., users/products may have zero or very few reviews) using the Sparse80, subset of the Yelp 2013 dataset provided in <ref type="bibr" target="#b0">(Amplayo et al., 2018a)</ref>. We compare our models with three competing models: NSC <ref type="bibr" target="#b4">(Chen et al., 2016)</ref>, which uses a hierarchical LSTM encoder coupled with customization on the attention mechanism, BiLSTM+CSAA (Amplayo et al., 2018a), which uses a BiLSTM encoder with customization on a cold-start aware attention (CSAA) mechanism, and HCSC (Amplayo et al., 2018a), which is a combination of CNN and BiLSTM encoder with customization on CSAA.</p><p>Results are reported in <ref type="table" target="#tab_9">Table 6</ref>, which provide us two observations. First, the BiLSTM model customized on the linear transformation matrix, which performs the best on the original Yelp 2013 dataset (see <ref type="table" target="#tab_3">Table 3</ref>), obtains a very sharp decrease in performance. We posit that this is because basis customization is not able to handle zero-shot cold-start entities, which are amplified in the Yelp 2013 Sparse80 dataset. We leave extensions of basis for zero-shot or cold-start, studied actively in machine learning <ref type="bibr" target="#b29">(Wang et al., 2019)</ref> and recommendation domains <ref type="bibr" target="#b27">(Sun et al., 2012)</ref> respectively. Inspired by CSAA (Amplayo et al., 2018a), using similar review texts for inferring the coldstart user (or product), we expect to infer meta context, similarly based on similar meta context, which may mitigate the zero-shot cold-start problem. Second, despite having no zero-shot learning capabilities, Basis-Customized BiLSTM on the attention mechanism performs competitively with HCSC and performs better than BiLSTM+CSAA, which is Customized BiLSTM on attention mechanism with cold-start awareness.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We presented a new study on customized text classification, a task where we are given, aside from the text, its categorical metadata information, to predict the label of the text, customized by the categories available. The issue at hand is that these categorical metadata information are hardly understandable and thus difficult to use by neural machines. This, therefore, makes neural-based models hard to train and optimize to find a proper categorical metadata representation. This issue is very critical, in such a way that a simple concatenation of these categorical information provides better performance than existing popular neuralbased methods. We proposed to solve this problem by using basis vectors to customize parts of a classification model such as the attention mechanism and the weight matrices in the hidden layers. Our results showed that customizing the weights using the basis vectors boosts the performance of a basic BiLSTM model, and also effectively outperforms the simple yet robust concatenation methods. We share the code and datasets used in our experiments here: https://github. com/zizi1532/BasisCustomize.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>of Customized BiLSTMAs explained in the previous section, Customized BiLSTM should perform better than Concatenated BiLSTM. However, that is only if the optimization of category-specific weights operates properly for machine usage. However, training the model to The full architecture of the proposed model, basis-customizing parts of the BiLSTM model:(1) the bias vector, (2) the linear transformation matrix, (3) the attention context vector, (4) the BiLSTM encoder weights, and (5) the word embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>the linear transformation matrix v = W (c) of the logistic regression classifier in Equation 8, (3) the context vector v = x of the attention mechanism in Equation 5, (4) the BiLSTM weights v = W (e) in Equations 2 and 3, and (5) the nonlinear transformation matrix v = W (w) on the residual vector in Equation 10 to modify the word embeddings. These correspond to the five versions of Customized BiLSTM discussed above.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Examples of attention vectors from three different pairs of users and products (u , p), (u, p ), (u, p), and from the basis vectors. Numbers in parenthesis are the ? i coefficient of the pair (u, p) with respect to basis b i .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>TSNE Visualization of the category vectors of Customized BiLSTM (first row) and Basis-Customized BiLSTM (middle row), and the ? coefficients of the latter model (last row), when epoch is equal to 1, 2, 4, and when training has finished (left to right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Accuracy per user/product review frequency on Yelp 2013 dataset. The review frequency value f represents the frequencies in the range [f, f + 10), except when f = 100, where it represents the frequencies in the range [f, inf).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The datasets, the split sizes(train, dev, test), and the available categories and their properties. Numbers inside the parenthesis are the number of unique category labels.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>for Paper Acceptance 2 http://ir.hit.edu.cn/~dytang 3 https://github.com/lancopku/AAPR Classification, and (3) PolMed dataset 4 for Political Message Type Classification. Statistics, categories, and properties of the datasets are reported in Table 1. Details about the datasets are discussed in the next sections.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison of previous and our best models in the Yelp 2013 dataset. Our best models perform better, even though we only use a single BiLSTM encoder.</figDesc><table><row><cell>Models</cell><cell>Accuracy</cell></row><row><cell cols="2">using full text (Yang et al., 2018)</cell></row><row><cell>LSTM</cell><cell>60.5</cell></row><row><cell>MHCNN</cell><cell>67.7</cell></row><row><cell cols="2">using abstract and categories (our setting)</cell></row><row><cell>LSTM</cell><cell>60.6</cell></row><row><cell>MHCNN</cell><cell>63.7</cell></row><row><cell>BiLSTM</cell><cell>61.7</cell></row><row><cell>BiLSTM+word-basis-cust</cell><cell>65.8</cell></row><row><cell>BiLSTM+attention-basis-cust</cell><cell>65.9</cell></row><row><cell>BiLSTM+encoder-basis-cust</cell><cell>66.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Performance comparison of models using</cell></row><row><cell>full texts and our implemented models using paper</cell></row><row><cell>abstracts (and authors and research areas as cate-</cell></row><row><cell>gories for basis-customized models) as inputs in</cell></row><row><cell>the AAPR dataset.</cell></row><row><cell>parison in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Example texts from the AAPR dataset (upper) and Political Media dataset (lower) with a variable category label (research field and political bias) that changes the classification label.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Performance comparison of competing models in the Yelp 2013 Sparse80 dataset.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We limit our scope to texts with categorical metadata information (e.g., product reviews, news articles, tweets, etc.), which covers most of the texts in the Web. Texts without metadata can use predicted categorical information, such as topics from a topic model, which are commonly used<ref type="bibr" target="#b33">(Zhao et al., 2017;</ref><ref type="bibr" target="#b5">Chou et al., 2017)</ref>. However, since the prediction may be incorrect, performance gains cannot be guaranteed. We leave the investigation of this area in future work.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://www.figure-eight.com/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">In reviewing scenarios, the use of authors as additional information is discouraged for fairness. We show how powerful these features are for prediction when properly modeled, which is useful for other scenarios, e.g., deciding which arXiv</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cold-start aware user and product attention for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinald</forename><surname>Kim Amplayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihyeok</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2535" to="2544" />
		</imprint>
	</monogr>
	<note>Sua Sung, and Seung-won Hwang. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Translations as additional contexts for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinald</forename><surname>Kim Amplayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungjae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyoung</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seung-Won</forename><surname>Hwang</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2018/550</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018<address><addrLine>Stockholm, Sweden.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07-13" />
			<biblScope unit="page" from="3955" to="3961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations, ICLR&apos;15. ICLR</title>
		<meeting>the 3rd International Conference on Learning Representations, ICLR&apos;15. ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A general framework for personalized text classification and annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Baruzzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonina</forename><surname>Dattolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nirmala</forename><surname>Pudota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tasso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Adaptation and Personalization for Web 2.0, AP WEB 2.0@UMAP</title>
		<meeting>the Workshop on Adaptation and Personalization for Web 2.0, AP WEB 2.0@UMAP<address><addrLine>Trento, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-06-22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural sentiment classification with user and product attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cunchao</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1171</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1650" to="1659" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Context-aware sentiment propagation using LDA topic modeling on chinese conceptnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Hao</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Tzong-Han Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane Yung-Jen</forename><surname>Hsu</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00500-016-2273-0</idno>
	</analytic>
	<monogr>
		<title level="j">Soft Comput</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2911" to="2921" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Capturing user and product information for document level sentiment analysis with deep memory network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi-Yi</forename><surname>Dou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1054</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="521" to="526" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Twitter sentiment classification using distant supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Go</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richa</forename><surname>Bhayani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">1</biblScope>
			<pubPlace>Stanford</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">CS224N Project Report</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An unsupervised approach to automatic classification of scientific literature utilizing bibliographic metadata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Joorabchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdulhussain</forename><forename type="middle">E</forename><surname>Mahdi</surname></persName>
		</author>
		<idno type="DOI">10.1177/0165551511417785</idno>
	</analytic>
	<monogr>
		<title level="j">J. Information Science</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="499" to="514" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1181</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Addressing cold-start problem in recommendation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><forename type="middle">Nhat</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thuc</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trong Duc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Duc</forename><surname>Duong</surname></persName>
		</author>
		<idno type="DOI">10.1145/1352793.1352837</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Conference on Ubiquitous Information Management and Communication</title>
		<meeting>the 2nd International Conference on Ubiquitous Information Management and Communication<address><addrLine>Suwon, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-01-31" />
			<biblScope unit="page" from="208" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">One-shot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2006.79</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="594" to="611" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning question classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2002: The 19th International Conference on Computational Linguistics</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for document modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muyun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1106</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="899" to="907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dual memory network model for biased product review classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfei</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Ren</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</title>
		<meeting>the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="140" to="148" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1166</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cascading multiway attentions for document-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="634" to="643" />
		</imprint>
	</monogr>
	<note>Asian Federation of Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1301.3781</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Thumbs up? sentiment classification using machine learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivakumar</forename><surname>Vaithyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2002 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The presentation of self in virtual life: Characteristics of personal home pages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizi</forename><surname>Papacharissi</surname></persName>
		</author>
		<idno type="DOI">10.1177/107769900207900307</idno>
	</analytic>
	<monogr>
		<title level="j">Journalism &amp; Mass Communication Quarterly</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="643" to="660" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1202</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Conference on Learning Representations, ICLR&apos;16. ICLR</title>
		<meeting>the 4th International Conference on Learning Representations, ICLR&apos;16. ICLR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The author-topic model for authors and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Rosen-Zvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padhraic</forename><surname>Smyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI &apos;04, Proceedings of the 20th Conference in Uncertainty in Artificial Intelligence</title>
		<meeting><address><addrLine>Banff, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-07-07" />
			<biblScope unit="page" from="487" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Machine learning in automated text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
		<idno type="DOI">10.1145/505282.505283</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Survey of cold-start problem in collaborative filtering recommender system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Ting</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-Hai</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer and Modernization</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="59" to="63" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning semantic representations of users and products for document level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P15-1098</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1014" to="1023" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A survey of zero-shot learning: Settings, methods, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Miao</surname></persName>
		</author>
		<idno type="DOI">10.1145/3293318</idno>
		<idno>13:1-13:37</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Automatic academic paper rating based on modularized hierarchical convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="496" to="502" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1174</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">ADADELTA: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno>abs/1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Topic-aware deep compositional models for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kezhi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kezhi</forename><surname>Mao</surname></persName>
		</author>
		<idno type="DOI">10.1109/TASLP.2016.2632521</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech and Lang. Proc</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="248" to="260" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Text classification improved by integrating bidirectional lstm with two-dimensional max pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suncong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3485" to="3495" />
		</imprint>
	</monogr>
	<note>The COLING 2016 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Parallel multi-feature attention on neural sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujiu</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3155133.3155193</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Symposium on Information and Communication Technology, Nha Trang City, Viet Nam</title>
		<meeting>the Eighth International Symposium on Information and Communication Technology, Nha Trang City, Viet Nam</meeting>
		<imprint>
			<date type="published" when="2017-12-07" />
			<biblScope unit="page" from="181" to="188" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
