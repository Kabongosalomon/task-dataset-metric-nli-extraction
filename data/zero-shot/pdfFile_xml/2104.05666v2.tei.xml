<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">View-Guided Point Cloud Completion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuancheng</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Feng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqing</forename><surname>Zou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Wan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xibin</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Guo</surname></persName>
							<email>yandong.guo@live.com</email>
							<affiliation key="aff2">
								<orgName type="institution">OPPO Research Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Gao</surname></persName>
							<email>gaoyue@tsinghua.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">THUICBS</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
								<address>
									<addrLine>3 Huawei Technologies Canada Co</addrLine>
									<settlement>Ltd</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bnrist</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kliss</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">View-Guided Point Cloud Completion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a view-guided solution for the task of point cloud completion. Unlike most existing methods directly inferring the missing points using shape priors, we address this task by introducing ViPC (view-guided point cloud completion) that takes the missing crucial global structure information from an extra single-view image. By leveraging a framework that sequentially performs effective cross-modality and cross-level fusions, our method achieves significantly superior results over typical existing solutions on a new large-scale dataset we collect for the view-guided point cloud completion task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Point cloud has attracted increasing research interest due to its wide range of applications in various fields such as auto-driving <ref type="bibr" target="#b16">[16]</ref>, robotics <ref type="bibr" target="#b27">[27]</ref>, geography <ref type="bibr" target="#b29">[29]</ref>, phenomics <ref type="bibr" target="#b19">[19]</ref>, and archaeology <ref type="bibr" target="#b4">[4]</ref>. In practice, the point cloud's data quality directly acquired by the depth scanning devices can be affected by many factors such as occlusions between objects and low scanning precision, which may lead to the poor-quality point cloud, e.g., incomplete, sparse, and noisy point cloud.</p><p>Existing methods, mainly including point cloud completion <ref type="bibr" target="#b42">[42]</ref>, denoising <ref type="bibr" target="#b5">[5]</ref>, and super-resolution (up-sampling) <ref type="bibr" target="#b38">[38]</ref>, have been proposed for the task of point cloud enhancement. Early methods generated enhanced point cloud by mainly using shape prior information <ref type="bibr" target="#b17">[17]</ref> or handcrafted geometric regularities <ref type="bibr" target="#b35">[35]</ref>. In recent years, datadriven methods, especially deep learning techniques like PointNet <ref type="bibr" target="#b28">[28]</ref> and DGCNN <ref type="bibr" target="#b37">[37]</ref>, have made significant progress on this problem. Compared to traditional methods, these deep learning based methods have demonstrated significant advantages in processing objects with irregular structure and geometry. In this paper, we focus on the following point completion task: the input point cloud is incomplete but with limited noise, while our method outputs a complete point cloud. Studying this problem addresses a common problem in realworld 3D data acquisition where a 3D scanner with a RGB camera is occluded by other objects in the environment. The most recent solutions to this problem are data-driven, leveraging an encoder-decoder architecture <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b39">39]</ref>. In those methods, an encoder transfers the incomplete input point cloud into the feature space, and then a decoder reconstructs a complete point cloud by transferring the features back to Euclidean space. The whole network works as a parameterized model by learning a mapping between the two latent spaces of incomplete and complete point cloud. In the cases where there is a large degree of incompleteness in the input point cloud, learning this mapping with only the single-modality point cloud data is challenging because of the following factors: 1) there is a great uncertainty in inferring the missing points due to the limited amount of information available, 2) point cloud is of an unstructured data, together with inherent sparseness, it is difficult to determine whether a blank 3D space is caused by inherent spareness or incompleteness.</p><p>In this paper, we seek a more applicable solution to the point cloud completion task. Specifically, we address the task with the help of the image modality and propose a view-guided point completion framework (ViPC) as illustrated in <ref type="figure">Figure.</ref> 1. This setting of sensor fusion is increasingly common as the hardware cost decreases (e.g., the In-tel Real Sense D455 and Microsoft Kinect devices). The key challenge of solving this problem is how to effectively fuse the information of pose and local details provided by the partial point cloud and the global structure information provided by the single-view image. This is not trivial because it involves a two-dimensional challenge: "crossmodality" (the information is from both image and point cloud modalities) and "cross-level" (local details and global structure are information from different levels). We address the problem by a three-stage framework which first address the cross-modality challenge and then the crosslevel challenge. Specifically, the cross-modality challenge is addressed by reconstructing a coarse point cloud from the single-view image and transferring all the information required by the completion to the identical point cloud domain. The cross-level challenge is addressed by a differential refinement strategy empowered by a network called "Dynamic Offset Predictor" which can deferentially refine the points in a coarse point cloud: performing a light refinement for low-quality points while a heavy refinement for high-quality points.</p><p>To better investigate the problem, we have built a largescale dataset called ShapeNet-ViPC on existing ShapeNet dataset <ref type="bibr" target="#b6">[6]</ref>. Our dataset includes 38,328 objects from 13 categories. Each object has 24 sets of ground-truth data consisting of two incomplete point clouds produced under two typical data acquisition scenarios, a view-aligned image and a complete ground-truth point cloud. Extensive evaluations on ShapeNet-ViPC demonstrate that the proposed approach can achieve significantly superior results than existing stateof-the-art approaches.</p><p>In summary, the main contributions of our methods are threefold:</p><p>1. We propose a new solution for point cloud completion, in which an extra single-view image explicitly provides the crucial global structural prior information for completion.</p><p>2. We design a new general deep network for point cloud refinement which can deferentially refine the points in a point cloud.</p><p>3. We build a large-scale dataset for the point cloud completion task on the ShapeNet dataset. This dataset simulates point cloud defects caused by various kinds of occlusions. It could be used as a benchmark for future research of point cloud completion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Existing point cloud completion methods can be generally classified into three types of approaches: geometry-based, alignment-based and learning-based.</p><p>Geometry-based Methods. Geometry-based methods predict the invisible shape part from the observed shape part directly by prior geometric assumptions <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b14">14]</ref>. More specifically, some methods fill the surface holes locally by generating smooth interpolations <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b34">34]</ref>, such as Laplacian smoothing <ref type="bibr" target="#b23">[23]</ref> and Poisson surface reconstruction <ref type="bibr" target="#b17">[17]</ref>. Other methods detect the regularities in model structures and repeat them to predict missing data based on the identified symmetry axes <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b35">35]</ref>,?hese methods infer the missing data directly from the observed region and show impressive results. However, they need the hand-crafted geometric regularities that are predefined for specific kinds of models and only applied to models with a small degree of incompleteness.</p><p>Alignment-based Methods. Alignment-based methods retrieve identical models similar to the target object in a shape database, then align the input with temple models and complete the missing region. Some methods retrieve 3D shapes directly, such as the whole model <ref type="bibr" target="#b25">[25]</ref> or part of them <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b18">18]</ref>. Other methods use synthesized models after deformation <ref type="bibr" target="#b30">[30]</ref> or non-3D geometric primitives such as planes and quadrics <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b41">41]</ref> in place of 3D shapes in the database. These methods are applicable to many different types of models and can be applied to varying degrees of incompleteness, but they require expensive cost during inference optimization and database construction; also they are sensitive to noise.</p><p>Learning-based Methods. Learning-based methods construct a parameterized model to learn a mapping between the two feature spaces of the incomplete and complete point cloud of a shape. Most of them are encoderdecoder based neural networks. As for shape representation, most existing models use voxels to represent a shape <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b13">13]</ref>, which are intuitive and convenient for 3D convolution. In order to preserve more geometric information (i.e., local geometric details) in the completed point cloud, several models perform the operation on point sets directly <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b33">33]</ref>. Since both points or voxels are a mono-modality input, it is difficult to infer an accurate mapping between an incomplete point cloud with a large-scale incompleteness and a complete point cloud. Therefore, those methods may perform well only on specific categories of objects or the shapes with a small-scale incompleteness. The work leveraging auxiliary data to supplement the missing information of the input point cloud for the enhancement task has rarely been studied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>Problem Definition. The proposed view-guided completion solution is based on an assumption that the input image contains the necessary structural information of the missing shape part. Our goal is to recover a 3D shape S consisting of two parts, i.e., S = {? 0 , ?S}, where? 0 is a shape part that is close to the input partial shape S 0 (i.e., the difference between? 0 and S 0 is limited), and ?S is the unknown missing shape part. Formally, we denote M(S) as the representation of shape S in modality M, i.e. M = P for point cloud and M = I for image. The input of the task could be formulated as:</p><p>? The partial point cloud P(S 0 ).</p><p>? The single-view image I(S I ), where S I refers to the observed shape from the view of image.</p><p>Pipeline. The proposed three-stage framework is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. The first stage is used to address the crossmodality fusion problem. It maps I to a coarse representation of point cloud in P. The second stage generates a coarse point cloud and the last stage enhances it and produce a higher-quality completed point cloud. These two stages work together to perform the cross-level fusion.</p><p>Specifically, the first stage termed as Modality Transfer maps I to a coarse representation of point cloud in P and then aligns the reconstructed point cloud to the input partial shape S 0 in 3D space. Then the second stage termed as Part Filter generates a coarse point cloud from the two aligned point cloud. Additionally, this stage also makes a distinction between the points which are mainly from the input partial point cloud and the other points which are mainly reconstructed from input image I(S I ). We term the former as the Fine Part and the latter as the Coarse Part. In general, the Fine part has much higher shape quality than the Coarse part and needs only a light refinement. Lastly, the third stage termed as Part Refinement takes as input both the Coarse and Fine parts and produces a completed higherquality point cloud. In this stage, we mainly refine the shape of the Coarse but use the Fine part as the constraint for the refinement. This can achieve better results than a global refinement which refines all the points without any constraint  as confirmed by the ablation study in the experimental section. We achieve this by a novel neural network called "Dynamic Offset Predictor". Next, we detail the above three stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Modality Transfer</head><p>It would be challenging to directly reconstruct a highquality point cloud which preserves rich local details from the image I(S I ). Thus, the transferred point cloud is expected to work as an initialized shape mainly used for structure guidance. Let the point cloud reconstructed from the image I(S I ) as P r (S I ). A light-weight point cloud reconstruction network with an encoder-decoder architecture shown in <ref type="figure" target="#fig_2">Figure 3</ref> is employed. The encoder maps the input image into a latent space vector, and the decoder outputs an N r ? 3 matrix, each row of which represents the Cartesian coordinates of a point. In our implementation, the encoder comprises a series of convolutional layers with ReLU activation, and outputs feature map in 7 ? 7 ? 512 as the latent space vector. The decoder applies a series of deconvolutional layers and flatten the output to generate N r point coordinates. Feature maps from each layer of the encoder are also preserved as additional guidance for the following processes in this stage. After reconstructing P r (S I ) from the single-view image, we align it with the input partial point cloud.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Part Filter</head><p>The reconstructed P r (S I ) can roughly describe the view-observed shape S I , which is assumed to contain the major information of the missing shape part ?S. In this stage, we firstly merge P(S 0 ) and P r (S I ) and extract a subset (N c points) of it as a coarse representation of shape S 0 ? S I . Since point density may differ in P(S 0 ) and the reconstructed P r (S I ) and the concatenation leads to redundant points in the overlap space between S 0 and S I . We would like this coarse point cloud with N c points to be uniformly dense and preserve the global structure but also as many local details of the shape as possible. Specifically, we use the farthest point sampling (FPS) <ref type="bibr" target="#b22">[22]</ref>) on P c (S 0 ? S I ) to achieve this goal.</p><p>After that, we identify the points that do not need heavy refinement as the Fine part and leave the remaining points as the Coarse part, i.e., dividing the coarse point cloud with N c points into the Coarse part (N m points) and Fine part (N c ? N m points). To search N c ? N m points that are close to the points in P(S 0 ) for the Fine part, we construct the correspondence between points in P(S 0 ) and P c (S 0 ? S I ) by Chamfer Distance (CD) <ref type="bibr" target="#b11">[11]</ref>. For each point p in P c (S 0 ? S I ), we calculate its distance to the closest point in P(S 0 ), i.e. d(p) = min q?P(S0) p ? q 2 2 . If the distance d(p) is less than an adaptive threshold d thr , we select it as a candidate point in the the Fine part. In the implementation, the threshold d thr can adapt to the density varying of point cloud. Technically, we randomly divide the points from P c (S 0 ? S I ) into two subsets, and calculate the closest distance d(p) of all points between the two subsets. Then the average distance value could be an estimation of the point cloud density and it serves as the threshold d thr . 3D Guidance. In recent learning-based completion methods, it has been verified that completion based on the existing shape and prior knowledge is a feasible way. For example, the symmetry of airplane could help us to complete the missing wing in one side. Thus, we extract global features by applying the common encoder PointNet <ref type="bibr" target="#b28">[28]</ref> on the partial point cloud P(S 0 ) and the reconstructed P r (S I ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Part Refinement</head><p>2D Guidance. The single-view image I(S I ) serves the  guidance for the recovery of both the structure and geometrical details of the Coarse part. During the stage of modality transfer, the feature of the image I(S I ) has been extracted into feature maps with different sizes, e.g. 56 ? 56, 28 ? 28, 14 ? 14, 7 ? 7. Inspired by the perceptual feature pooling in Pixel2Mesh <ref type="bibr" target="#b36">[36]</ref>, for each point, we search the features in the image feature maps corresponding to each point in the coarse point cloud using the coordinates and camera parameters. These features are stacked as f pixel for a kind of guidance feature. In addition, to increase variations among the local points and prevent predicting the same offset for different points, inspired by FoldingNet <ref type="bibr" target="#b40">[40]</ref>, we generate a 2D-grid and reshape to a feature vector as f grid to boost slight disturbance.</p><p>Dynamic Offset Predictor. Unlike the previous foldingbased or tree structure methods, Dynamic Offset Predictor predicts the spatial offset of each point towards its current position rather than directly predicting the coordinates, which simplifies the regression. The data flow of Dynamic Offset Predictor shown in <ref type="figure" target="#fig_4">Figure 4</ref> is as follows: 1. The f f usion is repeated N c times and fed into a series of 1D convolutional layers to output the hidden embeddings f point .</p><p>2. The f point is tiled R times for point movements and upsampling. 3. The 2D convolutional layers with kernel size 1 ? R are applied to perceive the local information of each point with R offsets. 4. The offset vector in R ? N c ? 3 is predicted by the 1D convolutional layers. 5. A protective mask in 1 ? (N c ? N m ) ? 3 with a small offset value is generated to cover the offset vector, aiming to limit the movement of Fine Part's points. The coordinates of P coarse are then stacked R times (R ? N c ) and added with corresponding offsets as the completed point cloud. Therefore, Part Refinement with Dynamic Offset Predictor refines and up-samples P coarse without changing the Fine Part's detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Loss Function</head><p>The loss function measures the difference between dense point cloud and ground truth. Since the point cloud are unordered data, the loss function must be permutation invariant, Chamfer Distance (CD) and Earth Mover's Distance (EMD) is considered in this work. Given two subsets P ? R 3 and Q ? R 3 , Chamfer distance calculates the average closest point distance between P and Q. We use the symmetric version of CD as formulated in <ref type="formula">(1)</ref>, where the first term forces the output point cloud to move close to ground truth, and the second term ensures that the output point cloud covers the ground truth point cloud.</p><formula xml:id="formula_0">L CD = 1 |P | p?P min q?Q p?q 2 2 + 1 |Q| q?Q min p?P p?q 2 2 (1)</formula><p>The Earth Mover's Distance (EMD) is an algorithm to evaluate the dissimilarity between two multi-dimensional distributions. Let ? : P ? Q be a bi-jection between two point clouds, which finds the minimal average distance between two point sets. In practice, searching the optimal ? is computationally expensive, we thereby use an iterative (1 + ) approximation scheme as <ref type="bibr" target="#b2">[2]</ref>.</p><formula xml:id="formula_1">L EM D = min ?:P ?Q 1 |P | p?P p ? ?(p) 2 2 (2)</formula><p>We combine these two distance with L = ?L CD +?L EM D as the loss function, where ?, ? are the trade-off hyperparameters. We use ? = 1 and ? = 0.0001 as a default value in the training stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">ShapeNet-ViPC</head><p>To simulate the defects related to the task of ViPC and evaluate the performance of the proposed approach, we build a new dataset called ShapeNet-ViPC based on the ShapeNetRendering <ref type="bibr" target="#b6">[6]</ref>. It contains 38,328 objects from 13 categories, i.e., airplane, bench, cabinet, car, chair, monitor, lamp, speaker, firearm, sofa, table, cellphone, watercraft. For each object, we generate two types incomplete point cloud (with or without noise) under 24 view points, as illustrated in <ref type="figure">Figure 5</ref>. The 24 view points follow the same view point setting as ShapeNetRendering <ref type="bibr" target="#b6">[6]</ref> (this setting is also used in 3D-R2N2 <ref type="bibr" target="#b8">[8]</ref>, PointSetGeneration <ref type="bibr" target="#b11">[11]</ref> and Pixel2Mesh <ref type="bibr" target="#b36">[36]</ref>). For each set we uniformly sample 2,048 points from the mesh surface of a target shape under the corresponding view point setting as the ground-truth complete point cloud. Specifically, each 3D shape is normalized into the bounding sphere with radius of 1, and rotated to the pose corresponding to a specific view point lastly. For the image data, we use the same 24 rendered views as ShapeNetRendering. ShapeNet-ViPC contains 38, 328 ? 24 = 919, 872 Scanner Scanner <ref type="figure">Figure 5</ref>. Illustration of two typical types of point cloud acquisition scenarios. Top: the target shape is occluded by other objects in the environment as well as a part of itself (self-occlusion); bottom: besides of self-and inter-object occlusion, the locations of the acquired points are disturbed because of device noise. sets of training data in total. Each set contains one groundtruth complete point cloud, two incomplete (partial) point clouds, and an image view. In this paper, we use 31,650 objects (759,600 sets) of eight categories for all experiments, 80% for training and 20% for test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self occlusion + Inter-object occlusion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self occlusion + Inter-object occlusion + Noise</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation details and evaluation metrics</head><p>In our implementation, the size of input image is 224 ? 224, from which we reconstruct a point cloud with N m = 784 points. The partial input contains 2,048 points. We sample a coarse point cloud with N c = 1024 points by FPS <ref type="bibr" target="#b22">[22]</ref> from the combination of the input partial point cloud and the reconstructed point cloud (i.e., P c (S 0 ? S I )). The output complete cloud contains 2,048 points (i.e.,R = 2). The network for modality transfer is pre-trained with batch size of 64 and learning rate of 1e-4 for 100 epochs. The Part Refinement network is trained with batch size of 1 and learning rate 1e-6 for 200 epochs. Category-specific parameters are trained for each object category. To quantify the completion performance, we use both the Chamfer Distance (CD) <ref type="bibr" target="#b32">[32]</ref> and F-Score <ref type="bibr" target="#b32">[32]</ref> as the quantitative evaluation metrics. Results with lower CD value and/or higher F-Score correspond to better completion quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Comparisons</head><p>We compare our method with several state-of-the-art methods on the task of point cloud completion, including AtlasNet <ref type="bibr" target="#b12">[12]</ref>, FoldingNet <ref type="bibr" target="#b40">[40]</ref>, point completion network (PCN) <ref type="bibr" target="#b42">[42]</ref> and TopNet <ref type="bibr" target="#b33">[33]</ref>. AtlasNet recovers a complete point cloud by estimating a collection of parametric surface elements. FoldingNet, 2D-grid based auto-encoder, is a pioneer grid based method for point cloud completion. PCN is an encoder-decoder framework which completes a partial input point cloud with a typical coarse-to-fine scheme. Top-Net completes an imperfect point cloud by a tree structure network. All the above baseline methods take as input only a partial point cloud. <ref type="table">Avg  Airplane  Cabinet  Car  Chair  Lamp  Sofa  Table  Watercraft</ref> AtlasNet <ref type="bibr">[</ref> Quantitative results. We normalize the output point clouds produced by the comparison methods and calculate the CD and F-Score on the 2,048 points of each shape. The results on each categories and the average are summarized in <ref type="table" target="#tab_2">Tables 1 and 2</ref>. It is found that the proposed method consistently outperforms other methods with a significant margin on all the eight categories on both CD and F-Score metrics. Besides, our method demonstrates more advantages on the categories of airplane, watercraft, lamp, and car.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean Chamfer Distance per point</head><p>Qualitative results. We also visualize the results produced by the comparison methods for a more comprehensive evaluation. Results on the representative examples from the eight categories are shown in <ref type="figure">Figure 6</ref>. It is easy to observe that the completed point clouds produced by Fold-ingNet are relatively messy. The generated point clouds do not show clear structures on some shape parts, e.g., the wings of the airplane, the chair legs, the table legs. PCN and AtlasNet produce improved qualitative results compared to FoldingNet overall. However, local small-scale structural details are still missing (e.g., the fuel tank of the airplanes, the arms of the chairs) in the results. The structured-tree based TopNet method achieves better visual results than PCN and AtlasNet in general. We can see the evidences on the airplane, lamp, sofa, and watercraft, which exhibits much clearer part structures and points arranged more neatly. However, some part details in the input partial point clouds are not preserved in the completed point clouds, e.g., the points of the fuel tank of the airplane and the trestle of the table have been moved to other parts. Results produced by our method have no this problem and show visually better performance on all eight categories than baselines. This is because unlike other comparison methods inferring the locations of all points, our method instead uses the points in the other part of the shape (i.e., Fine part) as a completion constraint and infers a local point distribution of a part of the shape (i.e., Coarse part). This makes the network preserve the local detail from the input partial point cloud, produce a more reasonable completion for the missing structure (see the reconstructed left trestles of the table), as well as converge much faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Experimental Analysis</head><p>Ablation Studies. We conduct ablation experiments to study the individual contributions of each stage in our method. As shown in <ref type="table">Table 3</ref>, we quantitatively compare the point cloud qualities of the reconstructed point clouds P rec generated by Modality Transfer, the coarse point clouds P coarse generated by Part Filter, and the completed point clouds P complete generated by Part Refinement.</p><p>In addition, we also study how much the differential refinement strategy that differentiates between the Fine and Coarse parts contributes. Specifically, we modify the Dynamic Offset Predictor network and disable the constraint that the points in the Fine part should be moved within a small sphere, i.e., the points from both the Fine and Coarse parts are processed without any difference. We term the results produced by this architecture as P global . By comparing with P global and P complete , we can obtain the quantitative contribution of the differential refinement strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I ma g e P a r t i a l I n p u t At l a s Ne t F o l d i n g Ne t P C N T o p Ne t</head><p>Ou r s GT <ref type="figure">Figure 6</ref>. Qualitative comparison on ShapeNet-ViPC. Our method outperforms other baseline methods with significant margins. The resolution for partial, completed and groud truth point clouds are 2,048.  <ref type="figure">Figure 7</ref>. Views provides more complementary information for the input partial point clouds can produce better completion results. Each input partial point clouds are shown on the left; quantitative completion performance measured by the average CD (unit: 10 ?3 ) is reported below each input reference view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Incomplete Point Cloud Top1 View Top7 View</head><p>Contribution of the Single-view Image. In this set of experiments, we study what kind of input view can better improve the completion. We randomly select 50 ? 8 = 400 partial point clouds (50 partial point clouds for each category) from the test set of ShapeNet-ViPC for the evaluation. For each partial point cloud, we produce 24 complete point clouds, each of which is generated with the reference of an image from the 24 rendered views. We quantify the completion quality of those 400 completion point clouds with the CD metric and demonstrate some representative results in <ref type="figure">Figure 7</ref>. It indicates that different image views can provide different improvement. The image views which can provide more information for the missing part of the partial point would produce better results.  Contrast to SVR Methods. In this set of experiments, we study if the proposed ViPC is superior to the SToA single-view based reconstruction method PSG <ref type="bibr" target="#b11">[11]</ref>. We compare three different architectures: PSG, the point generation network in the Modality Transfer stage (i.e., P rec in <ref type="table">Table 3</ref>), and the entire proposed framework (i.e., P complete in <ref type="table">Table 3</ref> on the test set of ShapeNet-ViPC. We compute the average CD value of the results produced by the three comparison architectures. We summarize the quantitative results in <ref type="table">Table 4</ref> and visualize representative results in <ref type="figure" target="#fig_5">Figure 8</ref>. Those results indicate the entire proposed framework outperform exceed PSG with a large performance margin. We also find the results generated by PSG show better quality than P ren . Replacing the adopted network in Modality Transfer with a more effective network like PSG may provide a better solution.</p><p>Feature Ablation Studies. We remove xyz, f partial , f recon , f pixel , f grid from f f usion , respectively, to study the contribution of different features in f f usion during the Part Refinement. The results of completion performance are shown in <ref type="table">Table 5</ref>. Their contributions can be ranked as xyz&gt;f partial ? f recon &gt;f pixel &gt;f grid .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Limitations</head><p>Point cloud registration. To ensure the Part Filter and Part Refinement giving full play to their effects, the input partial point cloud and the reconstructed point cloud should be aligned accurately. Because the camera parameters are not hard to obtain in calibrated devices. We align the input partial point cloud with the reconstructed point cloud by using camera parameters in Modality Transfer stage. As for non-calibrated devices, we tried several classical unsupervised point cloud registration methods such as ICP <ref type="bibr" target="#b3">[3]</ref> and ICP-MCC <ref type="bibr" target="#b10">[10]</ref> to perform the point cloud registration. Unfortunately, due to the sparsity of the reconstructed point clouds and the incompleteness of the partial input, it is hard to achieve an accurate alignment. Replacing the current registration solution in Modality Transfer with an effective learning based one is worthy of further study in the future. Completion in real-world scenes. We have also evaluated the proposed method in real-world scenes where we captured the single view images by mobile phone and collected the partial point clouds by the Li-DAR device in the iPad Pro. Because we train our model on rendered images where the rendered textures can not reflect the illumination in the real environment, it leads to poor-quality reconstructed point clouds in the Modality Transfer stage and thus produces unsatisfactory completion results. Training on real-world data may solve this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We propose a pioneer sensor fusion work called ViPC for the task of point cloud completion. ViPC is a view-guided point cloud completion framework. It takes the missing global structure information from an extra single-view image to complete a partial point cloud. The core technical contribution in ViPC is a point cloud refinement network called "Dynamic Offset Predictor" which can deferentially refine the points in a coarse point cloud. We compare ViPC with existing single-modality based STOA methods that reconstruct a complete point cloud based on either the point cloud modality or the image modality. It demonstrates significant quality improvement on a new large-scale dataset we collected for the point cloud completion task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>ViPC is a new approach completing a partial point cloud by leveraging the complementary information from an extra single-view image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Architecture of the proposed ViPC; see the Pipeline section in the text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Network structure for point cloud generation from a single view image in Modality Transfer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Part</head><label></label><figDesc>Refinement stage further refines (up-samples) the coarse point cloud consisting of the Fine and Coarse parts to produce a complete point cloud. To achieve an effective refinement, besides of the coarse point cloud itself (represented by the point coordinates [x, y, z]), the refinement also leverages the guidance of four other types of features which can be obtained from the stages of Modality Transfer and Part Filter as shown in Figure 4. Those four types of features which can be classified into 2D-or 3D-guidance, together with the point coordinates [x, y, z] of the coarse point cloud, are concatenated into a global feature vector shared by each points in the coarse point cloud. The proposed Dynamic Offset Predictor network takes N c repeated global feature vectors as input and predicts the coordinate offset value for N c ? R (R defines the up-sampling rate) output points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>We concatenate five types of features (top row) as the input of Dynamic Offset Predictor whose architecture is shown in the bottom row.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Qualitative comparison with PSG. The input views are shown on the left; extra input partial point clouds for P complete are shown in the top right corner of each corresponding results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Reshape Reconstructed Point Cloud Single View Image</head><label></label><figDesc></figDesc><table><row><cell>Conv2d</cell><cell>Conv2d</cell><cell>Conv2d</cell><cell>Conv2d</cell><cell>Conv2d</cell><cell>Conv2d</cell><cell>Conv2d</cell></row><row><cell>3x3, 16</cell><cell>3x3, 16</cell><cell>3x3, 32</cell><cell>3x3, 64</cell><cell>3x3, 128</cell><cell>5x5, 256</cell><cell>5x5, 512</cell></row><row><cell></cell><cell>Conv2d 3x3, 3</cell><cell>Conv2d 3x3, 32</cell><cell>Deconv2d 3x3, 64</cell><cell>Deconv2d 5x5, 128</cell><cell>Deconv2d 5x5, 256</cell><cell>7x7x512</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Quantitative results on ShapNet-ViPC using Chamfer Distance with 2,048 points. The best results are highlighted in bold.</figDesc><table><row><cell>12]</cell><cell>6.062</cell><cell>5.032</cell><cell>6.414</cell><cell>4.868</cell><cell></cell><cell>8.161</cell><cell cols="2">7.182</cell><cell>6.023</cell><cell>6.561</cell><cell>4.261</cell></row><row><cell>FoldingNet [40]</cell><cell>6.271</cell><cell>5.242</cell><cell>6.958</cell><cell>5.307</cell><cell></cell><cell>8.823</cell><cell cols="2">6.504</cell><cell>6.368</cell><cell>7.080</cell><cell>3.882</cell></row><row><cell>PCN [42]</cell><cell>5.619</cell><cell>4.246</cell><cell>6.409</cell><cell>4.840</cell><cell></cell><cell>7.441</cell><cell cols="2">6.331</cell><cell>5.668</cell><cell>6.508</cell><cell>3.510</cell></row><row><cell>TopNet [33]</cell><cell>4.976</cell><cell>3.710</cell><cell>5.629</cell><cell>4.530</cell><cell></cell><cell>6.391</cell><cell cols="2">5.547</cell><cell>5.281</cell><cell>5.381</cell><cell>3.350</cell></row><row><cell>Ours</cell><cell>3.308</cell><cell>1.760</cell><cell>4.558</cell><cell>3.138</cell><cell></cell><cell>2.476</cell><cell cols="2">2.867</cell><cell>4.481</cell><cell>4.990</cell><cell>2.197</cell></row><row><cell>Methods</cell><cell>Avg</cell><cell>Airplane</cell><cell>Cabinet</cell><cell>Car</cell><cell cols="3">F-Score@0.001 Chair</cell><cell>Lamp</cell><cell>Sofa</cell><cell>Table</cell><cell>Watercraft</cell></row><row><cell>AtlasNet [12]</cell><cell>0.410</cell><cell>0.509</cell><cell>0.304</cell><cell cols="2">0.379</cell><cell>0.326</cell><cell></cell><cell>0.426</cell><cell>0.318</cell><cell>0.469</cell><cell>0.551</cell></row><row><cell>FoldingNet [40]</cell><cell>0.331</cell><cell>0.432</cell><cell>0.237</cell><cell cols="2">0.300</cell><cell>0.204</cell><cell></cell><cell>0.360</cell><cell>0.249</cell><cell>0.351</cell><cell>0.518</cell></row><row><cell>PCN [42]</cell><cell>0.407</cell><cell>0.578</cell><cell>0.270</cell><cell cols="2">0.331</cell><cell>0.323</cell><cell></cell><cell>0.456</cell><cell>0.293</cell><cell>0.431</cell><cell>0.577</cell></row><row><cell>TopNet [33]</cell><cell>0.467</cell><cell>0.593</cell><cell>0.358</cell><cell cols="2">0.405</cell><cell>0.388</cell><cell></cell><cell>0.491</cell><cell>0.361</cell><cell>0.528</cell><cell>0.615</cell></row><row><cell>Ours</cell><cell>0.591</cell><cell>0.803</cell><cell>0.451</cell><cell cols="2">0.5118</cell><cell>0.529</cell><cell></cell><cell>0.706</cell><cell>0.434</cell><cell>0.594</cell><cell>0.730</cell></row></table><note>Table 2. Quantitative results on ShapNet-ViPC using F-Score with 2,048 points. The best results are highlighted in bold.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>CategoryMean Chamfer Distance per point P rec P coarse P global P complete</figDesc><table><row><cell>Airplane</cell><cell>4.479</cell><cell>2.360</cell><cell>1.993</cell><cell>1.760</cell></row><row><cell>Cabinet</cell><cell>7.381</cell><cell>5.531</cell><cell>4.807</cell><cell>4.558</cell></row><row><cell>Car</cell><cell>4.975</cell><cell>3.921</cell><cell>3.308</cell><cell>3.138</cell></row><row><cell>Chair</cell><cell>12.198</cell><cell>6.967</cell><cell>6.098</cell><cell>2.476</cell></row><row><cell>Lamp</cell><cell>4.573</cell><cell>3.549</cell><cell>3.186</cell><cell>2.867</cell></row><row><cell>Sofa</cell><cell>7.809</cell><cell>5.340</cell><cell>4.681</cell><cell>4.481</cell></row><row><cell>Table</cell><cell>10.967</cell><cell>6.719</cell><cell>5.891</cell><cell>4.990</cell></row><row><cell>Watercraft</cell><cell>5.626</cell><cell>3.156</cell><cell>2.669</cell><cell>2.197</cell></row><row><cell>mean</cell><cell>7.241</cell><cell>4.693</cell><cell>4.079</cell><cell>3.308</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .Table 5 .</head><label>35</label><figDesc>Quantitative results for the ablation study; average CD (unit: 10 ?3 ) are reported. Quantitative results of removing different features and their relative decline compared with fusion of all features.</figDesc><table><row><cell>Methods</cell><cell></cell><cell>PSG [11]</cell><cell>P rec</cell><cell>P complete</cell></row><row><cell>CD.(10 ?3 )</cell><cell></cell><cell>7.092</cell><cell>7.241</cell><cell>3.308</cell></row><row><cell cols="5">Table 4. Quantitative comparison with PSG.</cell></row><row><cell>Remove</cell><cell cols="4">xyz f partial f recon f pixel f grid none</cell></row><row><cell cols="5">CD.(10 ?3 ) 3.348 3.325 3.324 3.321 3.316 3.308</cell></row><row><cell cols="2">Decline(%) 1.22</cell><cell>0.52</cell><cell cols="2">0.49 0.39 0.24</cell><cell>-</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><forename type="middle">M</forename><surname>Seversky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Alliez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">A</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Sharf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cl?udio</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">State of the art in surface reconstruction from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eurographics Association</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="161" to="185" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A distributed asynchronous relaxation algorithm for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dimitri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bertsekas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CDC</title>
		<imprint>
			<date type="published" when="1985" />
			<biblScope unit="page" from="1703" to="1704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Method for registration of 3-d shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil D</forename><surname>Besl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mckay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sensor fusion IV: control paradigms and data structures</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">1611</biblScope>
			<biblScope unit="page" from="586" to="606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Computer vision tools for 3d modelling in archaeology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lo Brutto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paola</forename><surname>Meli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Heritage in the Digital Era</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Point cloud segmentation and denoising via constrained nonlinear least squares normal estimates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Innovations for Shape Analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="283" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
		<idno>abs/1512.03012</idno>
		<title level="m">An information-rich 3d model repository. CoRR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robust piecewise-planar 3d reconstruction and completion from large-scale unstructured point data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne-Laure</forename><surname>Chauve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Labatut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Philippe</forename><surname>Pons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1261" to="1268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">3d-r2n2: A unified approach for single and multi-view 3d object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="628" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Shape completion using 3d-encoder-predictor cnns and shape synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">Ruizhongtai</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Niener</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5868" to="5877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Robust rigid registration algorithm based on pointwise correspondence and correntropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoyi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanglin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sirui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuetao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Badong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="91" to="98" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A point set generation network for 3d object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqiang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="605" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">AtlasNet: A Papier-M?ch? Approach to Learning 3D Surface Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Groueix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Aubry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">High-resolution shape completion using deep neural networks for global structure and local geometry inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int. Conf. Comput. Vis</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="85" to="93" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Local frequency interpretation and non-local self-similarity on graph for point cloud inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4087" to="4100" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A probabilistic model for component-based shape synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">55</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Autoware on board: Enabling autonomous vehicles with embedded systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinpei</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shota</forename><surname>Tokunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuya</forename><surname>Maruyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seiya</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manato</forename><surname>Hirabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Kitsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><surname>Monrroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomohito</forename><surname>Ando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Fujii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Azumi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="287" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Screened poisson surface reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kazhdan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Hoppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning part-based templates from large collections of 3d shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vladimir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilmot</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Diverdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">70</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Lidar: An important tool for next-generation phenotyping technology of high potential for plant phenomics? Computers and electronics in Agriculture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="61" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Partial difference operators on weighted graphs for image processing on surfaces and point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lozes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elmoataz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>L?zoray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3896" to="3909" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Partial and approximate symmetry detection for 3d geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pauly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="560" to="568" />
			<date type="published" when="2006" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Fast marching farthest point sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Moenning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neil A Dodgson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
		<respStmt>
			<orgName>University of Cambridge, Computer Laboratory</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Laplacian mesh optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Nealen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Igarashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Sorkine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Alexa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>ACM</publisher>
			<biblScope unit="page" from="381" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deepsdf: Learning continuous signed distance functions for shape representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeong Joon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Florence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Lovegrove</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Example-based 3d scan completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Pauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Giesen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Markus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Geometry Processing, number CONF</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="23" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Discovering structural regularity in 3d geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Pauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Wallner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Pottmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">43</biblScope>
			<date type="published" when="2008" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A review of point cloud registration algorithms for mobile robotics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Pomerleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Colas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Siegwart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Robotics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="104" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Data infrastructure for multitemporal airborne lidar point cloud analysisexamples from physical geography in high mountain environments. Computers, Environment and Urban Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Rieg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Volker Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Rutzinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Sailer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johann</forename><surname>Geist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>St?tter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="137" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Completing 3d object shape from one depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Rock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmay</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Thorsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daeyun</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2484" to="2493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Completion and reconstruction with primitive shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruwen</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Degener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="503" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">What do single-view 3d reconstruction networks learn?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stephan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuwen</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3405" to="3414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Topnet: Structural point cloud decoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A field model for repairing 3d shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binh-Son</forename><surname>Duc Thanh Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khoi</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quang-Hieu</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai-Kit</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Shape from symmetry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Wegbreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1824" to="1831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pixel2mesh: Generating 3d mesh models from single rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dynamic graph CNN for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
		<idno>146:1-146:12</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Daniel Cohen-Or, and Olga Sorkine-Hornung. Patch-based progressive 3d point set upsampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Huang</surname></persName>
		</author>
		<idno>2019. 1</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<biblScope unit="page" from="5958" to="5967" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Grnet: Gridding residual network for dense point cloud completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxun</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangchen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiageng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<idno>2020. 1</idno>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Foldingnet: Point cloud auto-encoder via deep grid deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiru</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Morfit: interactive surface reconstruction from incomplete point clouds with curve-driven topology and geometry control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kangxue</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minglun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="202" to="203" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pcn: Point completion network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Mertz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="728" to="737" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
