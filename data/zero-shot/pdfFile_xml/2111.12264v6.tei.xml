<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pixel-wise Energy-biased Abstention Learning for Anomaly Segmentation on Complex Urban Driving Scenes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tian</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Australian Institute for Machine Learning</orgName>
								<orgName type="institution">University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyuan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Australian Institute for Machine Learning</orgName>
								<orgName type="institution">University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guansong</forename><surname>Pang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Singapore Management University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengbei</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Australian Institute for Machine Learning</orgName>
								<orgName type="institution">University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhong</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Australian Institute for Machine Learning</orgName>
								<orgName type="institution">University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Australian Institute for Machine Learning</orgName>
								<orgName type="institution">University of Adelaide</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Pixel-wise Energy-biased Abstention Learning for Anomaly Segmentation on Complex Urban Driving Scenes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>State-of-the-art (SOTA) anomaly segmentation approaches on complex urban driving scenes explore pixel-wise classification uncertainty learned from outlier exposure, or external reconstruction models. However, previous uncertainty approaches that directly associate high uncertainty to anomaly may sometimes lead to incorrect anomaly predictions, and external reconstruction models tend to be too inefficient for real-time self-driving embedded systems. In this paper, we propose a new anomaly segmentation method, named pixel-wise energy-biased abstention learning (PEBAL), that explores pixel-wise abstention learning (AL) with a model that learns an adaptive pixel-level anomaly class, and an energy-based model (EBM) that learns inlier pixel distribution. More specifically, PEBAL is based on a non-trivial joint training of EBM and AL, where EBM is trained to output high-energy for anomaly pixels (from outlier exposure) and AL is trained such that these high-energy pixels receive adaptive low penalty for being included to the anomaly class. We extensively evaluate PEBAL against the SOTA and show that it achieves the best performance across four benchmarks. Code is available at https://github.com/tianyu0207/PEBAL.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent advances in semantic segmentation have shown tremendous improvements on complex urban driving scenes <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b51">52]</ref>. Despite the accurate predictions on the inlier classes, the model fails to properly recognise anomalous objects that deviate from the training inlier distribution (col. 2 of <ref type="figure">Fig. 1</ref>). Addressing such failure cases is crucial to road safety for autonomous driving vehicles. For example, anomalies can be represented by unexpected objects in the middle of the road, such as a large rock or an unexpected animal that can be incorrectly predicted as a part of the road class, leading to potentially fatal traffic collisions.</p><p>Current methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b48">49]</ref> to detect and segment anomalous objects in complex urban driving scenes tend to depend on classification uncertainty or image reconstruction. The association of high classification uncertainty with anomaly is intuitive, but it has a few caveats. For instance, classification uncertainty happens when samples are close to classification decision boundaries, but Input Image Initial Prediction Final Prediction (Ours) Anomaly Prediction (Meta-OoD) Anomaly Prediction (Ours) <ref type="figure">Fig. 1</ref>: Anomaly segmentation overview. From the input image (anomaly highlighted with a yellow box), the initial prediction shows the original segmentation results with anomalies classified as a one of the pre-defined inlier classes. Anomaly predictions by the previous SOTA Meta-OoD <ref type="bibr" target="#b5">[6]</ref> and our method show an anomaly map with high scores (in yellow and red) for anomalous pixels, where our approach shows less false positive and false negative detections. Consequently, our method can detect small and distant anomalies (row 2) and blurry/unclear anomalies (rows 1, 3, 4) more accurately than Meta-OoD <ref type="bibr" target="#b5">[6]</ref>. In our final prediction, anomalous pixels are coloured as cyan. Some anomalies are small and blurred (e.g., row 2), so please zoom in the PDF for better visualisation.</p><p>there is no guarantee that all anomalies will be close to classification boundaries. Furthermore, samples close to classification boundaries may not be anomalies at all, but just hard inlier samples. Hence, these uncertainty based methods may detect a large number of false positive and false negative anomalies. For example, <ref type="figure">Fig. 1</ref> shows that the previous SOTA Meta-OoD <ref type="bibr" target="#b5">[6]</ref> misses important anomalous pixels (all rows), while misclassifying anomalies (e.g., vegetation in rows 1, 2, 3), even with the use of the outlier exposure (OE) strategy <ref type="bibr" target="#b20">[21]</ref>. In fact, the OE strategy maximises the uncertainty for proxy anomalies, which can cause the model to be more uncertain for all inlier classes and detect false positive anomalies (e.g., Meta-OoD mis-classifies trees or bush with high anomaly scores - <ref type="figure">Fig. 1  col 4</ref>). Reconstruction methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b48">49]</ref> add an extra network to reconstruct the input images from the estimated segmentation, where differences are assumed to be anomalous. Not only does this approach depend on accurate segmentation results for precise reconstruction, but they also require an extra reconstruction network that is hard to train and inefficient to run in real-time self-driving embedded systems. Moreover, reconstruction methods that rely on a discrepancy module require re-training whenever the inlier segmentation model changes due to input distribution shift <ref type="bibr" target="#b11">[12]</ref>, limiting their applicability in real-world systems. Furthermore, previous approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">32]</ref> ignore a couple of important constraints for anomaly segmentation, namely smoothness (e.g., Meta-OoD fails to classify neighbouring anomaly pixels in <ref type="figure">Fig. 1</ref>, rows 1, 4) and sparsity (e.g., Meta-OoD incorrectly detects a large number of anomalous pixels-see yellow and red regions in <ref type="figure">Fig. 1</ref>, rows 1, 2, 3). Another common issue shared by previous methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b31">32]</ref> is that they usually rely on the re-training of the entire network for OE, which is inefficient and can also bias the classification towards outliers. In this paper, we propose a new anomaly segmentation method, the pixelwise energy-biased abstention learning (PEBAL), that directly learns a pixellevel anomaly class, in addition to the pre-defined inlier classes, to reject/abstain anomalous pixels that are dissimilar to any of the inlier classes. It is achieved by a joint optimisation of a novel pixel-wise anomaly abstention learning (PAL) and an energy based model (EBM) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b32">33]</ref>. Particularly, abstention learning (AL) <ref type="bibr" target="#b35">[36]</ref> was originally developed to learn an image-level anomaly class, which is significantly challenged by the pixel-wise anomaly segmentation task that requires pixel-level anomaly class learning. This is because the original AL model treats all pixel inputs equally with a single pre-defined fixed penalty factor to regularise the classification of anomalous pixels, while adaptive penalties are typically required for different pixels in a complex driving scene, e.g., pixels in small (distant) objects vs. large (near) objects, or centred pixels vs fringe pixels of objects. PEBAL is designed to address this issue by learning adaptive pixel-wise energy-based penalties, which automatically decreases the penalty for pixels that are likely to be anomalies. Hence, our model does not explore previously proposed uncertainty measures (e.g., entropy or softmax criteria) or image reconstruction, and instead, for the first time, explicitly learns a new pixel-wise anomaly class. The learned penalty factors are jointly optimised with EBM, resulting in a mutually beneficial optimisation of anomaly and inlier segmentation. Additionally, we impose smoothness and sparsity constraints to the learning of the anomaly segmentation by PEBAL, incorporating local and global dependencies into the pixel-wise penalty estimation and anomaly score learning. Finally, the training of PEBAL is efficient given that we only need to fine-tune the last block of the segmentation model to achieve accurate inference. To summarise, our contributions are the following:</p><p>-We propose the pixel-wise energy-biased abstention learning (PEBAL) that jointly optimises a novel pixel-wise anomaly abstention learning (PAL) and energy-based models (EBM) to learn adaptive pixel-level anomalies. PEBAL mutually reinforces PAL and EBM in detecting anomalies, enabling accurate segmentation of anomalous pixels without compromising the segmentation of inlier pixels (cols. 4,5 of <ref type="figure">Fig. 1</ref>). -We introduce a new pixel-wise energy-biased penalty estimation, which can learn adaptive energy-based penalties to highly varying pixels in a complex driving scene, allowing a robust detection of small/distant and blurry anomalous objects ( <ref type="figure">Fig. 1 row 2</ref>). -We further refine our PEBAL training, using a novel smoothness and sparsity regularisation on anomaly scores to consider the local and global dependencies of the pixels, enabling the reduction of false positive/negative anomaly predictions.</p><p>We validate our approach on Fishyscapes leaderboard <ref type="bibr" target="#b3">[4]</ref>, and achieve SOTA classification accuracy on all relevant benchmarks. We also achieve the best classification results on LostandFound <ref type="bibr" target="#b41">[42]</ref> and Road Anomaly <ref type="bibr" target="#b31">[32]</ref> test sets, significantly surpassing other competing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Uncertainty-based Anomaly Segmentation. Early uncertainty-based methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b44">45]</ref> focused on the estimation of image-level anomalies, but they tended to misclassify object boundaries as anomalies <ref type="bibr" target="#b21">[22]</ref>. Jung et al. <ref type="bibr" target="#b21">[22]</ref> mitigate this issue by iteratively replacing false anomalous boundary pixels with neighbouring non-boundary pixels that have low anomaly score. In <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b37">38]</ref>, the boundary issue was tackled with a pixel-wise uncertainty estimated with MC dropout, but they showed a low pixel-wise anomaly detection accuracy <ref type="bibr" target="#b31">[32]</ref>. Without fine-tuning using a proxy outlier dataset, uncertainty estimation may not be accurate enough to detect anomalies and can predict high uncertainty for challenging inliers or low uncertainty for outliers due to overconfident misclassification.</p><p>Reconstruction-based Anomaly Segmentation. Anomalies can also be segmented from the errors between the input image and its reconstruction obtained from its predicted segmentation map <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49]</ref>. Those approaches are challenged by the dependence on an accurate segmentation prediction, by the complexity of reconstruction models that usually require long training and inference processes, and also by the low quality of the reconstructed images. Anomaly Segmentation via Outlier Exposure. Hendrycks et al. <ref type="bibr" target="#b20">[21]</ref> propose the outlier exposure (OE) strategy that uses an auxiliary dataset of outliers that do not overlap with the real outliers/anomalies to improve the anomaly detection performance. This OE strategy uses outliers from ImageNet <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b46">47]</ref>, void class of Cityscape <ref type="bibr" target="#b11">[12]</ref> or COCO <ref type="bibr" target="#b5">[6]</ref>, where the expectation is that the model can generalise to unseen outliers. Maximising uncertainty for outliers using the OE strategy can lead to a deterioration of the segmentation of inliers <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b46">47]</ref>. Another major drawback of OE methods is that they are trained using outlier images or objects without considering the fact that outliers are rare events that appear around inliers. Hence, the training contains a disproportionately high amount of outliers <ref type="bibr" target="#b5">[6]</ref> that can bias the segmentation toward the anomaly class. We address this issue by respecting the anomaly detection assumption, where anomalous objects are rare, contribute to a small proportion of the training set, and appear around inliers. Abstention Learning. The abstention learning mechanism <ref type="bibr" target="#b12">[13]</ref> adds a "reserve" (i.e., anomaly) class that is predicted when the classification predictions for all inlier classes are not high enough. This method shows good performance in learning holistic image-level anomaly class with a single pre-defined penalty factor for the whole training set, but it fails to learn fine-grained pixel-level anomaly class as an adaptive pixel-wise penalty is required for highly varying pixel-level anomalies (see <ref type="table" target="#tab_6">Table 5</ref>). We address this issue by learning a novel pixel-wise energy-biased penalty estimator that is jointly trained with fine-grained abstention learning. It is worth noting that differently from uncertainty-based methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22]</ref> that assume anomaly even when the model is uncertain but confident, abstention learning requires all classes to have low confidence to predict the anomaly class. Energy-based Models. EBM is trained such that inlier training samples have low energy, whereas non-training outlier samples (i.e., anomalies) are expected to have high energy <ref type="bibr" target="#b25">[26]</ref>. This energy value can then be used to compute the probability of a sample to belong to the inlier distribution. Recently, EBMs are being implemented with deep learning models <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40]</ref>, and to learn them, it is necessary to compute the partition function, which is generally estimated with Markov Chain Monte Carlo (MCMC) <ref type="bibr" target="#b15">[16]</ref>, but this estimation cannot generate accurate high-resolution images. Hence, we follow the simpler idea of estimating the energy score with the logsumexp operator <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b32">33]</ref>, where we minimise the energy of inliers and use an OE strategy <ref type="bibr" target="#b20">[21]</ref> to maximise the energy of outliers. Hence, we do not need to compute the partition function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>We present our PEBAL in this section (see <ref type="figure">Fig. 2</ref>), where we first describe the dataset, then introduce abstention learning and EBM. Next, we present the loss function to train the model, followed by the training and inference procedures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training Set</head><p>We assume to have a set of inlier training images and annotations</p><formula xml:id="formula_0">D in = {(x i , y in i )} |D in | i=1</formula><p>, where x ? X ? R H?W ?C denotes an image with C colour channels, and y in ? Y in ? {0, 1} H?W ?Y denotes the inlier pixel level labels that can belong to Y classes. We also have a set of outlier images and annotations</p><formula xml:id="formula_1">D out = {(x i , y out i )} |D out | i=1 , where y out ? Y out ? {0, 1} H?W ?(Y +1</formula><p>) denotes the outlier pixel-level labels, with the class Y + 1 reserved for pixels belonging to the anomaly class. Note that similarly to previous papers <ref type="bibr" target="#b5">[6]</ref>, the types of anomalies in training set D out do not overlap with the anomalies to be found in the testing set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pixel-wise Energy-biased Abstention Learning (PEBAL)</head><p>The PEBAL model is denoted by</p><formula xml:id="formula_2">p ? (y|x) ? = exp(f ? (y; x) ? ) y ? ?{1,...,Y +1} exp(f ? (y ? ; x) ? ) ,<label>(1)</label></formula><p>where ? is the model parameter, ? indexes a pixel in the image lattice ?, p ? (y|x) ? represents the probability of labelling pixel ? with y ? {1, ..., Y + 1}, and f ? (y; x) ? is the logit for class y at pixel ?. To train the model in <ref type="formula" target="#formula_2">(1)</ref>, we formulate a cost function that jointly trains PAL and EBM to classify anomalous pixels. An important training hyper-parameter</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inlier Logits</head><p>Outlier Logits Seg. Model <ref type="figure">Fig. 2</ref>: PEBAL. The pixel-wise anomaly abstention (PAL) loss ? pal learns to abstain the prediction of outlier pixels from x out containing OE objects (i.e., cyan coloured masks) and calibrate the logit of inlier classes (i.e., reduction of the inlier logits) from both inlier image x in and outlier image x out . The EBM loss ? ebm pushes the free energy E ? to low values for inlier pixels and pulls that to high values for outlier pixels, where a regularisation loss ? reg enforces the smoothness and sparsity constraints on the energy maps. Such EBM learning reduces the logit of inlier classes to share similar values at the same time, facilitating the ? pal learning. Then, the pixel-wise penalty a ? associated with the abstention class at position ? is estimated to bias the penalty to be low for outlier pixels and high for inlier pixels, which in turn encourages high free energy for anomalies and enforces ? pal to abstain the anomalous pixels.</p><p>for PAL is the penalty to abstain from the classification into one of the inlier classes in {1, ..., Y }-this penalty is generally tuned to a single value for all training samples through model selection (e.g., cross validation) <ref type="bibr" target="#b35">[36]</ref>. Instead of treating this as a tunable hyper-parameter, we propose the use of EBM (defined below in (4)) to automatically estimate this penalty during the training process for each pixel within each training image. More specifically, the cost function to train the PEBAL model in <ref type="formula" target="#formula_2">(1)</ref> is:</p><formula xml:id="formula_3">?(D in , D out , ?) = (x,y in )?D in ? pal (?, y in , x, E ? (x)) + ?? in ebm (E ? (x)) + ? reg (E ? (x)) + (x,y out )?D out ? pal (?, y out , x, E ? (x)) + ?? out ebm (E ? (x)) + ? reg (E ? (x)) .<label>(2)</label></formula><p>where ? pal (.) denotes the PAL loss defined as</p><formula xml:id="formula_4">? pal (?, y, x, E ? (x)) = ? ??? log f ? (y ? ; x) ? + f ? (Y + 1; x) ? a ? ,<label>(3)</label></formula><p>with y ? ? {1, ..., Y } for y in , y ? ? {1, ..., Y + 1} for y out1 , and a ? denotes the pixel-wise penalty associated with abstaining from the classification of the inlier classes. The minimisation of the loss in (3) will abstain from classifying outlier pixels into one of the inlier classes, where a pixel is estimated to be an outlier with a ? . Before formulating a ? , let us define the inlier free energy at pixel ?, which is denoted by E ? (x) ? and computed with the logsumexp operator as follows <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b32">33]</ref>:</p><formula xml:id="formula_5">E ? (x) ? = ? log y?{1,...,Y } exp(f ? (y; x) ? ).<label>(4)</label></formula><p>The pixel-wise penalty associated with abstaining from the classification of the inlier classes is defined by</p><formula xml:id="formula_6">a ? = (?E ? (x) ? ) 2 ,<label>(5)</label></formula><p>which means that the larger the a ? (i.e., low inlier free energy, so the sample is an inlier), the higher the loss to abstain from classifying into one of the Y classes, and low value of a ? (i.e., high free inlier energy, which means an outlier sample) implies a lower loss to abstain from classifying one of the Y classes. Also in <ref type="formula" target="#formula_3">(2)</ref>, ? in ebm (.) (weighted by hyper-parameter ?) represents the EBM loss that pushes the inlier free energy in (4) for samples in D in to low values, with</p><formula xml:id="formula_7">? in ebm (E ? (x)) = ??? max(0, E ? (x) ? ? m in ) 2 ,<label>(6)</label></formula><p>representing the loss of having inlier samples with free energy larger than threshold m in , and</p><formula xml:id="formula_8">? out ebm (E ? (x)) = ?in?? max(0, E ? (x) ? ?m in ) 2 + ?out?? max(0, m out ?E ? (x) ? ) 2 ,<label>(7)</label></formula><p>denoting the loss of having outlier samples with inlier free energy smaller than threshold m out , where the margin losses in <ref type="formula" target="#formula_7">(6)</ref> and <ref type="formula" target="#formula_8">(7)</ref> effectively create an energy gap between normal and abnormal pixels 1 . The last term to define in (2) is the inlier free energy regularisation loss to enforce that anomalous pixels are sparse and pixel anomaly classification is smooth (i.e., anomalous pixels tend to have anomalous neighbouring pixels), which is defined as</p><formula xml:id="formula_9">? reg (E ? (x)) = ??? ? 1 |E ? (x) ? ? E ? (x) N (?) | + ? 2 |E ? (x) ? |,<label>(8)</label></formula><p>where ? 1 and ? 2 are hyper-parameters that weight the contributions of the smoothness and sparsity and sparsity regularisations, and N (?) denotes neighbouring pixels in horizontal and vertical directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training and Inference</head><p>Training. An important point of the training process is how to setup the inlier and outlier datasets D in and D out . A recently published paper <ref type="bibr" target="#b5">[6]</ref> carefully selects images to be included in D out by making sure that the segmentation labels presented in those images do not overlaps with the inlier labels. In particular for <ref type="bibr" target="#b5">[6]</ref>, D in has images and annotations from Cityscape and D out has images and annotations from COCO <ref type="bibr" target="#b30">[31]</ref>. We argue that there are two issues with this strategy to form D out , which are: 1) the selected COCO images generally only contain anomalous pixel labels, leading to unstable training of the outlier losses (i.e., second summation in (2)) given the exclusive presence of the anomaly class (in effect, this becomes a one-class segmentation problem); 2) re-training the model with images containing only anomalous pixels removes the semantic context of inlier pixels when training for the outlier losses, which can deteriorate the segmentation accuracy of the inlier labels.</p><p>To mitigate these issues, we form D out using a novel extension based on Cut-Mix and CutPaste <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b49">50]</ref>, which we refer to as AnomalyMix. AnomalyMix cuts the anomalous objects from an outlier dataset (e.g., COCO) using its labelled masks and paste them into the images of the inlier dataset (e.g., CitySpace), where we label the pixels of the anomalous object with the class Y + 1 -these images are then inserted into D out . AnomalyMix addresses the two issues above because the outlier images now contain a combination of inlier and outlier pixels, allowing a balanced learning and keeping the visual context of inlier labels when training for the outlier losses. Furthermore, AnomalyMix can form a potentially infinite number of training images for D out given the range of transformations to be applied to the cut objects and the locations of the inlier images that the objects can be pasted. Previous papers <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b21">22]</ref> argue that re-training the whole segmentation model can jeopardise the segmentation accuracy for the inlier classes. Furthermore, such re-training requires a long training time, leading to inefficient optimisation. In this work, we propose to fine-tune only the final classification block using the loss in (2), instead of re-training the whole segmentation model. Besides being efficient, this fast fine-tuning keeps the segmentation accuracy of the model in the original dataset used for pre-training the model. Furthermore, an interesting side-effect of our training is that the cost function in (2) will calibrate the segmentation prediction for the inlier classes. This happens because the terms ? pal (.), ? in ebm (.) and ? out ebm (.) jointly constrain the maximisation of logits and naturally calibrate classification confidence (See supplementary material). Inference. During inference, pixel-wise anomaly detection is performed by computing the inlier free energy score E ? (x) ? from (4) for each pixel position ? given a test image x and inlier segmentation is obtained from the inlier classes from the PEBAL model in <ref type="bibr" target="#b0">(1)</ref>. Following <ref type="bibr" target="#b21">[22]</ref>, we also apply a Gaussian smoothing kernel to produce the final energy map.</p><p>Fishyscapes <ref type="bibr" target="#b3">[4]</ref> is a high-resolution dataset for anomaly estimation in semantic segmentation for urban driving scenes. The benchmark has an online testing set that is entirely unknown to the methods. The dataset is composed by two data sources: Fishyscapes LostAndFound that contains a set of real road anomalous objects <ref type="bibr" target="#b41">[42]</ref> and a blending-based Fishyscapes Static dataset. The Fishyscapes LostAndFound validation set consists of 100 images from the aforementioned Lo-stAndFound dataset with refined labels and the Fishyscapes Static validation set contains 30 images with the blended anomalous objects from Pascal VOC <ref type="bibr" target="#b13">[14]</ref>. For all datasets, we select the checkpoints based on the results on the public validation sets, but submitted our code and checkpoints to the benchmark website to be evaluated on their hidden test sets. Road Anomaly <ref type="bibr" target="#b31">[32]</ref> contains real-world road anomalies in front of the vehicles. The dataset has 60 images from the Internet, containing unexpected animals rocks, cones and obstacles.Unlike the LostAndFound and Fishyscapes, this dataset contains abnormal objects with various scales and sizes, making it even more challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>Following <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>, we use DeepLabv3+ <ref type="bibr" target="#b6">[7]</ref> with WideResnet38 trained by Nvidia <ref type="bibr" target="#b50">[51]</ref> and ResNet101 from <ref type="bibr" target="#b21">[22]</ref> as the backbone of our segmentation models. The training details of those models can be found in their original papers or our supplementary material. The models are trained on Cityscapes <ref type="bibr" target="#b9">[10]</ref> training set. For our PEBAL fine-tuning, we empirically set the m in and m out in Eq. 6 and Eq. 7 as -12 and -6, respectively. The weights ? 1 and ? 2 in Eq. 8 are set to 5e ? 4 and 3e?6 <ref type="bibr" target="#b42">[43]</ref>, ? in Eq. 2 to 0.1, and the weight of ? ebm to 0.1, respectively. Note that those hyper-parameters are selected at the first training epoch to normalise loss values to a similar scale. We also show our model can obtain consistently SOTA results regardless of the selection of hyper-parameters in the supplementary material. Our training consists of fine-tuning the final classification block of the model for 20 epochs. We use the same resolution of random crop as in <ref type="bibr" target="#b50">[51]</ref>, and use Adam with a learning rate of 1e ?5 . The batch size is set to 16. Following <ref type="bibr" target="#b5">[6]</ref>, for our AnomalyMix augmentation, we randomly sample 297 images as training data from the remaining COCO images that do not contain objects in Cityscapes or our anomaly validation/testing sets and randomly apply AnomalyMix to mix them into the Cityscape training images, following Chan et al. <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Measures</head><p>Following <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b21">22]</ref>, we compute the the area under receiver operating characteristics (AUROC), average precision (AP), and the false positive rate at a true positive rate of 95% (FPR95) to validate our approach. For Fishyscapes public leaderboard, we use AP and FPR95 to compare with other methods, same as their website.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison on Anomaly Segmentation Benchmarks</head><p>Comparison on LostAndFound. <ref type="table" target="#tab_0">Table 1</ref> shows the result on the testing set of LostAndFound. Notably, our approach surpasses the previous baseline approaches (i.e., MSP <ref type="bibr" target="#b18">[19]</ref>, Mahalanobis <ref type="bibr" target="#b27">[28]</ref>, Max Logit <ref type="bibr" target="#b19">[20]</ref> and Entropy [20]) by 10% to 40% AP, and 13% to 22% FPR95, respectively. When compared with previous SOTA approaches such as SynBoost <ref type="bibr" target="#b11">[12]</ref>, SML <ref type="bibr" target="#b21">[22]</ref> and Meta-OoD <ref type="bibr" target="#b5">[6]</ref>, we improve the AP performance by a large margin (15% to 40%), and decrease the FPR95 by about 5% to 70%. This illustrates the robustness and effectiveness on detecting small and distant anomalous objects given that the dataset contains mostly real-world small objects. Our PEBAL also improves the EBM baseline <ref type="bibr" target="#b32">[33]</ref> and the AL baseline based on Deep Gambler <ref type="bibr" target="#b35">[36]</ref>. This demonstrates that a simple adaptation of AL and EBM is not enough to enable accurate pixel-wise anomaly detection. Previous SOTA SML <ref type="bibr" target="#b21">[22]</ref> aims to balance the inlier class-wise discrepancy on prediction scores, which is disadvantageous for measuring performance on LostAndFound test set since there may be no classes in the evaluation other than the road class (i.e., most of the inlier classes within LF test set is road class), thus leading to significant performance variations between LostAndFound and Fishyscapes. It is worth noting that our approach achieves 1.03% FPR95, significantly reducing the false positive pixels, improving the chances of applying it to real-world applications.</p><p>Comparison on Fishyscapes Leaderboard.    <ref type="bibr" target="#b18">[19]</ref> 89. <ref type="bibr" target="#b28">29</ref>  it is worth noting that PEBAL reduces the amount of false positive pixels to 7.58 and 1.73 FPR on the two datasets. This result is publicly available on the Fishyscapes website.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FS LostAndFound FS Static Road Anomaly</head><formula xml:id="formula_10">AUC ? AP ? FPR95 ? AUC ? AP ? FPR95 ? AUC ? AP ? FPR95 ? MSP</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison on Fishyscapes validation sets and Road Anomaly.</head><p>In <ref type="table" target="#tab_3">Tables 3 and 4</ref>, we compare our approach on the Fishyscapes validation sets and Road Anomaly using two different backbones. Our model outperforms the previous methods by a large margin on all three benchmarks, regardless of the backbones and their segmentation accuracy. To verify the applicability of our method, except for the modern WideResnet38 backbone, we use a ResNet101 DeepLabv3+ to investigate the performance in terms of the size of the architecture and its inlier segmentation accuracy. The results demonstrate that our approach is applicable to a wide-range of segmentation models, indicating the effectiveness of PEBAL to adapt to real-world systems. Moreover, our fine-tuning sacrifices only marginally the inlier segmentation accuracy (i.e., 0.2% -0.7% mIoU on Cityscapes) for both backbones, achieving good performance on both inlier and anomaly segmentation. We present details of all inlier segmentation models (i.e., Cityscapes training setup and mIoU), and include more experimental results of other DeepLabv3+ checkpoints in supplementary material.</p><p>Remarks -Superior Performance on Challenging Benchmarks. Each dataset has different challenges. For example, the LostAndFound testing set considers only drivable areas with homogeneous normal scenes (i.e., road) and limited categories of abnormalities (i.e., road obstacles), leading to a relatively less challenging benchmark on which most methods can obtain good AUC performance, as shown in Tables 1, 3 and 4. On the contrary, Fishyscapes and Road-Anomaly contain large number of heterogeneous inlier and outlier pixels from diverse classes, leading to significantly more difficult testbeds than the LostAnd-Found testing set. Furthermore, Fishyscapes and RoadAnomaly contain domain shift compared with Cityscapes (e.g., both datasets contain different scenes than Cityscapes) and have different types/sizes of OoD objects. Most existing SOTA methods work ineffectively on these two datasets due to those challenges, while our adaptive pixel-level anomaly class learning helps our model effectively detect these challenging inlier and outlier pixels in the aforementioned heterogeneous and domain-shifted scenes, yielding substantial improvements (i.e., 20% to 50%) to previous approaches, as shown in Tables 2, 3 and 4. <ref type="table" target="#tab_6">Table 5</ref> shows the contribution of each component of our PEBAL on the Lo-stAndFound testing set. All modules are trained with COCO OE images using AnomalyMix. Adding an extra OoD class to learn the OE training samples with entropy maximisation (EM) is our baseline (first row). To justify the effectiveness of our proposed joint training, we show the results using energy-based  Finally, the smoothness and sparsity regularisation losses stabilise the training and further improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Outlier Samples and Computational Efficiency</head><p>Outlier Diversity and Efficiency. In <ref type="table" target="#tab_7">Table 6</ref>, we randomly select 1%, 5%, 10%, 25% 50%, and 75% of COCO classes as the OE data during training and compute the mean results over six different random seeds. We achieve consistent AP and FPR performance regardless of the number of COCO classes used during the training on Fishyscapes. It is also worth noting that our approach can effectively learn the PEBAL model using only one class (1% in <ref type="table" target="#tab_7">Table 6</ref>) of outlier data, which selects some of the irrelevant classes of COCO objects that are not possible to be found on road in real life (e.g., dining table, laptop, and clock). The results indicate that our model can consistently achieve SOTA performance on Fishyscapes without a careful selection of OE classes, demonstrating the robustness of our approach under diverse outlier classes. We also investigate the outlier sample efficiency of our model w.r.t smaller OE training sets with a fixed 100% COCO classes (80 classes) on Fishyscapes in <ref type="table" target="#tab_8">Table 7</ref>, and we achieve consistently good performance regardless the number of outlier Moreover, our method also has a much faster mean inference time of 0.55s compared to 0.85s of Meta-OoD and 1.95s of Synboost. Those results suggest the practicability of our model in real-world self-driving systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Discussions</head><p>We proposed a simple yet effective approach, named Pixel-wise Energy-biased Abstention Learning (PEBAL), to fine-tune the last block of a segmentation model to detect unexpected road anomalies. The approach introduces a nontrivial training that jointly optimises a novel pixel-wise abstention learning and an energy-based model to learn an adaptive pixel-wise anomaly class, in which a new pixel-wise energy-biased penalty estimation method is proposed to improve the precision and robustness to detect small and distant anomalous objects.</p><p>The resulting model significantly reduces the false positive and false negative detected anomalies, compared with previous SOTA methods. The results on four benchmarks demonstrate the accuracy and robustness of our approach to detect anomalous objects regardless of the amount or diversity of exposed training outliers. Despite the remarkable performance on most datasets, PEBAL is not as effective on the most challenging dataset, Road Anomaly, that contains significantly more diverse and realistic anomalous objects. We plan to further enhance the generalisation of our model to accurately detect more unknown, diverse anomalies. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Image Initial Prediction Anomaly Prediction Final Prediction</head><p>Fig. S1: From the input image (anomaly highlighted with a yellow box), the initial prediction shows the original segmentation results with anomalies classified as a one of the pre-defined inlier classes. Anomaly predictions from our method show an anomaly map with high scores (in yellow and red) for anomalous pixels. In our final prediction, anomalous pixels are coloured in cyan.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Qualitative results</head><p>In <ref type="figure">Figure S1</ref>, we show some additional qualitative results. Our approach can effectively detect small and distant objects (rows 6 and 7) and objects with different scales (rows 1 to 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B More AUC results</head><p>In Tables S1 and S2, we show the AUC results in addition to the AP and FPR results in <ref type="table" target="#tab_7">Tables 6 and 7</ref> of the main paper. We achieve consistently SOTA AUC performance regardless of the selection of outlier classes or the number of outlier training samples.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Training Details on Cityscapes</head><p>Following <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>, we use the same DeepLabv3+ <ref type="bibr" target="#b6">[7]</ref> with WideResnet38 (90.3 mIoU on Cityscapes Val) trained by Nvidia <ref type="bibr" target="#b50">[51]</ref> as one of the backbones of our segmentation model. As mentioned in <ref type="bibr" target="#b50">[51]</ref>, the model is firstly pre-trained on Mapillary Vista dataset <ref type="bibr" target="#b38">[39]</ref>, and then fine-tuned on Cityscapes train set with their proposed label relaxation loss and sdc-aug label propagation. Their model uses a different {cv2: monchengladbach, strasbourg, stuttgart} validation split than the standard split {cv0: munster, lindau, frankfurt}. Please refer to their paper for more details. For DeepLabv3+ <ref type="bibr" target="#b6">[7]</ref> with Resnet101 backbone (80.3 mIoU on Cityscapes Val) from <ref type="bibr" target="#b21">[22]</ref>, the authors trained their model with the standard cv0 train/validation split using default formulations in <ref type="bibr" target="#b6">[7]</ref>. All those checkpoints are downloaded from their official Github pages. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Results Based on Different DeepLabv3+ Checkpoint</head><p>In this section, we show the results of another DeepLabv3+ <ref type="bibr" target="#b6">[7]</ref> with WideRes-net38 trained by Nvidia <ref type="bibr" target="#b50">[51]</ref> using the Cityscapes {cv0: munster, lindau, frankfurt} standard train/val split. The checkpoint is downloaded from the their official Github page <ref type="bibr" target="#b50">[51]</ref>, with a 81.8% mIoU on Cityscapes validation set. This model was firstly pre-trained on Mapillary Vista dataset <ref type="bibr" target="#b38">[39]</ref> and then fine-tuned on Cityscapes but without their label relaxation loss and sdc-aug label propagation. As shown in Tab. S3, our model outperforms the previous methods by a large margin on all three benchmarks, regardless of the backbones, the segmentation accuracy and the Cityscapes train/val splits. Notably, our method surpasses the previous SOTA SML by 40%, 50% and 20% of AP on three datasets, respectively. We also achieve best AUC and FPR results on all datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Anomaly segmentation results on LostAndFound testing set, with WideResnet38 backbone. All methods use the same segmentation models. * indicate that the model requires additional learnable parameters. ? indicates that the results are obtained from the official code with our WideResnet38 backbone.</figDesc><table><row><cell>Methods</cell><cell cols="2">AUC ? AP ? FPR95 ?</cell></row><row><cell>MSP [19]</cell><cell cols="2">85.49 38.20 18.56</cell></row><row><cell cols="3">Mahalanobis [28] 79.53 42.56 24.51</cell></row><row><cell>Max Logit [20]</cell><cell cols="2">94.52 65.45 15.56</cell></row><row><cell>Entropy [20]</cell><cell cols="2">86.52 50.66 16.95</cell></row><row><cell>Energy [33]</cell><cell cols="2">94.45 66.37 15.69</cell></row><row><cell>Meta-OoD [6]</cell><cell>97.95 71.23</cell><cell>5.95</cell></row><row><cell>? SML [22]</cell><cell cols="2">88.05 25.89 44.48</cell></row><row><cell cols="2">? SynBoost* [12] 98.38 70.43</cell><cell>4.89</cell></row><row><cell cols="2">Deep Gambler [36] 98.67 72.73</cell><cell>3.81</cell></row><row><cell>Ours</cell><cell cols="2">99.76 78.29 0.81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>shows the leaderboard results on the test set of Fishyscapes LostAndFound and Fishyscapes Static. Following<ref type="bibr" target="#b21">[22]</ref>, we compared the methods based on whether they require re-training of the entire segmentation network, adding the extra network, or utilising the OoD data. We achieve the SOTA performance by a large margin on Fishyscapes leaderboard when compared with the previous methods except<ref type="bibr" target="#b1">[2]</ref> (Static) that rely on an inefficient re-training segmentation model, extra learnable parameters, and extra OoD training data. Without re-training the entire network or</figDesc><table /><note>adding extra learnable parameters, our approach can work efficiently to surpass previous SOTA competing approaches that fall into the same category by about 13% to 42% on LostAndFound and 40% to 50% AP on Static. Such significant improvements indicate the generalisation ability of our proposed PEPAL on de- tecting a wide variety of unseen abnormalities (i.e., of different size, type, scene, and distance) substantially reducing false negative and positive pixels. Moreover,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison with previous approaches on Fishyscapes Leaderboard. We achieve a new state-of-the-art performance among the approaches that require extra OoD data, and without re-training the segmentation networks and extra networks on Fishyscapes Leaderboard. FPR95 ? AP ? FPR95 ?</figDesc><table><row><cell cols="3">Models AP ? Discriminative Outlier Detection Head [2] FS LostAndFound re-training Extra Network OoD Data 31.31 19.02</cell><cell cols="2">FS Static 96.76 0.29</cell></row><row><cell>MSP [19]</cell><cell>1.77</cell><cell>44.85</cell><cell cols="2">12.88 39.83</cell></row><row><cell>Entropy [20]</cell><cell>2.93</cell><cell>44.83</cell><cell cols="2">15.41 39.75</cell></row><row><cell>SML [22]</cell><cell>31.05</cell><cell>21.52</cell><cell cols="2">53.11 19.64</cell></row><row><cell>kNN Embedding -density [4]</cell><cell>3.55</cell><cell>30.02</cell><cell cols="2">44.03 20.25</cell></row><row><cell>Bayesian Deeplab [38]</cell><cell>9.81</cell><cell>38.46</cell><cell cols="2">48.70 15.05</cell></row><row><cell>Density -Single-layer NLL [4]</cell><cell>3.01</cell><cell>32.9</cell><cell cols="2">40.86 21.29</cell></row><row><cell>Density -Minimum NLL [4]</cell><cell>4.25</cell><cell>47.15</cell><cell cols="2">62.14 17.43</cell></row><row><cell>Image Resynthesis [32]</cell><cell>5.70</cell><cell>48.05</cell><cell>29.6</cell><cell>27.13</cell></row><row><cell>OoD Training -Void Class</cell><cell>10.29</cell><cell>22.11</cell><cell cols="2">45.00 19.40</cell></row><row><cell>Dirichlet Deeplab [37]</cell><cell>34.28</cell><cell>47.43</cell><cell cols="2">31.30 84.60</cell></row><row><cell>Density -Logistic Regression [4]</cell><cell>4.65</cell><cell>24.36</cell><cell cols="2">57.16 13.39</cell></row><row><cell>SynBoost [12]</cell><cell>43.22</cell><cell>15.79</cell><cell cols="2">72.59 18.75</cell></row><row><cell>Ours</cell><cell>44.17</cell><cell>7.58</cell><cell cols="2">92.38 1.73</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Anomaly segmentation results on Fishyscapes validation sets (Lo- stAndFound and Static), and the Road Anomaly testing set, with WideRes- net38 backbone. * indicate that the model requires additional learnable param- eters. ? indicates that the results are obtained from the official code with our WideResnet38 backbone. Best and second best results in bold.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>4.59 40.59 92.36 19.09 23.99 67.53 15.72 71.38 Max Logit [19] 93.41 14.59 42.21 95.66 38.64 18.26 72.78 18.98 70.48 Entropy [20] 90.82 10.36 40.34 93.14 26.77 23.31 68.80 16.97 71.10 Energy [33] 93.72 16.05 41.78 95.90 41.68 17.78 73.35 19.54 70.17 Mahalanobis [28] 96.75 56.57 11.24 96.76 27.37 Synboost* [12] 96.21 60.58 31.02 95.87 66.44 25.59 81.91 38.21 64.75 ? SML [22] 94.97 22.74 33.49 97.25 66.72 12.14 75.16 17.52 70.70 Deep Gambler [36] 97.82 31.34 10.16 98.88 84.57 3.39 78.29 23.26 65.12 Ours 98.96 58.81 4.76 99.61 92.08 1.52 87.63 45.10 44.58</figDesc><table><row><cell></cell><cell>11.7</cell><cell cols="3">62.85 14.37 81.09</cell></row><row><cell>Meta-OoD [12]</cell><cell>93.06 41.31 37.69 97.56 72.91 13.57</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Anomaly segmentation results on Fishyscapes validation sets (LostAndFound and Static), and the Road Anomaly testing set, with Resnet101 backbone. * indicate that the model requires additional learnable parameters. ? indicates that the results are obtained from the official code with our Resnet101 backbone. Best and second best results in bold. AP ? FPR95 ? AUC ? AP ? FPR95 ? AUC ? AP ? FPR95 ? Synboost* [12] 94.89 40.99 34.47 92.03 48.44 47.71 85.23 41.83 59.72 SML [22] 96.88 36.55 14.53 96.69 48.67 16.75 81.96 25.82 49.74 Deep Gambler [36] 97.19 39.77 12.41 97.51 67.69 15.39 85.45 31.45 48.79</figDesc><table><row><cell cols="4">FS LostAndFound AUC ? MSP [19] Methods 86.99 6.02 45.63 88.94 14.24 34.10 73.76 20.59 68.44 FS Static Road Anomaly</cell></row><row><cell>Max Logit [19]</cell><cell cols="3">92.00 18.77 38.13 92.80 27.99 28.50 77.97 24.44 64.85</cell></row><row><cell>Entropy [20]</cell><cell cols="3">88.32 13.91 44.85 89.99 21.78 33.74 75.12 22.38 68.15</cell></row><row><cell>Energy [33]</cell><cell cols="3">93.50 25.79 32.26 91.28 31.66 37.32 78.13 24.44 63.36</cell></row><row><cell cols="2">? SynthCP* [49] 88.34 6.54</cell><cell>45.95</cell><cell>89.9 23.22 34.02 76.08 24.86 64.69</cell></row><row><cell>Ours</cell><cell cols="3">99.09 59.83 6.49 99.23 82.73 6.81 92.51 62.37 28.29</cell></row></table><note>?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Ablation studies for anomaly segmentation on LostAndFound, with WideResnet38 backbone, where all proposed modules are trained with COCO OE images with AnomalyMix. EM denotes the baseline method that adds an extra OoD class to learn the OE training samples with entropy maximisation (first row).</figDesc><table><row><cell cols="6">EM ?ebm ?pal ?reg AUC ? AP ? FPR95 ?</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell>96.88</cell><cell>69.02</cell><cell>8.03</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell>97.88</cell><cell>70.24</cell><cell>8.92</cell></row><row><cell></cell><cell>?</cell><cell></cell><cell>98.67</cell><cell>72.73</cell><cell>3.81</cell></row><row><cell>?</cell><cell>?</cell><cell></cell><cell>99.63</cell><cell>77.19</cell><cell>1.19</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell cols="2">99.76 78.29</cell><cell>0.81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>The performance comparison of our approach on Fishyscapes benchmark w.r.t different diversity of OE classes (mean results over six random seeds), in terms of AP and FPR95. ?3.74 6.97 ?1.98 85.84 ?1.01 3.05 ?0.97 5% 52.16 ?3.88 6.58 ?1.95 90.57 ?1.75 1.93 ?0.52 10% 55.14 ?3.02 5.78 ?1.59 91.37 ?1.28 1.64 ?0.58 25% 55.48 ?3.32 5.98 ?1.27 91.28 ?1.94 1.77 ?0.18 50% 56.69 ?2.57 5.32 ?1.16 91.88 ?0.71 1.62 ?0.05 75% 57.86 ?2.83 5.11 ?1.69 91.85 ?0.56 1.63 ?0.09 models (? ebm without ? pal ) and pixel-wise abstention (? pal with pre-defined fixed penalty). Both outperform the baselines (AP=70.2, FPR=8.9 and AP=72.7, FPR=3.8 vs. AP=69, FPR=8.03), while our proposed joint training (? ebm + ? pal ) obtains 77.19% of AP and 1.19% of FPR, improving over each module by 4% to 7%. This indicates the effectiveness of our joint training and the significance of our proposed PAL with learnable adaptive energy-based penalties a ? .</figDesc><table><row><cell>Class Per.</cell><cell>FS LostAndFound AP ? FPR95 ?</cell><cell>FS Static AP ? FPR95 ?</cell></row><row><cell>1%</cell><cell>53.57</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>The performance comparison of our approach on Fishyscapes benchmark w.r.t different amount of OE training samples (mean results over six random seeds), in terms of AP and FPR95. ?1.89 5.77 ?2.38 89.11 ?1.52 2.23 ?0.65 10% 56.28 ?1.05 4.66 ?1.36 90.02 ?0.57 1.67 ?0.28 25% 56.18 ?1.69 4.81 ?1.44 91.23 ?0.95 1.63 ?0.22 50% 57.34 ?1.19 4.75 ?1.32 91.29 ?0.92 1.67 ?0.17 training samples. All those experiments show the applicability of our PEBAL to real-world autonomous driving systems. Computational Efficiency. We compare the computational efficiency of our PEBAL with previous SOTA Meta-OoD [6] and Synboost [12] in terms of the trainable parameters, training time and mean inference time per image, on an NVIDIA3090. As PEBAL requires the fine-tuning of the final classification block, it has only 1.3M parameters and each training epoch takes about 12 minutes, which is significantly less than the re-training approach Meta-OoD that has 137.1M parameters and each training epoch takes about 26 minutes, and the reconstruction based approach Synboost that takes about 33 minutes to train a epoch of its re-synthesis and dissimilarity networks with 157.3M parameters.</figDesc><table><row><cell>Train Size</cell><cell>FS LostAndFound AP ? FPR95 ?</cell><cell>FS Static AP ? FPR95 ?</cell></row><row><cell>5%</cell><cell>54.32</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Class Per. FS LF -AUC FS Static -AUC 1% 97.59 ?0.39 98.37 ?0.56 5% 98.17 ?0.45 98.25 ?0.71 10% 98.47 ?0.39 99.59 ?0.25 25% 98.39 ?0.28 99.52 ?0.17 50% 98.63 ?0.07 99.54 ?0.08 75% 98.71 ?0.05 99.59 ?0.03</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table S1 :</head><label>S1</label><figDesc>AUC testing results (mean results over six random seeds) of our approach on Fishyscapes benchmark w.r.t. different diversity of OE classes.</figDesc><table><row><cell cols="3">Train Size FS LF -AUC FS Static -AUC</cell></row><row><cell>5%</cell><cell>98.13 ?0.12</cell><cell>99.16 ?0.09</cell></row><row><cell>10%</cell><cell>98.35 ?0.15</cell><cell>99.57 ?0.07</cell></row><row><cell>25%</cell><cell>98.36 ?0.06</cell><cell>99.51 ?0.06</cell></row><row><cell>50%</cell><cell>98.69 ?0.05</cell><cell>99.37 ?0.07</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table S2 :</head><label>S2</label><figDesc>AUC testing results (mean results over six random seeds) of our approach on Fishyscapes benchmark w.r.t. different amount of OE training samples. C Hyper-parameters Selection For testing, we note a small performance gap with ? ? {0.1, 0.01} on LF test set, with AP=78.29 for ? = 0.01 and AP=77.15 for ? = 0.1. For the EBM margin, PEBAL reaches AP? [76.9, 78.3] and FPR? [0.8, 1.3] for m in ? [?12, ?22] and m out ? [?2, ?8] for different values of m in and m out on LF test set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table S3 :</head><label>S3</label><figDesc>Anomaly segmentation results on Fishyscapes validation sets (Lo-stAndFound and Static), and the Road Anomaly testing set, with WideRes-net38 backbone under cv0 standard train/val split.AP ? FPR95 ? AUC ? AP ? FPR95 ? AUC ? AP ? FPR95 ? MSP<ref type="bibr" target="#b18">[19]</ref> 89.<ref type="bibr" target="#b25">26</ref> 11.84 32.55 89.26 11.84 32.55 72.37 20.23 67.98 Max Logit [19] 93.14 12.78 38.15 93.27 18.89 25.49 76.39 23.46 64.55 Entropy [20] 89.01 8.79 47.81 90.28 15.19 31.71 73.70 22.13 67.42 Energy [33] 93.45 14.29 37.71 93.52 19.22 25.02 76.76 23.48 64.04 SML [22] 96.03 21.71 20.09 95.79 32.04 15.81 74.45 22.16 68.59 Ours 98.52 64.43 6.56 99.33 86.01 2.63 88.85 44.41 37.98</figDesc><table><row><cell>Methods</cell><cell>FS LostAndFound AUC ?</cell><cell>FS Static</cell><cell>Road Anomaly</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">When y? is an outlier pixel, we set y? = 1 for all the Y + 1 labels in Y out .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Please note that D out contains both iniler and outlier pixels.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Experiment4.1 DatasetsLostAndFound<ref type="bibr" target="#b41">[42]</ref> is one of the first publicly available urban driving scene anomaly detection datasets containing real-world anomalous objects. The dataset has an official testing set containing 1,203 images with small obstacles in front of the cars, collecting from 13 different street scenes, featuring 37 different types of anomalous objects with various sizes and material.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep autoencoding models for unsupervised anomaly segmentation in brain mr images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Baur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wiestler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albarqouni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International MICCAI Brainlesion Workshop</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Simultaneous semantic segmentation and outlier detection in presence of domain shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bevandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Discriminative out-of-distribution detection for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bevandi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kre?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Or?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>?egvi?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.07703</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The fishyscapes benchmark: Measuring blind spots in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Sarlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cadena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03215</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Segmentmeifyoucan: A benchmark for anomaly segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uhlemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rottmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Entropy maximization and meta classification for out-of-distribution detection in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rottmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gottschalk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation with cross pseudo supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep one-class classification via interpolated gaussian descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Real-time small obstacle detection on highways using compressive rbm road reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Creusot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Munawar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pixel-wise anomaly detection in complex driving scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Biase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cadena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On the foundations of noise-free selective classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Semi-supervised semantic segmentation needs strong, high-dimensional perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mackiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Finlayson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Your classifier is secretly an energy based model and you should treat it like one</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.03263</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Dense anomaly detection by robust learning on synthetic negative data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grci?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bevandi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>?egvi?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.12833</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">This is not what i imagined: Error detection for semantic segmentation through visual dissimilarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Haldimann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cadena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.00676</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Scaling out-of-distribution detection for real-world settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mostajabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.11132</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A baseline for detecting misclassified and out-ofdistribution examples in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02136</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04606</idno>
		<title level="m">Deep anomaly detection with outlier exposure</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Standardized max logits: A simple yet effective approach for identifying unexpected road obstacles in urban-scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">What uncertainties do we need in bayesian deep learning for computer vision?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04977</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Simple and scalable predictive uncertainty estimation using deep ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.01474</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Survey on semantic segmentation using deep learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lateef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ruichek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">338</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A tutorial on energybased learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Predicting structured data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">0</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Training confidence-calibrated classifiers for detecting out-of-distribution samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09325</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A simple unified framework for detecting outof-distribution samples and adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cutpaste: Self-supervised learning for anomaly detection and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Enhancing the reliability of out-of-distribution image detection in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02690</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Detecting the unexpected via image resynthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nakka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Energy-based out-of-distribution detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.03759</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Perturbed and strict mean teachers for semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Photoshopping colonoscopy video frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Maicas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Z C T</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Verjans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep gamblers: Learning to abstain with portfolio theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ueda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Malinin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gales</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.10501</idno>
		<title level="m">Predictive uncertainty estimation via prior networks</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Evaluating bayesian deep learning methods for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mukhoti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12709</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The mapillary vistas dataset for semantic understanding of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rota Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Learning non-convergent non-persistent short-run mcmc toward energy-based model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nijkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09770</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation with cross-consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ouali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hudelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Lost and found: detecting small road hazards for self-driving vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pinggera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Real-world anomaly detection in surveillance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sultani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Fewshot anomaly detection for polyp frames from colonoscopy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Maicas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Z C T</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Verjans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Weaklysupervised video anomaly detection with robust temporal feature magnitude learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Verjans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Constrained contrastive distribution learning for unsupervised anomaly detection and localisation in medical images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Verjans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.13379</idno>
		<title level="m">Revisiting multi-task learning in the deep learning era</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Road anomaly detection by partial image reconstruction with segmentation coupling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>?ipka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chumerin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">O</forename><surname>Reino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Synthesize then compare: Detecting failures and anomalies for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Improving semantic segmentation via video propagation and label relaxation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Pseudoseg: Designing pseudo labels for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.09713</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
