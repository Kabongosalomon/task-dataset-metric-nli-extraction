<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, MANUSCRIPT ID 1 HAGCN : Network Decentralization Attention Based Heterogeneity-Aware Spatiotemporal Graph Convolution Network for Traffic Signal Forecasting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junkyu</forename><surname>Jang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghyuk</forename><surname>Park</surname></persName>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, MANUSCRIPT ID 1 HAGCN : Network Decentralization Attention Based Heterogeneity-Aware Spatiotemporal Graph Convolution Network for Traffic Signal Forecasting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-graph neural network</term>
					<term>graph convolution network</term>
					<term>traffic forecasting</term>
					<term>spatiotemporal network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The construction of spatiotemporal networks using graph convolution networks (GCNs) has become one of the most popular methods for predicting traffic signals. However, when using a GCN for traffic speed prediction, the conventional approach generally assumes the relationship between the sensors as a homogeneous graph and learns an adjacency matrix using the data accumulated by the sensors. However, the spatial correlation between sensors is not specified as one but defined differently from various viewpoints. To this end, we aim to study the heterogeneous characteristics inherent in traffic signal data to learn the hidden relationships between sensors in various ways. Specifically, we designed a method to construct a heterogeneous graph for each module by dividing the spatial relationship between sensors into static and dynamic modules. We propose a network decentralization attention based heterogeneity-aware graph convolution network (HAGCN) method that aggregates the hidden states of adjacent nodes by considering the importance of each channel in a heterogeneous graph. Experimental results on real traffic datasets verified the effectiveness of the proposed method, achieving a 6.35% improvement over the existing model and realizing state-of-the-art prediction performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>ime series forecasting is crucial for dynamically predicting real-world phenomena. Among them, traffic forecasting is an important application of this technique for real-world scenarios. Traffic forecasting aims to predict future traffic conditions based on historical traffic data. Because accurate traffic forecasting can be highly beneficial for controlling urban traffic <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, researchers have used deep learning methods to develop traffic forecasting. In particular, because traffic conditions in neighboring areas may affect each other, a graph convolution network (GCN) is a widely used method to adequately capture spatial correlations for traffic forecasting <ref type="bibr" target="#b2">[3]</ref>. Recent studies have focused on spatiotemporal graph modeling to maximize GCN utilization. For example, DCRNN <ref type="bibr" target="#b3">[4]</ref> models spatial dependencies between nodes using a bidirectional graph random walk, and captures temporal relationships with a recurrent neural network. In addition, GraphWaveNet <ref type="bibr" target="#b4">[5]</ref> and GMAN <ref type="bibr" target="#b5">[6]</ref> have attempted to elaborate spatial-correlation training using adaptive graphs. Furthermore, models such as the DMSTGCN <ref type="bibr" target="#b6">[7]</ref>, ACRGN <ref type="bibr" target="#b7">[8]</ref>, and Z-GCNETs <ref type="bibr" target="#b8">[9]</ref> have been proposed to model the dynamic spatial dependency between traffic nodes.</p><p>Despite efforts to accurately learn spatial dependencies between traffic sensors, there has been little discussion of how to learn relationships between hidden channels that can be extracted from sensors. Traffic sensors typically only capture one or two functions, such as traffic speed or traffic volume. In most existing studies, one or two traffic signal data go through the process of embedding as a hidden channel to learn hidden patterns. But the spatial dependency for each channel of the embedded traffic data is considered to be the same and has not been trained.</p><p>However, hidden patterns of traffic signals can imply different types of information, and the spatial correlation between traffic sensors for each hidden pattern is completely different. For example, suppose that the sensor on the upper left road in <ref type="figure" target="#fig_0">Fig. 1(a)</ref> captured the average traffic speed of the road. However, there are different types of vehicles on the road, such as public transport and private cars, which are hidden patterns for road speed, and they work  together to create traffic speeds. In <ref type="figure" target="#fig_0">Fig. 1(a)</ref>, the traffic patterns generated by public transport and private cars on upper left road are spatially related to the other roads below and to the right, respectively. This is because the roads mainly used by the two types of vehicles are different. As in previous papers, if only one spatial correlation between sensors is learned from the data accumulated by the sensor, different spatial correlations hidden in the data cannot be learned like other spatial relationships hidden in public transportation and private cars in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>. Therefore, it is necessary to discover and learn these hidden relationships in various ways. We hypothesize that learning the spatial relationships of potential patterns of traffic signals separately for each hidden channel as shown in <ref type="figure" target="#fig_0">Fig. 1(b)</ref> can help predict traffic signals.</p><p>In this process, a few problems may exist because of the complexity of accurately designing the spatial relationship of the hidden channels extracted from the nodes. The spatial relationships between nodes are not uniformly dynamic or static. To accurately reflect the spatial relationship between nodes, one must learn the dynamic and static relationships separately. In addition, even if a heterogeneous graph model of the relationship between the hidden features is constructed, the importance of each hidden channel cannot be reflected by the existing GCN method.</p><p>In this study, we propose a heterogeneity-aware GCN for traffic signal prediction using a heterogeneous graph. First, the heterogeneous graph generator aims to accurately model the spatial dependency between sensors using Tucker decomposition. The spatial relationship between each sensor is modeled as a three-dimensional (3D) and four-dimensional (4D) tensor by dividing it into two adjacency matrices that capture static and dynamic relationships. In addition, we propose a heterogeneity-aware channel-attention GCN to reflect the importance of hidden channels in learning spatial relationships through graph convolution. And when we perform channel attention, we conceived of a network decentralization attention module based on network analysis. Attention using network decentralization pooling is modeled to give attention to a hidden channel network that utilizes information from various nodes. In our model, several parallel structures are used to capture various spatiotemporal relationships, and each layer is composed of a block of the proposed heterogeneous graph structure and a heterogeneous channel-attention GCN. The traffic signal was transformed using temporal convolution networks (TCNs) to capture various temporal attributes, and the final traffic signal prediction was obtained using skip connections at each layer.</p><p>The main contributions of this study are summarized as follows:</p><p>?Method. We proposed HAGCN, a learning method for spatial correlation using a heterogeneity-aware graph. HAGCN learns two relationships by designing a static/dynamic generator to accurately learn spatial correlation. We show that this static/dynamic division of spatial correlation is effective in predicting traffic signals.</p><p>?Theory. We provide a heterogeneity-aware GCN that applies channel attention inspired by network decentralization to learn the spatial correlation of hidden channels. We formulate the concept of network decentralization of traditional network analysis and propose a method to give attention to hidden channels that utilize information from various nodes.</p><p>?Experiments. we perform extensive experiments on real traffic data. HAGCN shows state-of-the-art performance in predicting various time intervals for various types of traffic data such as traffic speed and traffic volume prediction. We also verified our proposed channel attention method and static/dynamic module. Finally, a robustness check was performed on noisy data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Time Series and Traffic Prediction</head><p>Time series forecasting has been studied for several decades, and traffic prediction is one of the most active research areas in time series forecasting. Traditionally, timeseries forecasting has been developed using statisticalbased methods such as ARIMA <ref type="bibr" target="#b9">[10]</ref> and VAR <ref type="bibr" target="#b10">[11]</ref>. However, these methods have limitations because they are based on the assumption that the time series is stationary and has a linear combination with the time lag variable. Many researchers have recently used deep learning approaches to represent complex time series relationships. As deep learning methods have emerged as the dominant methodology, recurrent neural networks (RNNs) have become popular for time series prediction. In particular, long short-term memory (LSTM) <ref type="bibr" target="#b11">[12]</ref> and gate recurrent unit (GRU) <ref type="bibr" target="#b12">[13]</ref> architectures for time-series prediction were highlighted by partially solving the instability of the RNN by exploiting the concept of flexible learning based on the memory and forget gates. Subsequently, TCNs using convolution networks were proposed separately from the RNN series method to predict time series. TCN-based algorithms have also been widely used for time series predictions <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. In the case of traffic forecasting, researchers have attempted to consider actual spatial (sensor location) correlations between traffic time series, in addition to the temporal correlations that are generally considered. In these studies, a GCN-based model was used to capture the spatial correlation of regions close to each other based on the assumption that a traffic time series is generated for a specific location <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16]</ref>. Although GCN-based research has achieved state-of-the-art traffic forecasting in recent years, most studies have constructed spatial TCNs using a predefined spatial graph structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Spatio-Temporal Graph Neural Networks</head><p>Advancements in graph neural networks and time series data learning methods have made it possible to perform spatiotemporal learning. In contrast to the relatively simple application of GCNs to train networks using spatial graph structures, researchers have attempted to construct networks that appropriately execute spatio-temporal learning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>In order to capture complex spatial temporal relation between traffic data using GCN, it was essential to accurately check the relationship between nodes by properly learning the adjacency matrix rather than predefining it as in the existing method. Among them, GraphWavenet <ref type="bibr" target="#b4">[5]</ref> used a GCN in the spatial domain and 1D convolution along the time domain to separately learn the adjacency matrix and time series. STSGCN <ref type="bibr" target="#b17">[18]</ref> proposed a method for capturing local correlations through a localized synchronous spatiotemporal graph convolution module independent of global mutual effects. LSGCN <ref type="bibr" target="#b18">[19]</ref> generates features using a novel method called cosine graph attention in spatially gated blocks. AGCRN <ref type="bibr" target="#b7">[8]</ref> learns the spatiotemporal correlation for each node by separating the parameters into spatial and temporal components using node-adaptive parameter learning. SFTGNN <ref type="bibr" target="#b19">[20]</ref> effectively captures hidden spatial dependencies by fusing a spatial graph with a temporal graph. STNN <ref type="bibr" target="#b20">[21]</ref> also designed a novel space time module to learn the local spatio-temporal correlations to attempts to capture spatial and temporal patterns well. Z-GCNETs <ref type="bibr" target="#b8">[9]</ref> incorporate a zigzag topological layer into the GCN. DMSTGCN <ref type="bibr" target="#b6">[7]</ref> is used to generate a dynamic graph and construct a framework that uses auxiliary features separately. In STG-NCDE <ref type="bibr" target="#b21">[22]</ref>, two neural-controlled differential equations for spatiotemporal processing were constructed. Although these methods have significantly contributed to the development of spatiotemporal learning, learning about hidden channels of traffic signals that are essential in spatiotemporal network construction has not been considered. Unlike existing methods, this study proposes a novel heterogeneity-aware and dynamic learning module to accurately capture spatio-temporal relationships for each hidden channel of a traffic signal. Our model is designed to precisely capture the attention of the network for each hidden channel by using the channel attention technique appling network decentralization pooling. In addition, two different modules that learn static and dynamic patterns were created. By fusing and integrating the two types of patterns, the spatial relation between traffic signals is learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARIES</head><p>This section describes the heterogeneous graph and the notation of the traffic features used in this study. Subsequently, we formulated the problem of predicting traffic signals using historical traffic features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Time Series and Traffic Prediction</head><p>In this study, we assume that the traffic network has a dynamic characteristic that varies according to time t and heterogeneous hidden embedding. We define a heterogeneous dynamic traffic network as follows: A heterogeneous dynamic graph is defined as a graph = ( , , ) associated with an edge-type mapping function ?: ? , where V denotes the set of nodes, is the set of edges, and denotes sets containing the weights of connected edges.</p><p>In practice, a node may represent a detection sensor located on a road in a traffic network. Each node records traffic flow data such as vehicle speed and traffic volume at regular intervals. Therefore, at time step t, graph has a feature matrix ? ? where d is the input feature dimension, and N is the number of nodes. Given the graph signals , we pass a linear layer through to create F hidden channels and embed the traffic signal. Therefore, we aim to learn adjacency matrix ? ? ? ? with F heterogeneity properties.</p><p>is a 4D tensor, and the elements , , , denote the edge weights of feature f at time t of vertices and . Furthermore, at time t, the 3D adjacency matrix in graph is defined as .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Problem Definition</head><p>Traffic forecasting aims to predict future traffic signals based on historical traffic features. At time step t, given the adjacency matrix and historical graph signals of P steps, we want to learn mapping function f to predict the graph signal of the next step Q. This can be defined as</p><formula xml:id="formula_0">[ ? +1: , ] ? +1: + (?)</formula><p>where ? +1: ? ? ? and +1: + ? ? ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHODOLOGY</head><p>In this section, we introduce our proposed model in detail. First, we explain the construction method of a heterogeneous graph that separately captures the correlation between sensors for static and dynamic graphs. Next, we describe the network decentralization channel attention based heterogeneity-aware GCN used to learn spatiotemporal dependencies while capturing the relative importance of the hidden channels. Finally, we briefly describe the architecture of the framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Heterogeneity-Aware Graph Constructor</head><p>The spatial correlation between traffic sensors changes over time owing to the dynamic nature of traffic. Because the spatial correlation between hidden channels of traffic signals may vary across features, it is essential to properly design the adjacency matrix of the graph. In previous studies, the spatial correlations between sensors were predefined using a two-dimensional matrix. Although some studies have considered dynamic correlation for learning, the homogeneity-aware graph does not consider the heterogeneity of correlations between unknown hidden traffic channels. In this study, the relationship between the nodes of a heterogeneous traffic signal is captured by dividing it into dynamic and static relationships, regardless of time. We divide the 4D adjacency matrix into two matrices: the 4D matrix , which captures the dynamic spatial relationship, and the 3D matrix , which captures the static spatial relationship, and define it as follows.</p><p>, , , = , , , + , ,</p><p>where ? ? ? ? captures the dynamic relationship between sensors as a 4D tensor, and ? ? ? captures the static relationship between the sensors as a 3D tensor. where F is the number of hidden channels, N the number of nodes, and T the number of time intervals. and were utilized through graph convolution in the dynamic and static modules of the heterogeneity-aware channel attention graph convolution network (HAGCN) model network, respectively.</p><p>The easiest way to learn these two adjacency matrices is to determine the relationship between nodes by directly assigning parameters to all elements of the dynamic and static matrices. However, with this assignment, the computational complexity of the adjacency tensor is ( + ), If different dynamic correlations are learned for all time signals of the train traffic signal, not only is the dynamic correlation undetermined when applied to the validation and test sets, but the number of parameters that need to be learned increases in proportion to the number of training time intervals. Therefore, we assume that traffic signals have a certain periodicity and that traffic signals detected during the same time of day share the same dynamic correlation. and are the numbers of original and target nodes, respectively; and m is the embedding dimension to reduce the dimension.</p><formula xml:id="formula_2">, , = (0, ? ? ? , ,<label>, , , ? , , , , =1 =1 =1</label></formula><formula xml:id="formula_3">) (?) , , , = ? ? ? ? , , ,<label>, , , ? , , , , =1 =1 =1</label></formula><p>we can train 3D tensor and 4D tensor with large n umber of parameters by training only E which is the embedding vectors of tucker decomposition. In fact, traffic signal relationships hide many repeating patterns. Information concerning duplicate patterns can be filtered via dimensionality reduction using the Tucker decomposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Heterogeneity-Aware Channel Attention GCN</head><p>The spatial correlation of the hidden channels of different nodes can contribute to improving the prediction performance of the traffic signal that we want to predict. In previous studies, graph convolution has been used to extract and utilize features unique to each node by aggregating the spatial correlation and signals of neighboring nodes. However, in this study, we propose a dynamic-heterogeneity-aware GCN to model the heterogeneous properties of the hidden features of nodes with different spatial dependencies for each channel and the dynamic correlation of nodes.</p><p>First, the heterogeneity-aware graph convolution method, which aggregates and updates the hidden state of nodes in each learning step using the 3D adjacency matrix ? representing the heterogeneous graph, is defined as:  where K is the maximum diffusion step, H l is the temporal convolution output of the -th block of the l-th layer, ? is the element-wise product, and is the convolution parameter of the k-th diffusion step. ? is the adjacency matrix used for graph convolution.</p><p>In our model, the adjacency matrix uses to capture dynamic relationships and to capture static spatial dependencies. First, when generating and using the adjacency matrix in the GCN in the static module, is substituted for ? and used for graph convolution. However, when using in the graph convolution in the dynamic module to learn dynamic relationships, graph convolution is used by considering only ?( ) , which corresponds to the dependency of time t in . Using the function ?( ) to calculate the number of time intervals from the period at time t, we utilize 3D tensor ?( ) .</p><p>In this study, the channel attention function ( ? ) is used in the adjacency matrix ? to assign weight to the hidden channel using spatial correlation with more neighbor nodes. In particular, the importance of channels in the 3D heterogeneity-aware adjacency matrix differs. For example, because one hidden channel has a frequent and large spatial correlation with neighboring nodes, many edge weights are greater than zero. Such a channel with frequent correlation can be considered valuable in utilizing graph convolution, which is advantageous for the use of information from the neighboring node when an edge weight appears. In addition, because we argue that channels in the adjacency matrix influence each other, we designed an attention module using a layer that calculates the interaction between channels. Consequently, channel attention ( ? ) is defined as follows :</p><formula xml:id="formula_4">( ? ) = ( { , } ( ( ?)))<label>(7)</label></formula><p>where ? is a softmax function and ( ?) is a channelwise aggregate function. In general, in the channel attention module in the field of computer vision, each channel means one image convolution map. Therefore, channel attention is calculated for each channel using methods such as global average pooling or generalized mean pooling as a channelwise aggregate function to give attention according to the activation level of the entire channel pixel <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>. However, since a channel in a heterogeneous graph convolution network means a network of each hidden channel, the aggregate function must be tuned differently. Network decentralization is a measure of how non-centralized the most central node is compared to all other nodes <ref type="bibr" target="#b11">[12]</ref>. Therefore, if the network decentralization is high, it means that the graph is organically connected with various nodes. We define the network decentralization function ( ?) as follows to give attention to hidden channels that actively utilize information from various nodes when performing graph convolution.</p><formula xml:id="formula_5">( ? )[ ] = 1 ? ? ( ? () ? ? ( )) ? ? ? ? ? ( () ? ( )) ? (8)</formula><p>where c is the each channel, and ( ? )[ ]is the c-th element of the aggregate function.</p><p>is node centrality function of graph G. We use degree centrality that means the sum of degrees of nodes among various node centrality functions to calculate network decentralization. ? stands for argmax which is the vertex with the highest degree centrality in graph G. In order to have the maximum value of the decentralization (8)'s denominator, it should be an N-star graph in which all edges are gathered at one node. Therefore, when calculating (8) considering degree centrality as follows:</p><formula xml:id="formula_6">( ?)[c] = 1 ? N ? max ? (? ? , , =1 ) ? ? ? ? , , =1 =1 (N ? 1)(N ? 2) ? max ? , ( ? , , )<label>(9)</label></formula><p>Let = ( ?), ? F ? 1, and F is the number of hidden channels. { 1 , 2 } takes the form </p><p>To avoid the high complexity of the attention module, the sizes of 1 and 2 were designated as F ? and ? F, respectively <ref type="bibr" target="#b9">[10]</ref>. { 1 , 2 } uses one fully connected (FC) layer to project the output feature of the pooling layer to a low-dimensional space. Subsequently, it is re-mapped to indirectly create a correspondence between the output feature of the pooling layer and attention weight of the adjacency matrix channel. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Temporal Convolution Layer</head><p>The traffic signal of a node is highly correlated with its historical data. Dilated causal convolution with a TCN was used to capture the temporal trends of the nodes. Dilated causal convolution increases the depth of the TCN layer, thereby enabling exponentially larger receptive fields. Unlike RNN-based time-series approaches, dilated causal convolution networks adequately process sequences over long distances in a non-recursive manner. Therefore, dilated causal convolution networks are suitable for data with periodicity, are easy to compute in parallel, and alleviate the problem of gradient explosion. However, the gating mechanism is important in RNN-based approaches. Their ability to control the information flow through layers in temporal convolutional networks has been demonstrated <ref type="bibr" target="#b12">[13]</ref>. Therefore, we used the gated TCN in our model to learn complex time dependencies.</p><formula xml:id="formula_8">H l b = tanh( 1 * ) ??(Z 2 * )<label>(11)</label></formula><p>where 1 and Z 2 are model parameters, ? is the elementwise product, and ?(?) is a sigmoid function that determines the proportion of information transferred to the next layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Other Components</head><p>In the proposed model, F l is the input of the b-th block of the l-th layer and the output of the ( ? 1)-th block of the -th layer. The data 1 1 input to the first block of the first layer are transformed by the FC layer as follows:</p><formula xml:id="formula_9">1 1 = ? +1: +<label>(12)</label></formula><p>In this model, the output F b+1 of the block is calculated by using the residual link at the end of each block. Subsequently, the sum of the outputs from the static and dynamic modules of the last block of each layer was used as the skip connection, and the concatenated value of the skip connections of each layer was sent to the output layer and used for the final prediction. The output layer was constructed using two FC layers. We chose the L1 loss as our training objective and optimized the loss for multistep prediction. Thus, the loss function of our model for multistep traffic prediction can be formulated as follows :</p><formula xml:id="formula_10">( ) = ? | ??| + = +1 (14)</formula><p>where represents all learnable parameters, ,? ? and ,? ? are outputs of the static and dynamic modules of the last block of layer , respectively, and ? is the concatenate function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we compare the performance of the proposed method with that of existing state-of-the-art methods using PeMSD4 and PeMSD8. Subsequently, we verify whether the proposed module and method improved the performance of the model. Finally, we test the performance of the model on noisy data using a robustness check.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We conducted experiments on two public real-world traffic datasets, PeMSD4 and PeMSD8, to evaluate task performance. PeMS is the Caltrans Performance Measure System (PeMS), which measures California's highway traffic in real time every 30 s <ref type="bibr" target="#b13">[14]</ref>.</p><p>PeMSD4: The pemsd4 dataset contains traffic flow data from the San Francisco Bay area. Data from 307 sensors were collected from January 1, 2018, to February 28, 2018.</p><p>PeMSD8: The pemsd8 dataset contains traffic flow signals from the San Bernardino area. Data from 170 sensors were selected from July 1, 2016, to August 31, 2018.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Preprocessing</head><p>For all datasets, we filled in the missing values using the linear interpolation method and used the training set data to perform normalization using the Z-score method for training.</p><p>To facilitate the training of our model after injecting the initial edge weights into the adjacency matrices and , we applied Tucker decomposition by defining the initial adjacency matrix to apply the initial parameter to E(?), which is the embedding parameter constituting and for training. The initial adjacency matrix is defined as follows: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Baselines</head><p>To verify the performance of our model, we selected models that include traditional statistical methods, various graph neural network models, and methods proposed in recent key papers as benchmarks. The benchmark methods are described as follows:</p><p>? DCRNN [4]: A model for achieving multistep prediction by employing the diffusion process and graph convolution. A diffusion-convolutional neural network was built by combining a GCN and recurrent models using the encoder-decoder method.</p><p>? GMAN <ref type="bibr" target="#b5">[6]</ref>: A multi-attention model based on spatial and temporal embeddings of traffic signals.</p><p>? GraphWavenet <ref type="bibr" target="#b4">[5]</ref>: A model that applies an adaptive graph network and combines dilated causal convolution and graph convolution.</p><p>? DMSTGCN <ref type="bibr" target="#b6">[7]</ref>: A spatiotemporal network that generates a dynamic graph and introduces a framework that uses auxiliary features separately.</p><p>? AGCRN <ref type="bibr" target="#b7">[8]</ref>: A model that uses node adaptive parameter learning to separate parameters into spatial and temporal components.</p><p>? Z-GCNETs <ref type="bibr" target="#b8">[9]</ref>: A spatiotemporal graph that integrates a time-aware zigzag topological layer with graph convolution.</p><p>? STG-NCDE <ref type="bibr" target="#b14">[15]</ref>: Two neural-controlled differential equations for spatiotemporal processing were constructed and studied in a new manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Experimental Setup</head><p>Our experiments were performed in a computer environment with an Intel? Xeon? Gold 5119T CPU at 1.90</p><p>GHz and four NVIDIA RTX 3090 GPU cards. In our model, to span the length of the input sequence, we used eight dilations using the dilation factor <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> for each of the two blocks in the four layers. Because of the characteristics of our model, the output channel sizes of the dilated and graph convolutions were set to 32. The batch size was set to 64 and the initial learning rate was set to 0.001. The core dimensions of the Tucker decomposition were reduced to 40, and the maximum depth of the graph convolution layers was set to 2. The model was optimized using the Adam optimizer, and all the datasets were split for training, validation, and testing at a ratio of 6:2:2. All the models were tested over five iterations, and the average performance was recorded. Three evaluation metrics were applied to evaluate the performance of each model: mean absolute error (MAE), mean absolute percentage error (MAPE), and root mean squared error (RMSE).   <ref type="figure" target="#fig_0">1291 0.0217 2.3255 1.3336 0.0270 2.9264 1.4667 0.0306 3.2949 1.5677 0.0334</ref>   <ref type="figure" target="#fig_0">HAGCN  0.9377 0.0181 2.0494 1.1136 0.0227 2.6491 1.2281 0.0260 3.0427 1.3133 0.0284 3.</ref>2770</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Comparison with Baseline Methods</head><p>In this study, traffic speed prediction was used as the main task, but in studies such as Z-GCNETs and STG-NCDE, traffic volume prediction was used as the main task. Therefore, traffic volume prediction was also compared with the benchmark models and showed superior performance. This implies that our model works even on a general task. The DCRNN model is highly dependent on a predefined graph; therefore, it does not accurately capture the spatial correlation between the nodes. Therefore, its performance was slightly lower than that of the other benchmark models. In the GMAN and GraphWavenet methods, a marginally better performance was achieved because modeling using an adaptive graph was performed to precisely capture the spatial correlation between nodes that could not be captured by the DCRNN. In addition, the DMSTGCN, ACRGN, and Z-GCNET models demonstrated high performance by designing networks that effectively utilize the dynamic characteristics between nodes in different ways. STG-NCDE achieved competitive performance by applying a neural-controlled differential equation. However, the improvement gain of our model for the next most accurate method was between 4.87% and 6.35% in the MAE of horizon 12 for the PeMSD4 and PeMSD8 datasets, and our model outperformed other models over horizons 3 to 9. Our method for capturing subtle relationships by defining separate spatial dependencies for each hidden channel performed better than that achieved by providing the same graph to each channel. In addition, the method of separately calculating the importance of the adjacency matrix channel at each instance wherein time-graph convolution is performed also significantly contributed to the performance of this method. Our experiments showed that considering these factors can provide significant advantages for predicting traffic signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Evaluating Effectiveness of Key Designs</head><p>To better understand the importance of the different components of our model, we conducted ablation studies, and the corresponding results are presented in TABLE 2. The most essential part of the proposed model is the division of the spatial correlation between nodes into static and dynamic adjacency matrices and the graph channel attention module used for graph convolution. Therefore, to verify the validity of the core module, we constructed and trained three types of models and compared their results.</p><p>?without dynamic module: In this model, we trained using only the static graph module to validate the dynamic graph module.</p><p>?without static module: In this model, we trained using only the dynamic graph module to demonstrate the validity of the static graph module.</p><p>?without heterogeneous graph: This model used general graph convolution after generating a dynamic graph and static graph by changing the adjacency matrix to a homogenous graph.</p><p>?without channel attention: In this model, we used only simple graph convolution to demonstrate the importance of graph channel attention when performing graph convolution.</p><p>?without decentralization pooling layer: In this model, we do not use network decentralization pooling when using channel attention. We train with channel attention using general global average pooling.  <ref type="figure" target="#fig_0">9377 0.0181 2.0494 1.1136 0.0227 2.6491 1.2281 0.0260 3.0427 1.3133 0.0284 3.</ref>2770</p><p>The results show that all the major modules contribute to the learning outcomes of the model. Learning with the dynamic module alone achieved a better performance than learning with only the static module. However, we observed that the performance of the model improved significantly only when two modules were used simultaneously. In addition, using the channel attention module for graph convolution outperformed the model without attention, with a relative gain of 0.6% to 0.9% in MAE for each dataset. This indicates the importance of modeling the attention of adjacency matrix channels when using heterogeneityaware graph neural networks. Moreover, using the network decentralization-based attention module, MAE has a relative gain between 1.0% and 1.8% for each data set compared to the global average pooling module. These results mean that giving attention to the proposed decentralization module significantly affects the results of the prediction model. The heatmaps in <ref type="figure" target="#fig_8">Fig. 4</ref> were trained with the same initial weight, but compared to the completed heatmap, they were trained with completely different weights. It can be seen that the spatial correlation between each sensor was well trained by dividing it into a static relationship and dynamically changing relationship. We visualized and compared the results for some nodes on the PeMSD4 test dataset. In <ref type="figure">Fig. 5</ref>, it can be observed that the prediction results of our proposed model estimate the true value better. This is the result of HAGCN organically using information from other nodes when performing traffic forecasting. <ref type="figure">Fig. 6</ref> and <ref type="figure">Fig. 7</ref> are heatmaps of channel attention values of all HAGCN blocks in the static/dynamic module. The xaxis means each channel and the y-axis means each HAGCN block. Each time graph convolution occurs, the importance of each hidden channel changes. This is because the characteristic of the hidden feature that is calculated every time a temporal relationship is learned from the HAGCN block to the TCN changes. Since we set the number of hidden channels to 32 when learning the  PeMSD4 dataset, if the attention values of all channels are the same or similar, it should have a value close to 1/32. However, in <ref type="figure">Fig. 6</ref> and <ref type="figure">Fig. 7</ref>, whenever our model performs graph convolution, it captures changes in channel attention well and has various attention values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8">How did heterogeneity channel attention work?</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.9">Traffic Volume Task Comparision</head><p>To examine whether the HAGCN model can predict other traffic signals in addition to the traffic speed prediction task, we have also experimented with the traffic volume prediction in the PeMSD dataset. The experimental settings for the traffic volume prediction task were the same, except that the initial learning rate was 0.003 and the size of the output channels of the dilated and graph convolutions was set to 64. All the models were tested for five iterations, and the average performance was recorded in TA-BLE 3. The improvement gain of our model compared to the second most accurate method, the STG-NCDE model, was between 2.65% and 3.88% in the MAE of horizon 12 for the PeMSD4 and PeMSD8 datasets, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.10">Robustness Check</head><p>To test the robustness of our model under noisy conditions, it was trained with Gaussian noise. The added noise followed a Gaussian density with zero mean and a variance of 2,4. TABLE 4 presents an MAE comparison with the benchmark model with two different levels of noise training. It can be seen that our model consistently outperformed the other models on both the PeMSD4 and PeMSD8 datasets when noise was added. Moreover, compared to other methods, it can be seen that our model's performance deteriorates slightly when Gaussian noise is added gradually. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this study, we presented a novel framework for modeling spatiotemporal graphs. By dividing the spatial relationship between the sensors into two static/dynamic relationships, we proposed a generator that creates a heterogeneous graph because the spatial dependency of each hidden channel is different. In addition, using the advantage of a heterogeneous graph, we proposed an effective framework for learning spatial dependency by automatically learning the importance of each hidden channel during graph convolution. Our model achieves state-of-the-art results for two sets of traffic data. We consider that the use of a GCN by appropriately generating heterogeneity-aware graphs utilizing hidden channels is a promising method for spatiotemporal processing and improves prediction performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>TFig. 1 .</head><label>1</label><figDesc>Heterogeneous spatial correlation. (a) A road network that has a variety of unknown relationships as features measured by sensors. (b) Transform the input data into c channels and capture the unknown relation using a heterogeneous graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig 2 .</head><label>2</label><figDesc>Network architecture of HAGCN. The HAGCN block of the framework consists of parallel parts that learn two spatial correlations of static/dynamic modules. Each static/dynamic module consists of a part that learns temporal correlation as a temporal convolution layer and a heterogeneity-aware graph convolution layer that learns a graph generated by a generator. For the final prediction, the ouput captured by the last block's graph convolution of each layer is taken and used for prediction Heterogeneous spatial correlation. (a) A road network that has a variety of unknown relationships as features measured by sensors. (b) Transform the input data into c channels and capture the unknown relation using a heterogeneous graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>{ 1 , 2 }</head><label>12</label><figDesc>(y) = 2 max (0, 1 )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig 3 .</head><label>3</label><figDesc>Network decentralization based channel attention method</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 4</head><label>4</label><figDesc>shows a part of an adjacency matrix trained with a static/dynamic generator trained on the PeMSD4 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig 4 .Fig 5 .Fig 6 .Fig 7 .</head><label>4567</label><figDesc>(a) Heatmap of the learned static module's adjacency matrix for the first 50 nodes in the first channel (b) Time-averaged heatmap of the adjacency matrix of dynamic module for the first 50 nodes Traffic speed forecasting visualization example Channel attention values in 8 HAGCN blocks of the static module's adjacency matrix Channel attention values in 8 HAGCN blocks of the dynamic module's adjacency matrix</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>compares the proposed HAGCN and benchmark models for traffic speed forecasting. TABLE ? reveals that the proposed model consistently outperformed the existing state-of-the-art models on both the PeMSD4 and PeMSD8 datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 1 PERFORMANCE</head><label>1</label><figDesc>COMPARISON OF DIFFERENT APPROACHES FOR TRAFFIC SPPED PREDICTION ON THE PEMSD4 AND PEMSD8</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell>MAE</cell><cell>Horizon 3 MAPE RMSE MAE</cell><cell>Horizon 6 MAPE RMSE MAE</cell><cell>Horizon 9 MAPE RMSE MAE</cell><cell>Horizon 12 MAPE RMSE</cell></row><row><cell></cell><cell>DCRNN</cell><cell cols="5">1.6247 0.0382 3.5105 1.9831 0.0412 4.0364 2.2974 0.0503 4.3821 2.4154 0.0529 4.7099</cell></row><row><cell></cell><cell>GMAN</cell><cell cols="5">1.3072 0.0269 2.6123 1.5023 0.0304 3.3124 1.6842 0.0351 3.8146 1.8462 0.0405 4.2783</cell></row><row><cell></cell><cell>GWNET</cell><cell cols="5">1.2832 0.0248 2.5261 1.5405 0.0311 3.2851 1.7442 0.0353 3.8592 1.9189 0.0399 4.3187</cell></row><row><cell></cell><cell cols="6">DMSTGCN 1.2080 0.0234 2,5053 1.4663 0.0295 3.2636 1.6661 0.0346 3.8278 1.8356 0.0390 4.2697</cell></row><row><cell>PeMSD4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>ACGRN</cell><cell cols="5">1.2409 0.0251 2.5817 1.4301 0.0298 3.1458 1.5627 0.0333 3.8354 1.6663 0.0360 3.8354</cell></row><row><cell></cell><cell>Z-GCNETs</cell><cell cols="5">1.1900 0.0235 2.4342 1.3951 0.0287 3.0220 1.5364 0.0325 3.7452 1.6480 0.0355 3.7452</cell></row><row><cell></cell><cell cols="6">STG-NCDE 1.1724 0.0221 2.3867 1.3936 0.0283 3.0545 1.5357 0.0316 3.4675 1.6756 0.0349 3.7424</cell></row><row><cell></cell><cell>HAGCN</cell><cell>1.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 2 ABALATION</head><label>2</label><figDesc>STUDY OF THE NETWORK ARCHITECTURE</figDesc><table><row><cell>Data set</cell><cell>Method</cell><cell>MAE</cell><cell>Horizon 3 MAPE RMSE MAE</cell><cell>Horizon 6 MAPE RMSE MAE</cell><cell>Horizon 9 MAPE RMSE MAE</cell><cell>Horizon 12 MAPE</cell><cell>RMS E</cell></row><row><cell></cell><cell>w/o dynamic</cell><cell cols="6">1.2406 0.0251 2.5695 1.4389 0.0302 3.1200 1.5763 0.0339 3.4983 1.6814 0.0367 3.7738</cell></row><row><cell></cell><cell>w/o static</cell><cell cols="6">1.2908 0.0272 2.6991 1.4636 0.0314 3.1733 1.5801 0.0343 3.4906 1.6701 0.0368 3.7280</cell></row><row><cell>PeM</cell><cell>w/o heterogene-ous</cell><cell cols="6">1.1949 0.0235 2.4694 1.3916 0.0285 2.9917 1.5387 0.0334 3.4138 1.6421 0.0356 3.7001</cell></row><row><cell>SD4</cell><cell>w/o channel at-tention</cell><cell cols="6">1.2532 0.0258 2.5946 1.4276 0.0301 3.0904 1.5439 0.0332 3.4245 1.6328 0.0355 3.6687</cell></row><row><cell></cell><cell>w/o decentral ization pooling</cell><cell cols="6">1.1515 0.0221 2.3399 1.3479 0.0272 2.9247 1.4776 0.0308 3.3026 1.5829 0.0336 3.5812</cell></row><row><cell></cell><cell>HAGCN</cell><cell cols="6">1.1291 0.0217 2.3255 1.3336 0.0270 2.9264 1.4667 0.0306 3.2949 1.5677 0.0334 3.5963</cell></row><row><cell></cell><cell>w/o dynamic</cell><cell cols="6">1.0267 0.0202 2.1675 1.2039 0.0247 2.7097 1.3261 0.0280 3.0806 1.4216 0.0307 3.3588</cell></row><row><cell></cell><cell>w/o static</cell><cell cols="6">1.0541 0.0221 2.3332 1.1997 0.0287 2.8016 1.2982 0.0287 3.1212 1.3744 0.0305 3.3555</cell></row><row><cell>PeM SD8</cell><cell>w/o heterogene-ous w/o channel at-tention</cell><cell cols="6">0.9654 0.0183 2.0625 1.1494 0.0233 2.6636 1.2740 0.0272 3.0912 1.3677 0.0302 3.3950 0.9932 0.0194 2.1142 1.1547 0.0242 2.7087 1.2648 0,0270 3.0813 1,3502 0.0295 3.3186</cell></row><row><cell></cell><cell>w/o decentrali ization pooling</cell><cell cols="6">0.9497 0.0181 2.0460 1.1267 0.0229 2.6512 1.2461 0.0265 3.0677 1.3376 0.0291 3.2915</cell></row><row><cell></cell><cell>HAGCN</cell><cell>0.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 3 PERFORMANCE</head><label>3</label><figDesc>COMPARISON FOR TRAFFIC VOLUME PREDIC-TION ON THE PEMSD4 AND PEMSD8 DATASETS</figDesc><table><row><cell>Model</cell><cell></cell><cell>PeMSD4</cell><cell></cell><cell></cell><cell>PeMSD8</cell><cell></cell></row><row><cell>Horizon 12</cell><cell>MAE</cell><cell>RMSE</cell><cell>MAPE</cell><cell>MAE</cell><cell cols="2">RMSE MAPE</cell></row><row><cell>Graph WaveNet</cell><cell>24.89</cell><cell>39.66</cell><cell>0.1729</cell><cell>18.28</cell><cell>30.08</cell><cell>0.1215</cell></row><row><cell>AGCRN</cell><cell>19.83</cell><cell>32.26</cell><cell>0.1297</cell><cell>15.95</cell><cell>25.22</cell><cell>0.1009</cell></row><row><cell>Z-GC NETs</cell><cell>19.50</cell><cell>31.61</cell><cell>0.1278</cell><cell>15.76</cell><cell>25.11</cell><cell>0.1001</cell></row><row><cell>STG-NCDE</cell><cell>19.21</cell><cell>31.09</cell><cell>0.1276</cell><cell>15.45</cell><cell>24.81</cell><cell>0.0990</cell></row><row><cell>HAGCN</cell><cell>18.70</cell><cell>30.61</cell><cell>0.1260</cell><cell>14.85</cell><cell>23.87</cell><cell>0.0957</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 4 ROBUSTNESS CHECK</head><label>4CHECK</label><figDesc></figDesc><table><row><cell>Dataset</cell><cell>Noise</cell><cell>AGCRN</cell><cell>Z_GCNETs</cell><cell>HAGCN</cell></row><row><cell></cell><cell>X</cell><cell>1.6663</cell><cell>1.6480</cell><cell>1.5677</cell></row><row><cell>PeMSD4</cell><cell>N(0,2)</cell><cell>1.7140</cell><cell>1.7118</cell><cell>1.6497</cell></row><row><cell></cell><cell>N(0,4)</cell><cell>1.7984</cell><cell>1.7957</cell><cell>1.7204</cell></row><row><cell></cell><cell>X</cell><cell>1.4077</cell><cell>1.4024</cell><cell>1.3133</cell></row><row><cell>PeMSD8</cell><cell>N(0,2)</cell><cell>1.5129</cell><cell>1.4856</cell><cell>1.4010</cell></row><row><cell></cell><cell>N(0,4)</cell><cell>1.5769</cell><cell>1.5709</cell><cell>1.4607</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Lc-rnn: A deep learning model for traffic speed prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in IJCAI</title>
		<imprint>
			<biblScope unit="page" from="3470" to="3476" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deepstd: Mining spatio-temporal disturbances of multiple context factors for citywide traffic flow prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3744" to="3755" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shahabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Graph WaveNet for Deep Spatial-Temporal Graph Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 28th International Joint Conference on Artificial Intelligence (IJCAI), 2019: International Joint Conferences on Artificial Intelligence Organization</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Gman: A graph multiattention network for traffic prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="1234" to="1241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dynamic and Multi-faceted Spatio-temporal Deep Learning for Traffic Speed Forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="547" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive Graph Convolutional Recurrent Network for Traffic Forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Z-GCNETs: Time Zigzags at Graph Convolutional Networks for Time Series Forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Segovia-Dominguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">R</forename><surname>Gel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.04100</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Modeling and forecasting vehicular traffic flow as a seasonal ARIMA process: Theoretical basis and empirical results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hoel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of transportation engineering</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="664" to="672" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Vector autoregressive models for multivariate time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zivot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Modeling Financial Time Series with S-Plus?</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="385" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2014 Workshop on Deep Learning</title>
		<imprint>
			<date type="published" when="2014-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Think globally, act locally: A deep neural network approach to high-dimensional time series forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Trellis Networks for Sequence Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Lc-rnn: A deep learning model for traffic speed prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in IJCAI</title>
		<imprint>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spatiotemporal graph structure learning for traffic forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="1177" to="1185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spatial-temporal synchronous graph convolutional networks: A new framework for spatial-temporal network data forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="914" to="921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">LSGCN: Long Short-Term Traffic Prediction with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="2355" to="2361" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spatial-Temporal Fusion Graph Neural Networks for Traffic Flow Forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page" from="4189" to="4196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Space Meets Time: Local Spacetime Neural Network For Traffic Flow Forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="817" to="826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Park</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.03558</idno>
		<title level="m">Graph Neural Controlled Differential Equations for Traffic Forecasting</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Tensor decompositions and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Kolda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Bader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM review</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="455" to="500" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Orthogonal convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11505" to="11515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Centrality in social networks conceptual clarification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social networks</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="215" to="239" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">WaveNet: A generative model for raw audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oord</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>SSW</publisher>
			<biblScope unit="volume">125</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Freeway performance measurement system: mining loop detector data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Petty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Skabardonis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Varaiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Record</title>
		<imprint>
			<biblScope unit="volume">1748</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="96" to="102" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
