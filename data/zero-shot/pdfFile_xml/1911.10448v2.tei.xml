<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">3FabRec: Fast Few-shot Face alignment by Reconstruction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Browatzki</surname></persName>
							<email>browatbn@korea.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Artificial Intelligence</orgName>
								<orgName type="institution">Korea University</orgName>
								<address>
									<settlement>Seoul</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wallraven</surname></persName>
							<email>wallraven@korea.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Artificial Intelligence</orgName>
								<orgName type="institution">Korea University</orgName>
								<address>
									<settlement>Seoul</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">3FabRec: Fast Few-shot Face alignment by Reconstruction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Figure 1: The framework of 3FabRec consisting of: (leftmost box) a first, unsupervised training stage that trains a low-dimensional latent space via an adversarial (generative) autoencoder on a large dataset of unlabeled faces, (middle box) subsequent supervised training with few annotated faces. The rightmost box shows results from testing the framework trained on only the 10 images of the middle box with original faces (top row), reconstructed faces via the autoencoder (middle row), and confidence heatmaps (bottom row).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Current supervised methods for facial landmark detection require a large amount of training data and may suffer from overfitting to specific datasets due to the massive number of parameters. We introduce a semi-supervised method in which the crucial idea is to first generate implicit face knowledge from the large amounts of unlabeled images of faces available today. In a first, completely unsupervised stage, we train an adversarial autoencoder to reconstruct faces via a low-dimensional face embedding. In a second, supervised stage, we interleave the decoder with transfer layers to retask the generation of color images to the prediction of landmark heatmaps. Our framework (3FabRec) achieves state-of-the-art performance on several common benchmarks and, most importantly, is able to maintain impressive accuracy on extremely small training sets down to as few as 10 images. As the interleaved layers only add a low amount of parameters to the decoder, inference runs at several hundred FPS on a GPU.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Accurate and robust localization of facial landmarks is a critical step in many existing face processing applications, including tracking, expression analysis, and face identification. Unique localization of such landmarks is severely affected by occlusions, partial face visibility, large pose variations, uneven illumination, or large, non-rigid deformations during more extreme facial expressions <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b45">46]</ref>. These challenges have to be overcome in order to achieve a low landmark localization error, implying high robustness to appearance changes in faces while guaranteeing high localization accuracy for each landmark.</p><p>The recent advances in deep learning techniques <ref type="bibr" target="#b2">[3]</ref> coupled with the availability of large, annotated databases have allowed steady progress with localization accuracy on a typical benchmark increasing by 100% (from <ref type="bibr" target="#b52">[53]</ref> to <ref type="bibr" target="#b50">[51]</ref> see below for more related work). Most approaches use a combination of highly-tuned, supervised learning schemes in order to achieve this performance and almost always are specifically optimized on the particular datasets that are tested, increasing the potential of overfitting to that dataset <ref type="bibr" target="#b6">[7]</ref>. Similarly, it has been shown that annotations in datasets can be imprecise and inconsistent (e.g., <ref type="bibr" target="#b13">[14]</ref>).</p><p>Given that in addition to the existing annotated facial landmark datasets, there is an even larger number of datasets available for other tasks (face detection, face identification, facial expression analysis, etc.), it should be possible to leverage the implicit knowledge about face shape contained in this pool to both ensure better generalizability across datasets and easier and faster, few-shot training of landmark localization. Here, we present such a framework that is based on a two-stage architecture (3FabRec, see Figs. <ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2)</ref>: the key to the approach lies in the first, unsupervised stage, in which an (generative) adversarial autoencoder <ref type="bibr" target="#b29">[30]</ref> is trained on a large dataset of faces that yields a low-dimensional embedding capturing "face knowledge" <ref type="bibr" target="#b47">[48]</ref> from which it is able to reconstruct face images across a wide variety of appearances. With this embedding, the second, supervised stage then trains the landmark localization task on annotated datasets, in which the generator is retasked to predict the locations of a set of landmarks by generating probabilistic heatmaps <ref type="bibr" target="#b4">[5]</ref>. This two-stage approach is a special case of semi-supervised learning <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b60">61]</ref> and has been successful in other domains, including general network training <ref type="bibr" target="#b19">[20]</ref>, text classification <ref type="bibr" target="#b21">[22]</ref> and translation <ref type="bibr" target="#b12">[13]</ref>, and visual image classification <ref type="bibr" target="#b55">[56]</ref>.</p><p>In the current study, we show that our method is able to achieve state-of-the-art results running at &gt;300 FPS on the standard benchmark datasets. Most importantly, it yields impressive localization performance already with a few percent of the training data -beating the leading scores in all cases and setting new standards for landmark localization from as few as 10 images. The latter result demonstrates that landmark knowledge has, indeed, been implicitly captured by the unsupervised pre-training. Additionally, the reconstructed autoencoder images are able to "explain away" extraneous factors (such as occlusions or make-up), yielding a best-fitting face shape for accurate localization and adding to the explainability of the framework.</p><p>Source code is available at https://github.com/ browatbn2/3FabRec.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Before the advent of deep learning methods, explicitlyparametrized landmark models such as active shape <ref type="bibr" target="#b8">[9]</ref>, active appearance <ref type="bibr" target="#b7">[8]</ref> or cascade regression models <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b52">53]</ref> provided the state-of-the-art in facial landmark detection. Current models using deep convolutional neural networks, however, quickly became the best-performing approaches, starting with deep alignment networks <ref type="bibr" target="#b43">[44]</ref>, fullyconvolutional networks <ref type="bibr" target="#b26">[27]</ref>, coordinate regression mod-els <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b46">47]</ref>, or multi-task learners <ref type="bibr" target="#b36">[37]</ref>, with the deep networks being able to capture the pixel-to-landmark correlations across face appearance variations.</p><p>The recent related work in the context of our approach can be structured into supervised and semi-supervised approaches (for a recent, interesting unsupervised method -at lower performance levels -see <ref type="bibr" target="#b44">[45]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Supervised methods</head><p>Several recent, well-performing supervised methods are based on heatmap regression, in which a deep network will infer a probabilistic heatmap for each of the facial landmarks with its corresponding maximum encoding the most likely location of that landmark <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b26">27]</ref> -an approach we also follow here. In order to provide additional geometric constraints, extensions use an active-appearance-based model-fitting step based on PCA <ref type="bibr" target="#b30">[31]</ref>, explicit encoding of geometric information from the face boundary <ref type="bibr" target="#b50">[51]</ref>, or additional weighting from occlusion probabilities <ref type="bibr" target="#b57">[58]</ref>. The currently best-performing method on many benchmarks uses a heatmap-based framework together with optimization of the loss function to foreground versus background pixels <ref type="bibr" target="#b48">[49]</ref>. Such supervised methods will typically require large amounts of labelled training data in order to generalize across the variability in facial appearance (see <ref type="bibr" target="#b9">[10]</ref> for an architecture using high-resolution deep cascades that tries to address this issue).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Semi-supervised methods</head><p>In addition to changes to the network architecture, the issue of lack of training data and inconsistent labeling quality is addressed in semi-supervised models <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b60">61]</ref> that augment the training process to make use of partially-or weakly-annotated data. Data augmentation based on landmark perturbation <ref type="bibr" target="#b28">[29]</ref> or from generating additional views from a 3D face model <ref type="bibr" target="#b61">[62]</ref> can be applied to generate more robust pseudo landmark labels. <ref type="bibr" target="#b13">[14]</ref> uses constraints from temporal consistency of landmarks based on optic flow to enhance the training of the landmark detector -see also <ref type="bibr" target="#b54">[55]</ref>. In <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b56">57]</ref>, multi-task frameworks are proposed in which attribute-networks tasked with predicting other facial attributes including pose and emotion are trained together with the landmark network, allowing for gradient transfer from one network to the other. Similar to this, <ref type="bibr" target="#b34">[35]</ref> show improvements using data augmentation with style-translated examples during training. In <ref type="bibr" target="#b14">[15]</ref>, a teacher-supervisesstudents (TS 3 ) framework is proposed in which a teacher is trained to filter student-generated landmark pseudolabels into "qualified" and "unqualified" samples, such that the student detectors can retrain themselves with better-quality data. Similarly, in <ref type="bibr" target="#b39">[40]</ref>, a GAN framework produces "fake" heatmaps that the main branch of the network needs to discriminate, hence improving performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2:</head><p>Overview of the 3FabRec pipeline, including the architecture of the autoencoder, as well as training paths for unsupervised, supervised, and the fine-tuning stages (see text for more details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Our approach</head><p>Most of the semi-supervised approaches discussed above use data augmentation on the same dataset as done for testing. Our approach (see Figs. 1,2) starts from an unsupervised method in which we leverage the implicit knowledge about face shape contained in large datasets of faces (such as used for face identification <ref type="bibr" target="#b5">[6]</ref>). This knowledge is captured in a low-dimensional latent space of an autoencoder framework. Importantly, the autoencoder also has generative capabilities, i.e., it is tasked during training to reconstruct the face from the corresponding latent vector. This step is done because the following, supervised stage implements a hybrid reconstruction pipeline that uses the generator together with interleaved transfer layers to both reconstruct the face as well as probabilistic landmark heatmaps. Hence, the changes in the latent vector space will be mapped to the position of the landmarks trained on labeled datasets. Given that the first, unsupervised stage has already captured knowledge about facial appearance and face shape, this information will be quickly made explicit during the second, supervised stage allowing for generalization across multiple datasets and enabling low-shot and few-shot training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Unsupervised face representation</head><p>The unsupervised training step follows the framework of <ref type="bibr" target="#b3">[4]</ref> in which an adversarial autoencoder is trained through four loss functions balancing faithful image reconstruction with the generalizability and smoothness of the embedding space needed for the generation of novel faces. A reconstruction loss L rec penalizes reconstruction errors through a pixel-based L1 error. An encoding feature loss L enc <ref type="bibr" target="#b18">[19]</ref> ensures the creation of a smooth and continuous latent space. An adversarial feature loss L adv pushes the encoder E and generator G to produce reconstructions with high fidelity since training of generative models using only image reconstruction losses typically leads to blurred images.</p><p>As the predicted landmark locations in our method follow directly from the locations of reconstructed facial elements, our main priority in training the autoencoder lies in the accurate reconstruction of such features. Thus, we trade some of the generative power against reconstruction accuracy by replacing the generative image loss, L gen , used in <ref type="bibr" target="#b3">[4]</ref> with a new structural image loss L cs .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Structural image loss:</head><p>To penalize reconstructions that do not align facial structures well with input images, we add a structural image loss based on the SSIM <ref type="bibr" target="#b49">[50]</ref> image similarity metric, which measures contrast c(a, b) and correlation s(a, b) between two image windows a and b:</p><formula xml:id="formula_0">c(a, b) = 2? a ? a + c ? 2 a + ? 2 b + c , s(a, b) = ? ab + c/2 ? a ? b + c/2<label>(1)</label></formula><p>The values ? a and ? b denote intensity variances of windows a,b and ? ab denotes their covariance. The constant c adds stability against small denominators. It is set to c = 255 0.01 for images with 8-bit channels. The calculation is run for each k ? k window across the images:</p><formula xml:id="formula_1">cs(x, y) = 1 |w| w c(x w , y w )s(x w , y w ))<label>(2)</label></formula><p>We obtain the structural image loss by evaluating cs(x, y) with the original image and its reconstructions:</p><formula xml:id="formula_2">L cs (E, G) = E x?p(x) [cs(x, G(E(x))]<label>(3)</label></formula><p>This loss improves the alignment of high-frequency image elements and imposes a penalty for high-frequency noise introduced by the adversarial image loss. Hence, L cs also serves as a regularizer, stabilizing adversarial training.</p><p>Full autoencoder objective: The final training objective is a weighted combination of all loss terms:</p><formula xml:id="formula_3">min E,G max Dz,Dx L AE (E, G, D z , D x ) = ? rec L rec (E, G) + ? cs L cs (E, G) + ? enc L enc (E, D z ) + ? adv L adv (E, G, D x )<label>(4)</label></formula><p>We set ? enc and ? adv to 1.0. ? rec and ? cs are selected so the corresponding loss terms yield similarly large values to each other, while at the same time ensuring a roughly 10 times higher weight in comparison to ? enc and ? adv (given the range of loss terms, we set ? rec ? 1.0, ? cs ? 60.0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Supervised landmark discovery</head><p>For landmark detection, we are not primarily interested in producing a RGB image but rather an L-channel image containing landmark probability maps. This can be seen as a form of style transfer in which the appearance of the generated face is converted to a representation that allows us to read off landmark positions. Hence, information about face shape that was implicitly present in the generation of color images before is now made explicit. Our goal is to create this transfer without losing the face knowledge distilled from the very large set of (unlabeled) images as the annotated datasets available for landmark prediction are only a fraction of that size and suffer from imprecise and inconsistent human annotations <ref type="bibr" target="#b13">[14]</ref>. For this, we introduce additional, interleaved transfer layers into the generator G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Interleaved transfer layers</head><p>Training of landmark generation starts by freezing all parameters of the autoencoder. We then interleave the inverted ResNet layers of the generator with 3 ? 3 convolutional layers. Each of these Interleaved Transfer Layers (ITL) produces the same number of output channels as the original ResNet layer. Activations produced by a ResNet layer are transformed by these layers and fed into the next higher block. The last convolutional layer mapping to RGB images is replaced by a convolutional layer mapping to Lchannel heatmap images (L = number of landmarks to be predicted). This approach adds just enough flexibility to the generator to produce new heatmap outputs by re-using the pre-trained autoencoder weights.</p><p>Given an annotated face image x, the ground truth heatmap H i for each landmark l i ? R 2 consists of a 2D Normal distribution centered at l i and a standard deviation of ?. During landmark training and inference the activations a 1 produced by the first inverted ResNet layer for an encoded image z = E(x) are passed to the first ITL layer. This will transfer the activations and feed it into the next, frozen inverted ResNet layer, such that the full cascade of ResNet and ITLs can reconstruct a landmark heatmapH. The heatmap prediction loss L H is defined as the L2 distance between predicted (H) and ground truth heatmap (H)</p><formula xml:id="formula_4">L H (IT L) = E x?p(x) [ H ? IT L(a 1 ) 2 ]<label>(5)</label></formula><p>The position of the landmark isl i = argmax u,vH i (u, v).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Encoder finetuning</head><p>Once training of the ITL layers reaches convergence we can perform an optional finetuning step. For this, the encoder E is unfrozen so that ITL layers and encoder are optimized in tandem (see <ref type="figure">Fig.2</ref>).</p><formula xml:id="formula_5">L H (IT L) ? L H (E, IT L)<label>(6)</label></formula><p>Since the updates are only based on landmark errors, this will push E to encode input faces such that facial features are placed more precisely in reconstructed faces. At the same time, other attributes like gender, skin color, or illumination may be removed as these are not relevant for the landmark prediction task. Overfitting is avoided since the generator remains unchanged, which acts as a regularizer and limits the flexibility of the encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments 1</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>VGGFace2 &amp; AffectNet The dataset used for unsupervised training of the generative autoencoder combines two datasets: the VGGFace2 dataset <ref type="bibr" target="#b5">[6]</ref>, which contains a total of 3.3 million faces collected with large variability in pose, age, illumination, and ethnicity in mind. From the full dataset, we removed faces with a height of less than 100 pixels resulting in 1.8 million faces (from 8631 unique identities). In addition, we add the AffectNet dataset <ref type="bibr" target="#b33">[34]</ref> that was designed for capturing a wide variety of facial expressions (thus providing additional variability in face shape), which contains 228k images, yielding a total of 2.1M images for autoencoder training.</p><p>300-W This dataset was assembled by <ref type="bibr" target="#b40">[41]</ref> from several sources, including LFPW <ref type="bibr" target="#b1">[2]</ref>, AFW <ref type="bibr" target="#b25">[26]</ref>, HELEN <ref type="bibr" target="#b62">[63]</ref>, XM2VTS <ref type="bibr" target="#b31">[32]</ref>, and own data and annotated semiautomatically with 68 facial landmarks. Using the established splits reported in <ref type="bibr" target="#b38">[39]</ref>, a total of 3,148 training images and 689 testing images were used in our experiments. The latter is further split into 554 images that constitute the common subset and a further 135 images that constitute the challenging subset. Additionally, 300-W contains 300 indoor and 300 outdoor images that define the private testset of the original 300-W challenge.</p><p>AFLW This dataset <ref type="bibr" target="#b24">[25]</ref> contains 24,386 in-the-wild faces with an especially wide range of face poses (yaw angles from ?120 ? -120 ? ] and roll and pitch angles from ?90 ? -90 ? ). Following common convention, we used splits of 20,000 images for training and 4,386 for testing and trained with only 19 of the 21 annotated landmarks <ref type="bibr" target="#b27">[28]</ref>.</p><p>WFLW The newest dataset in our evaluation protocol is from <ref type="bibr" target="#b50">[51]</ref> containing a total of 10,000 faces with a 7,500/2,500 train/test split. Images were sourced from the WIDER FACE dataset <ref type="bibr" target="#b53">[54]</ref> and were manually annotated with a much larger number of 98 landmarks. Training procedure We train the autoencoder for 50 epochs with an input/output size of 128 ? 128 and a batchsize of 100 images. Upon convergence we add an additional ResNet layer to both the encoder and decoder and train for another 50 epochs with an image size of 256 ? 256 to increase reconstruction fidelity with a batchsize of 50. We use the Adam optimizer <ref type="bibr" target="#b22">[23]</ref> (? 1 = 0.0, ? 2 = 0.999) with a constant learning rate of 2?10 ?5 , which yielded robust settings for adversarial learning. We apply data augmentations of random horizontal flipping (p = 0.5), translation (?4%) resizing (94% to 103%), rotation (?45 ? ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Supervised landmark training</head><p>Images are cropped using supplied bounding boxes and resized to 256 ? 256. For creating ground truth heatmaps, we set ? = 7. In all experiments we train four ITL layers and generate landmark heatmaps of size 128x128 by skipping the last generator layer (as detailed in 4.6, higher generator layers contain mostly decorrelated local appearance information). To train from the landmark dataset images, we apply data augmentations of random horizontal flipping (p = 0.5), translation (?4%) resizing (?5%), rotation (?30 ? ), and occlusion (at inference time no augmentation is performed). The learning rate during ITL-only training is set to 0.001. During the optional finetuning stage we lower ITL learning rate to 0.0001 while keeping the encoder learning rate the same as during training (=2 ? 10 ?5 ) and resetting Adam's ? 1 to the default value of 0.9.</p><p>Evaluation Metrics Performance of facial landmark detection is reported here using normalized mean error (NME), failure rate (FR) at 10% NME and area-under-thecurve (AUC) of the Cumulative Error Distribution (CED) curve. For 300-W and WFLW we use the distance between the outer eye-corners as the "inter-ocular" normalization. Due to the high number of profile faces in AFLW, errors are normalized using the width of the (square) bounding boxes following <ref type="bibr" target="#b58">[59]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Qualitative results</head><p>The trained generator is able to produce a wide range of realistic faces from a low-dimensional (99D) latent feature vector z -this is shown in <ref type="figure" target="#fig_0">Fig.3</ref> with randomly-generated faces with overlaid, predicted landmark heatmaps. To achieve this, the model must have learned inherent information about the underlying structure of faces. We can further illustrate the implicit face shape knowledge by interpolating between face embeddings and observing that facial structures (such as mouth corners) in produced images are constructed in a highly consistent manner (see <ref type="figure" target="#fig_1">Fig. 4</ref> for a visualization). This leads to two insights: First, facial structures are actually encoded in the low-dimensional representation z. Second, this information can be transformed into 2D maps of pixel intensities (i.e., a color image) while maintaining high correlation with the originating encoding.</p><p>Further examples of the reconstruction quality on challenging images are shown in <ref type="figure" target="#fig_2">Fig. 5</ref>. As can be seen, the pipeline will try to reconstruct a full face as much as possible given the input, removing occlusions and make-up and  even "upsampling" the face <ref type="figure" target="#fig_2">(Fig. 5</ref>, first column) in the process. This is because the databases for training the autoencoder contained mostly unoccluded and non-disguised faces at roughly similar resolutions. Additionally we note that the reconstructed faces will not necessarily preserve the identity as the goal of the fully-trained pipeline is to reconstruct the best-fitting face shape. Although our method is able to handle considerable variations in resolution ( <ref type="figure" target="#fig_2">Fig. 5</ref>, first column), make-up ( <ref type="figure" target="#fig_2">Fig. 5, second column)</ref>, lighting ( <ref type="figure" target="#fig_2">Fig. 5, third column)</ref>, and pose ( <ref type="figure" target="#fig_2">Fig. 5, fourth column)</ref>, it does produce failed predictions in cases when these factors become too extreme, as shown in the fifth column of <ref type="figure" target="#fig_2">Fig. 5</ref>. Landmark prediction, however, typically degrades gracefully in these cases as the confidence encoded in the heatmaps will also be low. <ref type="table" target="#tab_2">Table 1</ref> shows comparisons of our semi-supervised pipeline with state-of-the-art on the 300-W and the AFLW datasets using the full amount of training. We achieve top-   2 accuracy on nearly all test sets with the exception of the common set from 300-W. This demonstrates that our framework is able to reach current levels of performance despite a much lighter, supervised training stage using only a few interleaved transfer layers on top of the generator pipeline. The results in <ref type="table" target="#tab_3">Table 2</ref> for AUC and FR for the commonly-reported 300-W dataset demonstrate that our framework achieves the lowest failure rate of all methods (our FR=0.17 corresponds to only 1 image out of the full set that has large enough errors to count as a failure). At the same time, the AUC is in the upper range but not quite as good as that of <ref type="bibr" target="#b50">[51]</ref>, for example, which means that overall errors across landmarks are low, but more equally distributed compared to the top-performing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with state-of-the-art</head><p>The NME results in <ref type="table" target="#tab_5">Table 3</ref> show that on the newest WFLW dataset, our approach performs at levels of the LAB method <ref type="bibr" target="#b50">[51]</ref> with most subsets, although we perform consistently below the current StyleAlign approach (SA, <ref type="bibr" target="#b34">[35]</ref> -note, however, that this approach could be easily implemented into our framework as well, which would allow us to disentangle the 99D-feature vector into style attributes <ref type="bibr" target="#b3">[4]</ref>   to generate augmented training data). The main reason for this is that WFLW contains much more heavy occlusions and extreme appearance changes compared to our training sets leading to more failure cases (see <ref type="figure" target="#fig_2">Fig.5</ref> fifth column).  <ref type="table" target="#tab_7">Table 4</ref> shows that performance is comparable to that of 2-year-old approaches trained on the full dataset (cf. <ref type="table" target="#tab_2">Table 1</ref>) although 3FabRec was trained only with 10% of the dataset. In addition, performance does not decrease much when going to lower values of 5% and 1.5% of training set size. Even when training with only 10 images or 1 image, our approach is able to deliver reasonably robust results (see <ref type="figure">Fig.1</ref> for landmark reconstruction results from training with 10 images).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Limited training data and few-shot learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>300-W</head><p>AFLW For this dataset <ref type="table" target="#tab_8">(Table 5)</ref>, our approach already starts to come ahead at 20% of training set size with little degradation down to 1%. Again, even with only a few images 3FabRec can make landmark predictions.</p><p>WFLW For this more challenging dataset <ref type="table" target="#tab_9">(Table 6)</ref>, our approach easily outperforms the StyleAlign <ref type="bibr" target="#b20">[21]</ref> method as soon as less than 10% is used for training while being able to maintain landmark prediction capabilities down to only 10 images in the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Ablation studies 4.6.1 Effects of ITLs</head><p>In order to see where information about landmarks is learned in the interleaved transfer layers, <ref type="figure" target="#fig_4">Figure 6</ref> shows the reconstruction of the landmark heatmap when using all four layers versus decreasing subsets of the upper layers. As can be seen, the highest layer has only very localized information (mostly centered on eyes and mouth), whereas the lower layers are able to add information about the outlines -especially below layer 2.</p><p>Localization accuracy is reported on the 300-W dataset (NME of 51 inner landmarks and outlines, as well as FR) in <ref type="table" target="#tab_10">Table 7</ref>. As can be expected from the visualization, performance is bad for the upper layers only, but quickly recovers (especially when including the outlines) below layer 2. The reason for this is that the upper layers of the generator will mostly contain localized, de-correlated information at the pixel level, whereas the lower layers are closer to the more global and contextual information necessary to cover highly variable outlines (cf. blue curve in <ref type="figure" target="#fig_4">Figure 6</ref>, note that all ITLs have 3?3 convolutions). As the gray curve in <ref type="figure" target="#fig_4">Figure  6</ref> and <ref type="table" target="#tab_10">Table 7</ref> show as well, the ITLs can achieve this with only very few additional parameters.      </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.2">Effects of finetuning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Runtime performance</head><p>Since inference complexity is equivalent to two forwardpasses through a ResNet-18, our method is able to run at frame rates of close to 300fps on a TitanX GPU -an order of magnitude faster than state-of-the-art approaches with similar, high accuracy (LAB <ref type="bibr" target="#b50">[51]</ref>=16fps, Wing <ref type="bibr" target="#b17">[18]</ref>=30fps, Deep Regression <ref type="bibr" target="#b27">[28]</ref>=83fps, Laplace <ref type="bibr" target="#b39">[40]</ref>=20fps).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>With 3FabRec, we have demonstrated that an unsupervised, generative training on large amounts of faces captures implicit information about face shape, making it possible to solve landmark localization with only a minimal amount of supervised follow-up training. This paradigm makes our approach inherently more robust against overfitting to specific training datasets as well as against human annotation variability <ref type="bibr" target="#b13">[14]</ref>. The critical ingredients of 3FabRec that enable this generalization are the use of an adversarial autoencoder that reconstructs high-quality faces from a low-dimensional latent space, together with low-overhead, interleaved transfer layers added to the generator stage that transfer face reconstruction to landmark heatmap reconstruction.</p><p>Results show that the autoencoder is easily able to generalize from its unlabeled training set to data from unseen datasets. This offers generalization for training from only a few percent of the training set and still produces reliable results from only a few annotated images -far below anything reported so far in the literature. At the same time, since inference amounts to only two forward passes through a ResNet18, our method achieves much higher runtime performance than other highly accurate methods. <ref type="figure" target="#fig_5">Figure 7</ref> shows generated faces from a random sampling of the latent space (top four rows) together with the predicted landmark heatmaps (bottom four rows) using the final architecture from the main paper (trained on VGGFace2 and AffectNet with 256x256px). We note that the faces have high visual quality as well as large variability in facial appearance (pose, expression, hair style, accessories).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Random faces</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ablation studies</head><p>A critical part of our framework is the first step in which an adversarial autoencoder is trained in an unsupervised fashion on a large dataset of faces, which yields a lowdimensional embedding vector z that encapsulates the face representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Autoencoder losses</head><p>The adversarial autoencoder is trained through four loss functions balancing faithful image reconstruction with the generalizability and smoothness of the embedding space needed for the generation of novel faces. A reconstruction loss L rec penalizes reconstruction errors through a pixelbased L1 error. An encoding feature loss L enc <ref type="bibr" target="#b18">[19]</ref> ensures the creation of a smooth and continuous latent space. An adversarial feature loss L adv pushes the encoder E and generator G to produce reconstructions with high fidelity since training of generative model using only image reconstruction losses typically leads to blurred images. As the predicted landmark locations in our method follow directly from the locations of reconstructed facial elements, our main priority in training the autoencoder lies in the accurate reconstruction of such features, reconstruction accuracy is further enhanced by introducing a structural image loss L cs .</p><p>Here, we present results of the framework ablating different loss terms (except for the encoding feature loss L enc ) during the training of the autoencoder to study their impact on landmark localization accuracy (see <ref type="table" target="#tab_13">Table 9</ref>) using the 300-W dataset. In addition, we report the effects of the optional finetuning step on accuracy, in which the autoencoder is further tuned on the 300-W training dataset. All setups were trained on 128x128px images at a half of the resolution of the setup reported in the paper (see also <ref type="figure">Figure 8</ref>).</p><p>As benchmarks, the first two rows of <ref type="table" target="#tab_13">Table 9</ref> also list a standard ResNet-18 predictor of landmark locations (trained on 300-W) as well as a standard heatmap-based system (trained on 300-W). Both approaches offer roughly the same kind of performance on this dataset with a slight advantage for heatmap-based prediction.</p><p>If we only add the autoencoder (using L rec , L enc ) to our ResNet-architecture, then performance is comparable to that of the standard, non-bottlenecked ResNet-18 architecture, which shows that the 99 dimensions seem to be sufficient to capture the landmark "knowledge" -it is important to note, however, that this landmark knowledge was obtained from unsupervised training. Further (supervised) finetuning of the autoencoder on 300-W provides another, significant boost that goes beyond the performance of both supervised benchmark systems. Hence, the finetuning step on the dataset is able to sharpen the implicit landmark representation obtained during the unsupervised step.</p><p>Forcing the autoencoder to generate believable images by adding the adversarial loss (using L rec , L enc , L adv ) provides a further 7% improvement in NME for standard and finetuned training. Finally, the addition of the structural loss that further enhances small details in the reconstructed faces (using L rec , L enc , L adv , L cs ) yields another ?7% improvement. Overall, these results clearly show that losses that tune the face representation to be able to generate more detailed faces will also improve the landmark localization accuracy.</p><p>We note that the columns reporting "global" reconstruction errors (as RMSE or SSIM comparisons between the original and reconstructed images, respectively) and "local" reconstruction error (as SSIM errors evaluated for patches centered on the landmark locations of the original and reconstructed images) yield already good quality for the most "simple" loss setup. For this it is best to look at <ref type="figure">Figure  8</ref>, which shows how the different losses affect the visual quality of the reconstruction. When looking at rows (A), (B), (C), faces gain an increasing amount of high-frequency detail. When adding the GAN loss, these high-frequency details will not aid the reconstruction error at first as the details are "hallucinated" globally all over the face -these details, however, seem to be able to aid the landmark layers in providing a better mapping onto heatmaps and therefore landmark locations. The addition of the SSIM loss does improve the reconstruction error again as the loss forces the high-frequency details to better match with the trained source face images -again, the added details in this case will help landmark localization.</p><p>The effect of finetuning on face appearance is interesting to observe as the faces gain immediate detail for all loss setups, yet their overall reconstruction is sometimes more "different" to the source face compared to the non-finetuned version. This is because finetuning unfreezes the weights of the encoder but will train to predict the landmark locations more reliably -hence, the reconstructed faces will favor clear landmark localizability (through well-defined facial feature locations) at the expense of more faithful face reconstruction. Overall, the effect is therefore an increase of the reconstruction error.</p><p>As a final note, we observe that training the autoencoder setup on 256x256px provides another jump in performance as the system will learn to reconstruct facial details at an even higher fidelity (see final two rows in <ref type="figure">Figure 8</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Encoding length</head><p>The latent vector z reported in the main paper has a dimensionality of d = 99 which is comparable to other GANframeworks <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b42">43]</ref>.</p><p>In <ref type="table" target="#tab_2">Table 10</ref>, we report the effect of halving this dimensionality to d = 50 on landmark localization accuracy. Although yielding a slightly higher NME, the reduced autoencoder obtains a slightly lower FR, which overall means that both embedding dimensionalities result in similar performance levels. An issue with the reduced dimensionality embedding, however, was that the subsequent landmark training was notably less robust, requiring a much more conservative learning rate. Hence, for the task of landmark localization, the current framework may work with a lower-dimensional embedding space, however, it seems that pulling the implicit information out of the reduced dimensions is a harder task than for a richer embedding.</p><p>Further experiments are needed to investigate the effects of increasing the dimensionality as well as providing further constraints on the embedding vector z during the unsupervised training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Unsupervised training and few-shot learning</head><p>We next take a look at the effects of the unsupervised training step as well as the amount of supervised posttraining on 300-W. <ref type="table" target="#tab_2">Table 11</ref> shows again the ResNet-18 and heatmap hourglass baselines and then three different training setups for our full, finetuned system at 128x128px im-   age size. The first two rows report results of the full architecture without any unsupervised pre-training and hence without any implicit face knowledge. The next rows show results for the full architecture with different amounts of pre-training. Pre-training on the 300-W training dataset results in equal or slightly better performance compared to the baseline architectures showing that the system is able to pick up implicit knowledge already from only 3,200 images. Pretraining on 100,000 images provides a significant, further jump as does pre-training on the full 2,1M image dataset.</p><p>Importantly, the error increase in the presence of limited training data (columns labeled 1.5% in <ref type="table" target="#tab_2">Table 11</ref>) with just 50 images showcase the power of the pre-trained representation: whereas ResNet-18 increases around 54% in NMW from 100% to 1.5% training set size, our pre-trained architectures only reduce 47%, 34%, and 29% respectively owing to the more robust generalization from the latent representation.</p><p>C. Few-shot learning on different datasets <ref type="bibr">Figures 9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11</ref> show results for few-shot learning on the three different datasets (300-W, AFLW, WFLW) reported in the main paper. The first column has the entire training set (50, 10, or 1 labeled image(s)), and the second column shows predicted landmarks on nine or three images from the different testsets contained in the datasets. In all figures, training with even just one image produces reasonable localization results and a clear improvement in prediction accuracy can be traced as a few more images are added.</p><p>In <ref type="figure">Figure 11</ref>, the failure cases are most visible (see, for example, the top results for training with one image on the Blur testset). It should be noted that this is by far the most challenging dataset as it contains variability in face appearance (due to illumination, occlusion, and make-up) that is not fully present in the unsupervised datasets we used (cf. the randomly-generated faces in <ref type="figure" target="#fig_5">Figure 7</ref>). As a few more labeled faces are added, however, performance begins to quickly improve even in the presence of such severe changes.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Additional result visualizations</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Randomly-generated faces with overlaid generated landmark probability maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Predicted landmarks of generated faces by interpolation between embedded feature vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>3FabRec results on challenging test examples from WFLW. Rows show the original, and the reconstruction itself, with predicted landmarks, with ground-truth landmarks, and with predicted landmark heatmaps, respectively. The fifth column illustrates a failure case. For more examples, see supplementary materials.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Tables 4 , 5 ,</head><label>45</label><figDesc>6 showcase the central result of our framework: when training on only parts of the training set, 3Fab-Rec can beat the published benchmark performance values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Layer-analysis of 3FabRec. Gray curve: cumulative number of network parameters; blue curve: spatial dimension of each layer. The four red blocks indicate the ITL layers, with arrows showing how well the landmark heatmap can be predicted when starting from that layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Randomly-generated faces from the 3FabRec framework (top four rows) together with their predicted landmark confidence heatmaps (bottom four rows).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :Figure 9 :Figure 10 :Figure 11 :</head><label>891011</label><figDesc>Example reconstructions corresponding to Tab. 9. (A)-(C) are trained for 30 epoches on 128 ? 128 images. 'Final' denotes the fully trained model 256 ? 256 that was used for the experiments in Sec. 4 of the paper. Few-shot learning on 300-W Few-shot learning on AFLW Few-shot learning on WFLW Figure 12: 3FabRec results on WFLW Pose and Expression: Two blocks of rows show (1) original, (2) reconstruction, (3) reconstruction with predicted landmarks, (4) reconstruction with ground-truth landmarks, (5) reconstruction with predicted landmark heatmaps, and (6) original with predicted landmarks, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 13 :</head><label>13</label><figDesc>3FabRec results on WFLW Illumination and Make-Up: Two blocks of rows show (1) original, (2) reconstruction, (3) reconstruction with predicted landmarks, (4) reconstruction with ground-truth landmarks, (5) reconstruction with predicted landmark heatmaps, and (6) original with predicted landmarks, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 14 :</head><label>14</label><figDesc>3FabRec results on WFLW Occlusion and Blur: Two blocks of rows show (1) original, (2) reconstruction, (3) reconstruction with predicted landmarks, (4) reconstruction with ground-truth landmarks, (5) reconstruction with predicted landmark heatmaps, and (6) original with predicted landmarks, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The dataset contains different (partially overlapping) test subsets for evaluation where each subset varies in pose, expression, illumination, make-up, occlusion, or blur.</figDesc><table><row><cell>4.2. Experimental settings</cell></row><row><cell>4.2.1 Unsupervised autoencoder training</cell></row><row><cell>Network architecture Our implementation is based on</cell></row><row><cell>[4] which combines a standard ResNet-18 as encoder with</cell></row><row><cell>an inverted ResNet-18 (first convolution layers in each</cell></row><row><cell>block replaced by 4?4 deconvolution layers) as decoder.</cell></row><row><cell>Both encoder and decoder contain ?10M parameters each.</cell></row><row><cell>The encoded feature length is 99 dimensions.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>Method</cell><cell>AUC</cell><cell>FR</cell></row><row><cell>M 3 CSR [11]</cell><cell>47.52</cell><cell>5.5</cell></row><row><cell>CFSS [59]</cell><cell cols="2">49.87 5.05</cell></row><row><cell cols="3">DenseReg+MDM [1] 52.19 3.67</cell></row><row><cell>JMFA [12]</cell><cell cols="2">54.85 1.00</cell></row><row><cell>LAB [51]</cell><cell>58.85</cell><cell>0.83</cell></row><row><cell>3FabRec</cell><cell cols="2">54.61 0.17</cell></row></table><note>Normalized mean error (%) on 300-W dataset. Best results highlighted in bold, second best are underlined.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Area under the curve (AUC) and failure rate (FR in (%) @0.1) on the 300-W testset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>24.10 11.45 9.32 9.38 13.03 11.28 CFSS [60] 9.07 21.36 10.09 8.30 8.74 11.76 9.96 DVLN [52] 6.08 11.54 6.78 5.73 5.98 7.33 6.88 LAB [51] 5.27 10.24 5.51 5.23 5.15 6.79 6.32</figDesc><table><row><cell></cell><cell>Method</cell><cell>Full Pose Exp. Ill.</cell><cell>Mk.</cell><cell>Occ. Blur</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Up</cell></row><row><cell>NME (%)</cell><cell cols="4">SDM [53] 10.29 SAN [14] 5.22 10.39 5.71 5.19 5.49 6.83 5.80</cell></row><row><cell></cell><cell>Wing [51]</cell><cell cols="3">5.11 8.75 5.36 4.93 5.41 6.37 5.81</cell></row><row><cell></cell><cell>SA [35]</cell><cell cols="3">4.39 8.24 4.68 4.24 4.37 5.60 4.86</cell></row><row><cell></cell><cell>3FabRec</cell><cell cols="3">5.62 10.23 6.09 5.55 5.68 6.92 6.38</cell></row><row><cell>FR @0.1 (%)</cell><cell>SDM [53] CFSS [60] DVLN [52] LAB [51]</cell><cell cols="3">29.40 84.36 33.44 26.22 27.67 41.85 35.32 20.56 66.26 23.25 17.34 21.84 32.88 23.67 10.84 46.93 11.15 7.31 11.65 16.30 13.71 7.56 28.83 6.37 6.73 7.77 13.72 10.74</cell></row><row><cell></cell><cell>SAN [14]</cell><cell cols="3">6.32 27.91 7.01 4.87 6.31 11.28 6.60</cell></row><row><cell></cell><cell>Wing [51]</cell><cell cols="3">6.00 22.70 4.78 4.30 7.77 12.50 7.76</cell></row><row><cell></cell><cell>SA [35]</cell><cell cols="3">4.08 18.10 4.46 2.72 4.37 7.74 4.40</cell></row><row><cell></cell><cell>3FabRec</cell><cell cols="3">8.28 34.35 8.28 6.73 10.19 15.08 9.44</cell></row><row><cell></cell><cell>SDM [53]</cell><cell cols="3">0.300 0.023 0.229 0.324 0.312 0.206 0.239</cell></row><row><cell>AUC</cell><cell>CFSS [60]</cell><cell cols="3">0.366 0.063 0.316 0.385 0.369 0.269 0.304</cell></row><row><cell>@0.1</cell><cell>DVLN [52]</cell><cell cols="3">0.455 0.147 0.389 0.474 0.449 0.379 0.397</cell></row><row><cell></cell><cell>LAB [51]</cell><cell cols="3">0.532 0.235 0.495 0.543 0.539 0.449 0.463</cell></row><row><cell></cell><cell>SAN [15]</cell><cell cols="3">0.536 0.236 0.462 0.555 0.522 0.456 0.493</cell></row><row><cell></cell><cell>Wing [51]</cell><cell cols="3">0.534 0.310 0.496 0.541 0.558 0.489 0.492</cell></row><row><cell></cell><cell>SA [35]</cell><cell cols="3">0.591 0.311 0.549 0.609 0.581 0.516 0.551</cell></row><row><cell></cell><cell>3FabRec</cell><cell cols="3">0.484 0.192 0.448 0.496 0.473 0.398 0.434</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Evaluation results on WFLW dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 reports</head><label>8</label><figDesc>the effects of running the model with and without finetuning on the full testsets of the three evaluated datasets. The additional retraining of the autoencoder allows for better reconstruction of the faces and results in benefits of 10.9% on average (8.9% for 300-W, 15.2% for AFLW, and 8.5% for WFLW, respectively). .74 3.82 3.76 6.53 4.31 3.88 6.88 4.47 4.22 6.95 4.75 4.55 7.39 5.10 4.96 8.29 5.61 8.45 15.84 9.92</figDesc><table><row><cell>300-W dataset</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>NME (%) with reduced training sets on 300-W. ? RCN + reports errors normalized by eye-center distance -for better comparison values were rescaled by the known ratios of inter-ocular to inter-pupil distances, "-" denotes values not reported.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">AFLW dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Training set size</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>100%</cell><cell></cell><cell>20%</cell><cell></cell><cell>10%</cell><cell>5%</cell><cell>1%</cell><cell></cell><cell cols="5">50 (0.0025%) 10 (0.0005%) 1 (&lt;0.0001%)</cell></row><row><cell>RCN + [21]</cell><cell cols="2">1.61 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>2.17 -</cell><cell cols="2">2.88 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TS 3 [15]</cell><cell>-</cell><cell>-</cell><cell cols="6">1.99 1.86 2.14 1.94 2.19 2.03 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>3FabRec</cell><cell cols="11">1.87 1.59 1.96 1.74 2.03 1.74 2.13 1.86 2.38 2.03 2.74 2.23</cell><cell cols="2">3.05 2.56</cell><cell>4.93 4.04</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>NME (%) with reduced training sets for AFLW. The first column in each cell is the full testset, the second is the frontal testset, "-" denotes values not reported.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">WFLW dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell></cell><cell cols="3">Training set size</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">100% 20%</cell><cell>10%</cell><cell>5%</cell><cell>50</cell><cell>10</cell><cell>1</cell></row><row><cell>SA [21]</cell><cell>4.39</cell><cell>6.00</cell><cell>7.20</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell></row><row><cell>3FabRec</cell><cell>5.62</cell><cell>6.51</cell><cell>6.73</cell><cell>7.68</cell><cell>8.39</cell><cell>9.66</cell><cell>15.79</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>NME (%) with reduced training sets for WFLW.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Trained ITLs</cell><cell></cell></row><row><cell></cell><cell cols="2">1+2+3+4 2+3+4</cell><cell>3+4</cell><cell>4</cell></row><row><cell>Input size</cell><cell>256x8x8</cell><cell cols="3">128x16x16 64x32x32 64x64x64</cell></row><row><cell>Trainable params</cell><cell>881k</cell><cell>291k</cell><cell>143k</cell><cell>106k</cell></row><row><cell>300-W NME ?O</cell><cell>3.54</cell><cell>3.63</cell><cell>5.34</cell><cell>16.34</cell></row><row><cell>300-W NME O</cell><cell>6.58</cell><cell>7.32</cell><cell>18.17</cell><cell>40.24</cell></row><row><cell>300-W FR@0.1</cell><cell>1.45</cell><cell>2.03</cell><cell>22.93</cell><cell>91.44</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Parameters and training results for ITLs (?O=without outlines, O=outlines only)</figDesc><table><row><cell></cell><cell>300-W</cell><cell cols="2">AFLW WFLW</cell></row><row><cell>NME before FT</cell><cell>4.16</cell><cell>2.12</cell><cell>6.11</cell></row><row><cell>NME after FT</cell><cell>3.82</cell><cell>1.84</cell><cell>5.62</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>NME (%) before and after finetuning on full testsets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>ModelLrec L adv Lcs FT Global Reconstr. Local Reconstr. NME FR@0.1</figDesc><table><row><cell></cell><cell></cell><cell>RMSE</cell><cell>SSIM</cell><cell>Patch SSIM</cell><cell>%</cell><cell>% (#)</cell></row><row><cell>(R)</cell><cell>ResNet-18</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>5.64</cell><cell>4.64 (32)</cell></row><row><cell>(HG)</cell><cell>Heatmap HG</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>5.48</cell><cell>4.21 (29)</cell></row><row><cell>(A)</cell><cell>Adv. Autoencoder</cell><cell>12.61</cell><cell>0.68</cell><cell>0.64</cell><cell>5.67</cell><cell>4.94 (34)</cell></row><row><cell cols="2">(A-FT) Adv. Autoencoder (FT)</cell><cell>25.03</cell><cell>0.57</cell><cell>0.55</cell><cell>4.92</cell><cell>2.47 (17)</cell></row><row><cell>(B)</cell><cell>AE + GAN</cell><cell>15.10</cell><cell>0.60</cell><cell>0.58</cell><cell>5.30</cell><cell>3.77 (26)</cell></row><row><cell cols="2">(B-FT) AE + GAN (FT)</cell><cell>27.48</cell><cell>0.49</cell><cell>0.50</cell><cell>4.71</cell><cell>2.03 (14)</cell></row><row><cell>(C)</cell><cell>AE + GAN + Struct.</cell><cell>15.91</cell><cell>0.62</cell><cell>0.64</cell><cell>4.92</cell><cell>2.61 (18)</cell></row><row><cell cols="2">(C-FT) AE + GAN + Struct. (FT)</cell><cell>27.65</cell><cell>0.50</cell><cell>0.53</cell><cell>4.41</cell><cell>1.45 (10)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>Results of autoencoder ablation study. Rows (R) and (HG) are benchmark results from fully supervised methods with a comparable ResNet-18 architecture. Rows (A), (B), (C) show the effects of adding loss terms on both global and local reconstruction errors as well as on landmark localization accuracy and failure rate. Rows (A-FT), (B-FT), (C-FT) report results on post-finetuning the autoencoder on the 300-W dataset. NME = Normalized mean error, FR@0.1 = failure rate at 10% NME. All results reported for the full testset of 300-W.</figDesc><table><row><cell cols="3"># Dims NME FR@0.1</cell></row><row><cell></cell><cell>%</cell><cell>% (#)</cell></row><row><cell>50  ?</cell><cell>4.59</cell><cell>1.02 (07)</cell></row><row><cell>99</cell><cell>4.41</cell><cell>1.45 (10)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10 :</head><label>10</label><figDesc>Number of dimension of embedded feature vectors.</figDesc><table /><note>? Landmark training was instable and required multi- ple restarts and a reduction of the learning rate.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>Figures 12,<ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14</ref> show additional, non-curated visualizations of the full system on images from the six test subsets of WFLW.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Unlabeled training data</cell><cell cols="3">Labeled training data</cell></row><row><cell>Model</cell><cell>Num.</cell><cell>Pre-train</cell><cell cols="2">Num. of External</cell><cell>100% (3,189)</cell><cell cols="2">1.5% (50)</cell></row><row><cell></cell><cell cols="2">param. dataset(s)</cell><cell>images</cell><cell>images</cell><cell cols="2">NME FR@0.1 NME</cell><cell>FR@0.1</cell></row><row><cell>ResNet-18</cell><cell>11M</cell><cell>None</cell><cell>0</cell><cell>no</cell><cell>5.64 4.64 (32)</cell><cell cols="2">8.70 22.21 (153)</cell></row><row><cell>Heatmap HG</cell><cell>22M</cell><cell>None</cell><cell>0</cell><cell>no</cell><cell cols="3">5.48 4.21 (29) 10.13 39.33 (271)</cell></row><row><cell>C-FT</cell><cell>23M</cell><cell>300-W</cell><cell>3,189</cell><cell>no</cell><cell>5.40 4.79 (33)</cell><cell cols="2">7.95 15.82 (109)</cell></row><row><cell>C-FT</cell><cell>23M</cell><cell>VGG + AN</cell><cell>100k</cell><cell>yes</cell><cell>4.73 1.74 (12)</cell><cell>6.34</cell><cell>9.29 (064)</cell></row><row><cell>C-FT</cell><cell>23M</cell><cell>VGG + AN</cell><cell>2.1M</cell><cell>yes</cell><cell>4.41 1.45 (10)</cell><cell>5.71</cell><cell>4.35 (030)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 11 :</head><label>11</label><figDesc>Effect of unsupervised pre-training when trained with full and reduced labeled training data on 300-W.</figDesc><table><row><cell>Input</cell></row><row><cell>images</cell></row><row><cell>(A)</cell></row><row><cell>NME 5.67</cell></row><row><cell>FR 4.94</cell></row><row><cell>(A-FT)</cell></row><row><cell>NME 4.92</cell></row><row><cell>FR 2.47</cell></row><row><cell>(B)</cell></row><row><cell>NME 5.30</cell></row><row><cell>FR 3.77</cell></row><row><cell>(B-FT)</cell></row><row><cell>NME 4.71</cell></row><row><cell>FR 2.03</cell></row><row><cell>(C)</cell></row><row><cell>NME 4.92</cell></row><row><cell>FR 2.03</cell></row><row><cell>(C-FT)</cell></row><row><cell>NME 4.41</cell></row><row><cell>FR 1.45</cell></row><row><cell>(Final)</cell></row><row><cell>NME 4.16</cell></row><row><cell>FR 0.87</cell></row><row><cell>(Final-FT)</cell></row><row><cell>NME 3.82</cell></row><row><cell>FR 0.73</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For experiments on parameter tuning, cross-database results, and further ablation studies, see supplementary materials.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Densereg: Fully convolutional dense shape regression in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Riza Alp Guler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Epameinondas</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snape</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6799" to="6808" />
		</imprint>
	</monogr>
	<note>Stefanos Zafeiriou, and Iasonas Kokkinos</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Localizing parts of faces using a consensus of exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neeraj</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2930" to="2940" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A review of facial landmark extraction in 2d images and videos using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Bodini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Big Data and Cognitive Computing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Robust discrimination and generation of faces using compact, disentangled embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Browatzki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wallraven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Two-stage convolutional part heatmap regression for the 1st 3D face alignment in the wild (3DFAW) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), volume 9914 LNCS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="616" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Vggface2: A dataset for recognising faces across pose and age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Face &amp; Gesture Recognition (FG 2018)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On over-fitting in model selection and subsequent selection bias in performance evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gavin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola Lc</forename><surname>Cawley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Talbot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2079" to="2107" />
			<date type="published" when="2010-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Timothy F Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="484" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Active shape modelssmart snakes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Timothy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher J</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page" from="266" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">De-CaFA: Deep Convolutional Cascade for Face Alignment In The Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Dapogny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K?vin</forename><surname>Bailly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6893" to="6901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">M3 csr: Multi-view, multi-scale and multi-component cascade shape regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingshan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="19" to="26" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Joint multi-view face alignment in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Style Aggregated Network for Facial Landmark Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="379" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Teacher Supervises Students How to Learn From Partially Labeled Images for Facial Landmark Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="783" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Supervision-by-registration: An unsupervised approach to improve the precision of facial landmark detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-I</forename><surname>Shoou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshuo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="360" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cascaded collaborative regression for robust facial landmark detection trained using a mixture of synthetic and real images with dynamic weighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Zhen-Hua Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Jun</forename><surname>Christmas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3425" to="3440" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Wing Loss for Robust Facial Landmark Localisation with Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Zhen-Hua Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrik</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Jun</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee-Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improving Landmark Localization with Semi-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1546" to="1555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Universal language model fine-tuning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.06146</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Durk P Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3581" to="3589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Annotated facial landmarks in the wild: A largescale, real-world database for facial landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Koestinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE International Conference on Computer Vision Workshops (ICCV workshops)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2144" to="2151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Interactive facial feature localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Vuong Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="679" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Unconstrained facial landmark localization with backbonebranches fully-convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhujin</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyong</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.03409</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A deep regression architecture with twostage re-initialization for high performance facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangjing</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohu</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings -30th IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>-30th IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Landmark perturbation-based data augmentation for unconstrained face recognition. Signal Processing: Image Communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang-Jing</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Dong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang-Dong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="465" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Frey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05644</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Adversarial autoencoders. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Robust Facial Landmark Detection via a Fully-Convolutional Local-Global Context Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Merget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Rock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Rigoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="781" to="790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Xm2vtsdb: The extended m2vts database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kieron</forename><surname>Messer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Luettin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilbert</forename><surname>Maitre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second international conference on audio and video-based biometric person authentication</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">964</biblScope>
			<biblScope unit="page" from="965" to="966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Direct shape regression networks for end-to-end face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiantong</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianglong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5040" to="5049" />
		</imprint>
	</monogr>
	<note>Vassilis Athitsos, and Heng Huang</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">AffectNet: A Database for Facial Expression, Valence, and Arousal Computing in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Mollahosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behzad</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Aggregation via Separation: Boosting Facial Landmark Detector with Semi-Supervised Style Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keqiang</forename><surname>Shengju Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hyperface: A deep multi-task learning framework for face detection, landmark localization, pose estimation, and gender recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="121" to="135" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Face alignment at 3000 fps via regressing local binary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1685" to="1692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Face alignment via regressing local binary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1233" to="1245" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Laplace Landmark Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuncheng</forename><surname>Joseph P Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tulyakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Epameinondas</forename><surname>Antonakos</surname></persName>
		</author>
		<title level="m">Georgios Tzimiropoulos, Stefanos Zafeiriou, and Maja Pantic. 300 Faces In-The-Wild Challenge: database and results. Image and Vision Computing</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="3" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">300 faces in-the-wild challenge: The first facial landmark localization challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="397" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep convolutional network cascade for facial point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3476" to="3483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unsupervised learning of landmarks by descriptor vector exchange</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Thewlis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6361" to="6371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Face alignment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Tresadern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Petrovi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of face recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="109" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Mnemonic descent method: A recurrent process applied for end-to-end face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Snape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mihalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Epameinondas</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4177" to="4187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Face-space: A unifying concept in face recognition research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Valentine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hills</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Quarterly Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1996" to="2019" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Adaptive Wing Loss for Robust Face Alignment via Heatmap Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fuxin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multiscale structural similarity for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Seventh Asilomar Conference on Signals</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1398" to="1402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Look at Boundary: A Boundary-Aware Face Alignment Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yici</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2129" to="2138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Facial Landmark Detection with Tweaked Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanggeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Medioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prem</forename><surname>Natarajan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>De La Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="532" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning feature pyramids for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1281" to="1290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Few-Shot Adversarial Learning of Realistic Neural Talking Head Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Egor</forename><surname>Zakharov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksandra</forename><surname>Shysheya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Egor</forename><surname>Burkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Split-brain autoencoders: Unsupervised learning by cross-channel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1058" to="1067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Facial Landmark Detection by Deep Multi-task Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eccv</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Robust Facial Landmark Detection via Occlusionadaptive Deep Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meilu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Sadiq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingjie</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cvpr</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Face alignment by coarse-to-fine shape searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Face alignment by coarse-to-fine shape searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Encyclopedia of Machine Learning and Data Mining</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1142" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Face Alignment Across Large Poses: A 3D Solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Face detection, pose estimation, and landmark localization in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2879" to="2886" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
