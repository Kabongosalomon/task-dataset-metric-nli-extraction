<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SIM-Trans: Structure Information Modeling Transformer for Fine-grained Visual Categorization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbo</forename><surname>Sun</surname></persName>
							<email>sunhongbo@pku.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangteng</forename><surname>He</surname></persName>
							<email>hexiangteng@pku.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Peng</surname></persName>
							<email>pengyuxin@pku.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Wangxuan Institute of Computer Technology</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Wangxuan Institute of Computer Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Wangxuan Institute of Computer Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SIM-Trans: Structure Information Modeling Transformer for Fine-grained Visual Categorization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS ? Computing methodologies ? Object recognition KEYWORDS Fine-grained Visual Categorization</term>
					<term>Structure Information Model- ing</term>
					<term>Transformer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fine-grained visual categorization (FGVC) aims at recognizing objects from similar subordinate categories, which is challenging and practical for human's accurate automatic recognition needs. Most FGVC approaches focus on the attention mechanism research for discriminative regions mining while neglecting their interdependencies and composed holistic object structure, which are essential for model's discriminative information localization and understanding ability. To address the above limitations, we propose the Structure Information Modeling Transformer (SIM-Trans) to incorporate object structure information into transformer for enhancing discriminative representation learning to contain both the appearance information and structure information. Specifically, we encode the image into a sequence of patch tokens and build a strong vision transformer framework with two well-designed modules: (i) the structure information learning (SIL) module is proposed to mine the spatial context relation of significant patches within the object extent with the help of the transformer's selfattention weights, which is further injected into the model for importing structure information; (ii) the multi-level feature boosting (MFB) module is introduced to exploit the complementary of multi-level features and contrastive learning among classes to enhance feature robustness for accurate recognition. The proposed two modules are light-weighted and can be plugged into any transformer network and trained end-to-end easily, which only depends on the attention weights that come with the vision transformer itself. Extensive experiments and analyses demonstrate that the proposed SIM-Trans achieves state-of-the-art performance on finegrained visual categorization benchmarks. The code is available at https://github.com/PKU-ICST-MIPL/SIM-Trans_ACMMM2022.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Fine-grained visual categorization (FGVC) task <ref type="bibr" target="#b34">[35]</ref> targets at recognizing object into specific subcategory from a given basic category, such as identifying bird species <ref type="bibr" target="#b29">[30]</ref>. It is different from generic image classification which only needs to predict the basic category, * Corresponding author. such as "Bird". There exist much more challenges due to its intrinsic large intra-class variance and small inter-class variance. For example, "Artic Tern" and "Common Tern" belong to the "Bird" basic category and only have subtle difference in the tail and beak, which are hard to distinguish. Thus, locating discriminative regions to extract features and designing high-order robust features are researched for addressing the above problem. The localization based methods are widely studied for its better interpretation and promising performance.</p><p>Early works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b36">37]</ref> are designed to localize discriminative regions with the help of human annotations, i.e., bounding box of object or part annotations. Whereas, human annotations for finegrained image classification are hard to obtain due to the strict requirements for expertise. Aiming at addressing this problem, a lot of researches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42]</ref> have been conducted on the weakly-supervised fine-grained visual categorization task, which only utilizes the image category labels. They mainly detect semantic parts explicitly <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b38">39]</ref> or conduct saliency regions positioning implicitly <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b41">42]</ref>, which follows the feature extraction and fusion for final classification. The first class of methods such as <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14]</ref> mainly adopt the region proposal network to obtain the location of discriminative image regions. The selected image regions are resized into fixed size and input into the backbone network for feature extraction and classification. And the second class of methods such as <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23]</ref> exploit the attention mechanism for salient regions detection and utilization, which can be flexibly designed along with the backbone network. However, the above methods generally ignore the relation among discriminative regions within object in the model designing. It may cause bad localization results with large area of irrelevant background, which leads to a sharp drop of classification performance. Meanwhile, when reviewing the intrinsic structure of the CNN based FGVC methods, we find that the stacked convolution and pooling operations bring both the expansion of the receptive field and the degradation of spatial discrimination. Large continuous image areas are focused on and discriminative details are generally overlooked, which are essential for distinguishing subtle difference in fine-grained visual categorization.</p><p>Recently, vision transformer (ViT) <ref type="bibr" target="#b7">[8]</ref> and its variants present a new solution of encoding image into a sequence of patch tokens for recognition, which has achieved promising performance. The multihead self-attention mechanism in transformer provides long-range dependency to enhance the interaction among image patches. And the discriminative patch information is kept with the deeper of the transformer layer for deciding the final classification. Thus, the vision transformer can alleviate the aforementioned problems in CNN based FGVC methods to some extent. RAMS-Trans <ref type="bibr" target="#b15">[16]</ref> proposes the dynamic patch proposal module to guide region amplification for multi-scale learning in fine-grained visual categorization. However, the above methods mainly focus on significant patch tokens selection while ignoring their relation in the holistic object structure, which is also essential for identifying discriminative regions. For example, models can be puzzled due to lacking the cognitive ability for object structure in many cases <ref type="bibr" target="#b41">[42]</ref>, such as localizing legs of a bird among twigs.</p><p>Therefore, we propose the structure information modeling transformer dubbed SIM-Trans to introduce structure information into the vision transformer for fine-grained visual categorization. SIM-Trans attempts to model the context information among regions within the object to highlight discriminative regions for accurate recognition. Firstly, we construct a vision transformer backbone to encode the image into a sequence of patch tokens for feature extraction. Secondly, in order to model the object structure information, we propose the structure information learning (SIL) module to mine spatial context relations among discriminative patches within the object extent. Benefiting from the self-attention characteristics of transformer, the attention weight between the cls token (standing for the whole image) and patch token is highly correlated with whether the patch token contains the object information. Thus, discriminative patches can be selected expediently in this way. The relative position relation and semantic relation among patches are calculated to construct the graph of depicting the object structure information, which is further extracted and injected into the backbone by graph convolution. The SIL module can boost the model to learn the object structural composition and highlight significant regions through end-to-end training. Thirdly, to further enhance the feature robustness and discrimination, a multi-level feature boosting (MFB) module is designed. We propose to concatenate the features from the last three transformer layers to take advantage of their complementary, which have been injected the object structure information by the aforementioned SIL module. Besides, contrastive learning is introduced to further boost model's performance, which enhances the feature representation similarity of samples from the same category and weaken that from different categories.</p><p>The main contributions made in this paper can be summarized as follows:</p><p>? We propose the structure information modeling transformer dubbed SIM-Trans with two well-designed modules for boosting fine-grained representation learning to contain both the appearance information and structure information. ? The structure information learning (SIL) module is proposed to mine the spatial context relations among discriminative regions within object extent, which boosts the model's understanding ability for object structure. The multi-level feature boosting (MFB) module is designed to exploit the complementary of multi-level features and contrastive learning for robust feature representation. ? The proposed SIL and MFB modules are light-weighted, which only depend on the attention weighs that come with the vision transformer itself. They can be easily plugged into any vision transformer backbones and trained endto-end. Extensive experiments and analyses on two typical fine-grained visual categorization benchmarks demonstrate that our proposed method achieves new state-of-theart. The rest of the paper is organized as follows: Section 2 briefly reviews the related work on fine-grained visual categorization and vision transformer. Section 3 elaborates our SIM-Trans approach and Section 4 introduces the experimental results and analyses, as well as ablation studies. Finally, Section 5 concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we briefly introduce the related work of fine-grained visual categorization and vision transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Fine-grained Visual Categorization</head><p>Recent approaches mainly focus on the discriminative regions discovery and feature extraction for fine-grained visual categorization <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>. Ding et al. <ref type="bibr" target="#b6">[7]</ref> propose S3N to utilize sparse attention to estimate informative regions and extract discriminative and complementary features for classification. He et al. <ref type="bibr" target="#b14">[15]</ref> propose M2DRL to automatically determine the location and number of discriminative regions with reinforcement learning paradigm. Song et al. <ref type="bibr" target="#b25">[26]</ref> propose Bi-Modal PMA to capture discriminative parts stage-by-stage with the progressive mask attention model. Rao et al. <ref type="bibr" target="#b23">[24]</ref> propose a counterfactual attention learning method to obtain more useful attention for fine-grained visual categorization. However, the above methods generally ignore object's holistic structure information, which is greatly helpful for localizing the whole object extent. In our proposed method, the object structure information is introduced explicitly for highlighting the discriminative regions within object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Vision Transformer</head><p>Recently, many vision transformer methods have been proposed for computer vision tasks. ViT <ref type="bibr" target="#b7">[8]</ref> is the first work to introduce the pure transformer into image classification, which splits the image into a sequence of patch tokens as input of transformer and achieves promising performance. Zheng et al. <ref type="bibr" target="#b40">[41]</ref> propose SETR to utilize ViT as the encoder for image segmentation. He et al. <ref type="bibr" target="#b12">[13]</ref> propose TransReID to introduce the jigsaw patch module and side information embeddings into transformer for object re-identification. Recently, RAMS-Trans <ref type="bibr" target="#b15">[16]</ref> is proposed for fine-grained visual categorization, which designs the recurrent attention multi-scale transformer to find and amplify significant regions for learning multi-scale features. However, the above vision transformer works generally ignore the spatial relation of patches, which is important for significant patches discovery. Our proposed SIM-Trans incorporates the object structure information into transformer to enhance discriminative patch features for accurate fine-grained visual categorization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">APPROACH</head><p>Our proposed SIM-Trans is based on the vision transformer with several critical improvements for specializing in fine-grained visual categorization task. The overall framework of SIM-Trans is shown   in <ref type="figure" target="#fig_2">Figure 1</ref>. The vision transformer backbone (the middle part) takes image patch tokens and works as an extractor. The self-attention weights of the transformer layer are borrowed by the structure information learning (SIL) module (the right part) to mine the spatial context information of discriminative patches within the object. The multi-level feature boosting (MFB) module (the left part) fuses the feature from different levels to exploit their complementary, and the feature is also boosted by contrastive learning simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-level Feature Boosting Vison Transformer</head><formula xml:id="formula_0">Q 1 Q 1 K 1 K 1 V 1 V 1 ... Q 2 K 2 V 2 Q H K H V H Object</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Vision Transformer Backbone</head><p>The vision transformer backbone is composed of feature extractor and classifier head, which is shown in the middle part of <ref type="figure" target="#fig_2">Figure 1</ref>. Before being input to the feature extractor, the image ? ? ?3 , where and denotes its height and width respectively, is generally split into patch tokens through non-overlapping splitting, denoted as { 1 ? , 2 ? , ..., ? }. However, this splitting way may cause the incomplete neighboring information within the patch due to the hard separation. Thus, we adopt the sliding window splitting method following <ref type="bibr" target="#b12">[13]</ref>. Specifically, the patch size and the window's sliding step are denoted as and , the patch sequence length for the image can be calculated as follows:</p><formula xml:id="formula_1">= ? = ? + 1 ? ? + 1<label>(1)</label></formula><p>where ??? denotes the floor operation, and denote the patch numbers in vertical and horizontal directions after splitting. In this way, the local neighboring information preservation problem is alleviated to some degree.</p><p>The patch token { ? } is then projected into -dimensional embedding through linear mapping (?). For representing the whole image, a learnable embedding token is introduced and put at the beginning of the input sequence embeddings. To incorporate position information, learnable position embeddings ? ( +1)? are added to the input sequence embeddings to get 0 as the input of the first transformer layer as follows:</p><formula xml:id="formula_2">0 = [ , ( 1 ? ), ( 2 ? ), ..., ( ? )] +<label>(2)</label></formula><p>The transformer feature extractor is composed of transformer layers, each of which consists of a multi-head self-attention (MSA) module and a feed forward neural network of two fully connected layers. The output of the ? transformer layer is calculated as follows:</p><formula xml:id="formula_3">? = ( ( ?1 ) + ?1 ) (3) = ( ( ? ) + ? ) (4) where</formula><p>(?) denote the layer normalization <ref type="bibr" target="#b0">[1]</ref>. The output tokens of the last three transformer layers are concatenated and regarded as image representation (more details in Sec. 3.3), which is forwarded into a classifier head to get the prediction vector ( ) for the input image . The classification loss is as follows:</p><formula xml:id="formula_4">= ? ?? ? ( ) ( ? log( ( )))<label>(5)</label></formula><p>where ( ) is the training set and is the one-hot label of image .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Structure Information Learning</head><p>The vision transformer backbone can achieve promising image classification results, which utilizes the self-attention mechanism to own a global receptive filed. However, the vision transformer framework generally ignores the spatial relation of patches, which is important for identifying discriminative patches in fine-grained visual categorization task. Thus, we propose the structure information learning (SIL) module to incorporate the object spatial context information into the vision transformer and the whole procedure is shown in the right part of <ref type="figure" target="#fig_2">Figure 1</ref>.</p><p>Localizing the object extent is the precondition for structure learning. In the transformer layer, the attention weight between the patch token and cls token depicts its importance for the final classification, which is highly correlated with whether the patch token contains the object information. Thus, object can be naturally localized with the help of the attention weights. Suppose the transformer layer has heads, and are -dimensional query vectors and key vectors of all tokens, then the attention weights can be calculated as follows:</p><formula xml:id="formula_5">? = max( ?? / )<label>(6)</label></formula><p>where ? ? ( +1)?( +1) , ? = 1, 2, ..., and = ? is the number of patches. The attention weight between patch token and the cls token for each head is extracted and denoted as ? ? ?1 . Corresponding total attention weights are calculated as follows:</p><formula xml:id="formula_6">= ?? ?=1 ?<label>(7)</label></formula><p>The attention weight between the patch token in ( , ) position and the cls token is denoted as <ref type="bibr">( , )</ref> . For filtering out insignificant patches, the mean value?is calculated as the threshold and the new attention weight is as follows:</p><formula xml:id="formula_7">( , ) = ( , ) ( , ) &gt;0 ?<label>(8)</label></formula><p>Inspired by <ref type="bibr" target="#b41">[42]</ref>, polar coordinates are applied to measure the spatial relation between the most discriminative patch and other patch to mine object's structure information. Specifically, the patch with the highest attention weight is regarded as the most discriminative patch, which is set as the reference patch. Given the reference patch 0 = 0 , 0 and the horizontal reference direction, where ( 0 , 0 ) are the indices in ? plane, the polar coordinates of patch , can be defined as:</p><formula xml:id="formula_8">, = ?? ( ? 0 ) 2 + ( ? 0 ) 2 (9) , = (arctan 2( ? 0 , ? 0 ) + ) 2<label>(10)</label></formula><p>where , is the relative distance between , and 0 and , is the normalized polar angle of , relative to the horizontal direction. To introduce this object structure information, we design the graph convolutional neural network to obtain the object structure feature. We firstly construct the graph which contains two components: (i) the image patch node features which depict the spatial context correlation based on the calculation of the polar coordinates and (ii) the edge weights obtained based on attention weight calculation between cls token and image patch token in the vision transformer layer, which summarize the significance of image patch token. Specifically, the matrix = ? ( ) denotes the edge weights among nodes based on the , where the edge weights related to insignificant patches are zeros to filter out their affect. The graph convolution is then adopted to further extract and incorporate the structure information into the vision transformer. The structure features are obtained by two-layer graph convolution as follows:</p><formula xml:id="formula_9">= ( ? ( ? ? 1 ) ? 2 )<label>(11)</label></formula><p>where 1 and 2 are learnable parameters and (?) is activation function. The feature of the reference patch node is regarded as the object structure feature, which is further added to the cls token feature for introducing structure information into the transformer backbone. Through the end-to-end training, the composition of the object can be modeled and the significant image patch can be highlighted, which improves the model's classification performance.</p><p>Overall, the structure information learning module can incorporate the object structure information, i.e., its spatial composition of key discriminative patches, into one or more vision transformer layers. The transformer network can be empowered to learn both appearance and structure information for accurate fine-grained classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multi-level Feature Boosting</head><p>After obtaining image features by the transformer feature extractor, the feature of the cls token of the last transformer layer is generally selected as the image representation for final classification in vision transformer methods, such as ViT. However, it generally ignores the complementary of different levels' features and the utilization of the inherent intra-class and inter-class semantic relations. Thus, we propose the multi-level feature boosting (MFB) module for enhancing feature robustness, which is shown in the left part of the <ref type="figure" target="#fig_2">Figure 1</ref>.</p><p>Specifically, the features of cls tokens from the last three transformer layers are concatenated as the final image feature representation, which all introduce structure information through the structure learning module. In this simple but effective way, their complementary can be exploited to bring performance gains. To take full advantage of the semantic relations for feature enhancement, contrastive learning is adopted to enhance the feature similarity of the same category and weaken that of different categories. To mine the hard negative sample pairs to contribute to the model's training, a hyper-parameter is adopted in the following contrastive learning loss. The negative pairs with similarity which is smaller than that of the positive pairs are filtered out. Formally, the contrastive learning loss on a batch of size is defined as follows:</p><formula xml:id="formula_10">, = max{0, + ( , ? ) ? 1 ? ( )= ( ), ? : ? ( , + )} (12) = 1 2 =1 [ : ( )= ( ) (1 ? ( , + )) + : ( )? ( ) , ? ( , ? )]<label>(13)</label></formula><p>where ( , + ) denotes the positive image representation pair which have the same category label, i.e., ( ) = ( ), ( , ? ) denotes the negative image representation pair which belongs to different categories, ? = , ? denotes the number of positive pairs, (?) denotes the cosine similarity calculation.</p><p>In summary, the vision transformer backbone, structure information learning module and feature boosting module of the proposed method are jointly trained end to end. The total objective is as follows:</p><p>= +</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we evaluate the performance of the proposed SIM-Trans approach on two standard fine-grained visual categorization datasets, i.e., the typical CUB-200-2011 <ref type="bibr" target="#b29">[30]</ref> dataset and the largescale iNaturalist 2017 <ref type="bibr" target="#b28">[29]</ref> dataset. We firstly introduce the datasets and evaluation metric. Then, we show specific implementation details, comparison experiments and analyses with recent stateof-the-art methods for each dataset respectively. Finally, in order to verify the effectiveness of the proposed modules, we conduct ablation experiments and analyses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Evaluation Metric</head><p>Two standard fine-grained visual categorization benchmarks are adopted in the experiments:</p><p>? CUB-200-2011 <ref type="bibr" target="#b29">[30]</ref> is the most widely used dataset in the fine-grained visual categorization task. This dataset consists of 11788 images of 200 bird subcategories, where 5994 images are selected as training set and 5794 images are selected as testing set. ? iNaturalist 2017 <ref type="bibr" target="#b28">[29]</ref> is one of the largest fine-grained visual categorization datasets. it contains more than 5,000 finegrained categories and more than 95,000 test images. The detailed statistics are shown in table 1, which includes category number and the split of training data and testing data. The dataset is very challenging for its large-scale finegrained category number and testing data, which provides a strong benchmark for fine-grained visual categorization performance contrast. Accuracy is adopted as the evaluation metric, which is generally used in fine-grained visual categorization task. And the definition is as follows:</p><formula xml:id="formula_12">= | right | | |<label>(15)</label></formula><p>where | | denotes the number of images in the testing set and | right | denotes the number of images that are correctly classified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiments and Analyses on CUB-200-2011</head><p>4.2.1 Implentation Details. In the experiments, the images are resized into 600 ? 600 and randomly cropped into the size of 448 ? 448 as input during the training stage. Random horizontal flip and normalization are used for preprocessing. All the above steps are standard pre-process steps which are generally used in fine-grained visual categorization methods, such as <ref type="bibr" target="#b15">[16]</ref>. In the testing phase, the images are resized into the size of 600 ? 600 and cropped into the size of 448 ? 448 from the center. We adopt ViT as the backbone and the initial weights are loaded from the official ViT-B_16 model pre-trained on the ImageNet 21k dataset, which is also adopted in RAMS-Trans <ref type="bibr" target="#b15">[16]</ref>. Thus, the comparison experiments with state-ofthe-art (SOTA) transformer-based methods are fair and convincing. The image is split into patches of size 16 and the sliding window step is set as 12, which are and in Eq. 1 respectively. We adopt the stochastic gradient descent (SGD) optimizer with a momentum of 0.9 for model optimization. The learning rate is initialized as 3e-2 and cosine annealing schedule <ref type="bibr" target="#b21">[22]</ref> is exploited to update the learning rate. The batch size is set as 5 and the number of total training steps is set to be 10000 and the first 500 steps are warm-up. We adopt the last three transformer layers to plug in the structure information learning module in Sec. 3.2 and the hyper-parameter in Eq. 12 is set to be 0.3. We perform the experiments with PyTorch using NVIDIA GeForce GTX 1080 Ti GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.2.2</head><p>Comparisons with State-of-the-art Methods . This subsection presents comparison experimental results and analyses with other state-of-the-art (SOTA) methods including CNN based methods and transformer based methods on CUB-200-2011. For fair comparison, all the methods adopt the same training and testing setting, such as the same input image resolution. From <ref type="table" target="#tab_1">Table 2</ref>, we can observe that:</p><p>? On CUB-200-2011, our proposed SIM-Trans approach achieves the best classification accuracy of 91.8%, which brings 1.2% improvement than the ViT baseline method. Compared with the optimal CNN based method CAL <ref type="bibr" target="#b23">[24]</ref>, our SIM-Trans achieves 1.2% performance improvement. In the CNN based methods, attention mechanism and multi-scale learning are widely used for discriminative information mining and feature robustness enhancement. For example, the counterfactual attention learning method is proposed to learn more effective attention for fine-grained classification in CAL. In PMG <ref type="bibr" target="#b8">[9]</ref>, mutli-granularity information with jigsaw operation is introduced to boost model's recognizing from different scales. However, the above CNN based methods are generally affected by the spatial resolution degradation, which is caused by the convolutional strides and pooling operation. The transformer based methods </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone Acc(%) KP (CVPR 2017) <ref type="bibr" target="#b5">[6]</ref> VGG16 86.2 MA-CNN (ICCV 2017) <ref type="bibr" target="#b38">[39]</ref> VGG19 86.5 PC (ECCV 2018) <ref type="bibr" target="#b9">[10]</ref> DenseNet161 86.9 NTS-Net (ECCV 2018) <ref type="bibr" target="#b35">[36]</ref> ResNet50 87.5 M2DRL (IJCV 2019) <ref type="bibr" target="#b14">[15]</ref> VGG16 87.2 S3N (ICCV 2019) <ref type="bibr" target="#b6">[7]</ref> ResNet50 88.5 FDL (AAAI 2020) <ref type="bibr" target="#b20">[21]</ref> ResNet50 88.6 LIO (CVPR 2020) <ref type="bibr" target="#b41">[42]</ref> ResNet50 88.0 PMG (ECCV 2020) <ref type="bibr" target="#b8">[9]</ref> ResNet50 89.6 DP-Net (AAAI 2021) <ref type="bibr" target="#b30">[31]</ref> ResNet50 89.3 GaRD (CVPR 2021) <ref type="bibr" target="#b37">[38]</ref> ResNet50 89.6 Chang et al. (CVPR 2021) <ref type="bibr" target="#b3">[4]</ref> ResNet50 89.9 SPS (ICCV 2021) <ref type="bibr" target="#b16">[17]</ref> ResNet50 88.7 Joung et al. (ICCV 2021) <ref type="bibr" target="#b18">[19]</ref> ResNet50 88.4 CAL (ICCV 2021) <ref type="bibr" target="#b23">[24]</ref> ResNet101 90.6 MCEN (ACM MM 2021) <ref type="bibr" target="#b19">[20]</ref> ResNet50 89.3 ViT (ICLR 2020) <ref type="bibr" target="#b7">[8]</ref> ViT-B_16 90.6 RAMS-Trans (ACM MM 2021) <ref type="bibr" target="#b15">[16]</ref> ViT-B_16 91.3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our SIM-Trans approach</head><p>ViT-B_16 91.8</p><p>can keep the patch information with the deeper of the layers, which can keep discriminative detailed information. Thus, the transformer based method have the advantage for fine-grained visual categorization and even the pure vision transformer ViT baseline can perform as well as the SOTA CNN based method, achieving 90.6% classification accuracy. ? Compared with the transformer based methods, our SIM-Trans also show the best performance, outperforming the RAMS-Trans by a margin of 0.5%. RAMS-Trans proposes to use the attention weights to guide the model to amplify discriminative regions for multi-scale learning, which is a heuristic work to combine the advantages from both the CNN and transformer. However, the multi-branch framework increases the computation cost and the parametersharing in different scales may cause the model's confusion. By contrast, our SIM-Trans guides the model to focus on the holistic object structure and its component discriminative patches. It is a soft way to boost the significant patches and inhibit the useless patches, such as the background patch. Our SIM-Trans approach makes an attempt to empower the model to conduct fine-grained recognition from both the object structure and the local discriminative patches. Learning both the object structure information and fine-grained appearance information makes our SIM-Trans approach achieve better classification performance with strong fine-grained representation ability. extent and highlight discriminative regions, which verifies its effectiveness in discriminative information mining. In order to visualize the structure learning effect of our SIM-Trans approach, we present the raw images and focus regions of our SIM-Trans model in <ref type="figure" target="#fig_4">Figure 3</ref>. The first and third rows show the raw images sampled from the CUB-200-2011 dataset and iNaturalist 2017 dataset respectively. And the second and fourth rows are corresponding focus regions. From <ref type="figure" target="#fig_4">Figure 3</ref>, we can observe that the entire object and discriminative regions are highlighted precisely, which verifies the effectiveness and improve the interpretability of our proposed SIM-Trans approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiments and Analyses on iNaturalist 2017</head><p>4.3.1 Implementation Details. For fair comparison with other methods, the size of images is set as 304 ? 304 following <ref type="bibr" target="#b15">[16]</ref>. We load the weights from the official ViT-B_16 model pre-trained on the ImageNet 21k dataset for fair comparisons with other SOTA transformer based methods. The batch size is set to be 16 and the number of total training steps is set to be 100000. Stochastic gradient descent (SGD) optimizer with a momentum of 0.9 is adopted, and the learning rate is set as 1e-2 with cosine annealing scheduler. We introduce the structure information in the last three transformer layers and the hyper-parameter in Eq. 12 is set as 0.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vermilion Flycatcher</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Anna Hummingbird Green Kingfisher Scarlet Tanager American Redstart Fish Crow</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Agalychnis dacnicolor Coccinella septempunctata</head><p>Eriophora pustulosa Eudryas grata Salamandra salamandra Orthodera novaezealandiae  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone Acc(%) ResNet152 (CVPR 2016) <ref type="bibr" target="#b11">[12]</ref> ResNet152 59.0 IncResNetV2 (AAAI 2017) <ref type="bibr" target="#b27">[28]</ref> InResNetV2 67.3 SSN (ECCV 2018) <ref type="bibr" target="#b24">[25]</ref> ResNet101 65.2 TASN (CVPR 2019) <ref type="bibr" target="#b39">[40]</ref> ResNet101 68.2 Huang et al. (CVPR 2020) <ref type="bibr" target="#b17">[18]</ref> ResNet101 66.8 ViT (ICLR 2020) <ref type="bibr" target="#b7">[8]</ref> ViT-B_16 67.0 RAMS-Trans (ACM MM 2021) <ref type="bibr" target="#b15">[16]</ref> ViT-B_16 68.5 Our SIM-Trans approach ViT-B_16 69.9</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.3.2</head><p>Comparisons with State-of-the-art Methods . To fully validate the effectiveness of our proposed SIM-Trans approach for fine-grained visual categorization, comparison experiments with SOTA methods on the large-scale fine-grained visual categorization benchmark iNaturalist 2017 are conducted. <ref type="table" target="#tab_2">Table 3</ref> summarizes the result, and we can observe that:</p><p>? On iNaturalist 2017, our proposed SIM-Trans approach achieves the best performance with 69.9% classification accuracy, which outperforms the pure resnet152 and the pure ViT (our baseline) by 10.9% and 2.9% respectively. Compared with the optimal CNN based method TASN and transformer based method RAMS-Trans, our SIM-Trans achieves 1.7% and 1.4% improvements respectively. Thanks to the simplicity and effectiveness of introducing structure information into vision transformer, our SIM-Trans approach is able to extend to both academic dataset and large-scale challenging scenario. The above comparison experiment results verify the effectiveness of our proposed SIM-Trans approach on the large-scale FGVC benchmark, whose categories are more than 5,000 with more than 95,000 test images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Qualitative</head><p>Results . The visualization results of our SIM-Trans approach on iNaturalist 2017 are also shown in <ref type="figure" target="#fig_4">Figure 3</ref>. Visual objects of different subcategories belonging to different super categories can be highlighted precisely, which verifies the accurate object structure modeling and generalization ability of our proposed SIM-Trans model.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Effectiveness of Each Component in Our SIM-Trans Approach</head><p>Structure information learning (SIL) module and multi-level feature boosting (MFB) module are proposed in our SIM-Trans approach. We conduct ablation experiments on CUB-200-2011 dataset and the effectiveness of each component is shown in <ref type="table" target="#tab_3">Table 4</ref> and <ref type="figure" target="#fig_5">Figure 4</ref>, we can observe that:</p><p>? A classifier head is added to the ViT extractor as our baseline, which can achieve 90.6% classification accuracy on CUB-200-2011 dataset. Compared with our baseline, the structure information learning (SIL) module improves the classification accuracy by a margin of 0.5%, which is simply added to the last transformer layer as a plug-in. The SIL module boosts the model's understanding ability for the object structure and highlights significant regions to make the feature representation more discriminative. ? Based on the SIL module, we conduct multi-layer structure information injection and the multi-level feature boosting (MFB) for robust feature learning, which can bring another 0.7% performance gain. We attempt to add the SIL module from the top layer to the bottom layer, and <ref type="figure" target="#fig_5">Figure 4</ref> shows when the number of layers is set as three, the proposed model can achieve the best performance. It also demonstrates the combination of SIL's expansion and MFB's fusion contributes to each other for improving the classification performance. ? In MFB, the contrastive learning can improve the classification accuracy by a margin of 0.4%, which attends to hard negative samples and utilize the semantic relations to boost the model's feature discrimination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Hyper-parameter Experiments</head><p>There is only one hyper-parameter in our SIM-Trans model designing. The of MFB module in Eq. 12 controls the hardness degree of negative pairs in contrastive learning. We analyze the influence of on the classification performance in <ref type="figure" target="#fig_7">Figure 5</ref>. When is set to  be 0.3, the proposed SIM-Trans model can achieve the best classification performance. Continuing to increase , the performance degrades because the over-high threshold cannot filter out easy negative pairs, which is harmful for contrastive learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we propose the structure information modeling transformer (SIM-Trans) that introduces the object structure information into vision transformer for boosting the discriminative feature learning to contain both the appearance and structure information. Structure information learning (SIL) module is proposed to mine spatial context relation of significant patches within the object extent, which boosts model's understanding ability for object structure and highlights discriminative regions. Multi-level feature boosting (MFB) module is then proposed to exploit the complementary of multi-level features and contrastive learning to further enhance feature representation robustness for accurate fine-grained recognition. The combination of the proposed two modules boosts each other and promotes the feature discrimination. The proposed SIM-Trans approach provides an attempt to model object structure in transformer framework and achieves state-of-the-art for FGVC task on two typical fine-grained benchmarks, especially achieving promising performance on the large-scale iNaturalist 2017 benchmark.</p><p>In the future, the structure embedding to the initial layer of the transformer framework will be studied for better structure modeling to improve fine-grained visual categorization performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ACKNOWLEDGMENTS</head><p>This work was supported by the grants from the National Natural Science Foundation of China (61925201, 62132001, U21B2025) and the National Key R&amp;D Program of China (2021YFF0901502).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>The overall framework of our proposed SIM-Trans.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>4. 2 . 3 Figure 2 :</head><label>232</label><figDesc>Qualitative Results . As shown in Figure 2, compared with the ViT baseline, our proposed SIM-Trans can not only filter out irrelevant background information but also complete the object Attention visualization based on ViT baseline and our proposed SIM-Trans. The first column is original images and the second column shows the attention generated by ViT baseline. The last column shows the attention generated by SIM-Trans.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Visualization results of SIM-Trans on CUB-200-2011 and iNaturalist 2017 datasets. The first row and the third row are original images while the second and the fourth rows present focus regions generated by our SIM-Trans model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Layer number experiments of structure information introduction on CUB-200-2011 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Hyper-parameter experiments of on CUB-200-2011 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Introduction of the iNaturalist 2017 dataset.</figDesc><table><row><cell>Super Class</cell><cell>Class</cell><cell cols="2">Train Images Test Images</cell></row><row><cell>Plantae</cell><cell>2101</cell><cell>158407</cell><cell>38206</cell></row><row><cell>Insecta</cell><cell>1021</cell><cell>100479</cell><cell>18076</cell></row><row><cell>Aves</cell><cell>964</cell><cell>214295</cell><cell>21226</cell></row><row><cell>Reptilia</cell><cell>289</cell><cell>35201</cell><cell>5680</cell></row><row><cell>Mammalia</cell><cell>186</cell><cell>29333</cell><cell>3490</cell></row><row><cell>Fungi</cell><cell>121</cell><cell>5826</cell><cell>1780</cell></row><row><cell>Amphibia</cell><cell>115</cell><cell>15318</cell><cell>2385</cell></row><row><cell>Mollusca</cell><cell>93</cell><cell>7536</cell><cell>1841</cell></row><row><cell>Animalia</cell><cell>77</cell><cell>5228</cell><cell>1362</cell></row><row><cell>Arachnida</cell><cell>56</cell><cell>4873</cell><cell>1086</cell></row><row><cell>Actinopterygii</cell><cell>53</cell><cell>1982</cell><cell>637</cell></row><row><cell>Chromista</cell><cell>9</cell><cell>398</cell><cell>144</cell></row><row><cell>Protozoa</cell><cell>4</cell><cell>308</cell><cell>73</cell></row><row><cell>Total</cell><cell>5089</cell><cell>579184</cell><cell>95986</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison experiments with other state-of-theart methods on CUB-200-2011 dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison experiments with other state-of-theart methods on iNaturalist 2017 dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Ablation Experiments on CUB-200-2011 dataset.</figDesc><table><row><cell>Method</cell><cell>Acc(%)</cell></row><row><cell>Baseline</cell><cell>90.6</cell></row><row><cell>Baseline + SIL</cell><cell>91.1</cell></row><row><cell>Baseline + SIL + MFB_without_CL</cell><cell>91.4</cell></row><row><cell>Baseline + SIL + MFB</cell><cell>91.8</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Poof: Part-based one-vs.-one features for fine-grained categorization, face verification, and attribute estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="955" to="962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Bird species categorization using pose normalized deep convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grant</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2952</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Your &quot;Flamingo&quot; is My &quot;Bird&quot;: Fine-Grained, or Not</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyue</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11476" to="11485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Destruction and construction learning for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5157" to="5166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Kernel pooling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2921" to="2930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Selective sparse sampling for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6599" to="6608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<title level="m">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fine-grained visual classification via progressive multi-granularity training of jigsaw patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoyi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayan</forename><surname>Kumar Bhunia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="153" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pairwise confusion for fine-grained visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhimanyu</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otkrist</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Naik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV</title>
		<meeting>the European conference on computer vision (ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="70" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Weakly supervised complementary parts models for fine-grained image classification from the bottom up</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangru</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3034" to="3043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Transreid: Transformer-based object re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuting</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15013" to="15022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast fine-grained image classification via weakly supervised discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangteng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1394" to="1407" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Which and how many regions to gaze: Focus discriminative regions for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangteng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="page" from="1235" to="1255" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">RAMS-Trans: Recurrent Attention Multi-scale Transformer for Fine-grained Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiwen</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4239" to="4248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stochastic Partial Swap: Enhanced Model Generalization and Interpretability for Fine-Grained Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoli</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="620" to="629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Interpretable and accurate fine-grained recognition via region grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8662" to="8672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning Canonical 3D Object Representation for Fine-Grained Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghun</forename><surname>Joung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ig-Jae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanghoon</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1035" to="1045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-branch Channelwise Enhancement Network for Fine-grained Visual Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangjun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengting</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5273" to="5280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Filtration and distillation: Enhancing region attention for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfeng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingyun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11555" to="11562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<title level="m">Sgdr: Stochastic gradient descent with warm restarts</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Object-part attention model for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangteng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1487" to="1500" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Counterfactual attention learning for fine-grained visual categorization and re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1025" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to zoom: a saliency-based sampling layer for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adria</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Kellnhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV</title>
		<meeting>the European Conference on Computer Vision (ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="51" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bi-Modal Progressive Mask Attention for Fine-Grained Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangbo</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren-Jie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="7006" to="7018" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fine-grained recognition: Accounting for subtle differences between similar classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisham</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12047" to="12054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-first AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The inaturalist species classification and detection dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Grant Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8769" to="8778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<title level="m">The caltech-ucsd birds-200-2011 dataset</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dynamic Positionaware Network for Fine-grained Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning a discriminative filter bank within a cnn for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vlad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4148" to="4157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Graph-Propagation Based Correlation Learning for Weakly Supervised Fine-Grained Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianjun</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12289" to="12296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Weakly Supervised Fine-Grained Image Classification via Guassian Mixture Model Oriented Discriminative Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianjun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zezhou</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9749" to="9758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fine-Grained Image Analysis with Deep Learning: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning to navigate for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiange</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV</title>
		<meeting>the European Conference on Computer Vision (ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="420" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Part-based R-CNNs for fine-grained category detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="834" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Graph-based high-order relation discovery for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15079" to="15088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning multiattention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5209" to="5217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Looking for the devil in the details: Learning trilinear attention sampling network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5012" to="5021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6881" to="6890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Look-intoobject: Self-supervised structure modeling for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11774" to="11783" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
