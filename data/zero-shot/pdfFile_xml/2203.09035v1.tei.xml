<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HybridNets: End-to-End Perception Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">T</forename><surname>Dat</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">FPT University</orgName>
								<address>
									<addrLine>Hoa Lac High Tech Park</addrLine>
									<postCode>10000</postCode>
									<settlement>Hanoi</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nvh</forename><surname>Bao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">FPT University</orgName>
								<address>
									<addrLine>Hoa Lac High Tech Park</addrLine>
									<postCode>10000</postCode>
									<settlement>Hanoi</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">D</forename><surname>Hung</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">FPT University</orgName>
								<address>
									<addrLine>Hoa Lac High Tech Park</addrLine>
									<postCode>10000</postCode>
									<settlement>Hanoi</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">HybridNets: End-to-End Perception Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Received: xx, 2022</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>End-to-end network</term>
					<term>multi-task learning</term>
					<term>detection</term>
					<term>segmentation</term>
					<term>autonomous-driving</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>End-to-end Network has become increasingly important in multi-tasking. One prominent example of this is the growing significance of a driving perception system in autonomous driving. This paper systematically studies an end-to-end perception network for multi-tasking and proposes several key optimizations to improve accuracy. First, the paper proposes efficient segmentation head and box/class prediction networks based on weighted bidirectional feature network. Second, the paper proposes automatically customized anchor for each level in the weighted bidirectional feature network. Third, the paper proposes an efficient training loss function and training strategy to balance and optimize network. Based on these optimizations, we have developed an end-to-end perception network to perform multi-tasking, including traffic object detection, drivable area segmentation and lane detection simultaneously, called HybridNets, which achieves better accuracy than prior art. In particular, HybridNets achieves 77.3 mean Average Precision on Berkeley DeepDrive Dataset, outperforms lane detection with 31.6 mean Intersection Over Union with 12.83 million parameters and 15.6 billion floating-point operations. In addition, it can perform visual perception tasks in real-time and thus is a practical and accurate solution to the multi-tasking problem. Code is available at https://github.com/datvuthanh/HybridNets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Background</head><p>Recent advances in embedded systems' computational power and neural networks' performance have made autonomous driving an active field in computer vision. Ideally, to create a vehicle capable of driving itself is to feed it with every bit of information available in its immediate surroundings. However, unlike conventional thinking, lidar and radar are not required to create an accurate perception field for intelligent vehicles. From time to time, it has been shown that such vehicles can make relatively good driving decisions with just the assistance of a single camera attached to the front. There is a general consensus that the three most critical tasks in guiding intelligent vehicles are: traffic object detection, drivable area segmentation, and lane line segmentation.</p><p>Each one of these tasks has got its state-of-the-art networks, including but not limited to SSD <ref type="bibr" target="#b13">[14]</ref>, YOLO <ref type="bibr" target="#b20">[21]</ref> for object detection; UNet <ref type="bibr" target="#b22">[23]</ref>, SegNet <ref type="bibr" target="#b0">[1]</ref>, ERNet <ref type="bibr" target="#b9">[10]</ref> for semantic segmentation; LaneNet <ref type="bibr" target="#b28">[29]</ref> and SCNN <ref type="bibr" target="#b18">[19]</ref> for lane line detection. Still, passing an image through three different networks creates unreasonable latency. Many researchers (MultiNet <ref type="bibr" target="#b27">[28]</ref>, DLT-Net <ref type="bibr" target="#b19">[20]</ref>, YOLOP <ref type="bibr" target="#b29">[30]</ref>) have thought about combining the networks into a simple encoder-decoder architecture, where the backbone and neck generate context for three different heads to process. The architecture can be improved even further with proper selection of the feature extractor and fusing lane line with drivable area into one segmentation head. This experiment achieves the highest recall of 92.8% and segmentation IoU of 70.8%, outperforming existing multi-task networks on the challenging BDD100K dataset <ref type="bibr" target="#b30">[31]</ref>, as shown qualitatively in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Improvements are made upon the excellent multi-scale feature fusion BiFPN in EfficientDet <ref type="bibr" target="#b25">[26]</ref>, together with an EfficientNet <ref type="bibr" target="#b26">[27]</ref> backbone pre-trained on ImageNet with its balanced trade-off between accuracy and computational overhead. A BiFPN decoder is constructed to utilize existing multi-scale features into the newly designed segmentation head.</p><p>For an input resolution of 640x384, the entire network comes in at 15.6 BFLOPS on 12.83M parameters, comparable to the latest multi-task network YOLOP at 18.6 BFLOPS on 7.9M parameters. A multi-stage learning strategy is employed to help with the convergence of multiple loss functions <ref type="bibr" target="#b4">[5]</ref>.</p><p>To finetune even further, we also tinker with anchor box generation in this study <ref type="bibr" target="#b21">[22]</ref>.</p><p>Because anchor boxes theoretically cannot be generalized well for every dataset, nevertheless having a significant impact on the performance of one-stage detectors, we empirically choose the best possible aspect ratios and scales for the driving dataset BDD100K, where objects vary from large upfront trucks to tiny further cars.</p><p>To sum it up, the main contributions of this research are: 1. HybridNets, an end-to-end perception network, achieving outstanding results in real-time on the BDD100K dataset. 2. Automatically customized anchor for each level in the weighted bidirectional feature network, on any dataset.</p><p>3. An efficient training loss function and training strategy to balance and optimize multi-task networks. The green areas indicate the drivable area, the blue lines are the lane lines, and the orange boxes are the traffic objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Related Works</head><p>This section will review some of the best networks in each respective task, then conclude with the latest multi-task networks to emphasize the strength of this unified architecture. Current developments in improving detectors' performance have nearly split the area into two distinct branches: region-based and one-stage detectors. While region-based methods are more accurate, one-stage detectors gained more attraction due to their efficiency in embedded systems with limited hardware constraints. When FPN came about, it initially supported RPNs by providing a top-down pathway to construct higher resolution layers from a semantic-rich layer <ref type="bibr" target="#b10">[11]</ref>. Then BiFPN officially showed the performance boost of bidirectional feature fusion to one-stage detectors. They can now take in multiple scales of the feature map in just one pass, alleviating the apparent weakness of YOLOs and the like.</p><p>Semantic segmentation has also made remarkable steps with deep-learning instead of the old-fashioned segmentation algorithms. FCN <ref type="bibr" target="#b24">[25]</ref> sparked the flame with the first fully convolutional segmentation network. From then on, researchers have found various ways to improve the performance, such as encoder-decoder architecture with UNet <ref type="bibr" target="#b22">[23]</ref>, the pyramid pooling module of PSPNet <ref type="bibr" target="#b31">[32]</ref>, or even semi-supervised learning based on generative adversarial networks <ref type="bibr" target="#b5">[6]</ref>. SSN <ref type="bibr" target="#b17">[18]</ref> incorporated conditional random field units in the post-processing stage to increase segmentation performance. Many data augmentation techniques have been tested throughout to enhance the learning generalization of road detection networks <ref type="bibr" target="#b16">[17]</ref>. Image analysis is still being explored in segmenting road scenes <ref type="bibr" target="#b8">[9]</ref>.</p><p>Traditional lane line detections algorithms have been in wide use until recently, a notable algorithm being Hough transform <ref type="bibr" target="#b32">[33]</ref>. Then LaneNet <ref type="bibr" target="#b28">[29]</ref> proposed individual lane lines as instances to be segmented. Spatial CNN <ref type="bibr" target="#b18">[19]</ref> preferred slice-by-slice convolutions over deep layer-by-layer convolutions, emphasizing objects with heavy spatial relationships but barely noticeable appearances, such as poles, traffic lights, or lane lines. ENet-SAD <ref type="bibr" target="#b7">[8]</ref> created self attention distillation, a technique allowing models to self-learn. It works by using attention maps generated in earlier training points as a form of supervision for later, surpassing SCNN by a large margin.</p><p>Many published papers attempted to combine perception tasks into a unified network.</p><p>Mask R-CNN <ref type="bibr" target="#b6">[7]</ref> inherited RPN from Faster R-CNN while adding a third output branch for object mask, enabling the parallelization of object detection and instance segmentation. BlitzNet <ref type="bibr" target="#b3">[4]</ref> also showed that object detection and semantic segmentation could benefit from each other.</p><p>LSNet <ref type="bibr" target="#b12">[13]</ref> came with a novel loss function named cross-IoU to add pose estimation into the output. MultiNet put forward the encoder-decoder structure, allowing DLT-Net to design special shared tensors between decoder heads for mutual information streams. Not long after, YOLOP became the first real-time state-of-the-art on the BDD100K dataset on three perception tasks: vehicle detection, drivable area, and lane line segmentation. However, the two similar segmentation heads left room for the obvious optimization task of reducing them to a single better-performing one. As hardware constraint is also of utmost importance to the application of any real-time decision-making network, model scaling must be taken into consideration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Network Architecture</head><p>Based on these challenges, this research has proposed an end-to-end network architecture that can multi-task named HybridNets. As shown in <ref type="figure" target="#fig_1">Figure 2</ref> two decoders: Detection Head and Segmentation Head. The backbone network generated 5 feature maps from to . By down-sampling the feature map , <ref type="bibr">1 5 5</ref> we obtain two feature maps and . <ref type="bibr">6 7</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Encoder</head><p>The feature extracting, serving as a backbone, is an essential part of the model that can bottom-up) path and adds weight for each feature to learn the importance of each level. We adopt the method to fuse features in our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Decoder</head><p>Each grid of the multi-scale fusion feature maps from the Neck network will be assigned nine prior anchors with different aspect ratios. Similar to YOLOv4 <ref type="bibr" target="#b1">[2]</ref>, this paper uses kmeans clustering <ref type="bibr" target="#b15">[16]</ref> to determine anchor boxes. In addition, we chose 9 clusters and 3 different scales for each grid cell. In order to various feature map levels, this paper use scale constant to create bounding box priors that covers all regions from small to large. Thus, this proposed network can work well on complex dataset. The detection head will predict the offset of bounding boxes and the probability of each class as well as the confidence of the prediction boxes. This is described channels with other levels. Then, we combine them to obtain a better feature fusion by summing all levels. Finally, we restore the output feature to the size , representing the ( , , 3) probability of each belonging pixel class. This research scales feature maps to the size of 2 level, because level is a strongly semantic feature map. Additionally, we feed feature map <ref type="bibr">2 2</ref> from backbone network which represents low-level feature into the final feature fusion that helps network improve output precision, as shown in <ref type="figure" target="#fig_3">Figure 3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Loss Function and Training</head><p>This paper used multi-task loss to train end-to-end network. Equation 2 expressed the total loss function by summing of two parts.</p><p>Where , are tuning parameters to balance the total loss, is the loss for object detection task and is the loss for segmentation task, the formulation can be written as follow and are focal loss <ref type="bibr" target="#b11">[12]</ref>, which is implemented for classifying class and the confidence of objects, respectively. The focal loss reduces the slope of loss function and focuses on misclassified examples. is computed by smooth L1 loss, which takes absolutely between the predicted box and ground truth box, can be expressed as Where is the prediction of bounding box and is the ground truth, and is determined a positive label has been assigned to a grid cell. In this paper, we force size some anchor boxes to the regression network can learn smoothly, can be written as Where is the anchor box , the total of anchor boxes is combining of each feature map level with is the resolution of feature map, and is the total of ground truth bounding boxes of each input image. Next is multiclass hybrid loss that is utilized for multi-class segmentation of background, drivable area and lane line. Small object segmentation is a challenge in semantic segmentation caused by imbalanced data distribution. Therefore, this paper combines Tversky loss <ref type="bibr" target="#b23">[24]</ref> and Focal loss <ref type="bibr" target="#b11">[12]</ref> to predict the class to which a pixel belongs. performs well at class-imbalanced problems and optimizes the maximization of score, whereas aims to minimize the classification error between pixels and focuses on hard labels.</p><p>Where , , are true positives, false negatives and false positives for class , is the predicted probability for pixel belonging to class , is the ground truth for pixel being in class . is the number of classes and is the total number of pixels in the input image.</p><p>During training, this paper did several experiments to finetune a lot of hyperparameters and suitable architecture networks. Training from an end-to-end approach will cost computation and training time. In addition, several optimization algorithms have also been experimented.</p><p>Therefore, to compress the training time and optimize hyperparameters, we construct a training strategy in order to train the model step by step and quickly transform the experiments.</p><p>Algorithm 1 illustrates the strategy of our training method.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Evaluation Metrics</head><p>On traffic object detection task, this proposed method uses mAP50. mAP50 is computed by the average of the Average Precision calculated for all the classes at single IoU threshold 0.5.</p><p>Average Precision is the area under the precision-recall curve. This paper only evaluates one class, focusing on how good the proposed method can find all the positives. This paper set the lowest confidence and all bounding boxes is computed by mAP50. On semantic segmentation task, IoU metric is used to evaluate drivable area and lane line segmentation. To be more specific, this paper presents mIoU as average of IoU for each class and IoU metric for single class. <ref type="table" target="#tab_1">Table 1</ref> compares HybridNets with other multi-task networks. Although our HybridNets has more extensive parameters (12.83M) than YOLOP (7.9M), the number of computations of HybridNets is lower than the compared networks. By adopting depth-wise separable convolutions <ref type="bibr" target="#b2">[3]</ref>, the computations are significantly reduced to 15.6 BFLOPs. In addition, we have also compared the inference latency on V100 GPU FP16. Specifically, our V100 latency is the time processing of the model, not including preprocessing and NMS postprocessing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Cost Computation Performance</head><p>Compared to previous multi-networks, HybridNets are up to 1.4x faster on GPU. Therefore, HybridNets can run in real-time on standard devices and embedded devices. Latency is for inference with batch size 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Multi-task Performance</head><p>The second experiment presents results on three tasks, including traffic object detection, drivable area segmentation, and lane line segmentation. We present the vehicle detection results and compare them to six models on the BDD100K dataset.  <ref type="table">Table.</ref> 2. The comparison result on traffic object detection task. The experiment settings include confidence threshold of 0.001 and NMS threshold of 0.6. This paper mainly focuses on obtaining highest Recall IoU.</p><p>As listed in <ref type="table">Table 2</ref>, HybridNets outperforms performance to previous networks on the BDD100K dataset. Our model achieves 3.6% better recall and achieves the best mAP50 at 77.3%. We can outperform all previous networks on recall and mAP50 metrics because our HybridNets can detect incredibly small objects ranging from 3 pixels to 10 pixels with input size (640,384,3) thanks to our automatically customized anchor aspect ratio and scale. <ref type="figure" target="#fig_5">Figure 4</ref> illustrates the visualization of traffic object detection. As shown in <ref type="figure" target="#fig_6">Figure 5</ref>, our proposed architecture makes further improvement compared to YOLOP. Specifically, HybridNets detects small objects and large objects in traffic object detection task, whereas YOLOP has high False Negative score and detects wrong objects. In addition, HybridNets works well in various complex weather conditions and the bounding boxes are more accurate. Next we evaluate the drivable area segmentation task. IoU metric is used to evaluate the segmentation performance of various networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Drivable mIoU (%)  <ref type="table">Table 3</ref>. Performance comparison on drivable area segmentation task. <ref type="table">Table 3</ref> shows the Drivable IoU of five networks. Our HybridNets achieves 90.5 % mIoU, pale in comparison to YOLOP (91.5%). We built a decoder network for multi-classes, whereas YOLOP constructed two decoders for specific tasks. Therefore, our HybridNets is more flexible and optimistic than theirs. <ref type="figure">Figure 6</ref> visualizes the semantic segmentation output of drivable areas in various conditions. As shown in <ref type="figure">Figure 7</ref>, the comparison between HybridNets and YOLOP on Drivable Segmentation task, HybridNets is more accurate than YOLOP. To be more specific, YOLOP focuses on evaluating the pixel to which class it belongs, while needing to consider the intersection of bounding boxes. Therefore, the YOLOP model does not work well in harsh conditions such as night or areas with a lot of noise. Based on our Neck Network using BiFPN architecture, the information of different receptive fields is combined from various feature map levels with weighted parameters. Thus, HybridNets can improve the performance of drivable area segmentation task. Finally, lane detection is one of the main challenges in autonomous driving. The evaluation metrics we use for lane detection are accuracy and IoU. As shown in <ref type="table" target="#tab_5">Table 4</ref>, our HybridNets outperforms all previous models with accuracy 85.4 % and IoU 31.6 %. The proposed method works well in various complex weather conditions as shown in <ref type="figure" target="#fig_8">Figure 8</ref>. As shown in <ref type="figure">Figure 9</ref>, the lane detection results from YOLOP have mismatched pixels and less accuracy, whereas HybridNets works well on lane detection task. The lane lines from HybridNets are continuous and have high accuracy with less sparse supervisory. However, lane line has low IoU because of our approach in preprocessing the training dataset, making lane line annotation easier to learn with the drawback of suboptimal results. Thus, this paper has added another metric of accuracy to evaluate in a more objective and fair manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Accuracy <ref type="formula">(</ref>   <ref type="figure">Fig. 9</ref>. Comparison between YOLOP and HybridNets on Lane Detection task. The first row shows the issue of mismatched of YOLOP and the second row shows the result of HybridNets.</p><p>The green regions are false positive and the yellow regions are false negative. <ref type="figure" target="#fig_0">Figure 10</ref> shows the results of HybridNets. The red lines are the lane lines, the green areas are the drviable area, and the orange bounding boxes are traffic objects. Our HybridNets has great performance in most scenarios. Based on the context structures, the drivable area provides information for the model to help train the model to converge faster. Moreover, each task provides context structure for other tasks. Therefore, our HybridNets can detect vehicles object easily, which challenges many other prior detection models. Therefore, the model can more easily predict traffic objects, which challenges many prior models. In general, our HybridNets works well in most complex scenarios such as severe reflective scenes and extreme weather conditions. However, our model is unable to adapt to the crossroads, the lane lines detection is broken and the drivable area is misjudged to be on the other side of the road in some cases. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION AND PERSPECTIVE</head><p>In this paper, we systematically study network architecture design choices for multi-tasking, propose an efficient end-to-end perception network, customize automatic aspect ratios for each level in the weighted bidirectional feature network, and build efficient training loss function and training strategy to improve accuracy and performance. Based on these optimizations, we develop a new end-to-end multi-network, named HybridNets, which achieves better accuracy and efficiency than prior art across a broad spectrum of resource constraints.</p><p>Most importantly, our network HybridNets achieves state-of-the-art accuracy with fewer FLOPS than previous multi-network models.</p><p>In future works, we would like to propose a robust network, which can perform many tasks related to perception and improve parameters and FLOPs of network. To be more specific, our work will focus on processing problems in autonomous driving such as building a decoder network that can detect 3-D object detection with only one input and classify several objects. We will try to ameliorate lane lines performance as well as context of structures in drivable area segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">ACKNOWLEDGEMENT</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Results for inference of HybridNets. Our proposed network performs three tasks, including traffic object detection, drivable area segmentation and lane line detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>, our one-stage network includes one sharing encoder and two separated decoders to solve distinct tasks. The resolution of each feature map level represents a feature level with resolution of of the input images. For 1/2 instance, if input resolution is 640x384, the represents feature level 2 ( HybridNets Architecture has one encoder: backbone network and neck network;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>help a variety of networks achieve excellent performance to in various tasks. Many modern network architectures currently reuse networks that have good accuracy in the ImageNet dataset to extract features. Recently, EfficientNet showed high accuracy and efficient performance over existing CNNs, reducing FLOPs by orders of magnitude. We choose EfficientNet-B3 as the backbone, which solves the problem of network optimization by finding depth, width, and resolution parameters based on neural architecture search to design a stable network. Therefore, our backbone can reduce the computational cost of the network and obtain several vital features. The feature maps from the backbone network are fed to the neck network pipeline. Multi-scale feature representation is the main challenge; FPN recently proposed a feature extractor design to generate multi-scale feature maps to obtain better information. However, the limitation of FPN is that information feature is inherited by a one-way flow. Therefore, our neck network uses a BiFPN module based on EfficientDet. BiFPN fuses feature at a different resolution based on cross-scale connection for each node by each bidirectional (top-down and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>The Segmentation branch of HybridNets architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Dataset (BDD100K) is used in training and validating the model. Since the test labels of 20K images are unavailable, we opt to evaluate on the validation set of 10K images. The dataset for three tasks is prepared according to existing multi-task networks trained on BDD100K to aid in comparison. Of all the ten classes in object detection, only {car, truck, bus, train} is selected and merged into a single class {vehicle} since DLT-Net and MultiNet can only detect vehicles. Two segmentation classes {direct, alternative} are also merged into {drivable}. We follow the practice of calculating two lane line annotations into a central one, dilating the annotations in training set to 8 pixels while keeping validation set intact<ref type="bibr" target="#b7">[8]</ref>. Images are resized from 1280x720 to 640x384 due to three main reasons, in order of importance: respecting the original aspect ratio, maintaining a good trade-off between performance and accuracy, and making sure the dimensions are divisible by 128 for BiFPN.Basic augmentation techniques such as rotating, scaling, translating, horizontal flipping, and HSV shifting are used. Mosaic augmentation, first introduced in YOLOv4 with great results<ref type="bibr" target="#b1">[2]</ref>, is utilized while training detection head specifically.We jump-start the model by using EfficientNet-B3 weights pre-trained on ImageNet. The custom anchor box settings found automatically have scales of and ratios of . The chosen optimizer is AdamW<ref type="bibr" target="#b14">[15]</ref> with . When the model stucks around for 3 epochs, learning rate is decreased tenfold. For object detection, the model uses smooth L1 loss with for regression and focal loss with for classification. When matching anchor boxes to annotations, the model uses an IoU threshold of 0.5 for annotations larger than 100 pixels in area but only 0.25 for those smaller. We emphasize regression 4 times more than classification because one-class classification is easy to converge. For drivable area and lane segmentation, the model uses a combination of Tversky loss with and Focal loss with . We train with a batch-size of 16 on a RTX 3090 for 200 epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Visualization of the traffic object detection results of HybridNets. Fig. 4. (a) shows results in day-time series with different weather conditions such as clear, heat stroke and heat-wave. Fig. 4. (b) shows results in night-time series with different weathers such as cool and flurries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Comparison between YOLOP and HybridNets on Traffic Object Detection task. The first row shows issues of YOLOP and the second row shows the result of HybridNets. The red bounding boxes are the false positive, the yellow bounding boxes are the false negative and the purple bounding boxes are not accurate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>6 .Fig. 7 .</head><label>67</label><figDesc>(a) Day-time result (b) Night-time result Fig. Visualization of the drivable area segmentation results of HybridNets. Fig. 6. (a) shows semantic segmentation results in day-time with various views. Fig. 6. (b) shows results in night-time series with various brightness views. Comparison between YOLOP and HybridNets on Drivable Area Segmentation task. The first row shows the issue of mismatched pixels of YOLOP and the second row shows the result of HybridNets. The red regions are false positive and the yellow regions are false negative.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Visualization of the lane detection results of HybridNets.Fig. 8. (a) shows results in day-time series with various weather conditions.Fig. 8. (b)shows results in night-time series with various brightness views.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 .</head><label>10</label><figDesc>Multi-task results using HybridNets. The red lines are the lane lines, the green areas are the drivable area, and the orange bounding boxes are traffic objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>First, this paper up-samples each level to have the same output feature map with size</figDesc><table><row><cell>as:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Where</cell><cell cols="5">is the center, width and height of each bounding box, respectively from</cell></row><row><cell cols="6">network prediction. Each anchor box has a center</cell><cell>, width</cell><cell>and height</cell><cell>.</cell></row><row><cell cols="6">Segmentation head has 3 classes for output, which are background, drivable area and lane</cell></row><row><cell cols="4">line. This paper keeps 5 feature levels</cell><cell>{</cell><cell>3 , ?,</cell><cell>7 }</cell><cell>from Neck network to segmentation branch.</cell></row><row><cell cols="2">. Second, feeding ( 4 , 4 , 64)</cell><cell>2</cell><cell cols="3">level to convolution layer to have the same feature map</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Algorithm 1 .</head><label>1</label><figDesc>HybridNets training stage. First, we only train Encoder and Detection Head as object detection task. Second, we freeze the Encoder, Detection head and unfreeze parameters from Segmentation Head. Finally, the final network is trained jointly for all tasks.</figDesc><table><row><cell cols="4">Input: Target end-to-end network</cell><cell>with parameter group</cell></row><row><cell></cell><cell>;</cell><cell></cell></row><row><cell></cell><cell>Training dataset</cell><cell>;</cell></row><row><cell></cell><cell cols="3">Threshold for convergence</cell><cell>;</cell></row><row><cell></cell><cell>Total loss function</cell><cell>;</cell></row><row><cell></cell><cell>Pivot strategy</cell><cell></cell></row><row><cell cols="3">Output: Proposed network:</cell></row><row><cell cols="2">1: procedure Train</cell><cell></cell></row><row><cell>2:</cell><cell cols="2">for = 0 to length</cell><cell>-1</cell></row><row><cell>3:</cell><cell></cell><cell cols="2">// Freeze parameters</cell></row><row><cell>4:</cell><cell>repeat</cell><cell></cell></row><row><cell>5:</cell><cell cols="3">Sample a mini-batch</cell><cell>from training dataset</cell></row><row><cell>.</cell><cell></cell><cell></cell></row><row><cell>6:</cell><cell></cell><cell></cell></row><row><cell>7:</cell><cell></cell><cell></cell></row><row><cell>8:</cell><cell>until</cell><cell></cell></row><row><cell>9:</cell><cell>if &lt; length</cell><cell></cell><cell>-1 then</cell></row><row><cell>10:</cell><cell></cell><cell></cell></row><row><cell>11:</cell><cell>endif</cell><cell></cell></row><row><cell>12:</cell><cell>end for</cell><cell></cell></row><row><cell cols="2">13: end procedure</cell><cell></cell></row><row><cell cols="2">14: Train</cell><cell></cell></row><row><cell cols="3">15: return Proposed network</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Cost computation result for various multi-networks. Params and FLOPs denote the number of parameters and the number of computations.</figDesc><table><row><cell>Table. 1.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Latency (ms)</cell></row><row><cell></cell><cell>Params</cell><cell>FLOPs</cell><cell>V100</cell></row><row><cell>YOLOP</cell><cell>7.9M</cell><cell>18.6B</cell><cell>52</cell></row><row><cell>HybridNets</cell><cell>12.83M</cell><cell>15.6B</cell><cell>37</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Performance comparison on lane detection task.</figDesc><table><row><cell>%)</cell><cell>Lane Line IoU (%)</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2016.2644615</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2016.2644615" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">YOLOv4: Optimal Speed and Accuracy of Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><forename type="middle">M</forename><surname>Liao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10934</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Xception: Deep Learning with Depthwise Separable Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.195</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.195" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1800" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">BlitzNet: A Real-Time Deep Network for Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dvornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shmelkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.447</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4174" to="4182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep Neural Networks: Selected Aspects of Learning and Application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Golovko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Kroshchanka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">V</forename><surname>Mikhno</surname></persName>
		</author>
		<idno type="DOI">10.1134/S1054661821010090</idno>
		<ptr target="https://doi.org/10.1134/S1054661821010090" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Image Anal</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="132" to="143" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semisupervised and Weakly Supervised Road Detection Based on Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/LSP.2018.2809685</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="551" to="555" />
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2018.2844175</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="386" to="397" />
			<date type="published" when="2020-02-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning Lightweight Lane Detection CNNs by Self Attention Distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.00110</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1013" to="1021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A New Method of Global Image Analysis and Its Application in Understanding Road Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Kiy</surname></persName>
		</author>
		<idno type="DOI">10.1134/S1054661818030100</idno>
		<ptr target="https://doi.org/10.1134/S1054661818030100" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Image Anal</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="483" to="495" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient Residual Neural Network for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<idno type="DOI">10.1134/S1054661821020103</idno>
		<ptr target="https://doi.org/10.1134/S1054661821020103" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Image Anal</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="212" to="220" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Feature Pyramid Networks for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.106</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="936" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Focal Loss for Dense Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2018.2858826</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="318" to="327" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">LSNet: Extremely Light-Weight Siamese Network For Change Detection in Remote Sensing Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.09156</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">SSD: Single Shot MultiBox Detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46448-0_2</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46448-0_2" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<editor>Leibe, B. et al.</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Decoupled Weight Decay Regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Some Methods for Classification and Analysis of Multivariate Observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Berkeley Symposium on Mathematical Statistics and Probability</title>
		<meeting>the 5th Berkeley Symposium on Mathematical Statistics and Probability<address><addrLine>Berkeley</addrLine></address></meeting>
		<imprint>
			<publisher>University of California Press</publisher>
			<date type="published" when="1967" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep fully convolutional networks with random data augmentation for enhanced generalization in road detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mu?oz-Bulnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Parra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fern?ndez-Llorca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Sotelo</surname></persName>
		</author>
		<idno type="DOI">10.1109/ITSC.2017.8317901</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE 20th International Conference on Intelligent Transportation Systems (ITSC</title>
		<meeting>the IEEE 20th International Conference on Intelligent Transportation Systems (ITSC</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="366" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Strong-Structural Convolution Neural Network for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ouyang</surname></persName>
		</author>
		<idno type="DOI">10.1134/S1054661819040126</idno>
		<ptr target="https://doi.org/10.1134/S1054661819040126" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Image Anal</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="716" to="729" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.06080</idno>
		<title level="m">Spatial As Deep: Spatial CNN for Traffic Scene Understanding</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">DLT-Net: Joint Detection of Drivable Areas, Lane Lines, and Traffic Objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TITS.2019.2943777</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4670" to="4679" />
			<date type="published" when="2020-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">You Only Look Once: Unified, Real-Time Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.91</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">YOLO9000: Better, Faster, Stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.690</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6517" to="6525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-4_28" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention -MICCAI 2015. MICCAI 2015</title>
		<editor>Navab N., Hornegger J., Wells W., Frangi A.</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<biblScope unit="volume">9351</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Tversky Loss Function for Image Segmentation Using 3D Fully Convolutional Deep Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S M</forename><surname>Salehi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erdogmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gholipour</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-67389-9_44</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-67389-9_44" />
	</analytic>
	<monogr>
		<title level="m">Machine Learning in Medical Imaging</title>
		<editor>Wang Q., Shi Y., Suk HI., Suzuki K.</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">10541</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fully Convolutional Networks for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2016.2572683</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017-04-01" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="640" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">EfficientDet: Scalable and Efficient Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR42600.2020.01079</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10778" to="10787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<title level="m">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">MultiNet: Real-time Joint Semantic Reasoning for Autonomous Driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Teichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Z?llner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="DOI">10.1109/IVS.2018.8500504</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Intelligent Vehicles Symposium (IV)</title>
		<meeting>the IEEE Intelligent Vehicles Symposium (IV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1013" to="1020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">LaneNet: Real-Time Lane Detection Networks for Autonomous Driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qiu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.01726</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">YOLOP: You Only Look Once for Panoptic Driving Perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.11250</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR42600.2020.00271</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2633" to="2642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pyramid Scene Parsing Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.660</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6230" to="6239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improved Lane Line Detection Algorithm Based on Hough Transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C-W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M-C</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1134/S1054661818020049</idno>
		<ptr target="https://doi.org/10.1134/S1054661818020049" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Image Anal</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="254" to="260" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">His current research interests include Artificial Intelligence, Computer Vision, Pattern Recognition</title>
	</analytic>
	<monogr>
		<title level="m">Machine Learning, and Autonomous Driving</title>
		<meeting><address><addrLine>Hanoi, Vietnam</addrLine></address></meeting>
		<imprint/>
		<respStmt>
			<orgName>Vu Thanh Dat received his B.S.E degree in Computer Science from FPT University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Vietnam with a degree in Computer Science in 2022</title>
		<imprint>
			<pubPlace>Hanoi</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Ngo Viet Hoai Bao graduated from FPT University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">His current research interests include Artificial Intelligence and Deep Learning</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Since 2009, he has worked as a Lecturer, and served as the Head of Department and the Director of the Master Program in Software engineering at FPT University, Hanoi, Vietnam. His current research interests include Digital Signal and Image processing</title>
	</analytic>
	<monogr>
		<title level="m">Phan Duy Hung received his Ph.D. degree from INP</title>
		<meeting><address><addrLine>Grenoble France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Internet of Things. Industrial networking</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
