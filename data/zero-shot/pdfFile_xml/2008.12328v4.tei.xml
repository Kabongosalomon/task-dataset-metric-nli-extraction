<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Background-Agnostic Framework with Adversarial Training for Abnormal Event Detection in Video</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20151">AUGUST 2015 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariana-Iuliana</forename><surname>Georgescu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE, Fahad</roleName><forename type="first">Radu</forename><forename type="middle">Tudor</forename><surname>Ionescu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Shahbaz</forename><surname>Khan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Popescu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
						</author>
						<title level="a" type="main">A Background-Agnostic Framework with Adversarial Training for Abnormal Event Detection in Video</title>
					</analytic>
					<monogr>
						<title level="j" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE</title>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20151">AUGUST 2015 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-abnormal event detection</term>
					<term>anomaly detection</term>
					<term>auto-encoders</term>
					<term>adversarial training</term>
					<term>security and surveillance</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abnormal event detection in video is a complex computer vision problem that has attracted significant attention in recent years. The complexity of the task arises from the commonly-adopted definition of an abnormal event, that is, a rarely occurring event that typically depends on the surrounding context. Following the standard formulation of abnormal event detection as outlier detection, we propose a background-agnostic framework that learns from training videos containing only normal events. Our framework is composed of an object detector, a set of appearance and motion auto-encoders, and a set of classifiers. Since our framework only looks at object detections, it can be applied to different scenes, provided that normal events are defined identically across scenes and that the single main factor of variation is the background. This makes our method background agnostic, as we rely strictly on objects that can cause anomalies, and not on the background. To overcome the lack of abnormal data during training, we propose an adversarial learning strategy for the auto-encoders. We create a scene-agnostic set of out-of-domain pseudo-abnormal examples, which are correctly reconstructed by the auto-encoders before applying gradient ascent on the pseudo-abnormal examples. We further utilize the pseudo-abnormal examples to serve as abnormal examples when training appearance-based and motion-based binary classifiers to discriminate between normal and abnormal latent features and reconstructions. Furthermore, to ensure that the auto-encoders focus only on the main object inside each bounding box image, we introduce a branch that learns to segment the main object. We compare our framework with the state-of-the-art methods on four benchmark data sets, using various evaluation metrics. Compared to existing methods, the empirical results indicate that our approach achieves favorable performance on all data sets. In addition, we provide region-based and track-based annotations for two large-scale abnormal event detection data sets from the literature, namely ShanghaiTech and Subway.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>A BNORMAL events are defined as rare occurrences that deviate from the normal patterns observed in familiar events <ref type="bibr" target="#b0">[1]</ref>. Considering the prior work on video anomaly detection <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, we can devise a non-exhaustive taxonomy of abnormal events that includes: appearance anomalies (for example, a car or a truck in a pedestrian area), short-term motion anomalies (for example, a person running or a person throwing an object), long-term motion anomalies (for instance, loitering) and group anomalies (for example, several people running inside a public building). These anomalies, which are commonly observed in publicly available data sets <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b22">[23]</ref>, typically involve people and vehicles, which usually appear in surveillance videos captured in urban areas. This is because abnormal event detection is studied in the context of video surveillance. Additionally, we should emphasize that the classification of an event as normal or abnormal always depends on the context. For instance, driving a truck on the street is considered normal, but, if the truck enters a pedestrian area, the event becomes abnormal. Considering the commonly-adopted definition of abnormal events and the reliance on context, it is difficult to obtain a sufficiently representative set of anomalies for all possible contexts, making traditional supervised methods less applicable to abnormal event detection. Therefore, the majority of anomaly detection methods proposed so far are based on outlier detection <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, learning normality models from training videos containing only normal events. During inference, an event is labeled as abnormal if it deviates from the normality model. In general, existing abnormal event detection methods build the normality model using local features <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, global (frame-level) features <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b30">[31]</ref>, or both <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b31">[32]</ref>. Such methods work well when training and testing are conducted on the same scene. If we switch to a different scene at inference time, however, the methods based on local or global features tend to fail because the features used for the normality model are specific to the training scene. There are a few approaches arXiv:2008.12328v4 [cs.CV] 10 May 2021 that do not require any training data <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, instead employing different algorithms for change detection at test time. Such methods can be considered scene-agnostic, but they typically obtain much lower performance levels compared to methods that rely on training data to build normality models. Other works, such as <ref type="bibr" target="#b36">[37]</ref>, tackled frame anomaly detection as an action recognition task, considering only events that are always abnormal, irrespective of the context, e.g. arson attacks, burglaries or traffic accidents. By considering only generic abnormal events, the method developed by Sultani et al. <ref type="bibr" target="#b36">[37]</ref> is implicitly scene-agnostic. However, the method falls outside the commonly-accepted definition of abnormal events and is rather considered by others <ref type="bibr" target="#b0">[1]</ref> as an action recognition method.</p><p>A handful of methods <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref> also have the potential to become scene-agnostic, while still taking into account the reliance on context of anomalies. This is achieved by applying an object detector before feature extraction, allowing the model to learn the normality only with respect to the objects, while ignoring the background or other elements in the scene. However, such methods are affected by significant viewpoint changes, frame resolution differences, frame rate variations and different types of normal events across scenes. Such methods work well across different scenes, especially in case of background variations. Similar to the above mentioned works <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, we employ an object detector, analyzing the abnormality at the object level by extracting appearance or motion features to represent each object. Different from preliminary works that rely on object detection <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, we are the first to present a crossdatabase (cross-domain) evaluation, demonstrating that our proposed method is not severely affected by scene variations. Remarkably, our cross-domain results on Avenue <ref type="bibr" target="#b11">[12]</ref>, ShanghaiTech <ref type="bibr" target="#b12">[13]</ref>, Subway <ref type="bibr" target="#b22">[23]</ref> and UCSD Ped2 <ref type="bibr" target="#b13">[14]</ref> surpass many of the recently reported in-domain results (see <ref type="bibr">Section 4.9)</ref>.</p><p>We conduct experiments on four challenging benchmarks, namely Avenue <ref type="bibr" target="#b11">[12]</ref>, ShanghaiTech <ref type="bibr" target="#b12">[13]</ref>, Subway <ref type="bibr" target="#b22">[23]</ref> and UCSD Ped2 <ref type="bibr" target="#b13">[14]</ref>, reporting favorable performance levels compared to the state-of-the-art methods <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>. While we report results in terms of the standard frame-level area under the curve (AUC) metric, we also report our performance levels in terms of the Region-Based Detection Criterion (RBDC) and Track-Based Detection Criterion (TBDC). These criteria were recently introduced by Ramachandra et al. <ref type="bibr" target="#b23">[24]</ref>, who argued that the frame-level AUC is inadequate to fully evaluate abnormal event detection systems, essentially because it does not take into consideration spatial localization, counting a frame as a correct detection even when the pixels predicted as abnormal do not overlap with the groundtruth abnormal pixels. Since Ramachandra et al. <ref type="bibr" target="#b23">[24]</ref> did not provide the region-based and track-based annotations required to compute RBDC and TBDC on ShanghaiTech and Subway, we labeled these data sets ourselves. We release the annotations along with our open source code at: https://github.com/lilygeorgescu/AED. Relation to preliminary CVPR 2019 version <ref type="bibr" target="#b5">[6]</ref>. Since our method stems from the method proposed in <ref type="bibr" target="#b5">[6]</ref>, we briefly present our preliminary work and explain our new design changes. Ionescu et al. <ref type="bibr" target="#b5">[6]</ref> was the first work to propose an object-centric framework, employing a single-stage detection framework (SSD <ref type="bibr" target="#b48">[49]</ref> with Feature Pyramid Networks (FPN) <ref type="bibr" target="#b49">[50]</ref>) on each frame in order to extract objects of interest. Representative deep unsupervised features for normal objects are learned using three convolutional autoencoders (CAEs), one for appearance and two for motion. Upon training the auto-encoders, the concatenated latent features are clustered using the k-means algorithm to obtain clusters representing various types of normality. For each normality cluster, an SVM classifier is trained to discriminate the corresponding cluster from the rest, using the one-versus-rest scheme. During inference, the one-versusrest SVM is applied to obtain a normality score for each detected object. The maximum among the scores assigned by the one-versus-rest SVM with respect to each normality cluster is the normality score for a given test sample. While our preliminary framework attained state-of-the-art results at the time of publication <ref type="bibr" target="#b5">[6]</ref>, we observed that the autoencoders sometimes produce excessively good reconstructions for abnormal examples, resulting in a higher false negative rate. Furthermore, the previous framework contains multiple components that are not integrated into an endto-end pipeline. In order to adopt an end-to-end processing pipeline in our current work, we remove the k-means clustering and the one-versus-rest classification steps, replacing them with three binary classifiers (two for motion and one for appearance) that are trained to discriminate between normal examples from the training set and pseudoabnormal examples from a generic data set. Additionally, we make significant changes to the auto-encoders. First, we introduce skip connections and two decoder branches, one for adversarial training and one for object segmentation. To cope with the lack of anomalies during training, we introduce a generic set of out-of-domain data samples that play the role of abnormal samples, which we call "pseudoabnormal". The pseudo-abnormal examples are databaseagnostic, meaning that they can be used for any type of scene, as demonstrated throughout our experiments. Indeed, we utilize the same pseudo-abnormal examples for every data set that we experiment with. We emphasize that the auto-encoders are not supposed to reconstruct the pseudo-abnormal examples, i.e. we expect good reconstructions for normal data samples only. To address the issue regarding the excessively good reconstructions for abnormal examples observed in the preliminary framework <ref type="bibr" target="#b5">[6]</ref>, we perform adversarial training on the pseudo-abnormal examples to prevent the auto-encoders from generalizing to such data samples, inherently inducing the same behavior for abnormal examples. Indeed, adversarial training helps us to obtain poor reconstructions for abnormal objects, such as bicycles or cars in pedestrian areas. One last difference from our previous work <ref type="bibr" target="#b5">[6]</ref> is to replace image gradients with optical flow. In summary, we propose the following changes with respect to our previous work <ref type="bibr" target="#b5">[6]</ref>: (i) to replace the k-means clustering step with binary classifiers that distinguish between normal and pseudo-abnormal objects, (ii) to integrate skip connections, (iii) to add a segmentation decoder, (iv) to perform adversarial training and (v) to use optical flow instead of image gradients. We emphasize that the proposed changes lead to a significantly different model, bringing consistently superior performance over the preliminary work <ref type="bibr" target="#b5">[6]</ref>, especially in terms of the recentlyintroduced RBDC and TBDC metrics <ref type="bibr" target="#b23">[24]</ref>.</p><p>In summary, our contribution is threefold:</p><p>? We propose a novel framework that leverages adversarial training in the context of abnormal event detection in video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We show that our framework is background agnostic by performing a series of cross-database experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We provide region-level and track-level annotations for ShanghaiTech and Subway, allowing future works to report results in terms of RBDC and TBDC. The rest of this paper is organized as follows. We present related work on abnormal event detection in video in Section 2. Our method is described in detail in Section 3. We present the anomaly detection experiments and results in Section 4. Finally, our conclusions are drawn in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Most of the recent works treat abnormal event detection as an outlier detection task <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b50">[51]</ref>, learning a model using only normal data. Then, at inference time, the events that diverge from the normality model are labeled as abnormal. Existing abnormal event detection methods can be categorized into distance-based approaches <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>, reconstructionbased models <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, probabilistic models <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b56">[57]</ref> and change detection methods <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>. A handful of preliminary works <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b50">[51]</ref> proposed to build a dictionary of atoms representing normal events, labeling the events that are not represented in the dictionary as abnormal. For example, Dutta et al. <ref type="bibr" target="#b27">[28]</ref> proposed an approach that builds a model of familiar events from training data using a sparse coding objective. Then, the model is incrementally updated in an unsupervised manner as new patterns are observed in the test data.</p><p>Other recent approaches <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b55">[56]</ref> employed deep learning in order to detect the anomalous frames in a video. For instance, Liu et al. <ref type="bibr" target="#b10">[11]</ref> proposed to detect abnormal frames by predicting the next frame in the video, given the previous four frames. Their hypothesis is that an abnormal frame should be harder to predict than a normal one. Thus, the peak signal-to-noise ratio between the predicted frame and the original frame is expected to be lower for abnormal frames. More recently, Ramachandra et al. <ref type="bibr" target="#b24">[25]</ref> used videopatches to detect the anomalies in videos. They stored a set of exemplars for each region in the video frames, comparing each video patch from the test video to each exemplar from the corresponding region. The minimum distance is interpreted as the anomaly score.</p><p>Similar to our work, which learns features in an unsupervised manner, there are a few works that employ unsupervised learning steps for abnormal event detection <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b50">[51]</ref>. There are also some works that are completely unsupervised <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, requiring no training data to perform anomaly detection. Different from such works, our method employs an object detector trained with supervision. The object detector helps our method to become background-agnostic. Another way we introduce supervision into our framework is through pseudoabnormal examples. Our auto-encoders learn to output poor reconstructions for pseudo-abnormal examples through adversarial training, while still being capable of reconstructing normal patterns. We also use the pseudo-abnormal examples to train binary classifiers on top of the auto-encoder features and reconstructions, in a supervised way.</p><p>More closely related to our work, some methods use auto-encoders in order to learn useful features <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>. For example, Nguyen et al. <ref type="bibr" target="#b40">[41]</ref> used an autoencoder with two branches to detect anomalous frames. The input of the auto-encoder is the current frame of the video. The first branch predicts the frame intensity, while the second branch predicts the motion between the current frame and the next frame. Unlike the first branch, which is a standard auto-encoder, the second branch follows a U-Net architecture. Different from Nguyen et al. <ref type="bibr" target="#b40">[41]</ref> and other methods based on auto-encoders, we train our autoencoders on detected objects, which helps our method to better localize anomalies and to become backgroundagnostic. We also employ adversarial training, which enables us to obtain good reconstructions for normal objects only.</p><p>There are also some works <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref> that use Generative Adversarial Networks (GANs) <ref type="bibr" target="#b57">[58]</ref> in order to detect abnormal events in videos. For instance, Nguyen et al. <ref type="bibr" target="#b40">[41]</ref> used a discriminator to distinguish between the generated optical flows and the real ones. Therefore, in order to reconstruct the motion, their network is also guided by the discriminator to produce realistic optical flows. Our method does not employ GANs, i.e. we do not train a generator and a discriminator in an adversarial fashion. In our case, the adversarial training consists of propagating the reversed gradients from an adversarial decoder through the encoder, forcing the auto-encoder to output bad reconstructions for pseudo-abnormal examples. While we integrate a binary classifier after each auto-encoder, the respective classifier is only trained to discriminate between normal and pseudoabnormal reconstructions (both kinds of examples being generated, not real), without interfering with the learning process of the auto-encoder. We refrain from referring to our binary classifiers as discriminators to avoid any confusion with GANs. Unlike these previous works <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, we are the first to perform adversarial training using a generic set of pseudo-abnormal examples.</p><p>We note that auto-encoders <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b60">[61]</ref> and adversarial models <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b62">[63]</ref> have been applied to anomaly detection in other domains, e.g. time series <ref type="bibr" target="#b58">[59]</ref>, images <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b61">[62]</ref> or network traffic analysis <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b62">[63]</ref>. Abnormal event detection is commonly viewed as an independent domain, requiring the design of specific methods that take into account both motion and appearance as well as other particularities of the video domain. Hence, we consider methods such as <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b62">[63]</ref> to be distantly related.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Motivation</head><p>We distinguish two realistic requirements that are especially desired when designing a framework for abnormal event detection in real-word scenarios. Based on these requirements, we introduce a set of design choices in our proposed framework.</p><p>The first requirement is to learn from training videos containing only normal events, deeming supervised learning methods needing both positive (normal) and negative (abnormal) samples unusable for abnormal event detection. Nonetheless, we believe that including any form of supervision is an important step towards obtaining better performance in practice. Motivated by this, we incorporate two approaches for including supervision into our framework. The first approach is to employ a single-shot object detector <ref type="bibr" target="#b63">[64]</ref>, which is trained with class and bounding-box supervision, in order to obtain object detections that are subsequently used throughout the rest of the processing pipeline. The second approach consists of gathering a large and generic pool of pseudo-abnormal examples, substituting the need for abnormal examples during training.</p><p>The second requirement is to apply the same model on multiple scenes with different backgrounds, eliminating the need to retrain the model for each and every scene. For example, a model that is trained on road traffic is expected to work well on multiple road traffic scenes. This has motivated us to design a background-agnostic framework. Since we employ an object detector and analyze abnormality at the object level, our framework is close to being background-agnostic. We take it a step further and equip our auto-encoders with a segmentation branch to focus on reconstructing only the corresponding segment in each bounding box, thus completely ignoring the background. We conduct cross-database experiments to demonstrate that our framework is indeed background-agnostic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Overview</head><p>Our abnormal event detection pipeline is illustrated in <ref type="figure">Figure</ref> 1. An object detector is applied on the video, resulting in a set of object detections. For each object detection, optical flow maps are computed with respect to the previous and next frames. The object detections are given as input to an appearance auto-encoder, while the optical flow maps are given as input to two motion auto-encoders. The autoencoders learn to reconstruct the detections and flow maps extracted from the input video. At the same time, the autoencoders are prevented from learning to reconstruct examples from the pool of pseudo-abnormal training samples passing through the encoder and the adversarial decoder branch. The appearance auto-encoder has a third decoder branch that learns to reconstruct object segments. The segmentation branch helps the model to focus on the foreground object, ignoring the background inevitably caught inside each bounding box. The segmentation branch is also depicted in <ref type="figure" target="#fig_0">Figure 1</ref>. The absolute differences between the inputs and reconstructions are subsequently used to learn three binary classifiers for discriminating between normal examples (labeled as positive) and pseudo-abnormal examples (labeled as negative). During inference, the average scores (class probabilities) provided by the three classifiers represent normality scores associated to the input object detections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Object Detection and Preprocessing</head><p>In this work, we employ a fast single-stage object detection framework, YOLOv3 <ref type="bibr" target="#b63">[64]</ref>, that is pre-trained on MS COCO <ref type="bibr" target="#b64">[65]</ref>. We do not fine-tune the object detector on abnormal event detection data sets due to the lack of bounding box annotations. Throughout our experiments, we did not observe any significant false negatives due to the employment of a pre-trained object detector. This is likely due to the fact that anomalous events are mostly associated with humans and their interaction with other humans or vehicles, e.g. humans riding bikes, people fighting, people stealing or throwing backpacks and so on. Such categories (human, bike, backpack, etc.) are present in the MS COCO data set. Nevertheless, the detector can be retrained in case of any missing categories.</p><p>We opted for YOLOv3 due to its combined advantage of superior detection performance (in terms of mean Average Precision) and high speed (72 frames per second on a single GPU). We are interested in achieving an optimal trade-off between accuracy and speed, YOLOv3 being an excellent choice in this regard. The object detector is applied on each frame t, resulting in a set of bounding boxes at each frame. To obtain the input for the appearance auto-encoder, we crop the objects according to the detected bounding boxes, then, we convert the resulting image crops to grayscale. To obtain segmentation maps to be used as ground-truth labels during training time, we employ Mask R-CNN <ref type="bibr" target="#b65">[66]</ref>. Since Mask R-CNN is slower than YOLOv3, we only use it to obtain segments, necessary at training time, but not during inference. To obtain the motion representation corresponding to an object, we use optical flow. We compute the optical flow using the pre-trained version of SelFlow <ref type="bibr" target="#b66">[67]</ref>, applying it on each tuple of three consecutive frames. More precisely, to compute the flow at frame t, the SelFlow network receives the frames t?1, t and t+1. The resulting optical flow is composed of the forward and the backward optical flow maps. In order to extract the optical flow corresponding to each object in a frame t, we apply the bounding boxes detected by YOLOv3 to crop out the corresponding flow maps. As we obtain two flow maps from SelFlow, we employ two motion auto-encoders, one being used to encode the forward motion and the other to encode the backward motion. This is consistent with our preliminary framework <ref type="bibr" target="#b5">[6]</ref>, which also employed two motion auto-encoders, although these were trained on top of image gradients instead of optical flow maps. Our motion auto-encoders are trained on tensors with two channels, one for the orientation and the other for the magnitude of the motion vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Pseudo-Abnormal Examples</head><p>We gathered a collection of pseudo-abnormal examples that are problem-agnostic, enabling us to use the same collection regardless of the data set. The purpose of using the pseudoabnormal examples is twofold. First of all, pseudo-abnormal images force the auto-encoders to forget generic reconstruction patterns, projecting only normal objects to the learned manifold. Second of all, they represent an excellent way to fill in for the lack of abnormal data that would be required to train the binary classifiers using classic supervision.</p><p>The collection of pseudo-abnormal examples does not contain objects that can appear in real abnormal event detection scenarios, such as people, cars, bicycles, firearms and so on. Instead, it contains texture images <ref type="bibr" target="#b67">[68]</ref>, flower images <ref type="bibr" target="#b68">[69]</ref>, anime images, butterfly images and some categories from Tiny ImageNet that are unrelated to abnormal event detection, such as hourglass and acorn. In total, the data set of pseudo-abnormal examples is composed of 66,918 images. Since an object can be classified both as normal or abnormal, the distinction being based on the context, we completely refrain from adding any object class that is likely to appear in a video surveillance scene. To obtain pseudo-abnormal motion patterns, we compute optical flow maps on frame triplets selected at time t ? k, t and t + k from the (normal) training video. We choose k = {3, 4, 5, 6} for the experiments, artificially magnifying the motion stored in the pseudo-abnormal optical flow maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Architecture</head><p>Our neural architecture is composed of three independent processing streams (appearance, backward motion and forward motion), which are combined using a late fusion strategy. Each stream uses a convolutional auto-encoder (CAE) followed by a binary neural network classifier. The three auto-encoders share the same lightweight architecture, with only their input and trainable weights being different. One CAE takes as input cropped grayscale images of objects and learns to encode the appearance of objects in its latent space. The other two CAEs receive as input the orientation and the magnitude of the motion vectors stored in the optical flow maps, learning to represent motion in their latent spaces. The input size for the appearance CAE is 64 ? 64, while the input size for the motion CAEs is 64 ? 64 ? 2.</p><p>Each encoder is composed of three convolutional (conv) layers, each followed by a max-pooling layer with a filter size of 2 ? 2 applied at a stride of 2. The conv layers are formed of 3 ? 3 filters. Each conv layer is followed by Rectified Linear Units (ReLU) <ref type="bibr" target="#b69">[70]</ref> as the activation function. The first two conv layers consist of 32 filters, while the third layer consists of 16 filters. The latent representation is composed of 16 activation maps of size 8 ? 8.</p><p>Each decoder starts with an upsampling layer, increasing the spatial support of the activation maps by a factor of 2?. The upsampling operation is based on nearest neighbor interpolation. After upsampling, we apply a conv layer with 16 filters of 3 ? 3. The first upsampling and conv block is followed by another two upsampling and conv blocks. The last conv layer of an appearance decoder is formed of a single conv filter, while the last conv layer of a motion decoder is formed of two filters. In both cases, the number of filters in the last conv layer is chosen such that the size of the output matches the size of the input.</p><p>We note that the appearance CAE incorporates one encoder e and three decoder branches. The first decoder d is used to reconstruct the normal objects, the second decoder d is used to decode the pseudo-abnormal objects and the third decoder d is used to generate a mask that segments the object and ignores the background of the input image. Different from the appearance CAE, the motion CAEs incorporate one encoder? * and only two decoder branches, one denoted byd * , which reconstructs the normal objects, and the other denoted byd * , which decodes the pseudoabnormal examples, where * can be replaced with values from the set {b, f }, where b represents the backward motion and f represents the forward motion.</p><p>In order to distinguish between normal and pseudoabnormal examples, we train binary classifiers using classic supervision. We designate a binary classifier for each of the three streams. The actual input of a binary classifier is the absolute difference between the input and the output of the corresponding CAE. In a set of preliminary experiments, we tried to use directly the reconstructions instead of the absolute differences, but the results were considerably worse. For the appearance CAE, the absolute difference is a matrix of 64?64 components, while for the motion CAEs, the absolute difference is a tensor of 64 ? 64 ? 2 components. Since the inputs of the binary classifiers are of the same shape as the CAEs, we employ the same sequence of three conv and max-pooling layers as in the encoders. The resulting activation maps are subsequently passed through a neural network that follows the LeNet architecture <ref type="bibr" target="#b70">[71]</ref>, except that the spatial support of the conv filters is always 3 ? 3 and the pooling operation is max-pooling instead of averagepooling. In summary, a binary classifier is composed of five conv layers, a fully-connected layer and a Softmax classification layer.</p><p>We note that our architecture is also equipped with skip connections. Each conv layer in an encoder has skip connections to the corresponding conv layer in each standard or adversarial decoder that belongs to the same CAE as the encoder, following the U-Net architecture <ref type="bibr" target="#b71">[72]</ref>. Instead of concatenating the features as in U-Net, we sum up the corresponding activation maps, as in ResNet <ref type="bibr" target="#b72">[73]</ref>. Skip connections are also added between the last conv layer of an encoder and the third conv layer of the corresponding binary classifier. Since the first part of the binary classifier coincides with the encoder in terms of architecture, we can sum up the corresponding activation maps, both forming a tensor of 8 ? 8 ? 16 components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Training the Auto-Encoders</head><p>We train the auto-encoders using adversarial training, propagating the reversed gradients from the adversarial branch of each CAE through the corresponding encoder. Let ? e , ? d , ? d and ? d be the parameters of the appearance encoder e, the main appearance decoder d, the adversarial appearance decoder d and the segmentation decoder d , respectively. Analogously, let ?? * , ?d * and ?d * be the parameters of the motion encoder? * , the main motion decoderd * and the adversarial motion decoderd * , where * ? {b, f }. For the motion auto-encoders, the loss function for reconstructing an input optical flowx * of h ? w ? c components is the pixel-wise mean squared error:</p><formula xml:id="formula_0">L * mot-rec (x * ,x * ) = 1 h ? w ? c h i=1 w j=1 c k=1 (x * ijk ?x * ijk ) 2 ,</formula><p>(1) wherex * =d * (? * (x * , ?? * ), ?d * ) is the main output of a motion auto-encoder, ? * ? {b, f }. We note that h = w = 64, while c = 2. Similarly, we define the loss for the adversarial branch of the motion CAEs as follows:</p><formula xml:id="formula_1">L * mot-adv (x * ,x * ) = 1 h ? w ? c h i=1 w j=1 c k=1 (x * ijk ?x * ijk ) 2 ,</formula><p>(2) wherex * =d * (? * (x * , ?? * ), ?d * ) is the output of an adversarial motion decoder, ? * ? {b, f }. In the above equations, it is important to highlight that the notations? * ,d * and d * are interchangeably used to denote components of both motion auto-encoders. In other words, * joins the notations e b ,d b andd b designated for the components of the backward motion CAE and the notations? f ,d f andd f designated for the components of the forward motion CAE. We note that the independent notations are used in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>For the appearance CAE, we also have the segmentation decoder d , which is trained jointly with the principal decoder d. Hence, the loss function for reconstructing an input x and a segmentation map s is the sum of the pixelwise mean squared error computed with respect to main decoder and the logistic error computed with respect to the segmentation decoder:</p><formula xml:id="formula_2">L app-rec (x,x, s,?) = 1 h ? w h i=1 w j=1 (x ij ?x ij ) 2 + + 1 h?w h i=1 w j=1 ?s ij ?log(? ij ) ? (1 ? s ij )?log(1 ?? ij ),<label>(3)</label></formula><p>where x is a grayscale image,x = d(e(x, ? e ), ? d ) is the main output of the auto-encoder and? = d (e(x, ? e ), ? d ) is the output of the segmentation decoder. We notice that the segmentation mask is estimated from the input x, the ground-truth segmentation mask s being used only for the comparison with the output?. According to the described architecture, the size of both inputs is h = w = 64. As the segmentation map is a binary map separating the foreground object from the background, we opted for the logistic loss on the segmentation branch. The loss for the adversarial branch of the appearance CAE is defined analogously to Equation <ref type="formula">(2)</ref>:</p><formula xml:id="formula_3">L app-adv (x,x) = 1 h ? w h i=1 w j=1 (x ij ?x ij ) 2 ,<label>(4)</label></formula><formula xml:id="formula_4">wherex = d (e(x, ? e ), ? d )</formula><p>is the output of the adversarial decoder. Since our goal is to obtain poor reconstructions for the pseudo-abnormal examples, we train the decoders d andd * as adversaries.</p><p>For the motion auto-encoders, the parameters ?d * are updated to optimize the loss defined in Equation <ref type="formula">(2)</ref>, while the parameters ?? * of the encoder are updated to fool the decoderd * . This leads to the following update rules for the parameters:</p><formula xml:id="formula_5">?d * ? ?d * ? ? ? ?L * mot-rec ??d * ,<label>(5)</label></formula><formula xml:id="formula_6">?d * ? ?d * ? ? ? ?L * mot-adv ??d * ,<label>(6)</label></formula><formula xml:id="formula_7">?? * ? ?? * ? ? ?L * mot-rec ??? * + ? ? ? ? ?L * mot-adv ??? * ,<label>(7)</label></formula><p>where ? is the learning rate and ? is a weight for the reversed gradient. We note that the encoder is trained using gradient descent with respect to the main decoder and gradient ascent with respect to the adversarial decoder.</p><p>To ensure convergence, ? must be less than 1 (otherwise, the gradient ascent step will be greater than the gradient descent step). As suggested in <ref type="bibr" target="#b73">[74]</ref>, we set ? = 0.2 for our experiments. For the appearance auto-encoder, the parameter update rules are equivalent, while only adding the standard gradient descent for the segmentation decoder:</p><formula xml:id="formula_8">? d ? ? d ? ? ? ?L app-rec ?? d ,<label>(8)</label></formula><formula xml:id="formula_9">? d ? ? d ? ? ? ?L app-adv ?? d ,<label>(9)</label></formula><formula xml:id="formula_10">? d ? ? d ? ? ? ?L app-rec ?? d ,<label>(10)</label></formula><formula xml:id="formula_11">? e ? ? e ? ? ?L app-rec ?? e + ? ? ? ? ?L app-adv ?? e .<label>(11)</label></formula><p>Once again, we set ? = 0.2 for the experiments. We train the auto-encoders using the Adam optimizer <ref type="bibr" target="#b74">[75]</ref> with the learning rate ? = 10 ?4 , keeping the default values for the other parameters of Adam.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Training the Binary Classifiers</head><p>After training the auto-encoders until convergence, we remove the additional decoder branches d , d andd * , while freezing the parameters ? e , ? d , ?? * and ?d * of the remaining components. We then pass all the training examples, including the adversarial ones, through the main appearance and motion decoders, obtaining the final reconstructions for the training data. The next step is to compute the absolute difference between each input example and its corresponding reconstruction. For a tensor x representing the absolute difference and the binary label y associated to x, we employ the binary cross-entropy to train the classifiers: <ref type="bibr" target="#b11">(12)</ref> where? represents the prediction (class probability) for sample x. In order to use the cross-entropy loss, the normal examples, taken from the training video, are labeled with y = 1 and the pseudo-abnormal examples are labeled with y = 0. Our classifiers are optimized using Adam <ref type="bibr" target="#b74">[75]</ref> with a learning rate of 10 ?3 .</p><formula xml:id="formula_12">L cross-entropy (y,?) = ?y ? log(?) + (1 ? y) ? log(1 ??),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8">Inference</head><p>During inference, we run the YOLOv3 detector to determine the bounding boxes of objects in the current frame. We then compute the optical flow maps for the entire frame. For each detected object, we apply the CAEs (without the additional decoder branches d , d andd * ) to obtain the appearance and motion reconstructions. Then, we compute the absolute differences and pass them to the binary classifiers. Because we have three binary classifiers, we obtain three class probabilities for each object. A class probability is interpreted as a normality score normalized between 0 and 1. The final anomaly score for an object x is obtained by subtracting the average of the three normality scores from 1, resulting in an anomaly score between 0 and 1:</p><formula xml:id="formula_13">s(x) = 1 ? mean ? (i) , ?i ? {1, 2, 3},<label>(13)</label></formula><p>where? (i) is a normality score provided by one of the three classifiers. We expect abnormal examples from the test video to be classified as pseudo-abnormal examples, having an abnormality score closer to 1. By reassembling the anomaly scores of the detected objects into an anomaly map for each frame, we obtain pixellevel anomaly detections. Hence, our framework can also perform anomaly localization. When the bounding boxes of two objects overlap, we keep the maximum anomaly score for the overlapping area. In order to make the pixel-level maps smoother, we apply a 3D mean filter. The frame-level anomaly score is obtained by taking the maximum inside the prediction map for the corresponding frame. We further apply a Gaussian filter to temporally smooth the frame-level anomaly scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data Sets</head><p>We present results on four benchmark data sets, namely Avenue <ref type="bibr" target="#b11">[12]</ref>, ShanghaiTech <ref type="bibr" target="#b12">[13]</ref>, Subway <ref type="bibr" target="#b22">[23]</ref> and UCSD Ped2 <ref type="bibr" target="#b13">[14]</ref>. Although UMN <ref type="bibr" target="#b14">[15]</ref> is among the most widelyused abnormal event detection benchmarks, we consider this data set as being small and saturated. Hence, we do not conduct experiments on UMN. Avenue. The Avenue data set <ref type="bibr" target="#b11">[12]</ref> consists of 16 training videos and 21 test videos. The training videos have a total of 15,328 frames, while the test videos have 15,324 frames. The resolution of each video frame is 360 ? 640 pixels. The original data set is annotated both at the pixel level and at the frame level. Region-level and track-level annotations are provided by Ramachandra et al. <ref type="bibr" target="#b23">[24]</ref>. ShanghaiTech. The ShanghaiTech Campus data set <ref type="bibr" target="#b12">[13]</ref> is one of the largest data sets for abnormal event detection. It contains 330 training videos and 107 test videos. The resolution of each video frame is 480 ? 856 pixels and the data set has a total of 316,154 frames. ShanghaiTech contains both frame-level and pixel-level annotations. We provide region-level and track-level annotations for ShanghaiTech. Subway. The Subway surveillance data set <ref type="bibr" target="#b22">[23]</ref> is formed of two videos, training and testing being conducted independently on the two videos. The length of one video (Entrance gate) is 96 minutes and the length of the other (Exit gate) is 43 minutes. The Entrance gate video has 144,251 frames, while the Exit gate has 64,903 frames. The resolution of each video frame is 384 ? 512 pixels. For the Entrance video, we follow <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b6">[7]</ref> and split the video in 53% frames for training and 47% frames for testing. For the Exit video, similar to <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b11">[12]</ref>, we use the first 15 minutes for training and the rest of the video for testing. This data set contains only frame-level labels. We provide pixel-level, region-level and track-level annotations for both videos in the Subway data set. UCSD Ped2. The UCSD Ped2 data set <ref type="bibr" target="#b13">[14]</ref> is formed of 16 training videos and 12 test videos. The training videos have a total of 2,550 frames, while the test videos have 2,010 frames. The resolution of each video frame is 240 ? 360 pixels. As for Avenue, region-level and track-level annotations are available due to Ramachandra et al. <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation</head><p>As our first evaluation metric, we consider the area under the curve (AUC) computed with respect to the ground-truth frame-level annotations. At a given threshold, a frame is labeled as abnormal if at least one pixel inside the frame is abnormal. With some exceptions, we note that many previous works do not mention if the frame-level AUC is computed by (i) concatenating all frames then computing the score (this is the micro-averaged AUC), or by (ii) computing the frame-level AUC for each video, then averaging the resulting scores (this is the macro-averaged AUC). We therefore report both frame-level AUC measures. Additionally, we report the Region-Based Detection Criterion (RBDC) and the Track-Based Detection Criterion (TBDC), two new metrics introduced by Ramachandra et al. <ref type="bibr" target="#b23">[24]</ref>. In their work, Ramachandra et al. <ref type="bibr" target="#b23">[24]</ref> demonstrated that the framelevel AUC and the pixel-level AUC are not representative metrics to evaluate abnormal event detection frameworks, thus proposing RBDC and TBDC as alternative metrics. RBDC takes into consideration every region that is detected as abnormal. If the intersection over union of a ground-truth region and a predicted region is at least ?, the predicted region is considered a true positive, otherwise it is a false positive. TBDC takes into consideration the detection of tracks. A track is an abnormal event that occurs across several consecutive frames. Each track is formed of a set of regions. A track is considered detected if at least a fraction ? of the ground-truth regions belonging to the track is detected. More details about these new evaluation metrics are presented in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b23">[24]</ref>. Following Ramachandra et al. <ref type="bibr" target="#b23">[24]</ref>, we set ? = 0.1, ? = 0.1 and compute the area under the ROC curve considering false positive rates that are less than or equal to 1.</p><p>As observed in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b23">[24]</ref>, a simple post-processing step can make the pixel-level AUC equal to the frame-level AUC. The post-processing step consists in labeling all pixels in a frame as abnormal if at least one pixel in the frame is abnormal, increasing the true positive rate without modifying the false positive rate. Hence, we refrain from independently reporting the pixel-level AUC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Parameter and Implementation Details</head><p>In the object detection stage, we employ the YOLOv3 <ref type="bibr" target="#b63">[64]</ref> detector, which is pre-trained on the MS COCO data set <ref type="bibr" target="#b64">[65]</ref>. During training and inference, we keep the detections with a confidence level higher than 0.8 for Avenue and ShanghaiTech. Because Subway and UCSD Ped2 have lower frame resolutions, we set the confidence level at 0.5 for these two data sets. We employ the pre-trained Mask R-CNN <ref type="bibr" target="#b65">[66]</ref> to obtain frame-level segmentation maps, and the pre-trained SelFlow <ref type="bibr" target="#b66">[67]</ref> to obtain frame-level optical flow maps. The CAEs and the binary classifiers are trained from scratch in TensorFlow <ref type="bibr" target="#b75">[76]</ref>. We train the CAEs for 20 epochs with the learning rate set to 10 ?4 , as in <ref type="bibr" target="#b5">[6]</ref>. The binary classifiers are trained for 30 epochs with the learning rate set to 10 ?3 . In each experiment, we set the mini-batch size to 64 samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Preliminary Results</head><p>In <ref type="figure" target="#fig_2">Figure 2</ref>, we present a set of qualitative results to analyze the behavior of the appearance and motion auto-encoders with and without adversarial training. For each input sample, we provide the corresponding output and the absolute difference between input and output, respectively. We notice that, without adversarial training, the auto-encoders provide excessively good reconstructions for the abnormal examples, indicating that the auto-encoders generalize well to abnormal data samples coming from a very close distribution to the training data distribution. Unfortunately, in our application domain, we do not want the auto-encoders to generalize to abnormal examples. Instead, we would prefer to obtain visibly worse reconstructions for the abnormal examples, thus enabling the detection of such examples. This is the main motivation behind our decision to introduce adversarial training. We observe that, in general, the auto-encoders based on adversarial training provide worse reconstructions. However, we are not interested in the quality of the reconstructions, but in the difference of quality between reconstructions for normal samples and reconstructions for abnormal samples.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Quantitative Results on Avenue</head><p>We first compare our approach with several state-of-theart methods <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref> reporting results on the Avenue data set. The corresponding micro-averaged frame-level AUC, macroaveraged frame-level AUC, RBDC and TBDC scores are presented in <ref type="table">Table 1</ref>. The existing methods attain framelevel AUC scores between 70.2% and 89.6%. Notably, in terms of the frame-level AUC values, our method surpasses all exiting methods. With a micro-averaged AUC of 92.3%, our method is the only method surpassing the 90% threshold on the Avenue data set by a certain margin. There are at least two works <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b15">[16]</ref> that report the macroaveraged AUC. Our current approach obtains the same macro-averaged AUC as our preliminary method <ref type="bibr" target="#b5">[6]</ref>, but our micro-averaged AUC is almost 5% higher than the result reported in <ref type="bibr" target="#b5">[6]</ref>. We conjecture that our approach attains superior results compared to previous methods because (i) it focuses specifically on detected objects, eliminating false positive events that are not caused by objects, and (ii) it uses adversarial training to increase reconstruction errors for abnormal objects, reducing the number of false negatives.</p><p>In terms of RBDC and TBDC, only Ramachandra et al. <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref> report the results on the Avenue data set. Additionally, we compute the RBDC and TBDC scores for our previous framework presented in <ref type="bibr" target="#b5">[6]</ref> and for Liu et al. <ref type="bibr" target="#b10">[11]</ref>. We should emphasize that the original implementation of Liu et al. <ref type="bibr" target="#b10">[11]</ref> produces extremely low RBDC and TBDC scores (close to zero). We had to modify their framework by adding a post-processing step that removes abnormal regions smaller than a certain area, leading to the better RBDC and TBDC scores shown in <ref type="table">Table 1</ref>. Compared to Ramachandra et al. <ref type="bibr" target="#b24">[25]</ref>, we obtain a considerable improvement of 23.85% in terms of RBDC. We believe that this improvement is due to the fact that we detect objects in the scene, resulting in higher overlaps between our predicted regions and the ground-truth regions. In terms of TBDC, Ramachandra et al. <ref type="bibr" target="#b23">[24]</ref> attains the state-of-the-art result of 80.90%. Since RBDC takes into consideration every region from the ground-truth and since we surpass Ramachandra et al. <ref type="bibr" target="#b23">[24]</ref> by a large margin, we conjecture that our method is able to detect many more ground-truth regions with a lower rate of false positives. However, given the fact that our TBDC is lower than that of Ramachandra et al. <ref type="bibr" target="#b23">[24]</ref>, we conjecture that our method does not detect all the tracks in the ground-truth, given the maximum false positive rate of 1. Finally, comparing our method to the preliminary version proposed in <ref type="bibr" target="#b5">[6]</ref>, we report improvements of almost 50% in terms of RBDC and almost 40% in terms of TBDC, respectively. We therefore consider that the proposed framework is significantly better compared to its earlier version <ref type="bibr" target="#b5">[6]</ref>.</p><p>In <ref type="figure">Figure 3</ref>, we present the frame-level anomaly scores (corresponding to a frame-level AUC of 98.83%) produced by our method versus the anomaly scores (corresponding to a frame-level AUC of 93.25%) produced by our previous  <ref type="table">1</ref> Micro-averaged AUC, macro-averaged AUC, RBDC and TBDC scores (in %) of our approach compared to the state-of-the-art methods <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref> on the Avenue data set. When it is unclear if the reported frame-level AUC is micro-averaged or macro-averaged, we placed the score in the middle. All results are copied from the corresponding papers, except for those marked with asterisk (which are computed by ourselves using the official implementations). The best results are highlighted in bold.</p><p>Method AUC RBDC TBDC Micro Macro Lu et al. <ref type="bibr" target="#b11">[12]</ref> 80.9 --Hasan et al. <ref type="bibr" target="#b4">[5]</ref> 70.2 --Del Giorno et al. <ref type="bibr" target="#b32">[33]</ref> 78.3 ---Smeureanu et al. <ref type="bibr" target="#b30">[31]</ref> 84.6 ---Ionescu et al. <ref type="bibr" target="#b33">[34]</ref> 80.6 ---Luo et al. <ref type="bibr" target="#b12">[13]</ref> 81.7 ---Liu et al. <ref type="bibr" target="#b10">[11]</ref> 85.1 81.7* 19.59* 56.01* Liu et al. <ref type="bibr" target="#b34">[35]</ref> 84.4 ---Lee et al. <ref type="bibr" target="#b39">[40]</ref> 87.2 --Lee et al. <ref type="bibr" target="#b8">[9]</ref> 90.0 --Ionescu et al. <ref type="bibr" target="#b6">[7]</ref> 88.9 ---Wu et al. <ref type="bibr" target="#b18">[19]</ref> 86.6 --Nguyen et al. <ref type="bibr" target="#b40">[41]</ref> 86.9 ---Ionescu et al. <ref type="bibr" target="#b5">[6]</ref> 87.4* 90.4 15.77* 27.01* Tang et al. <ref type="bibr" target="#b17">[18]</ref> 85.1 --Dong et al. <ref type="bibr" target="#b3">[4]</ref> 84.9 --Park et al. <ref type="bibr" target="#b15">[16]</ref> -88.5 --Doshi et al. <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref> 86.4 --Ramachandra et al. <ref type="bibr" target="#b23">[24]</ref> 72.0 35.80 80.90 Ramachandra et al. <ref type="bibr" target="#b24">[25]</ref> 87.2 41.20 78.60 Sun et al. <ref type="bibr" target="#b44">[45]</ref> 89.6 --Wang et al. <ref type="bibr" target="#b45">[46]</ref> 87.0 --Yu et al. <ref type="bibr" target="#b46">[47]</ref> 89  <ref type="figure">Fig. 3</ref>. Frame-level anomaly scores (on the vertical axis) provided by our current approach versus the earlier version proposed in <ref type="bibr" target="#b5">[6]</ref>, for test video 03 from Avenue <ref type="bibr" target="#b11">[12]</ref>. Ground-truth abnormal events are represented in cyan, our scores are depicted in red and the scores of the earlier method are depicted in blue. Best viewed in color.</p><p>approach <ref type="bibr" target="#b5">[6]</ref> on test video 03 from Avenue. According to the ground-truth labels, which are also illustrated in <ref type="figure">Figure 3</ref>, there are two abnormal events in the respective test video. Our approach is able to identify both events, without including any false positive detections, while our earlier approach <ref type="bibr" target="#b5">[6]</ref> only identifies the second event. We would like to emphasize that adversarial training plays a key role in detecting the first abnormal event, which is missed by our preliminary framework <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Quantitative Results on ShanghaiTech</head><p>We further compare our method with the state-of-the-art approaches <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, (in %) of our approach compared to the state-of-the-art methods <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref> on the ShanghaiTech data set. When it is unclear if the reported frame-level AUC is micro-averaged or macro-averaged, we placed the score in the middle. All results are copied from the corresponding papers, except for those marked with asterisk (which are computed by ourselves using the official implementations). The best results are highlighted in bold.  <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref> on the ShanghaiTech data set, presenting the corresponding results in <ref type="table" target="#tab_2">Table 2</ref>. The state-of-the-art results in terms of the micro-averaged and the macro-averaged frame-level AUC are attained by our previous method <ref type="bibr" target="#b5">[6]</ref>. The method proposed in the current work outperforms its previous version <ref type="bibr" target="#b5">[6]</ref> by a margin of 4% in terms of the micro-averaged AUC and a margin of 4.4% in terms of the macro-averaged AUC, respectively. All other methods attain lower frame-level AUC scores, ranging from 60.9% to 79.3%. We note that there are no previous works reporting RBDC and TBDC scores on ShanghaiTech, since we are the first to provide the necessary region-level and track-level annotations for this data set. Nevertheless, we compute the RBDC and TBDC scores for our previous method proposed in <ref type="bibr" target="#b5">[6]</ref> and for Liu et al. <ref type="bibr" target="#b10">[11]</ref>. As on Avenue, we had to add a post-processing step to significantly improve the RBDC and TBDC scores of Liu et al. <ref type="bibr" target="#b10">[11]</ref>. Our current method surpasses its earlier version by more than 20% in terms of RBDC. The improvement is even higher in terms of TBDC, the difference being 34.25% in favor of the current method. Compared to Liu et al. <ref type="bibr" target="#b10">[11]</ref>, our improvements are higher than 24% for both RBDC and TBDC metrics. This demonstrates that our method is able to better localize the anomalies, having a lower rate of false positives per frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Since ShanghaiTech is one of the largest anomaly detection data sets, we consider our results reported in <ref type="table" target="#tab_2">Table 2</ref> as noteworthy.</p><p>In <ref type="figure">Figure 4</ref>, we display the frame-level anomaly scores (corresponding to a frame-level AUC of 99.65%) of our method against the ground-truth labels on a ShanghaiTech test video with one abnormal event. On this video, we can clearly observe a strong correlation between our anomaly scores and the ground-truth labels. The preliminary framework proposed in <ref type="bibr" target="#b5">[6]</ref> produces anomaly scores that are less correlated to the ground-truth, its frame-level AUC for the selected test video being 96.12%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 3</head><p>Frame-level AUC, RBDC and TBDC scores (in %) of our approach compared to the state-of-the-art methods <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b35">[36]</ref> on the Subway data set. All results are copied from the corresponding papers, except for those marked with asterisk (which are computed by ourselves using the official implementations). The best results are highlighted in bold.  <ref type="figure">Fig. 4</ref>. Frame-level anomaly scores (on the vertical axis) provided by our current approach versus the earlier version proposed in <ref type="bibr" target="#b5">[6]</ref>, for test video 05 0024 from ShanghaiTech <ref type="bibr" target="#b12">[13]</ref>. Ground-truth abnormal events are represented in cyan, our scores are depicted in red and the scores of the earlier method are depicted in blue. Best viewed in color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exit</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Quantitative Results on Subway</head><p>The Subway data set was originally annotated at the framelevel only. Therefore, in order to determine the ground-truth regions and tracks, we first had to annotate the data set at the pixel-level. During the manual annotation process, our annotators observed that there are some frames that were labeled as abnormal, but those frames did not contain any abnormal objects or events. In these circumstances, many false positive frames were counted as correct detections in previous works, which is wrong. In order to rectify this problem, we also relabeled the data set at the frame-level based on the identified abnormal regions, such that, if there is an abnormal region in a frame, the respective frame is considered anomalous. We note that our new labels were subject to the agreement of two independent annotators. Hence, we consider our new labels to be more accurate that the original ones. In order to compare our work with all previous works <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b35">[36]</ref>, we report the performance obtained using the original frame labels. Since we have access to the exact implementation of the top scoring method on the Subway data set, namely that of Ionescu et al. <ref type="bibr" target="#b6">[7]</ref>, we were able to compute the framelevel AUC, the RBDC and TBDC scores for the respective method. In addition, we compute the results of our preliminary framework <ref type="bibr" target="#b5">[6]</ref> on Subway. The comparative results are reported in <ref type="table">Table 3</ref>. As the Subway data set contains only one testing video per scene, the micro-averaged frame-level <ref type="figure">Fig. 5</ref>. Frame-level anomaly scores (on the vertical axis) provided by our approach versus the approach of Ionescu et al. <ref type="bibr" target="#b6">[7]</ref>, for a chunk of video trimmed out from Subway Exit. Ground-truth abnormal events are represented in cyan, our scores are depicted in red and the scores of Ionescu et al. <ref type="bibr" target="#b6">[7]</ref> are depicted in blue. Best viewed in color.</p><p>AUC is equivalent to the macro-averaged frame-level AUC. Results on Subway Exit. Considering the old labels, on the Exit video, the state-of-the-art result of 95.1% in terms of the frame-level AUC is obtained by Ionescu et al. <ref type="bibr" target="#b6">[7]</ref>, our method being able to achieve the fourth best score of 92.1%. Even though our results are lower than those of Ionescu et al. <ref type="bibr" target="#b6">[7]</ref> on the original labels, when we switch to the new ones, we outperform their method by 0.9%. The same happens when we compare our current method with its previous version <ref type="bibr" target="#b5">[6]</ref>. In terms of RBDC and TBDC, we surpass the state-of-the-art method of Ionescu et al. <ref type="bibr" target="#b6">[7]</ref> by very large margins, namely 24.1% in terms of RBDC and 15.34% in terms of TBDC, respectively. Our RBDC and TBDC scores are also higher than those of the preliminary version <ref type="bibr" target="#b5">[6]</ref>, the differences being around 5% in favor of our method.</p><p>We compare our frame-level anomaly scores (corresponding to a frame-level AUC of 98.30%) against the ground-truth labels on a chunk of the Exit test video in <ref type="figure">Figure 5</ref>. There are several abnormal events, which seem to be grouped into two temporal clusters. Our approach correctly identifies both groups of abnormal events, without false positives. As reference, the frame-level scores produced by the method of Ionescu et al. <ref type="bibr" target="#b6">[7]</ref> are also included in <ref type="figure">Figure 5</ref>. While the approach of Ionescu et al. <ref type="bibr" target="#b6">[7]</ref> seems to identify both clusters of abnormal events, it also produces high anomaly scores for some normal frames, thus having a <ref type="figure">Fig. 6</ref>. Frame-level anomaly scores (on the vertical axis) provided by our approach versus the approach of Ionescu et al. <ref type="bibr" target="#b6">[7]</ref>, for a chunk of video trimmed out from Subway Entrance. Ground-truth abnormal events are represented in cyan, our scores are depicted in red and the scores of Ionescu et al. <ref type="bibr" target="#b6">[7]</ref> are depicted in blue. Best viewed in color.</p><p>higher false positive rate compared to our approach. Results on Subway Entrance. Considering the frame-level AUC on the old labels for the Entrance video, it seems that our method is surpassed by many other approaches. Hasan et al. <ref type="bibr" target="#b4">[5]</ref> attained the state-of-the-art score of 94.3%, being closely followed by Ionescu et al. <ref type="bibr" target="#b6">[7]</ref> with a score of 93.5%. Although our frame-level AUC score is lower on the old labels, when considering the new labels, we report an improvement of over 10% compared to Ionescu et al. <ref type="bibr" target="#b6">[7]</ref>. We also obtain superior results in terms of RBDC and TBDC, respectively. Our approach outperforms the method presented in <ref type="bibr" target="#b6">[7]</ref> by 41.27% in terms of RBDC, being able to detect many more regions with a lower rate of false positives. With a TBDC of 76.97%, we surpass the method of Ionescu et al. <ref type="bibr" target="#b6">[7]</ref> by more than 27%. When compared to its previous version <ref type="bibr" target="#b5">[6]</ref>, the proposed framework attains superior results, regardless of the metric. Notably, we observe RBDC and TBDC improvements higher than 10%.</p><p>As for Subway Exit, we select a chunk of the Subway Entrance test video to compare our frame-level anomaly scores to those of Ionescu et al. <ref type="bibr" target="#b6">[7]</ref> as well as to the groundtruth labels, illustrating the comparison in <ref type="figure">Figure 6</ref>. There are three abnormal events in the selected chunk and our method provides peak anomaly scores for all three events, attaining a frame-level AUC of 94.56% on the selected chunk of video. However, our method also provides high anomaly scores for some frames before the first abnormal event. Nevertheless, the method of Ionescu et al. <ref type="bibr" target="#b6">[7]</ref> exhibits high anomaly scores for many normal frames, thus having a much higher false positive rate. Consequently, the framelevel AUC obtained by Ionescu et al. <ref type="bibr" target="#b6">[7]</ref> is only 91.76%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Quantitative Results on UCSD Ped2</head><p>UCSD Ped2 is one of the most popular benchmarks in video anomaly detection, with a broad range of methods reporting results on it <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>. We compare the results of our approach with the results reported in literature in <ref type="table" target="#tab_5">Table 4</ref>. Our preliminary approach <ref type="bibr" target="#b5">[6]</ref> and five other recent methods <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref> obtain frame-level scores higher than 97%. Nonetheless, our framework outperforms all these methods, reaching a micro-averaged frame-level AUC of 98.7% and a macro-averaged frame-level AUC of Micro-averaged AUC, macro-averaged AUC, RBDC and TBDC scores (in %) of our approach compared to the state-of-the-art methods <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref> on the UCSD Ped2 data set. When it is unclear if the reported frame-level AUC is micro-averaged or macro-averaged, we placed the score in the middle. All results are copied from the corresponding papers, except for those marked with asterisk (which are computed by ourselves using the official implementations). The best results are highlighted in bold.</p><p>Method AUC RBDC TBDC Micro Macro Kim et al. <ref type="bibr" target="#b7">[8]</ref> 69.3 --Mehran et al. <ref type="bibr" target="#b14">[15]</ref> 55.6 --Mahadevan et al. <ref type="bibr" target="#b13">[14]</ref> 82.9 --Hasan et al. <ref type="bibr" target="#b4">[5]</ref> 90.0 --Zhang et al. <ref type="bibr" target="#b29">[30]</ref> 91.0 --Ionescu et al. <ref type="bibr" target="#b33">[34]</ref> 82.2 ---Luo et al. <ref type="bibr" target="#b12">[13]</ref> 92.2 ---Ravanbakhsh et al. <ref type="bibr" target="#b25">[26]</ref> 93.5 --Xu et al. <ref type="bibr" target="#b19">[20]</ref> 90.8 --Liu et al. <ref type="bibr" target="#b10">[11]</ref> 95.4 98.1* 38.34* 56.76* Liu et al. <ref type="bibr" target="#b34">[35]</ref> 87.5 ---Ravanbakhsh et al. <ref type="bibr" target="#b16">[17]</ref> 88.4 --Gong et al. <ref type="bibr" target="#b41">[42]</ref> 94.1 ---Ionescu et al. <ref type="bibr" target="#b5">[6]</ref> 94.3* 97.8 52.76* 72.88* Lee et al. <ref type="bibr" target="#b8">[9]</ref> 96.6 --Nguyen et al. <ref type="bibr" target="#b40">[41]</ref> 96.2 ---Dong et al. <ref type="bibr" target="#b3">[4]</ref> 95.6 --Doshi et al. <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref> 97.8 --Ji et al. <ref type="bibr" target="#b42">[43]</ref> 98.1 --Lu et al. <ref type="bibr" target="#b43">[44]</ref> 96.2 --Park et al. <ref type="bibr" target="#b15">[16]</ref> -97.0 --Ramachandra et al. <ref type="bibr" target="#b23">[24]</ref> 88.3 62.50 80.50 Ramachandra et al. <ref type="bibr" target="#b24">[25]</ref> 94.0 74.00 89. <ref type="bibr" target="#b29">30</ref> Tang et al. <ref type="bibr" target="#b17">[18]</ref> 96.3 --Yu et al. <ref type="bibr" target="#b46">[47]</ref> 97.3 ---Zaheer et al. <ref type="bibr" target="#b47">[48]</ref> 98 99.7%. In addition to <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, we report the RBDC and TBDC scores for our previous method <ref type="bibr" target="#b5">[6]</ref> and for Liu et al. <ref type="bibr" target="#b10">[11]</ref>. Once again, we had to add a post-processing step to significantly improve the RBDC and TBDC scores of Liu et al. <ref type="bibr" target="#b10">[11]</ref>. In terms of RBDC, our method outperforms Liu et al. <ref type="bibr" target="#b10">[11]</ref>, Ramachandra et al. <ref type="bibr" target="#b23">[24]</ref>, as well as our previous method <ref type="bibr" target="#b5">[6]</ref>. The same can be said about the TBDC scores of these methods. In terms of RBDC and TBDC, our method is somewhat comparable to that of Ramachandra et al. <ref type="bibr" target="#b24">[25]</ref>, the former method attaining a superior TBDC score, while the latter one yielding a better RBDC score. An advantage of our method is its superiority over Ramachandra et al. <ref type="bibr" target="#b24">[25]</ref> in terms of the frame-level AUC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9">Cross-Database Quantitative Results</head><p>In order to demonstrate that our framework is indeed scene-agnostic, we conduct cross-database experiments considering seven pairs of data sets, namely ShanghaiTech?Avenue, Avenue?ShanghaiTech, Avenue?UCSD Ped2, ShanghaiTech?UCSD Ped2, Avenue?Subway Exit, UCSD Ped2?Subway Exit and Avenue + UCSD Ped2?Subway Exit. The corresponding results are presented in <ref type="table" target="#tab_7">Table 5</ref>. We consider our in-domain results as upper bounds for the cross-database results. ShanghaiTech?Avenue. As expected, the performance of our method on Avenue is lower when training is performed on ShanghaiTech instead of Avenue. For example, the microaveraged AUC score degrades from 92.3% to 83.6%. Yet, this result is still better than many state-of-the-art results <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b23">[24]</ref> (see <ref type="table">Table 1</ref>) that rely on models trained on the target data set (Avenue). Our cross-domain method also outperforms models that perform change detection at test time <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, which do not require any training data.</p><p>In terms of RBDC, our cross-domain approach outperforms the in-domain method of Ramachandra et al. <ref type="bibr" target="#b23">[24]</ref> by 5.36%, being able to accurately detect the abnormal regions with a lower false positive rate. Remarkably, the TBDC score of our cross-domain method only drops by 4.09% compared to our in-domain framework. This confirms that the abnormal tracks are still being accurately detected, even though we train our framework on scenes from a different data set. Avenue?ShanghaiTech. In the second cross-domain experiment, we train our framework on the Avenue data set, evaluating it on the ShanghaiTech data set. Even though ShanghaiTech is much larger than Avenue, we are still able to obtain compelling results. In terms of the micro-averaged frame-level AUC, our cross-domain method surpasses most of the state-of-the-art methods <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b37">[38]</ref> (see <ref type="table" target="#tab_2">Table 2</ref>), which were trained on the ShanghaiTech training set. In terms of RBDC, we observe a performance degradation of only 8.79% for our cross-domain method compared to its in-domain version. In terms of TBDC, our performance decreases from 78.79% to 63.89%, but we still outperform the state-of-the-art method of Ionescu et al. <ref type="bibr" target="#b5">[6]</ref> by a significant margin of 19.35%. Avenue?UCSD Ped2. When we train our method on Avenue and test it on UCSD Ped2, we observe significant performance drops in terms of RBDC and TBDC with respect to the in-domain baseline. The performance degradation is likely caused by camera viewpoint, frame rate and frame resolution discrepancies between Avenue and UCSD Ped2. Still, the RBDC and TBDC scores of our cross-domain framework are very close to those of the previous version of our in-domain method <ref type="bibr" target="#b5">[6]</ref> (see <ref type="table" target="#tab_5">Table 4</ref>). Furthermore, the macroaveraged AUC only drops by 2.5% with respect to the upper bound (99.7%) provided by our in-domain method. In terms of the frame-level AUC scores, our cross-domain model outperforms in-domain methods such as <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b33">[34]</ref> (see <ref type="table" target="#tab_5">Table 4</ref>).</p><p>ShanghaiTech?UCSD Ped2. When we train our framework on ShanghaiTech and test it on UCSD Ped2, we notice that our cross-domain method exhibits performance drops similar to those observed for the Avenue?UCSD Ped2 experiment, as the differences between ShanghaiTech and UCSD Ped2 are roughly the same. This time, our micro-averaged frame-level AUC is slightly higher (90.6%), leading to superior performance with respect to in-domain methods such as <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref> (see <ref type="table" target="#tab_5">Table 4</ref>). Avenue?Subway Exit. In terms of frame rate and frame resolution, Avenue and Subway Exit are very well aligned. Hence, it comes as no surprise that, when we train our background-agnostic framework on Avenue and apply it on Subway Exit, we observe generally low performance reductions with respect to the in-domain framework, the only exception being the larger TBDC drop from 67.96% to 63.93%. Still, the RBDC and TBDC scores reported for the Avenue?Subway Exit experiment are higher than the scores of the state-of-the-art in-domain methods <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. UCSD Ped2?Subway Exit. The frame resolution discrepancy between Subway Exit and UCSD Ped2 is only slightly larger than the discrepancy between Subway Exit and Avenue. It is therefore natural to expect minor variations between Avenue?Subway Exit and UCSD Ped2?Subway Exit. While the frame-level AUC is moderately higher when UCSD Ped2 is used as source data set, we attain a slightly better RBDC score when Avenue is used as source data set. We conclude that both Avenue and UCSD Ped2 data sets provide useful training data for Subway Exit. Avenue + UCSD Ped2?Subway Exit. To study if we can gain additional performance by combining multiple source data sets, we considered an experiment where Avenue and UCSD Ped2 are jointly used as source data sets and Subway Exit is used as target data set. We observe insignificant performance changes when we put the source data sets together as opposed to considering them as independent source data sets. This result hints that trying to combine normal training data for multiple sources is not always helpful.</p><p>Overall. Since our cross-domain method is able to outperform many state-of-the-art methods <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b37">[38]</ref> that are trained on the target data sets, we conclude that our method is background-agnostic, providing good results even when testing is performed on scenes never seen during training. <ref type="figure" target="#fig_3">Figure 7</ref> illustrates a set of true positive, false positive and false negative abnormal event localizations from Avenue, ShanghaiTech, Subway Exit and Subway Entrance. We note that the anomaly maps overlapped over the presented frames were subject to a 3D mean filter, hence, the shapes of the detections do not coincide with object bounding boxes. Avenue. The examples selected from the Avenue data set are illustrated on the first row. From left to right, the true positive detections are a person running, a person walking besides a bike and a person throwing an object. The false positive example is a person entering the scene from an unusual location (closer to the camera than usual). The false negative example is represented by papers thrown in the air, which are The false positive example is represented by two people interacting, one of them facing the wrong direction (according to the definition of abnormal events for Subway Entrance <ref type="bibr" target="#b22">[23]</ref>). The false negative example is a person walking in the wrong direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.10">Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.11">Ablation Study</head><p>We perform an ablation study on the Avenue and the ShanghaiTech data sets to emphasize the effect of each component over the overall performance of our framework.</p><p>The ablation results are presented in <ref type="table" target="#tab_8">Table 6</ref>. The first row corresponds the method presented in <ref type="bibr" target="#b5">[6]</ref>, which attains good AUC scores, yet seems to underperform in terms of RBDC and TBDC. Our first design change is to eliminate the k-means clustering and the one-versus-rest (OVR) SVM. The model is left with three auto-encoders, the anomaly score being computed as the mean absolute error (MAE) between the input and the reconstruction of each CAE. Although the AUC scores record significant drops with respect to the original method <ref type="bibr" target="#b5">[6]</ref>, we observe some performance improvements in terms of RBDC and TBDC. Our next design change is the replacement of the SSD-FPN <ref type="bibr" target="#b49">[50]</ref> with YOLOv3 <ref type="bibr" target="#b63">[64]</ref>. This change causes further performance drops on Avenue, in terms of AUC, and on ShanghaiTech, in terms of all metrics. However, YOLOv3 brings significant improvements on Avenue, in terms of RBDC and TBDC. Even though the results obtained with YOLOv3 <ref type="bibr" target="#b63">[64]</ref> are not very encouraging, we have decided to continue the experiments with YOLOv3 <ref type="bibr" target="#b63">[64]</ref>, as it detects four times more objects than SSD-FPN, while having a lower false positive rate. The models presented so far use the image gradients as input to the motion auto-encoders, as proposed in <ref type="bibr" target="#b5">[6]</ref>. Our next change is to replace image gradients with optical flow maps given by SelFlow <ref type="bibr" target="#b66">[67]</ref>. This seems to be a very important design change, leading to significant performance gains with respect to all metrics. We continue our ablation study by considering the integration of an adversarial branch in each auto-encoder, while also switching from conventional training to adversarial training. Through adversarial training, we obtained improvements of around 1% in terms of the macro-averaged frame-level AUC on both data sets. In a similar manner, we updated the architecture and the loss Micro-averaged AUC, macro-averaged AUC, RBDC and TBDC scores (in %) obtained by making gradual design changes to our original method presented in <ref type="bibr" target="#b5">[6]</ref>, until the framework converges to our current proposal. Best results are highlighted in bold. Notations: MAE represents the MAE between the CAE input and output; AD is short for absolute differences between the CAE input and output; LF is short for the CAE latent features. of the appearance CAE such that the model can also output segmentation maps. This change seems to bring some slight improvements on Avenue, in terms of RBDC. Upon integrating the binary classifiers into our framework, we observe major improvements with respect to all performance metrics. We note that training the binary classifiers would not be possible without the adversarial component. Hence, the adversarial component plays an indirect yet important role in our framework, giving us a good reason to keep it. As input for the binary classifiers, we considered three options. Our first option is to use the absolute differences (AD) between inputs and reconstructions of the CAEs, obtaining improvements of more than 12% in terms of TBDC and more than 6% in terms of the micro-averaged frame-level AUC, on both data sets. Our second option, which is based on providing the latent features (LF) as input to the binary classifiers, leads to further performance gains. Our third option is to combine the absolute differences and the latent features. In the combination, the latent features are passed to the binary classifiers through some skip connections, as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. By combining the absolute differences and the latent features, we reach the state-of-the-art macroaveraged frame-level AUC of 90.6% on the Avenue data set, surpassing the method presented in <ref type="bibr" target="#b5">[6]</ref>. Interestingly, we note that replacing YOLOv3 back with SSD-FPN, while keeping all the other design changes presented so far, does not seem to be effective, confirming that YOLOv3 is a better choice in the end. Our next design change is to integrate skip connections into the CAEs, drawing our inspiration from U-Net <ref type="bibr" target="#b71">[72]</ref>. This design change seems to degrade performance. Our last design change is to integrate skip connections by summing up the corresponding features instead of concatenating them as in U-Net. The resulting micro-averaged frame-level AUC scores of 92.3% on the Avenue data set and 82.7% on the ShanghaiTech data set are state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.12">Running Time</head><p>Compared to our previous method described in <ref type="bibr" target="#b5">[6]</ref>, one of the most important changes in terms of time is the replacement of SSD-FPN with YOLOv3. For optimal performance, we process the video in mini-batches of 64 frames. Even though YOLOv3 <ref type="bibr" target="#b63">[64]</ref> is much faster than SSD <ref type="bibr" target="#b48">[49]</ref> with Feature Pyramid Networks <ref type="bibr" target="#b49">[50]</ref>, it still requires about 0.84 seconds to process 64 frames, thus running at 72 frames per second (FPS). The slowest component of our framework is SelFlow <ref type="bibr" target="#b66">[67]</ref>, which runs at 20 FPS on mini-batches of 32 frames. Due to the fact that our CAEs are very light, they require only 1.5 milliseconds to extract the latent features and to output the reconstruction for one input image. Our binary classifiers are even faster, requiring less than 1 millisecond to obtain the normality score for one object. Reassembling the anomaly scores of the detected objects into an anomaly map for each frame takes less than 1 millisecond. Putting all the components together, our framework runs at 18 FPS with a reasonable average of 5 objects per frame. We note that the reported speed of 18 FPS is for a sequential processing pipeline on a single thread. This is to fairly compare with other methods from the literature. However, we note that running the pipeline on two threads in parallel increases the speed to 24 FPS, while still using a single GPU. Hence, our framework can process the video in real time using parallel processing. Nevertheless, we would like to note that 76% of the processing time is spent computing the optical flow. Therefore, one way to further speed up the running time is to replace SelFlow with a faster optical flow predictor. The reported running times were measured on a GeForce GTX 3090 GPU with 24 GB of VRAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.13">Impact of Adversarial Training</head><p>Throughout the experiments presented above, we set ? (the weight of the reversed gradient) to 0.2, as recommended in <ref type="bibr" target="#b73">[74]</ref>. While this setting relinquishes us from the need to validate ? on each data set, we recognize that it may also produce suboptimal results. In order to study the effect of ? on the performance level, we present supplementary results on Avenue, ShanghaiTech, Subway Exit and UCSD Ped2, considering values of ? between 0 and 1 taken at a regular step of 0.1. In the subsequent experiments, we underline that ? = 0 is equivalent to removing adversarial training altogether. <ref type="figure" target="#fig_4">Figure 8</ref> illustrates the effect of ? on the micro-averaged frame-level AUC. We observe that ? has almost no effect on the Subway Exit data set. For the other data sets, we discover that adversarial training, i.e. when ? ? 0.1, is instrumental, bringing significant performance gains. However, there is no agreement across data sets regarding the optimal value for ?. Indeed, ? = 0.9 produces the best results (83.1%) on ShanghaiTech, while ? = 0.6 looks like a better choice for UCSD Ped2, leading to a micro-averaged AUC of 99.7%. The recommendation of McHardy et al. <ref type="bibr" target="#b73">[74]</ref> to use ? = 0.2 provides the best results (92.3%) only on Avenue. <ref type="figure" target="#fig_5">Figure 9</ref> shows the effect of ? on the macro-averaged frame-level AUC. First, we note that the results on Subway Exit are the same as in <ref type="figure" target="#fig_4">Figure 8</ref>, since the micro-averaged and the macro-averaged AUC are the same for this data set containing a single and very long test video. On UCSD Ped2,   we notice that there are multiple values of ? that produce a macro-averaged AUC of 100%. On ShanghaiTech, we obtain the top macro-averaged frame-level AUC of 89.9% with ? = 0.9, while on Avenue, ? = 0.1 produces the top macroaveraged frame-level AUC of 91.3%.</p><p>The effect of ? on RBDC is illustrated in <ref type="figure" target="#fig_0">Figure 10</ref>. We underline that our default configuration, i.e. ? = 0.2, produces optimal RBDC scores for both Avenue (65.05%) and ShanghaiTech (41.34%). On UCSD Ped2, we obtain the top RBDC score of 71.60% with ? = 0.4, while on Subway Exit, the RBDC scores are nearly constant, regardless of ?. <ref type="figure" target="#fig_0">Figure 11</ref> show the effect of ? on TBDC. We observe that values of ? ? 1 produce fairly low TBDC fluctuations on Avenue, Subway Exit and UCSD Ped2. There are noticeable fluctuations on ShanghaiTech, where the ? = 0.9 produces the highest TBDC score (79.45%). We also underline that completely removing adversarial training (? = 0) produces significantly lower TBDC scores on Avenue, ShanghaiTech and UCSD Ped2.</p><p>Overall, we observe that the results are consistent across the four evaluation metrics. We conclude that the results discussed above indicate that it is better to tune the hyperparameter ? on each data set, provided that a validation set would be available. Nevertheless, we emphasize that adversarial training brings major improvements, even when the value of ? is not optimally chosen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.14">Testing Additional Design Choices</head><p>In <ref type="table" target="#tab_10">Table 7</ref>, we present results considering additional design choices on four benchmarks: Avenue, ShanghaiTech, Subway Exit and UCSD Ped2. Another perspective of the results  is provided with the bar charts illustrated in <ref type="bibr">Figures 12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr">14 and 15.</ref> In the proposed method, we computed the average of the normality scores of two motion classifiers and one appearance classifier, implicitly assigning a higher weight to motion. When we double the weight of the appearance classifier, we observe some noticeable improvements on Avenue (in terms of RBDC) and ShanghaiTech (in terms of RBDC and TBDC), the other positive or negative performance changes being minor.</p><p>In the proposed framework, we employed the L 2 loss to measure reconstruction errors. However, an alternative option is to use the L 1 loss. The results reported in <ref type="table" target="#tab_10">Table 7</ref> show generally lower performance levels for the L 1 loss on Avenue, ShanghaiTech and Subway Exit. The L 1 loss brings improvements only on UCSD Ped2. Overall, we believe that the L 2 loss is a better choice.</p><p>To keep the similar designed as in <ref type="bibr" target="#b5">[6]</ref> for our appearance CAE, we used a grayscale input. However, replacing the grayscale input with a color (RGB) input does not seem to bring any improvements for Avenue and ShanghaiTech. We underline that the videos in Subway and UCSD Ped2 are already grayscale, so it does not make sense to use auto-encoders with RGB input as this would not make any difference.</p><p>As input to our binary classifiers, we considered the absolute differences between inputs and reconstructions. Alternatively, the binary classifiers could also take the signed differences as input. We observe that the performance obtained with absolute differences is generally higher than using signed differences. Intuitively, for abnormal event detection, we believe that knowing if the difference is small or large is more important than knowing if the reconstruction contains pixels of lower or higher intensities than the input. Hence, the absolute difference, i.e. the L 1 distance between inputs and reconstructions, is more representative.</p><p>. <ref type="figure" target="#fig_0">Fig. 16</ref>. Two frames from Street Scene <ref type="bibr" target="#b23">[24]</ref> illustrating failure cases of our framework. Objects detected by YOLOv3 are surrounded by green bounding boxes, while abnormal objects are surrounded by yellow bounding boxes. There is no overlap between abnormal and detected objects. Best viewed in color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.15">Failure Cases</head><p>Besides Avenue, ShanghaiTech, Subway and UCSD Ped2, we also tried to apply our anomaly detection framework on the recently introduce Street Scene data set <ref type="bibr" target="#b23">[24]</ref>. Street Scene was relevant to us because it is a large data set and we would have been able to compare our method in terms of RBDC and TBDC with an existing work <ref type="bibr" target="#b23">[24]</ref>. Street Scene contains a single outdoor scene which is filmed from above, as shown in the frames illustrated in <ref type="figure" target="#fig_0">Figure 16</ref>. Since the objects are relatively small and filmed from an atypical perspective, the pre-trained YOLOv3 detector fails to detect the objects of interest in most cases. In <ref type="figure" target="#fig_0">Figure 16</ref>, we present two examples in which the YOLOv3 detections (surrounded by green bounding boxes) do not include the abnormal objects (surrounded by yellow bounding boxes), which are hard to see even with the naked eye. In such cases, our anomaly detection framework has no chance of detecting the anomalies. In order to demonstrate that it is impossible to surpass the state-of-the-art method <ref type="bibr" target="#b23">[24]</ref> due to the poor performance of the object detector, we compute the RBDC for the YOLOv3 detections obtained for the confidence level 0.5, while assuming that the anomaly detection framework would output perfect results on the test set. In this setting, our RBDC score is 17.12%, which is 5% under the RBDC score of 21% reported in <ref type="bibr" target="#b23">[24]</ref>. We thus conclude that our framework fails to perform well when the objects of interest are too small or filmed in atypical perspectives. Certainly, the obvious solution is to train or fine-tune the object detector on the training video, but this would require manual labeling, which is not available for Street Scene or other video anomaly detection benchmarks.</p><p>We note that failures of the object detector can also affect our results on the other data sets. For example, on Avenue, the object detector does not detect papers or backpacks thrown in the air. Papers are not within the MS COCO object categories, while backpacks are not detected due to motion blur. However, the person performing the abnormal action (throwing papers or a backpack in the air) is detected and labeled as abnormal by our framework. This can generate false negatives when we consider the region-level or tracklevel metrics. However, the frame-level anomaly scores will remain unaffected in such cases.</p><p>In addition, we note that our sampling approach for pseudo-abnormal examples is based on the supposition that training and testing videos are typically collected at the same frame rate. For example, if the test frame rate is significantly lower, it may cause normal events to be falsely reported as abnormal. This is another potential failure case, which we observed to cause performance drops in the crossdomain experiments involving the UCSD Ped2 data set as target. However, this should not represent a problem in practice, as long as we have control over the frame sampling rate.</p><p>As the chosen benchmarks do not contain location-based anomalies, we did not consider location information in our framework. To address this issue, one possible solution is to encode the location of object bounding boxes as pyramidal one-hot vectors, as proposed in <ref type="bibr" target="#b6">[7]</ref>. At a given level of the pyramid, the frame is divided into a number of spatial bins. Upon quantizing bounding boxes into bins, the one-hot encoding of a bounding box can be immediately obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we have presented a set of significant design changes to our abnormal event detection approach presented at CVPR 2019 <ref type="bibr" target="#b5">[6]</ref>. More specifically, we replaced the k-means clustering and the one-versus-rest SVM with a set of binary classifiers that learn from normal and pseudoabnormal examples. Additionally, we modified the convolutional auto-encoders by adding adversarial and segmentation branches, as well as skip connections. Our design changes resulted in significant performance improvements in terms of both RBDC and TBDC. As a secondary contribution of our work, we release region-level and tracklevel annotations for the ShanghaiTech <ref type="bibr" target="#b12">[13]</ref> and the Subway <ref type="bibr" target="#b22">[23]</ref> data sets. Our experiments conducted on Avenue, ShanghaiTech, Subway and UCSD Ped2 indicate that our approach generally attains state-of-the-art results. We also demonstrated that our approach is background-agnostic, outperforming many approaches from the recent literature <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b37">[38]</ref>, when our method is tested on scenes not seen during training (unlike the related works that are trained and tested on the same scenes).</p><p>In future work, we aim to study new ways to improve the computational time of our abnormal event detection framework. More precisely, we will pursue faster object detection and optical flow estimation methods, which currently account for 95% of our total processing time.</p><p>Marius Popescu is associate professor at the University of Bucharest, Department of Computer Science. He defended his PhD in 2004 with the thesis "Machine Learning Applied in Natural Language Processing". His domains of interest are: artificial intelligence, machine learning, computational linguistics, information retrieval, authorship identification, computer vision. His achievements in these fields include: a method for word sense disambiguation that was awarded third prize at Senseval 3 in 2004; the ENCOPLOT method for plagiarism detection that won the first international competition in plagiarism detection in 2009, followed by ranking 4th at PAN@CLEF 2010 and 2nd at PAN@CLEF 2011; a method for authorship analysis obtaining the best results in author identification at PAN@CLEF 2012; methods that ranked on 3rd place in the Native Language Identification Shared <ref type="table">Task</ref>  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Our anomaly detection framework based on training convolutional auto-encoders with skip connections on top of object detections. In the learning phase, pseudo-abnormal examples are used to train the adversarial decoder branch using gradient ascent. The absolute differences between the inputs and the reconstructions are provided as input to a binary classifier corresponding to each convolutional auto-encoder. In the inference phase, we can label a test sample as abnormal if the average classification score is negative, i.e. the sample is labeled as pseudoabnormal. Components represented in dashed lines are removed during inference. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Indeed, we observe that the auto-encoders based on adversarial training exhibit visibly worse reconstructions for abnormal examples than for normal examples, this being the desired effect. We notice the same effect on the pseudo-abnormal examples, only at a greater level. This happens because the pseudo-abnormal examples are used in the adversarial training procedure, while the abnormal examples are selected from the test set. In summary, we conclude that introducing adversarial training is helpful in achieving the desired goal, that of obtaining visibly better reconstructions for normal examples than for abnormal examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Normal (top), pseudo-abnormal (middle) and abnormal (bottom) examples and optical flow maps with reconstructions provided by the appearance and the motion convolutional auto-encoders, which are trained either without adversarial training (left) or with adversarial training (right). The auto-encoders provide worse reconstructions for pseudo-abnormal and abnormal examples after adversarial training, which is the desired effect. The normal and abnormal samples are selected from the Avenue [12] and the ShanghaiTech [13] test sets, while the pseudoabnormal examples are select from our pool of generic pseudo-abnormal examples. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 .</head><label>7</label><figDesc>True positive (left) versus false positive (middle) and false negative (right) detections of our framework. Examples are selected from the Avenue<ref type="bibr" target="#b11">[12]</ref> (first row), the ShanghaiTech<ref type="bibr" target="#b12">[13]</ref> (second row), the Subway Exit<ref type="bibr" target="#b22">[23]</ref> (third row) and the Subway Entrance<ref type="bibr" target="#b22">[23]</ref> (fourth row) data sets. Best viewed in color.not detected because paper is not among the classes known by the pre-trained object detector. ShanghaiTech. The examples from ShanghaiTech are illustrated on the second row ofFigure 7. The true positive abnormal events detected by our framework are (from left to right) two people running, a person jumping and a person riding a bike. The false positive example consists of two people that are detected in the same bounding box by the object detector, generating a very unusual motion. The false negative example is a person riding a skateboard, which is not detected by YOLOv3 because the person is too small (just entering the scene from the far end).Subway Exit. The examples from Subway Exit are presented on the third row. From left to right, the first two true positive detections represent a person walking in the wrong direction, while the third true positive example is a person loitering. The false positive example is a person crossing the scene from left to right. The false negative example is a person loitering. Subway Entrance. The examples from Subway Entrance are presented on the last row of Figure 7. From left to right, the first two true positive examples represent a person jumping over the gate, while the third true positive example is a person loitering.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 .</head><label>8</label><figDesc>Effect of ? (the weight of the reversed gradient) on the microaveraged frame-level AUC. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 .</head><label>9</label><figDesc>Effect of ? (the weight of the reversed gradient) on the macroaveraged frame-level AUC. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 .</head><label>10</label><figDesc>Effect of ? (the weight of the reversed gradient) on RBDC. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 11 .</head><label>11</label><figDesc>Effect of ? (the weight of the reversed gradient) on TBDC. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 14 .</head><label>14</label><figDesc>Micro-averaged AUC, macro-averaged AUC, RBDC and TBDC scores on Subway Exit with various design changes applied on our framework. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 15 .</head><label>15</label><figDesc>Micro-averaged AUC, macro-averaged AUC, RBDC and TBDC scores on UCSD Ped2 with various design changes applied on our framework. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>of BEA-8, on 4th place in the Facial Expression Recognition Challenge of WREPL 2013, 2nd place in the Arabic Dialect Identification Shared Task of VarDial 2016 and 1st place in the Native Language Identification Shared Task of BEA-12. Mubarak Shah , the UCF Trustee chair professor, is the founding director of the Center for Research in Computer Vision at the University of Central Florida (UCF). He is a fellow of the NAI, IEEE, AAAS, IAPR and SPIE. He is an editor of an international book series on video computing, was editor-in-chief of Machine Vision and Applications and an associate editor of ACM Computing Surveys and IEEE T-PAMI. He was the program cochair of CVPR 2008, an associate editor of the IEEE T-PAMI and a guest editor of the special issue of the International Journal of Computer Vision on Video Computing. His research interests include video surveillance, visual tracking, human activity recognition, visual analysis of crowded scenes, video registration, UAV video analysis, among others. He has served as an ACM distinguished speaker and IEEE distinguished visitor speaker. He is a recipient of ACM SIGMM Technical Achievement award; IEEE Outstanding Engineering Educator Award; Harris Corporation Engineering Achievement Award; an honorable mention for the ICCV 2005 "Where Am I?" Challenge Problem; 2013 NGA Best Research Poster Presentation; 2nd place in Grand Challenge at the ACM Multimedia 2013 conference; and runner up for the best paper award in ACM Multimedia Conference in 2005 and 2010. At UCF he has received Pegasus Professor Award; University Distinguished Research Award; Faculty Excellence in Mentoring Doctoral Students; Scholarship of Teaching and Learning award; Teaching Incentive Program award; Research Incentive Award.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2</head><label>2</label><figDesc>Micro-averaged AUC, macro-averaged AUC, RBDC and TBDC scores</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 4</head><label>4</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 5</head><label>5</label><figDesc>Micro-averaged AUC, macro-averaged AUC, RBDC and TBDC scores (in %) of our method for a series of cross-domain experiments. For each experiment, we specify the training and the test data sets.</figDesc><table><row><cell>Training?Test</cell><cell cols="2">AUC</cell><cell>RBDC TBDC</cell></row><row><cell></cell><cell cols="2">Micro Macro</cell><cell></cell></row><row><cell>Avenue?Avenue</cell><cell>92.3</cell><cell>90.4</cell><cell>65.05 66.85</cell></row><row><cell>ShanghaiTech?Avenue</cell><cell>83.6</cell><cell>81.0</cell><cell>46.56 62.76</cell></row><row><cell>ShanghaiTech?ShanghaiTech</cell><cell>82.7</cell><cell>89.3</cell><cell>41.34 78.79</cell></row><row><cell>Avenue?ShanghaiTech</cell><cell>76.3</cell><cell>86.3</cell><cell>32.55 63.89</cell></row><row><cell>UCSD Ped2?UCSD Ped2</cell><cell>98.7</cell><cell>99.7</cell><cell>69.23 93.15</cell></row><row><cell>Avenue?UCSD Ped2</cell><cell>87.0</cell><cell>97.2</cell><cell>47.43 68.58</cell></row><row><cell>Shanghai?UCSD Ped2</cell><cell>90.6</cell><cell>95.7</cell><cell>41.71 70.49</cell></row><row><cell>Training?Test</cell><cell cols="2">AUC</cell><cell>RBDC TBDC</cell></row><row><cell></cell><cell>Old</cell><cell>New</cell><cell></cell></row><row><cell>Subway Exit?Subway Exit</cell><cell>92.1</cell><cell>93.7</cell><cell>47.95 67.96</cell></row><row><cell>Avenue?Subway Exit</cell><cell>92.1</cell><cell>92.4</cell><cell>47.54 63.93</cell></row><row><cell>UCSD Ped2?Subway Exit</cell><cell>92.8</cell><cell>92.8</cell><cell>46.60 65.73</cell></row><row><cell cols="2">Avenue+UCSD Ped2?Subway Exit 92.1</cell><cell>92.3</cell><cell>47.50 64.18</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 6</head><label>6</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>SVM 87.4 90.4 15.77 27.01 78.70 84.90 20.65 44.54 SSD-FPN motion gradients MAE 77.8 78.8 16.12 30.17 72.20 79.30 24.24 49.69 AD+LF 91.6 90.6 62.11 67.55 82.40 88.90 41.26 77.29 SSD-FPN fusion SelFlow classifier on AD+LF 82.6 80.7 21.29 34.12 78.40 81.70 28.73 59.10 YOLOv3 motion SelFlow concatenate classifier on AD+LF 87.4 87.2 52.87 64.17 81.00 87.40 37.24 75.54 YOLOv3 appearance -concatenate classifier on AD+LF 81.7 83.7 46.08 49.94 68.50 75.80 33.05 72.37 YOLOv3 fusion SelFlow concatenate classifier on AD+LF 90.9 90.5 64.59 65.12 82.50 88.30 38.79 76.68 YOLOv3 motion SelFlow sum classifier on AD+LF 84.8 84.0 46.58 63.57 81.40 88.10 37.78 76.05 90.4 65.05 66.85 82.70 89.30 41.34 78.79</figDesc><table><row><cell>Object</cell><cell>CAE</cell><cell>Motion</cell><cell>CAE</cell><cell>CAE</cell><cell>CAE</cell><cell>Anomaly</cell><cell cols="2">Avenue</cell><cell>ShanghaiTech</cell></row><row><cell>Detector</cell><cell>Input</cell><cell cols="3">Type Adversarial Segmentation</cell><cell>Skip</cell><cell>Detection</cell><cell>AUC</cell><cell>RBDC TBDC</cell><cell>AUC</cell><cell>RBDC TBDC</cell></row><row><cell></cell><cell>Type</cell><cell></cell><cell>Branch</cell><cell>Branch</cell><cell>Connections</cell><cell>Method</cell><cell>Micro Macro</cell><cell></cell><cell>Micro Macro</cell></row><row><cell cols="7">SSD-FPN fusion gradients k-means+OVR SSD-FPN appearance -MAE</cell><cell cols="3">77.6 77.4 16.77 29.90 75.60 81.90 26.30 50.44</cell></row><row><cell cols="3">SSD-FPN fusion gradients</cell><cell></cell><cell></cell><cell></cell><cell>MAE</cell><cell cols="3">78.3 79.2 16.42 30.01 72.40 79.70 24.35 49.74</cell></row><row><cell cols="3">YOLOv3 motion gradients</cell><cell></cell><cell></cell><cell></cell><cell>MAE</cell><cell cols="3">72.4 74.6 47.88 48.34 65.60 76.80 19.11 44.72</cell></row><row><cell cols="2">YOLOv3 appearance</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell>MAE</cell><cell cols="3">78.3 76.5 49.94 44.22 67.50 78.40 21.69 48.13</cell></row><row><cell cols="3">YOLOv3 fusion gradients</cell><cell></cell><cell></cell><cell></cell><cell>MAE</cell><cell cols="3">74.8 75.4 51.49 48.80 65.80 77.70 19.54 45.16</cell></row><row><cell cols="3">YOLOv3 motion SelFlow</cell><cell></cell><cell></cell><cell></cell><cell>MAE</cell><cell cols="3">80.3 81.7 50.60 52.65 69.30 79.80 22.84 52.66</cell></row><row><cell cols="2">YOLOv3 fusion</cell><cell>SelFlow</cell><cell></cell><cell></cell><cell></cell><cell>MAE</cell><cell cols="3">81.6 82.3 52.89 52.70 69.80 80.60 22.90 52.70</cell></row><row><cell cols="3">YOLOv3 motion SelFlow</cell><cell></cell><cell></cell><cell></cell><cell>MAE</cell><cell cols="3">77.6 81.3 44.96 49.66 69.00 80.50 23.23 52.62</cell></row><row><cell cols="2">YOLOv3 appearance</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell>MAE</cell><cell cols="3">76.7 79.7 48.84 45.28 67.30 79.00 21.48 48.67</cell></row><row><cell cols="2">YOLOv3 fusion</cell><cell>SelFlow</cell><cell></cell><cell></cell><cell></cell><cell>MAE</cell><cell cols="3">80.0 83.4 49.98 51.69 69.20 81.20 23.23 52.68</cell></row><row><cell cols="2">YOLOv3 appearance</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell>MAE</cell><cell cols="3">77.3 79.5 49.86 43.45 67.90 77.10 20.67 48.29</cell></row><row><cell cols="2">YOLOv3 fusion</cell><cell>SelFlow</cell><cell></cell><cell></cell><cell></cell><cell>MAE</cell><cell cols="3">79.9 83.7 50.72 51.38 69.20 81.10 23.11 52.71</cell></row><row><cell cols="3">YOLOv3 motion SelFlow</cell><cell></cell><cell></cell><cell></cell><cell>classifier on AD</cell><cell cols="3">84.8 85.7 46.30 62.54 78.80 87.90 34.79 72.97</cell></row><row><cell cols="2">YOLOv3 appearance</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell>classifier on AD</cell><cell cols="3">79.7 81.5 36.98 49.66 62.30 75.10 24.21 54.28</cell></row><row><cell cols="2">YOLOv3 fusion</cell><cell>SelFlow</cell><cell></cell><cell></cell><cell></cell><cell>classifier on AD</cell><cell cols="3">86.2 85.9 50.68 63.74 77.50 87.70 33.78 69.79</cell></row><row><cell cols="3">YOLOv3 motion SelFlow</cell><cell></cell><cell></cell><cell></cell><cell>classifier on LF</cell><cell cols="3">86.5 86.2 49.96 63.10 81.60 88.60 36.43 74.31</cell></row><row><cell cols="2">YOLOv3 appearance</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell>classifier on LF</cell><cell cols="3">89.2 83.4 48.13 53.38 63.30 74.10 30.44 66.80</cell></row><row><cell cols="2">YOLOv3 fusion</cell><cell>SelFlow</cell><cell></cell><cell></cell><cell></cell><cell>classifier on LF</cell><cell cols="3">86.6 88.2 51.79 63.26 81.90 89.10 37.31 75.33</cell></row><row><cell cols="3">YOLOv3 motion SelFlow</cell><cell></cell><cell></cell><cell></cell><cell cols="4">classifier on AD+LF 86.1 82.5 45.29 65.02 80.00 87.30 37.49 75.71</cell></row><row><cell cols="2">YOLOv3 appearance</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell cols="4">classifier on AD+LF 87.4 83.3 49.15 53.23 64.80 77.20 31.00 68.35</cell></row><row><cell cols="10">YOLOv3 fusion classifier on YOLOv3 appearance SelFlow -sum classifier on AD+LF 83.0 82.4 39.57 52.83 63.90 73.50 33.72 69.31</cell></row><row><cell cols="2">YOLOv3 fusion</cell><cell>SelFlow</cell><cell></cell><cell></cell><cell>sum</cell><cell cols="2">classifier on AD+LF 92.3</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 7</head><label>7</label><figDesc>Micro-averaged AUC, macro-averaged AUC, RBDC and TBDC scores (in %) with various design changes applied on our framework. Results are reported on the Avenue, ShanghaiTech, Subway Exit and UCSD Ped2 data sets. 90.4 65.05 66.85 82.7 89.3 41.34 78.79 92.1 93.7 47.95 67.96 98.7 99.7 69.23 93.15 2?weight for appearance classifier 92.1 90.0 66.77 63.69 82.3 87.5 42.41 79.46 92.1 93.6 47.82 67.18 98.9 99.7 69.06 92.27 L 1 loss for CAEs 91.3 90.0 64.92 64.54 82.1 88.8 39.62 78.11 91.5 93.3 47.70 67.48 99.4 99.9 69.36 92.46 RGB input for appearance CAE 91.7 90.3 63.97 65.48 81.9 89.5 38.90 78.90.1 61.98 64.71 81.7 87.7 39.73 78.07 92.3 93.3 47.85 67.12 99.2 99.6 67.12 92.37Fig. 12. Micro-averaged AUC, macro-averaged AUC, RBDC and TBDC scores on Avenue with various design changes applied on our framework. Best viewed in color.Fig. 13. Micro-averaged AUC, macro-averaged AUC, RBDC and TBDC scores on ShanghaiTech with various design changes applied on our framework. Best viewed in color.</figDesc><table><row><cell></cell><cell cols="2">Avenue</cell><cell cols="2">ShanghaiTech</cell><cell cols="2">Subway Exit</cell><cell></cell><cell cols="2">UCSD Ped2</cell></row><row><cell>Method</cell><cell>AUC</cell><cell>RBDC TBDC</cell><cell>AUC</cell><cell cols="4">RBDC TBDC AUC RBDC TBDC</cell><cell>AUC</cell><cell>RBDC TBDC</cell></row><row><cell></cell><cell>Micro Macro</cell><cell></cell><cell>Micro Macro</cell><cell cols="2">Old New</cell><cell></cell><cell></cell><cell cols="2">Micro Macro</cell></row><row><cell>Proposed method</cell><cell cols="4">92.3 63 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Signed differences for classifiers</cell><cell>89.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The research leading to these results has received funding from the EEA Grants 2014-2021, under Project contract no. EEA-RO-NO-2018-0496. This article has also benefited from the support of the Romanian Young Academy, which is funded by Stiftung Mercator and the Alexander von Humboldt Foundation for the period 2020-2022. The work is also supported by starting grant (GR010) and VR starting grant (2016-05543).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Iuliana Georgescu is a PhD student at the Faculty of Mathematics and Computer Science, University of Bucharest. She received the B.Sc. degree from the Faculty of Mathematics and Computer Science, University of Bucharest, in 2017 and the M.Sc. degree in Artificial Intelligence from the same university in 2019. Although she is early in her research career, she is the first author of 6 papers published at conferences and journals. Her research interests include artificial intelligence, computer vision, machine learning, deep learning and medical image processing.  His research interests include a wide range of topics within computer vision, such as object recognition, object detection, action recognition and visual tracking. He has published articles in high-impact computer vision journals and conferences in these areas.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A Survey of Single-Scene Video Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramachandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Vatsavai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Video parsing for abnormality detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Antic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2415" to="2422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Video anomaly detection and localization using hierarchical feature representation and Gaussian process regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-H</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2909" to="2917" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dual Discriminator Generative Adversarial Network for Video Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="88" to="170" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning temporal regularity in video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="733" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Object-Centric Auto-Encoders and Dummy Anomalies for Abnormal Event Detection in Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-I</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7842" to="7851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Detecting abnormal events in video using Narrowed Normality Clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Smeureanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WACV</title>
		<meeting>WACV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1951" to="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Observe locally, infer globally: A spacetime MRF for detecting abnormal activities with incremental updates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2921" to="2928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BMAN: Bidirectional Multi-Scale Aggregation Networks for Abnormal Event Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Ro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2395" to="2408" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Anomaly detection and localization in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="18" to="32" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Future Frame Prediction for Anomaly Detection -A New Baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6536" to="6545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Abnormal Event Detection at 150 FPS in MATLAB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2720" to="2727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A Revisit of Sparse Coding Based Anomaly Detection in Stacked RNN Framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="341" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Anomaly Detection in Crowded Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bhalodia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1975" to="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Abnormal crowd behavior detection using social force model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mehran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="935" to="942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning Memory-guided Normality for Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR, 2020</title>
		<meeting>CVPR, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="14" to="372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Plug-and-Play CNN for Crowd Motion Analysis: An Application in Abnormal Event Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mousavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WACV</title>
		<meeting>WACV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1689" to="1698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Integrating prediction and reconstruction for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="123" to="130" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A Deep One-Class Neural Network for Anomalous Event Detection in Complex Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2609" to="2622" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Detecting Anomalous Events in Videos by Learning Deep Representations of Appearance and Motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">156</biblScope>
			<biblScope unit="page" from="117" to="127" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Online Detection of Unusual Events in Videos via Dynamic Sparse Coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3313" to="3320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Video Anomaly Detection and Localization using Motion-field Shape Description and Homogeneity Testing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="page">107394</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Robust Real-Time Unusual Event Detection Using Multiple Fixed-Location Monitors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shimshoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Reinitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="555" to="560" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Street Scene: A new dataset and evaluation protocol for video anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramachandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WACV, 2020</title>
		<meeting>WACV, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="2569" to="2578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning a distance function with a Siamese network to localize anomalies in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramachandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vatsavai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WACV, 2020</title>
		<meeting>WACV, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="2598" to="2607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Abnormal Event Detection in Videos using Generative Adversarial Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Marcenaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Regazzoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICIP</title>
		<meeting>ICIP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1577" to="1581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep-cascade: Cascading 3D Deep Neural Networks for Fast Anomaly Detection and Localization in Crowded Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Online Detection of Abnormal Events Using Incremental Coding Length</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Banerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3755" to="3761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Video anomaly detection based on local statistical aggregates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2112" to="2119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Video anomaly detection based on locality sensitive hashing filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sakai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="302" to="311" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep Appearance Features for Abnormal Behavior Detection in Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Smeureanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICIAP</title>
		<meeting>ICIAP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">10485</biblScope>
			<biblScope unit="page" from="779" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sparse reconstruction cost for abnormal event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3449" to="3456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A Discriminative Framework for Anomaly Detection in Large Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Giorno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="334" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unmasking the abnormal events in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Smeureanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2895" to="2903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Classifier Two-Sample Test for Video Anomaly Detections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>P?czos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Selftrained Deep Ordinal Regression for End-to-End Video Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Real-World Anomaly Detection in Surveillance Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sultani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6479" to="6488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Any-Shot Sequential Anomaly Detection in Surveillance Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yilmaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPRW, 2020</title>
		<meeting>CVPRW, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="934" to="935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Continual Learning for Anomaly Detection in Surveillance Videos</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPRW, 2020</title>
		<meeting>CVPRW, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="254" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">STAN: Spatio-temporal adversarial networks for abnormal event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Ro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1323" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Anomaly detection in video sequence with appearance-motion correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-N</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Meunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Memorizing Normality to Detect Anomaly: Memory-Augmented Deep Autoencoder for Unsupervised Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1705" to="1714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">TAM-Net: Temporal Enhanced Appearance-to-Motion Generative Network for Video Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCNN, 2020</title>
		<meeting>IJCNN, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Few-Shot Scene-Adaptive Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV, 2020</title>
		<meeting>ECCV, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="125" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Scene-Aware Context Reasoning for Unsupervised Abnormal Event Detection in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACMMM, 2020</title>
		<meeting>ACMMM, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="184" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Cluster Attention Contrast for Video Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACMMM</title>
		<meeting>ACMMM</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2463" to="2471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Cloze Test Helps: Effective Video Anomaly Detection via Learning to Complete Video Events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACMMM</title>
		<meeting>ACMMM</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="583" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Old is Gold: Redefining the Adversarially Learned One-Class Classifier Training Paradigm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Astrid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-I</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR, 2020</title>
		<meeting>CVPR, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="14" to="183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">SSD: Single Shot MultiBox Detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Unsupervised Behavior-Specific Dictionary Learning for Abnormal Event Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Olsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="28" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep-Anomaly: Fully convolutional neural network for fast anomaly detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Moayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page" from="88" to="97" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Online growing neural gas for anomaly detection in changing surveillance scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">C</biblScope>
			<biblScope unit="page" from="187" to="201" />
			<date type="published" when="2017-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Anomaly Detection using a Convolutional Winner-Take-All Autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hogg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning deep event models for crowd anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">219</biblScope>
			<biblScope unit="page" from="548" to="556" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Joint Detection and Recounting of Abnormal Events by Learning Deep Generic Knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hinami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3639" to="3647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Chaotic Invariants of Lagrangian Particle Trajectories for Anomaly Detection in Crowded Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2054" to="2060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Outlier Detection for Time Series with Recurrent Autoencoder Ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2725" to="2732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Lipschitz Continuous Autoencoders in Application to Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Paik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AISTATS, 2020</title>
		<meeting>AISTATS, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="2507" to="2517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Anomaly Detection with Robust Deep Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Paffenroth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD</title>
		<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="665" to="674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Robust Anomaly Detection in Images Using Adversarial Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beggel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bischl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECML-PKDD</title>
		<meeting>ECML-PKDD</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="206" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Generative Adversarial Attributed Network Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM, 2020</title>
		<meeting>CIKM, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="1989" to="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">YOLOv3: An incremental improvement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">SelFlow: Self-Supervised Learning of Optical Flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4571" to="4580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">A Sparse Texture Representation Using Local Affine Regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1265" to="1278" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">A Visual Vocabulary for Flower Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1447" to="1454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Rectified Linear Units Improve Restricted Boltzmann Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of MICCAI</title>
		<meeting>MICCAI</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Adversarial Training for Satire Detection: Controlling for Confounding Variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mchardy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="660" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">TensorFlow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of OSDI</title>
		<meeting>OSDI</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
