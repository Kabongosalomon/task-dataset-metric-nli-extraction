<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ACP++: Action Co-occurrence Priors for Human-Object Interaction Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20151">AUGUST 2015 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">ACP++: Action Co-occurrence Priors for Human-Object Interaction Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20151">AUGUST 2015 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Human-object interaction</term>
					<term>visual relationship</term>
					<term>co-occurrence</term>
					<term>label hierarchy</term>
					<term>knowledge distillation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A common problem in the task of human-object interaction (HOI) detection is that numerous HOI classes have only a small number of labeled examples, resulting in training sets with a long-tailed distribution. The lack of positive labels can lead to low classification accuracy for these classes. Towards addressing this issue, we observe that there exist natural correlations and anti-correlations among human-object interactions. In this paper, we model the correlations as action co-occurrence matrices and present techniques to learn these priors and leverage them for more effective training, especially on rare classes. The efficacy of our approach is demonstrated experimentally, where the performance of our approach consistently improves over the state-of-the-art methods on both of the two leading HOI detection benchmark datasets, HICO-Det and V-COCO. P(operate-hair_drier) = 0.001 P(operate-hair_drier | hold-hair_drier) = 0.958 P(blow-cake) = 0.007 P(blow-cake | cut-cake) = 0.005</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>H UMAN-object interaction (HOI) detection aims to localize humans and objects in an image and infer the relationships between them. An HOI is typically represented as a human-action-object triplet with the corresponding bounding boxes and classes. Detecting these interactions is a fundamental challenge in visual recognition that requires both an understanding of object information and high-level knowledge of interactions.</p><p>A major issue that exists in HOI detection is that its datasets suffer from long-tailed distributions, in which many HOI triplets have few labeled instances. Similar to datasets for general visual relationship detection (VRD) <ref type="bibr" target="#b52">[53]</ref>, a reason for this is missing labels, where the annotation covers only a subset of the interactions present in an image. For the widelyused HICO-Det dataset <ref type="bibr" target="#b2">[3]</ref>, 462 out of the 600 HOI classes have fewer than 10 training samples. For such classes, the lack of positive labels can lead to inadequate training and low classification performance. In particular, since the supervision for the layer weights is mostly 0 for rare classes, confusion occurs for these classes. How to alleviate performance degradation on rare classes is thus a key issue in HOI detection.</p><p>To address the problem of long-tailed distributions, we propose to take advantage of natural co-occurrences in human actions. In other words, given a pair of a human and an object, <ref type="figure">Fig. 1</ref>. Examples of action co-occurrence in HOI detection datasets. The marginal/conditional probability values are computed from the distribution of training labels. Intuitively, detection of rarely labeled HOIs (operate-hair dryer) can be facilitated by detection of commonly co-occurring HOIs (holdhair dryer). Also, non-detection of rare HOIs (blow-cake) can be aided by detection of incompatible HOIs (cut-cake). We leverage this intuition as a prior to learn an HOI detector effective on long-tailed datasets. multiple actions or interactions can happen at the same time. For example, the HOI of 'operate-hair dryer' is rarely labeled and consequently hard to detect in the left image of <ref type="figure">Fig. 1</ref>. However, 'operate-hair dryer' often occurs when the more commonly labeled HOI of 'hold-hair dryer' is present. As a result, detection of 'operate-hair dryer' can be facilitated by detection of 'hold-hair dryer' in an image. On the other hand, the detection of an HOI may preclude other incompatible HOIs, such as for 'cut-cake' and 'blow-cake' in the right image of <ref type="figure">Fig. 1</ref>.</p><p>In this paper, we introduce the new concept of utilizing co-occurring actions as prior knowledge, termed as action co-occurrence priors (ACPs), to train an HOI detector. In particular, we count the co-occurrences of class labels present in the training data and leverage this knowledge to effectively train our model. In contrast to language-based prior knowledge which requires external data sources <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b91">[92]</ref>, co-occurrence priors can be easily obtained from the label statistics of the target dataset. We also propose two novel ways to exploit them. First, we design a neural network with hierarchical structure where the classification is initially performed with respect to action groups. Each action group is defined by one anchor action, where the anchor actions are mutually exclusive according to the co-occurrence prior. Then, our model predicts the fine-grained HOI class within the action group. With this approach, higher performance is attained by learning dedicated classifiers for distinguishing closely-related actions within a group. Second, we present a technique that employs knowledge distillation <ref type="bibr" target="#b23">[24]</ref> to expand HOI labels so they can have more positive labels for potentially co-occurring arXiv:2109.04047v1 [cs.CV] 9 Sep 2021 actions. During training, the predictions are regularized by the refined objectives to improve robustness, especially for rare classes. To the best of our knowledge, this is the first work to leverage label co-occurrences in HOI detection to alleviate the long-tailed distribution problem.</p><p>The main contributions of this work can be summarized as: <ref type="bibr" target="#b0">(1)</ref> The novel concept of explicitly leveraging correlations among HOI labels to address the problem of long-tailed distributions in HOI detection; <ref type="bibr" target="#b1">(2)</ref> Two orthogonal ways to leverage action co-occurrence priors, namely through a proposed hierarchical architecture and HOI label expansion via knowledge distillation. The resulting model is shown to be consistently advantageous in relation to state-of-the-art techniques on both the HICO-Det <ref type="bibr" target="#b2">[3]</ref> and V-COCO <ref type="bibr" target="#b19">[20]</ref> benchmark datasets.</p><p>This work is an extension of our previous conference paper <ref type="bibr" target="#b36">[37]</ref>. The primary differences are as follows: <ref type="bibr" target="#b0">(1)</ref> We extend our architecture to incorporate a self-attention module, whose purpose is to enrich the semantic content of each human-object pair by accounting for surrounding human-object pairs. In this way, global image context is leveraged to better understand the interaction of a human and object. (2) We additionally exploit linguistic prior knowledge through a word embedding regression loss for object classes. In the word embedding, object categories that are more semantically similar are closer together. With this loss, learning for rare HOI labels can benefit from more common HOI labels that are semantically similar, further alleviating dataset bias. This extended method including both of the new components is called ACP++. We show that ACP++ outperforms our previous ACP in all the application scenarios we demonstrate. In addition, we expand our experimental results and analysis to show multiple aspects of our proposed method's algorithmic behavior.</p><p>II. RELATED WORK The HOI detection task is rooted in visual relationship detection (VRD) and scene graph generation. In this section, we summarize the recent work on HOI detection, VRD, and scene graph generation. We also review works that utilize a label hierarchy in multi-label learning. Human-Object Interaction Human-Object Interaction was originally studied in the context of recognizing the function or 'affordance' of objects <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b64">[65]</ref>. Early works focus on learning more discriminative features combined with variants of SVM classifiers <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b85">[86]</ref>, and leverage the relationship with human poses for better representation <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b87">[88]</ref> or mutual context modeling <ref type="bibr" target="#b86">[87]</ref>.</p><p>Recently, a completely data-driven approach based on convolutional neural networks (CNNs) has brought dramatic progress to HOI. Many of the pioneering works constructed large-scale image datasets <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b100">[101]</ref> to set new benchmarks in this field. Since then, significant progress has been achieved in using CNNs for this problem <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b70">[71]</ref>, <ref type="bibr" target="#b72">[73]</ref>, <ref type="bibr" target="#b75">[76]</ref>, <ref type="bibr" target="#b79">[80]</ref>, <ref type="bibr" target="#b80">[81]</ref>, <ref type="bibr" target="#b97">[98]</ref>.</p><p>Most of these works follow a two-step scheme of CNN feature extraction and multi-information fusion, where the multiple information may include human and object appearance <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b61">[62]</ref>; box relation (either box configuration or spatial map) <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b80">[81]</ref>; object category <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b58">[59]</ref>; human pose <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>; and particularly, linguistic prior knowledge <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b58">[59]</ref>. More recent works tend to combine these various cues <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b72">[73]</ref>, <ref type="bibr" target="#b79">[80]</ref>. These works differ from one another mainly in their techniques for exploiting external knowledge priors. Kato et al. <ref type="bibr" target="#b30">[31]</ref> incorporate information from WordNet <ref type="bibr" target="#b54">[55]</ref> using a Graph Convolutional Network (GCN) <ref type="bibr" target="#b38">[39]</ref> and learn to compose new HOIs. Xu et al. <ref type="bibr" target="#b80">[81]</ref> also use a GCN to model the general dependencies among actions and object categories by leveraging a VRD dataset <ref type="bibr" target="#b52">[53]</ref>. Li et al. <ref type="bibr" target="#b47">[48]</ref> utilize interactiveness knowledge learned across multiple HOI datasets. Peyre et al. <ref type="bibr" target="#b58">[59]</ref> transfer knowledge from triplets seen at training to new unseen triplets at test time by analogy reasoning.</p><p>Different from the previous works that focus on network architecture and human representation, we propose an orthogonal perspective to reformulate the target action label space and corresponding loss function by leveraging co-occurrence relationships among action classes for HOI detection. To the best of our knowledge, this is the first work that leverages the co-occurrence relationship between actions for HOI recognition. In principle, our method is complementary to all of the previous works and can be combined with any of them. We have implemented our approach on several existing HOI detection architectures <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b49">[50]</ref>, and we mainly evaluated our method with a baseline presented in <ref type="bibr" target="#b21">[22]</ref> with details described in Sec. III-C. Visual Relationship Detection and Scene Graph Generation The closest problems to HOI detection are Visual Relationship Detection (VRD) <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b84">[85]</ref>, <ref type="bibr" target="#b90">[91]</ref>, <ref type="bibr" target="#b91">[92]</ref>, <ref type="bibr" target="#b93">[94]</ref>, <ref type="bibr" target="#b94">[95]</ref>, <ref type="bibr" target="#b96">[97]</ref>, <ref type="bibr" target="#b99">[100]</ref> and scene graph generation <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b78">[79]</ref>, <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b76">[77]</ref>, <ref type="bibr" target="#b81">[82]</ref>, <ref type="bibr" target="#b83">[84]</ref>, <ref type="bibr" target="#b92">[93]</ref>, which deal with general visual relationships between two arbitrary objects. In VRD and scene graph datasets <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b52">[53]</ref>, the types of visual relationships that are modeled include verb (action), preposition, spatial and comparative phrase. Scene graph generation, VRD, and HOI detection share common challenges such as long-tailed distributions and even zero-shot problems <ref type="bibr" target="#b52">[53]</ref>. In this paper, we focus on HOI detection, as co-occurrences of humanobject interactions are often strong, but the proposed technique could be extended to model the general co-occurrences that exist in visual relationships. While several other works on different tasks explore co-occurrence or contextual relation implicitly <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b68">[69]</ref>, <ref type="bibr" target="#b71">[72]</ref>, <ref type="bibr" target="#b88">[89]</ref>, <ref type="bibr" target="#b89">[90]</ref>, we explicitly model the co-occurrence of the data so that we can more efficiently exploit co-occurrence priors. In addition, as mentioned before, our method is complementary to the existing scene graph generation methods. Label Hierarchy in Multi-label Learning The hierarchical structure of label categories has long been exploited for multi-label learning in various vision tasks, e.g., image/object classification <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b82">[83]</ref>, detection <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b53">[54]</ref>, and human pose estimation <ref type="bibr" target="#b66">[67]</ref>, <ref type="bibr" target="#b67">[68]</ref>. In contrast, label hierarchy has rarely been considered in HOI detection. Inspired by previous work <ref type="bibr" target="#b9">[10]</ref> that uses Hierarchy and Exclusion (HEX) graphs to encode flexible relations between object labels, we introduce the first method to take advantage of an action label hierarchy for HOI recognition. While label hierarchies have  <ref type="figure">Fig. 2</ref>. Examples of co-occurrence matrices constructed for several objects (bicycle, boat, dog). Along the Y-axis is the given action, and the X-axis enumerates conditional actions. Each element represents the conditional probability that an action occurs when another action is happening.</p><p>commonly been used, our method is different in that it is defined by co-occurrences (rather than privileged semantics or a taxonomy <ref type="bibr" target="#b9">[10]</ref>). This co-occurrence based hierarchy can be determined statistically, without direct human supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head><p>Our method for utilizing the co-occurrence information of HOI labels consists of three key components: (1) establishing action co-occurrence priors (Sec. III-A), (2) hierarchical learning including anchor action selection (Sec. III-B) and devising the hierarchical architecture (Sec. III-C), followed by an extension of the architecture via self-attention (Sec. III-D), and (3) ACP projection for knowledge distillation (Sec. III-E), followed by an extension of the loss function via a language prior (Sec. III-F).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Action Co-occurrence Priors</head><p>Here, we formalize the action co-occurrence priors. The priors for the actions are modeled by a co-occurrence matrix C ? R N ?N where an entry c ij in C represents the conditional probability that action j occurs when action i is happening:</p><formula xml:id="formula_0">c ij = p(j|i), i, j ? [0, N ),<label>(1)</label></formula><p>where N (117 for the HICO-Det <ref type="bibr" target="#b2">[3]</ref> dataset) denotes the total number of action classes and i, j are indices of two actions. By definition, the diagonal entries are one. C is constructed from the target HOI detection dataset by counting the imagelevel statistics of its training labels. Examples of co-occurrence matrices constructed for a single object are visualized in <ref type="figure">Fig. 2</ref>. Meanwhile, we also consider the complementary event of action i (i.e., where the i-th action does not occur) and denote it as i , such that p(i ) + p(i) = 1. The complementary action co-occurrence matrix C ? R N ?N can thus be defined by entries c ij in C that represent the conditional probability that an action j occurs when another action i does not occur:</p><formula xml:id="formula_1">c ij = p(j|i ), i, j ? [0, N ).<label>(2)</label></formula><p>In this matrix, the diagonal entries are zero. It can be seen from <ref type="figure">Fig. 2</ref> that different types of relationships can exist between actions. They can be divided into three forms. The first is the prerequisite relationship, where the given action is highly likely to co-occur with the conditional action. For example, the HOI 'sit on-bicycle' is a prerequisite of the HOI 'ride-bicycle'. In this case, p(sit on-bicycle|ride-bicycle) is close to 1. Next is exclusion, where the given action is highly unlikely to co-occur with the conditional action. An example is that the HOI 'washbicycle' and HOI 'ride-bicycle' are unlikely to happen together. As a result, p(wash-bicycle|ride bicycle) is close to 0. Finally, we have overlapping, where the given action and conditional action may possibly co-occur, for example HOI 'hold-bicycle' and HOI 'inspect-bicycle', such that p(hold-bicycle|inspect-bicycle) is in between 0 and 1. We introduce two ways to exploit the co-occurrence matrix by regarding exclusion (action space partitioning) and prerequisite actions (knowledge distillation). Detailed descriptions of the proposed methods are presented in the following sections.</p><p>The strong relationships that may exist between action labels can provide strong priors on the presence or absence of an HOI in an image. In contrast to previous works where models may implicitly learn label co-occurrence via relational architectures <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b95">[96]</ref>, we explicitly exploit these relationships between action labels as priors, to effectively train our model especially for rare HOIs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Anchor Action Selection via Non-Exclusive Suppression</head><p>From a co-occurrence matrix for an object, one can see that some actions are close in semantics or commonly cooccur while others are not. Intuitively, visually similar actions (e.g., 'ride-horse' and 'hop on-horse') tend to be harder to distinguish from each other, but these actions should not occur at the same time. If the positive labels for these actions are rare, then they become even more difficult to distinguish. Such cases require fine-grained recognition <ref type="bibr" target="#b10">[11]</ref> and demand more dedicated classifiers. This motivates us to learn HOI classes in a coarse-to-fine manner. In particular, as a pre-processing step, we first collect a set of mutually exclusive action classes, called anchor actions, which tend to be distinguishable from one another. The anchor actions are used to partition the entire action label space into fine-grained sub-spaces. The other action classes are attributed to one or more sub-spaces and recognized in the context of a specific anchor action. In summary, unlike previous HOI detection works which predict action probabilities independently of one another, we divide the whole action label set into two sets, one for anchor actions and one for regular actions, which are modeled in different ways as explained in detail in Sec. III-C.</p><p>Before training, in selecting anchor actions, we seek a set of action classes that are exclusive of one another. To this end, we define the exclusiveness of an action class as counting the number of actions that never occur if action i is happening:</p><formula xml:id="formula_2">e i = j (1 if (c ij = 0), else 0).<label>(3)</label></formula><p>e i will have a high value if few other actions can occur when i does. Based on exclusiveness values, the anchor action label set D is generated through non-exclusive suppression (NES) as described in Alg. 1. It iteratively finds the most exclusive action class as an anchor action and removes remaining action classes that are not exclusive to the selected anchor actions. The anchors in the list are action classes that never occur together in the training labels. For example, if an action such as 'toast' is on the anchor list, then actions like 'stand' and 'sit' cannot be on the list because they may co-occur with 'toast', while actions such as 'hunt' or 'hop on' can potentially be on the list. While there may exist other ways the anchor action selection could be done, we empirically found this approach to be simple, effective in terms of detection accuracy, and computationally efficient (less than 0.01 second).</p><p>Algorithm 1: Non-Exclusive Suppression (NES) algorithm for mutually exclusive anchor action selection.</p><formula xml:id="formula_3">Input: E = {e i , i ? [0, N )}, C = {c ij , i, j ? [0, N )}; Output: D begin D ? {} ; while E is not empty do # Find the most exclusive action; m ? argmaxE; D ? D ? {m} ; for e k ? E do # Remove the actions correlated (not exclusive) to m; if c mk &gt; 0 then E ? E ? e k ; C ? C ? {c ij , i or j = k} end end end end</formula><p>The anchor action label set acts as a finite partition of the action label space (a set of pairwise disjoint events whose union is the entire action label space). To form a complete action label space, we add an 'other' anchor action, denoted as O, for when an action class does not belong to D. Finally, we have |D| + 1 anchor actions including D and the 'other' action class O.</p><p>There are several benefits to having this anchor action label set. First, only one anchor action can happen at one time between a given human and object. Thus, we can use the relative (one-hot) probability representation with softmax activation instead of a sigmoid, where softmax features were shown to compare well against distance metric learningbased features <ref type="bibr" target="#b25">[26]</ref>. Second, anchor actions tend to be easier to distinguish from one another since they generally have prominent differences in an image. Third, it decomposes the action label space into several sub-spaces, which facilitates a coarse-to-fine solution. Each sub-task will have a much smaller solution space, which can improve learning. Finally, each subtask will use a standalone sub-network which focuses on image features specific to the sub-task, which is an effective strategy for fine-grained recognition <ref type="bibr" target="#b10">[11]</ref>.</p><p>After selecting anchor actions, the entire action label set A is divided into the anchor action label set D and the remaining set of 'regular' action classes R, so that A = {D, R}. Each of the regular action classes is then associated with one or more anchor actions to form |D| + 1 action groups G = {G i ; i ? D ? O}, one for each anchor action. A regular action class j ? R will be assigned to the group associated with anchor action i (G i ) if action j is able to co-occur with the anchor action i,</p><formula xml:id="formula_4">j ? G i , if c ij &gt; 0 (i ? D ? O, j ? R).<label>(4)</label></formula><p>Note that the anchor actions themselves are not included in the action groups and a regular action j can be assigned to multiple action groups since it may co-occur with multiple anchor actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Hierarchical Architecture</head><p>We implement our hierarchical approach upon several existing HOI detection architectures <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b49">[50]</ref>. In this section, we introduce our main baseline architecture, built on the 'No-Frills' (NFs) baseline presented in <ref type="bibr" target="#b21">[22]</ref> on account of its simplicity, effectiveness, and code availability <ref type="bibr" target="#b20">[21]</ref>. Here, we give a brief review of the NFs architecture. Baseline Network NFs follows the common scheme of CNN feature extraction followed by multi-information fusion. The feature extraction uses an off-the-shelf Faster R-CNN <ref type="bibr" target="#b62">[63]</ref> object detector with ResNet152 <ref type="bibr" target="#b22">[23]</ref> backbone network to detect human and object bounding boxes. From the final fully connected (FC) layer of the detector, the featuresx = {x h ,x o } are extracted, wherex h andx o denote human and object appearance, respectively. These CNN features, together with the human pose (k), object category (?) and bounding boxes (b including both object and human bounding boxes) compose the multiple information as X = {x,k,?,b}. As illustrated in <ref type="figure">Fig. 3</ref>, they are fed to our target model via corresponding network streams: human appearance (f h ), object appearance(f o ), bounding box and object category (f b ), and human pose and object category (f k ). Each individual type of information is first fed through a separate network of two FC layers to generate a fixed dimension (number of actions N ) feature. Then, all the features are added together and sent through a sigmoid activation to obtain the probability prediction for the action a:?   <ref type="figure">Fig. 3</ref>. Illustration of our overall network architecture. Our work differs from the baseline <ref type="bibr" target="#b21">[22]</ref> by the addition of a hierarchical action prediction module.</p><formula xml:id="formula_5">= sigmoid(F (X)) ? R N ,<label>(5)</label></formula><formula xml:id="formula_6">(512 ? ( + 1), softmax) 1 (?) (512 ? ( ? ) , sigmoid) 2 (?) (512 ? ( ? ) , sigmoid) +1 (?) (512 ? ( ? ) ,</formula><p>For our hierarchical architecture, anchor action probabilities are directly generated by a softmax sub-network. Regular action probabilities are generated by a matrix multiplication of the anchor probability and the output from a few sigmoid based conditional sub-networks.</p><p>where?(a) = p(a|X) represents the probability prediction for action class a. The multi-information fusion procedure F (X) is expressed as</p><formula xml:id="formula_7">F (X) = f h (x h ) + f o (x o ) + f k (k||?) + f b (b||?),<label>(6)</label></formula><p>where || denotes concatenation. The baselines built on other models <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b49">[50]</ref> are constructed in a similar manner.</p><p>To eliminate training-inference mismatch, NFs directly optimizes the HOI class probabilities instead of separating the detection and interaction losses as done in <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b15">[16]</ref>. The final HOI prediction is a joint probability distribution over M number of HOI classes (600 for HICO-Det dataset) computed from the probabilities?,?, and? for human, object, and action, respectively:</p><formula xml:id="formula_8">Y = joint(?,?,?) ? R M .<label>(7)</label></formula><p>Specifically, for an HOI class (h, o, a),</p><formula xml:id="formula_9">Y (h, o, a) =?(h) * ?(o) * ?(a) = p(h|I) * p(o|I) * p(a|X),<label>(8)</label></formula><p>where?(h) = p(h|I) and?(o) = p(o|I) are the probability of a candidate box pair being a human h and object o, provided by the object detector <ref type="bibr" target="#b62">[63]</ref>. Finally, the binary cross-entropy loss L(? , Y gt ) is directly computed from the HOI prediction. This 'No-Frills' baseline network is referred to as Baseline.</p><p>Modified Baseline Network For a stronger baseline comparison, we make two simple but very effective modifications on the baseline network. (1) Replace the one-hot representation with the Glove word2vec <ref type="bibr" target="#b57">[58]</ref> representation for the object category (? ).</p><p>(2) Instead of directly adding up the multiple information, we average them and forward this through another action prediction module to obtain the final action probability prediction. As a naive implementation of the Modified Baseline, we simply use a sub-network f sub of a few FC layers as the action prediction module. Then Eq. (5)-6 are modified t?</p><formula xml:id="formula_10">A = sigmoid(f sub (F (X))),<label>(9)</label></formula><formula xml:id="formula_11">F (X) = f h (x h ) + f o (x o ) + f k (k||?) + f b (b||?) n stream .<label>(10)</label></formula><p>In this case, n stream = 4.</p><p>Our hierarchical architecture further modifies the action prediction module by explicitly exploiting ACP information as described in the next paragraph. Proposed Hierarchical Architecture Now we introduce the action prediction module for our hierarchical architecture (illustrated in <ref type="figure">Fig. 3</ref>) that better exploits the inherent cooccurrence among actions. While the baseline network predicts all the action probabilities directly from F (?) with a single feed-forward sub-network f sub , we instead use |D| + 2 subnetworks where one (f anchor (?)) is first applied to predict the anchor action set and then one of the |D| + 1 other subnetworks (f Gi (?)) which corresponds to the predicted anchor action is used to estimate the specific action within the action group. Because of the mutually exclusive property of anchor actions, we use the softmax activation for anchor action predictions, while employing the sigmoid activation for regular action prediction conditioned on the action group:</p><formula xml:id="formula_12">A anchor = sof tmax(f anchor (F (X))) ? R |D|+1<label>(11)</label></formula><formula xml:id="formula_13">A Gi = sigmoid(f Gi (F (X))) ? R N ?|D| , where i ? D ? O,<label>(12)</label></formula><p>where? anchor (i) is directly used as the final probability predictions for the anchor actions:</p><formula xml:id="formula_14">p(i|X) =? anchor (i), i ? D.<label>(13)</label></formula><p>We let? Gi (j) represent the learned conditional probability that action j occurs when action i is happening, namely,</p><formula xml:id="formula_15">A Gi (j) = p(j|i, X).<label>(14)</label></formula><p>Since the anchor action set is a finite partition of the entire action label space, the probability of a regular action j is predicted according to the law of total probability:</p><formula xml:id="formula_16">A regular (j) = p(j|X) = i?D?O p(i|X) * p(j|i, X) = i?D?O? anchor (i) * ? Gi (j),<label>(15)</label></formula><p>where j ? R. Thus, instead of Eq. <ref type="formula" target="#formula_10">(9)</ref>, we obtain the final action probability predictions for our hierarchical architectur? A(a) = p(a|X) a?</p><formula xml:id="formula_17">A(a) = ? anchor (a), if a ? D i?D?O? anchor (i) * ? Gi (a), otherwise.<label>(16)</label></formula><p>We use the same method as in Eq. (8) and a cross-entropy loss L to compute the final HOI probability prediction? .</p><p>To demonstrate the effectiveness of the hierarchical learning, we introduce two other baselines, MultiTask and TwoStream, that lie between the Modified Baseline and our hierarchical learning. MultiTask only uses the anchor action classification as an additional multi-task element to the Modified Baseline. TwoStream separately predicts the anchor and the regular actions but without using the hierarchical modeling between anchor and regular actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Self-Attention Module Extension</head><p>In order to enhance the capability for holistic relational understanding across objects or humans, we additionally leverage a self-attention module that applies the non-local layer <ref type="bibr" target="#b77">[78]</ref> to each human-object pair. In particular, let Z ? R B?512 denote a stack of B merged human-object features from our F (?) (Z = F (X)). Then, we compute the relational association matrix by:</p><formula xml:id="formula_18">R = Softmax(?(ZW a )?(ZW b ) ) ? R B?B ,<label>(17)</label></formula><p>where ?(?) denotes ReLU and W a , W b ? R 512?128 are learnable weights that map the human-object features Z to their own roles, (e.g., key and query) and the softmax operation is applied row-wise. Then, the relational feature matrix is computed by:</p><formula xml:id="formula_19">A = R?(ZW x )W z ? R B?512 ,<label>(18)</label></formula><p>where W x ? R 512?128 and W z ? R 512?128 are again learnable weights. The matrix A encodes aggregated features across all the objects according to the degree of relational association given in R. This relational feature matrix is combined with the original feature Z byZ = Z + A, so that Z is enhanced by holistic relational information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. ACP Projection for Knowledge Distillation</head><p>Knowledge distillation <ref type="bibr" target="#b23">[24]</ref> was originally proposed to transfer knowledge from a large network to a smaller one. Recently, knowledge distillation has been utilized for various purposes such as lifelong learning <ref type="bibr" target="#b48">[49]</ref>, multi-task learning <ref type="bibr" target="#b34">[35]</ref> or modality transfer <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b24">[25]</ref>. Hu et al. <ref type="bibr" target="#b27">[28]</ref> extended this concept to distill prior knowledge in the form of logic rules into a deep neural network. Specifically, they propose a teacher-student framework to project the network prediction (student) to a rule-regularized subspace (teacher), where the process is termed distillation. The network is then updated to balance between emulating the teacher's output and predicting the true labels.</p><p>Our work fits this setting as the ACPs can act as a prior to distill. We first introduce ACP Projection to map the action distribution to the ACP constraints. Then, we use the teacherstudent framework <ref type="bibr" target="#b27">[28]</ref> to distill knowledge from ACPs. ACP Projection In ACP Projection, an arbitrary action distribution A = {p(i), i ? [0, N )} ? R N is projected into the ACP-constrained probability space:</p><formula xml:id="formula_20">A * = project(A, C, C ) ? R N ,<label>(19)</label></formula><p>where A * is the projected action prediction. The projected probability for the j-th action A * (j) = p(j * ) is generated using the law of total probability:</p><formula xml:id="formula_21">p(j * ) = 1 N N i=1 (p(i) * p(j|i) + p(i ) * p(j|i )) = 1 N ( N i=1 p(i) * c ij + N i=1 (1 ? p(i)) * c ij ).<label>(20)</label></formula><p>In matrix form, the ACP projection is expressed as</p><formula xml:id="formula_22">project(A, C, C ) = AC + (1 ? A)C N .<label>(21)</label></formula><p>In practice, we use the object-based action co-occurrence matrices C o ? R N ?N and C o ? R N ?N which only count actions related to a specific object o. <ref type="figure">Fig. 2</ref> shows examples of C o with respect to object classes. Also, we give different weights ? and ? as hyper-parameters to the action cooccurrence matrix C o and its complementary matrix C o , with the weights subject to ? + ? = 2, ? &gt; ?. The projection function is then modified to</p><formula xml:id="formula_23">project(A, C o , C o ) = ?AC o + ?(1 ? A)C o N .<label>(22)</label></formula><p>This is done because we empirically found the co-occurrence relationships in C o to generally be much stronger than the complementary actions in C o . Teacher-Student Framework Now we can distill knowledge from the ACPs using ACP Projection in both the training and inference phases. There are three ways ACP Projection can be used: (1) Directly project the action prediction? into the ACP-constrained probability space at the testing phase to obtain the final action output (denoted as PostProcess).</p><p>(2) Project the action prediction? in the training phase and use the projected action as an additional learning target <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b91">[92]</ref>. (3) Project the ground truth label H gt , O gt , and A gt1 to the ACP space in the training phase and use the projected action project(A gt , C O gt , C O gt ) as an additional learning target. The second and third items are incorporated into the teacher-student framework as terms in a new objective function (denoted as Distillation):</p><formula xml:id="formula_24">L distill = ? 1 L(? , Y gt ) + ? 2 L(? ,? projO ) + ? 3 L(? , Y gt projO ),<label>(23)</label></formula><p>wher?</p><formula xml:id="formula_25">Y projO = joint(?,?, project(?, C?, C ? )) ? R M , (24) Y gt projO = joint(H gt , O gt , project(A gt , C O gt , C O gt )) ? R M .<label>(25)</label></formula><p>? 1 , ? 2 , ? 3 are balancing weights among the ground truth HOI term and the teacher objectives. The object type can be easily determined from the object probability predictions? or the ground truth label O gt . With the new objective function, rare classes receive different supervisions, so confusion between them can be alleviated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Word Embedding Loss Extension</head><p>As an extension to enhance performance, we add another loss term, namely a word embedding regression loss. Given a word2vec representation for the object category?, we regress our model to the same object representation in the output by computing the regressed word embedding v. Similar to Peyre et al. <ref type="bibr" target="#b58">[59]</ref>, we design the word embedding regularization loss as the sigmoid loss function:</p><formula xml:id="formula_26">L emb = |O| i=1 1 (y o =i) log( 1 1 + e ???v ),<label>(26)</label></formula><p>where |O| is the number of objects (80 for HICO-Det datasets) and y o ? R |O| is the class for the object. Multi-task learning with the object prediction works as prior knowledge for action prediction, which leads to performance improvements. Therefore, the total loss function can be described as a weighted sum of the knowledge distillation loss and the embedding loss:</p><formula xml:id="formula_27">L total = L distill + ? 0 L emb ,<label>(27)</label></formula><p>where ? 0 is the additional balancing weight between Eq. (23) and Eq. <ref type="bibr" target="#b25">(26)</ref>. Note that, as we have an object word2vec vector as input, this word embedding regression loss can be seen as an auto-encoding loss which is commonly used as a regularizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>The goal of the experiments is to show the effectiveness and generalizability of our method. In particular, we show that our method can consistently alleviate the long-tailed distribution problem in various setups by improving performance, especially for rare HOI classes. In this section, we describe the experimental setups and competing methods, and provide extensive performance evaluations and analysis of HOI detection with both quantitative and qualitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets and Metrics</head><p>We evaluate the performance of our model on two popular HOI detection benchmark datasets, HICO-Det <ref type="bibr" target="#b2">[3]</ref> and V-COCO <ref type="bibr" target="#b19">[20]</ref>. HICO-Det <ref type="bibr" target="#b2">[3]</ref> extends the HICO (Humans Interacting with Common Objects) dataset <ref type="bibr" target="#b3">[4]</ref> which contains 600 human-object interaction categories for 80 objects. Different from the HICO dataset, HICO-Det additionally contains bounding box annotations for humans and objects of each HOI category. The vocabulary of objects matches the 80 categories of MS COCO <ref type="bibr" target="#b50">[51]</ref>, and there are 117 different verb (action) categories. The number of all possible triplets is 117 ? 80, but the dataset contains positive examples for only 600 triplets. The training set of HICO-Det contains 38,118 images and 117,871 HOI annotations for 600 HOI classes. The test set has 9,658 images and 33,405 HOI instances. For evaluation, HICO-Det uses the mean average precision (mAP) metric. Here, an HOI detection is counted as a true positive if the minimum of the human overlap IOU and object overlap IOU with the ground truth is greater than 0.5. Following <ref type="bibr" target="#b2">[3]</ref>, HOI detection performance is reported for three different HOI category sets: (1) all 600 HOI categories (Full), (2) 138 categories with fewer than 10 training samples (Rare), and (3) the remaining 462 categories with more than 10 training samples (Non-rare). Two evaluation settings are examined: 'Default' and 'Known Object'. For the 'Known Object' setting, we evaluate the detection only on the images containing the target object category (e.g., "bike") given the HOI class (e.g., "riding-bike").</p><p>V-COCO (Verbs in COCO) is a subset of MS-COCO <ref type="bibr" target="#b50">[51]</ref>, which consists of 10,346 images (2,533, 2,867, 4,946 for training, validation and test, respectively) and 16,199 human instances. Each person is annotated with binary labels for 26 action classes. For the evaluation metric, we use the AP score as done for the evaluation on HICO-Det.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Quantitative Results</head><p>Ablation study In the ablations, the 'No-Frills' baseline network <ref type="bibr" target="#b21">[22]</ref> we used is denoted as Baseline. We first evaluate the effectiveness of the core design components in the proposed method, including <ref type="formula" target="#formula_0">(1)</ref>     <ref type="table" target="#tab_2">TABLE IV  RESULTS ON THE V-COCO DATASET. FOR OUR   METHOD, WE SHOW RESULTS BOTH FOR   CONSTRUCTING ACP FROM V-COCO AND FOR  USING ACP CONSTRUCTED FROM HICO-DET  INSTEAD. BOTH OF THESE MODELS SHOW   FAVORABLE PERFORMANCE AGAINST THE  CURRENT STATE-OF-THE-ART MODELS.</ref> AP role Gupta et al. <ref type="bibr" target="#b19">[20]</ref> impl. by <ref type="bibr" target="#b15">[16]</ref> 31.8 InteractNet <ref type="bibr" target="#b15">[16]</ref> 40.0 GPNN <ref type="bibr" target="#b61">[62]</ref> 44.0 iCAN <ref type="bibr" target="#b13">[14]</ref> 45.3 iHOI <ref type="bibr" target="#b79">[80]</ref> 45.79 with Knowledge <ref type="bibr" target="#b80">[81]</ref> 45.9 Interactiveness Prior <ref type="bibr" target="#b47">[48]</ref> 48.7 Contextual Attention <ref type="bibr" target="#b74">[75]</ref> 47.3 RPNN <ref type="bibr" target="#b98">[99]</ref> 47.53 PMFNet <ref type="bibr" target="#b72">[73]</ref> 52.0 VCL <ref type="bibr" target="#b26">[27]</ref> 48.3 DRG <ref type="bibr" target="#b12">[13]</ref> 51.0 VSGNet <ref type="bibr" target="#b70">[71]</ref> 51.76 Interaction Points <ref type="bibr" target="#b75">[76]</ref> 52.3 PD-Net <ref type="bibr" target="#b97">[98]</ref> 52.  <ref type="table" target="#tab_2">Table I</ref> gives a comprehensive evaluation for each component. We draw conclusions from it one-by-one. First, our baseline network is strong. Our Modified Baseline achieves 19.09 mAP and surpasses the 'No-Frills' Baseline by 1.51 mAP (a relative 8.7% improvement). This is already competitive to the state-of-the-art result <ref type="bibr" target="#b58">[59]</ref> and serves as a strong baseline.</p><p>Second, both hierarchical learning and knowledge distillation are effective. This is concluded by adding Hierarchical and Distillation to the Modified Baseline, respectively. Specifically, +Hierarchical improves the modified baseline by 0.94 mAP (a relative 4.9% improvement), and +Distillation (training with Eq. (27)) improves the modified baseline by 0.89 mAP (a relative 4.7% improvement). Including both obtains 1.16 mAP improvement (relatively better by 6.1%).</p><p>Third, the proposed ACP method achieves a new stateof-the-art. Our final result is generated by further using the PostProcess step (introduced in Sec. III-E) that projects the final action prediction into the ACP constrained space. Our method achieves 20.59 mAP (relative 7.9% improvement) for Full HOI categories, 15.92 mAP (relative 21.6% improvement) for Rare HOI categories, and 21.98 mAP (relative 5.2% improvement) for Non-rare HOI categories. Note that our method made especially significant improvements for Rare classes, which supports the claim that the proposed method can alleviate the long-tailed distribution problem of HOI detection datasets. This result set the new state-of-the-art at the time of submission for our earlier version of this work <ref type="bibr" target="#b36">[37]</ref> on both the HICO-Det and V-COCO datasets as shown in <ref type="table" target="#tab_2">Table III  and Table IV</ref>.</p><p>Fourth, the extended ACP++ outperforms our previous ACP. The word embedding loss (L emb ) improves the performance of our previous ACP model in all the metrics, especially on rare classes, by alleviating the bias in the dataset. In addition, self-attention module (SA) enhances the representation power of the model, thus it elevates the overall mAP by improving the performance on non-rare classes. However, SA reduces the performance on rare classes. We conjecture that the model overfits on the major classes due to the additional parameters introduced by the SA module and degrades the generalization ability on rare classes. Combining both of the technical extensions upon our previous ACP model gives the final extended ACP++ result. Without PostProcess, our extended method improves upon our original ACP method by 20.96 mAP (relative 3.5% improvement), and our extended method improves upon our original ACP method by 21.27 mAP (relative 3.3% improvement) with PostProcess.</p><p>In addition, the MultiTask, TwoStream, and our Hierarchical architecture are compared in <ref type="table" target="#tab_2">Table II</ref>. From MultiTask, it can be seen that the softmax based anchor action classification already brings benefits to the Modified Baseline when used only in a multi-task learning manner. From TwoStream, separately modeling the anchor and the regular classes leads to slightly more improvement compared to MultiTask. Moreover, our Hierarchical architecture improves upon TwoStream by explicitly modeling the hierarchy between anchor and regular action predictions.</p><p>Comparison with the state-of-the-art We compare our method with the previous state-of-the-art techniques in Table III. The first and second column sections in <ref type="table" target="#tab_2">Table III are for</ref>   <ref type="bibr" target="#b2">[3]</ref>, the baseline models that we trained or modified from <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b49">[50]</ref>, and the current published state-of-the-art methods. As shown in <ref type="table" target="#tab_2">Table III</ref>, our ACP model (Ours) shows consistent improvements over our baseline models on all metrics, with especially significant gains over the 'No-Frills' baseline, and shows favorable performance against the current state-of-the-art models in terms of all the metrics. In particular, our model (No-Frills + ACP++) surpasses the state-of-the-art model at the time of submission [59] by 1.87 mAP.We also added the result of applying ACP++ priors to more recent HOI detection methods <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b49">[50]</ref>, and this also achieves consistent performance improvements for all the metrics. Note that 'PPDM + ACP++' model outperforms the state-of-the-art model without elaborate fine-tuning <ref type="bibr" target="#b46">[47]</ref>.</p><p>Results on V-COCO dataset To show the generalization ability of our method, we also evaluate it on the V-COCO dataset. Note that the exact same method is directly applied to both HICO-Det and V-COCO, including the co-occurrence matrix, anchor action selection, and architecture design. We also constructed a co-occurrence matrix from V-COCO, but the matrix was sparse. Thus, to better take advantage of our idea, we instead use the co-occurrence matrix collected from the HICO-Det dataset to train on V-COCO. <ref type="table" target="#tab_2">Table IV</ref> shows the performance of our model implemented upon the 'No-Frills' model (Ours, HICO-Det) compared to the recent stateof-the-art HOI detectors on the V-COCO dataset. In addition, we show results of our model with the co-occurrence matrix constructed from the V-COCO dataset (Ours, V-COCO). Both of these models show favorable performance on the V-COCO dataset against the existing state-of-the-art models <ref type="bibr" target="#b51">[52]</ref>. Experiments with Functional Generalization <ref type="bibr" target="#b0">[1]</ref> We also examine our approach together with an HOI detection method called 'Functional Generalization' <ref type="bibr" target="#b0">[1]</ref>. Since its paper does not provide the details for its elaborate fine-tuning (which led to improvements from 14.35 mAP to 21.96 mAP), we consider it to be unsuitable for comparison in <ref type="table" target="#tab_2">Table III</ref>. However, we found our method to be complementary to its untuned version. As shown in <ref type="table" target="#tab_6">Table V</ref>, according to our implementation, the functional generalization approach improves upon its baseline by 0.59 mAP (a relative 4.51% improvement), while adding our ACP method to the functional generalization model improves it by 1.70 mAP (a relative 12.43% improvement). This indicates that our ACP method can complement other methods such as Functional Generalization. In addition, the results suggest that the ACP method can achieve larger performance gains than Functional Generalization. Finally, note that the Functional Generalization method requires word vectors pretrained on an external dataset, whereas our ACP prior can be constructed from only the labels of our target dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results of the zero-shot setup on the HICO-Det dataset</head><p>The zero-shot setting on the HICO-Det dataset was defined by Peyre et al. <ref type="bibr" target="#b58">[59]</ref>. Specifically, we select a set of 25 HOI classes that we treat as unseen classes and exclude them and their labels in the training phase. However, we still let the model predict those 25 unseen classes in the test phase, which is known as the zero-shot problem. These HOI classes are randomly selected from among the set of non-rare HOI classes. Since Peyre et al. did not provide which specific HOI classes they selected, we select the unseen HOI classes such that the performance (mAP) for these classes in our Modified Baseline model (introduced in Sec. III-C) is similar to the corresponding Supervised baseline in <ref type="bibr" target="#b58">[59]</ref>. In (denoted as aggregation) requires large-scale linguistic knowledge to train a word representation, whereas our model only requires the co-occurrence information of the labels in the dataset, which is much easier to obtain. We conclude that the proposed method is effective for the zero-shot problem while being easy to implement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Qualitative Results and Analysis</head><p>Qualitative results In addition, <ref type="figure">Fig. 4</ref> shows examples of HOI detection results that our model predicts correctly with high probability. We show each image with the predicted HOI class  <ref type="figure">Fig. 5</ref>. The HOI probability before and after applying the projection function project(?) on our model's HOI prediction (PostProcess). Note that PostProcess can be done without any optimization. followed by the probability computed by our model. Also, we show the HOI probability change from before to after applying the projection function project(?) on our model's HOI prediction (i.e., the effect of PostProcess introduced in Sec. III-E) in <ref type="figure">Fig. 5</ref>. Leveraging co-occurrence matrix C not only increases the score for true classes (top) but also reduces the score for false classes (bottom). Note that this change can be achieved without any optimization process. Moreover, we show changes in HOI probability from before to after applying the self-attention in our model in <ref type="figure">Fig. 6</ref>. It is found that our self-attention module mostly reduces the probability for rare classes (even for true classes), which might be the reason for the mAP degradation on rare classes shown in <ref type="table" target="#tab_2">Table I</ref>. Number of anchor actions In addition, we investigate the effect of using different numbers of anchor actions |D| in <ref type="figure" target="#fig_2">Fig. 7</ref>. We measure the relative performance improvement from the +Hierarchical model to the Modified Baseline model by changing the number of anchor actions at intervals of five.</p><p>In principle, the more anchor actions we use, the better performance that can be attained. On one hand, the selected anchor actions can be more distinguishable from one another with more anchor action categories and training samples. On the other hand, the remaining regular actions can also benefit  <ref type="figure">Fig. 6</ref>. The HOI probability before and after applying the self-attention in our model for non-rare (top row) and rare (bottom row) HOI classes. For non-rare classes, our self-attention increases the score for true classes ('hold-bicycle' and 'wear-backpack') and reduces the score for false classes ('herd-sheep'). On the other hand, for rare classes, the self-attention mostly reduces the scores even for true classes (e.g., 'wash-bowl' and 'wash-broccoli'). from stronger co-occurrence priors. However, more anchor action will also result in more sub-networks to optimize, and this will cause over-fitting to a certain extent. Through observations, we found that an increase in the number of parameters of the HOI detector often causes a severe performance decrease. Thus, there is a trade-off between a large and small number of anchors, which requires us to empirically select the best anchor number. As depicted in <ref type="figure" target="#fig_2">Fig. 7</ref>, the hierarchical architecture shows the best overall mAP score (Full) with 15 anchors and the best mAP score on rare classes with 10 anchors. We finally use an experimentally overall bestperforming choice of 15 anchor actions (maximum anchor action number is 54). Performance on various sets with different number of training samples In <ref type="figure">Fig. 8</ref>, we show the relative mAP score improvements of our model compared to the baseline model by computing mAP on various sets of HOI classes that have different number of training samples. Our method shows positive performance improvements for all numbers of training samples. Also, there is a trend that HOI classes with a small number of training samples mostly show larger performance improvements. In particular, for HOI classes with the number of training samples between 0 and 9, our model achieves 38.24% improvement compared to the baseline model. These results indicate that the proposed method is able to improve the performance of an HOI detector, especially for classes with few training samples. Analysis of false predictions <ref type="figure">Fig. 9</ref> shows examples of false predictions by our model. We found three common reasons for predictions being evaluated as false. First is the wrong prediction of object label from the object detector (e.g., 'couch' as 'chair' or 'sheep' as 'cow') which is shown in the left column of <ref type="figure">Fig. 9</ref>. Second, the prediction is correct but the ground truth label for the prediction in a test image is missing (e.g., 'ridebus' or 'hold-horse') which is shown in the middle column of <ref type="figure">Fig. 9</ref>. Last, an HOI detector could have been predicted correctly if there were a sophisticated way to take context (the third object or the background) into account (e.g., the background for predicting 'repair-bicycle' or an object that person carries for predicting 'load-bus') which is shown in the right column of <ref type="figure">Fig. 9</ref>. The last issue could be solved by devising a better network architecture for effectively encoding context, which is a direction orthogonal to our work. All of these issues (the errors in the object detector, missing labels in datasets, and encoding context) are fundamental issues in HOI detection, which can be interesting topics for future work. Relative mAP Improvement (%) <ref type="figure">Fig. 8</ref>. The relative mAP score improvements for various HOI sets with different numbers of training samples. Our method is able to improve the performance especially when the number of training samples is small (38.24% improvement for 0-9 samples).  <ref type="figure">Fig. 9</ref>. Examples of the false predictions from our model. The false cases include wrong prediction of object label from the object detector (left column), correct prediction but missing ground truth labels (middle column), and predictions that could have been correct if the context were taken into account (right column).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We introduced a novel method to effectively train an HOI detector by leveraging prior knowledge on action co-occurrences in two different ways, via the architecture and via the loss function. Our proposed method consistently achieves favorable performance compared to the current state-of-the-art methods in various setups. Co-occurrence information not only is helpful for alleviating the long-tailed distribution problem but also can be easily obtained. Given the co-occurrence action/interaction priors, one open question is how to expand the co-occurrence priors to a larger vocabulary and more general domains. Therefore, a possible direction for future work would be obtaining more general co-occurrence priors by leveraging external knowledge from the web. Another direction for future work is to construct and utilize co-occurrence priors for other relationship-based vision tasks <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b52">[53]</ref> or other problems related to the dataset bias <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b69">[70]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>our simple modification to Baseline in Sec. III-C, denoted as Modified Baseline; (2) the hierarchical learning technique introduced in Sec. III-C, denoted as Hierarchical; and (3) the knowledge distillation technique presented in Eq. (27) of Sec. III-E, denoted as Distillation. Moreover, we show additional baselines which include either the selfattention module from Sec. III-D (denoted as SA) or the word embedding loss function from Sec. III-F (denoted as L emb ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 7 .</head><label>7</label><figDesc>Performance of the hierarchical architecture with different numbers of anchors at intervals of five. The models with 15 and 10 anchors show the best performance overall and on rare classes, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I ABLATION</head><label>I</label><figDesc>STUDY ON THE HICO-DET DATASET. OUR FINAL MODEL (ACP++) THAT INCLUDES BOTH HIERARCHICAL ARCHITECTURE AND DISTILLATION FOLLOWED BY POST PROCESSING, ALONG WITH THE TWO TECHNICAL EXTENSIONS OF WORD EMBEDDING LOSS AND SELF-ATTENTION, SHOWS THE BEST PERFORMANCE AMONG THE BASELINES.</figDesc><table><row><cell></cell><cell>Full</cell><cell>Rare</cell><cell>Non-rare</cell></row><row><cell>Baseline</cell><cell cols="2">17.56 13.23</cell><cell>18.85</cell></row><row><cell>Modified Baseline</cell><cell cols="2">19.09 13.09</cell><cell>20.89</cell></row><row><cell>+Hierarchical only</cell><cell cols="2">20.03 14.52</cell><cell>21.67</cell></row><row><cell>+Distillation only</cell><cell cols="2">19.98 13.67</cell><cell>21.86</cell></row><row><cell>+Hierarchical+Distillation</cell><cell cols="2">20.25 15.33</cell><cell>21.72</cell></row><row><cell>+Hierarchical+Distillation+Post (Ours, ACP)</cell><cell cols="2">20.59 15.92</cell><cell>21.98</cell></row><row><cell>+Hierarchical+Distillation+L emb</cell><cell>20.76</cell><cell>16.35</cell><cell>22.08</cell></row><row><cell>+Hierarchical+Distillation+SA</cell><cell cols="2">20.89 14.43</cell><cell>22.81</cell></row><row><cell>+Hierarchical+Distillation+L emb +SA (ACP++)</cell><cell cols="2">20.96 15.12</cell><cell>22.70</cell></row><row><cell cols="2">+Hierarchical+Distillation+L emb +SA (ACP++) +Post 21.27</cell><cell>15.41</cell><cell>23.02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II PERFORMANCE</head><label>II</label><figDesc>OF OUR MODELS WITH DIFFERENT ARCHITECTURES FOR ACTION PREDICTION MODULE. OUR MODEL ((D) HIERARCHICAL) SHOWS THE BEST PERFORMANCE AMONG THE DESIGN CHOICES.</figDesc><table><row><cell></cell><cell>Full</cell><cell>Rare</cell><cell>Non-rare</cell></row><row><cell cols="3">(A) Modified Baseline 19.09 13.09</cell><cell>20.89</cell></row><row><cell>(B) MultiTask</cell><cell cols="2">19.54 13.93</cell><cell>21.22</cell></row><row><cell>(C) TwoStream</cell><cell cols="2">19.63 13.67</cell><cell>21.41</cell></row><row><cell>(D) Hierarchical</cell><cell cols="2">20.03 14.52</cell><cell>21.67</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III RESULTS</head><label>III</label><figDesc>ON THE HICO-DET DATASET COMPARED TO THE EXISTING STATE-OF-THE-ART METHODS. THE FIRST ROW SECTION SHOWS THE SCORES REPORTED IN THE PREVIOUS WORKS WHILE THE SECOND ROW SECTION SHOWS THE RESULTS OF OUR IMPLEMENTATIONS. THE PERFORMANCE OF OUR METHOD CONSISTENTLY SURPASSES THOSE OF THE EXISTING HOI DETECTORS.</figDesc><table><row><cell></cell><cell></cell><cell>Default</cell><cell></cell><cell></cell><cell cols="2">Known Object</cell></row><row><cell></cell><cell>Full</cell><cell>Rare</cell><cell>Non-rare</cell><cell>Full</cell><cell>Rare</cell><cell>Non-rare</cell></row><row><cell>Shen et al. [64]</cell><cell>6.46</cell><cell>4.24</cell><cell>7.12</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>HO-RCNN [3]</cell><cell>7.81</cell><cell>5.37</cell><cell>8.54</cell><cell>10.41</cell><cell>8.94</cell><cell>10.85</cell></row><row><cell>Gupta et al. [20] impl. by [16]</cell><cell>9.09</cell><cell>7.02</cell><cell>9.71</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>InteractNet [16]</cell><cell>9.94</cell><cell>7.16</cell><cell>10.77</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>GPNN [62]</cell><cell>13.11</cell><cell>9.34</cell><cell>14.23</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>iCAN [14]</cell><cell cols="2">14.84 10.45</cell><cell>16.15</cell><cell>16.43</cell><cell>12.01</cell><cell>17.75</cell></row><row><cell>Interactiveness Prior [48]</cell><cell cols="2">17.22 13.51</cell><cell>18.32</cell><cell>19.17</cell><cell>15.51</cell><cell>20.26</cell></row><row><cell>Contextual Attention [75]</cell><cell cols="2">16.24 11.16</cell><cell>17.75</cell><cell>17.73</cell><cell>12.78</cell><cell>19.21</cell></row><row><cell>No-Frills [22]</cell><cell cols="2">17.18 12.17</cell><cell>18.68</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RPNN [99]</cell><cell cols="2">17.35 12.78</cell><cell>18.71</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PMFNet [73]</cell><cell cols="2">17.46 15.65</cell><cell>18.00</cell><cell>20.34</cell><cell>17.47</cell><cell>21.20</cell></row><row><cell>Peyre et al. [59]</cell><cell cols="2">19.40 15.40</cell><cell>20.75</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PPDM [50]</cell><cell cols="2">21.10 14.46</cell><cell>23.09</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DJ-RN [47]</cell><cell cols="2">21.34 18.53</cell><cell>22.18</cell><cell>23.69</cell><cell>20.64</cell><cell>24.60</cell></row><row><cell>FCMNet [52]</cell><cell cols="2">20.41 17.34</cell><cell>21.56</cell><cell>22.56</cell><cell>18.97</cell><cell>23.12</cell></row><row><cell>VCL [27]</cell><cell cols="2">19.43 16.55</cell><cell>20.29</cell><cell>22.00</cell><cell>19.09</cell><cell>22.87</cell></row><row><cell>DRG [13]</cell><cell cols="2">19.26 17.74</cell><cell>19.71</cell><cell>23.40</cell><cell>21.75</cell><cell>23.89</cell></row><row><cell>No-Frills (Reproduced)</cell><cell cols="2">17.56 13.23</cell><cell>18.85</cell><cell>22.02</cell><cell>16.97</cell><cell>23.53</cell></row><row><cell>No-Frills + ACP (Ours)</cell><cell>20.59</cell><cell>15.92</cell><cell>21.98</cell><cell>25.35</cell><cell>19.55</cell><cell>27.08</cell></row><row><cell>No-Frills + ACP++ (Ours)</cell><cell>21.27</cell><cell>15.41</cell><cell>23.02</cell><cell>25.61</cell><cell>18.93</cell><cell>27.60</cell></row><row><cell>PPDM (Reproduced)</cell><cell cols="2">20.81 13.69</cell><cell>22.94</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PPDM + ACP++ (Ours)</cell><cell cols="2">22.11 14.43</cell><cell>24.40</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DRG (Reproduced)</cell><cell cols="2">18.77 16.41</cell><cell>19.47</cell><cell>24.74</cell><cell>23.74</cell><cell>25.04</cell></row><row><cell>DRG + ACP++ (Ours)</cell><cell cols="2">18.90 16.80</cell><cell>19.52</cell><cell>24.78</cell><cell>23.87</cell><cell>25.05</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V COMPARISON</head><label>V</label><figDesc>OF OUR MODEL TO THE FUNCTIONAL GENERALIZATION METHOD OF BANSAL et al. [1]. THE TOP SET OF ROWS SHOWS THE SCORES OF THE BASELINE AND THE FUNCTIONAL GENERALIZATION MODEL WITHOUT TUNING AS REPORTED IN<ref type="bibr" target="#b0">[1]</ref>.IN BOTTOM SET OF ROWS, WE SHOW THE RESULTS OF OUR IMPLEMENTATIONS THAT INCLUDE THE FUNCTIONAL GENERALIZATION MODEL AND OUR ACP METHOD. THE ACP METHOD CAN ACHIEVE LARGER PERFORMANCE GAINS THAN FUNCTIONAL GENERALIZATION.</figDesc><table><row><cell></cell><cell>Full</cell><cell>Rare</cell><cell>Non-rare</cell></row><row><cell>Baseline of [1]</cell><cell>12.72</cell><cell>7.57</cell><cell>14.26</cell></row><row><cell>Functional Generalization [1]</cell><cell>14.35</cell><cell>9.84</cell><cell>15.69</cell></row><row><cell>Baseline of [1] (Reproduced)</cell><cell>13.09</cell><cell>7.87</cell><cell>14.65</cell></row><row><cell>Functional Generalization (Reproduced)</cell><cell>13.68</cell><cell>8.62</cell><cell>15.19</cell></row><row><cell cols="3">Functional Generalization + ACP (Ours) 15.38 10.30</cell><cell>16.90</cell></row><row><cell cols="4">'Default' and 'Known Object' matrices, respectively. Among</cell></row><row><cell cols="4">the methods included in this comparison are the benchmark</cell></row><row><cell>model of the HICO-Det dataset</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VI RESULTS</head><label>VI</label><figDesc>ON THE ZERO-SHOT TRIPLETS OF THE HICO-DET DATASET. OUR FINAL MODEL SHOWS BETTER PERFORMANCE THAN PEYRE et al. BY A LARGE MARGIN. OUR ACP MODEL UNDER THE ZERO-SHOT SETTING EVEN OUTPERFORMS THE SUPERVISED SETTING OF OUR BASELINE.</figDesc><table><row><cell></cell><cell>mAP</cell></row><row><cell>Peyre et al. [59] Supervised</cell><cell>33.7</cell></row><row><cell>Peyre et al. [59] Zero-Shot</cell><cell>24.1</cell></row><row><cell>Peyre et al. [59] Zero-Shot with Aggregation</cell><cell>28.6</cell></row><row><cell>Ours Supervised (Modified Baseline)</cell><cell>33.27</cell></row><row><cell>Ours Zero-Shot (Modified Baseline)</cell><cell>20.34</cell></row><row><cell>Ours Zero-Shot (ACP)</cell><cell>34.95</cell></row><row><cell>Ours Zero-Shot (ACP++)</cell><cell>35.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Table VI, we show results of our final model (ACP and ACP++) and our modified baseline model compared to the corresponding setting reported in [59]. Our ACP model shows better performance (35.0 mAP) than Peyre et al. (28.6 mAP) by a large margin (relative 22.4% improvement). In addition, our extended model (ACP++) further improves the performance of our ACP (35.1 mAP). This result is remarkable in that our ACP model under the zero-shot setting even outperforms the supervised setting of our baseline model, indicating the power of the proposed ACP method to effectively leverage prior knowledge on action co-occurrences. Furthermore, the analogy transfer method proposed by Peyre et al.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Fig. 4. Examples of bounding boxes and HOI detection scores from our model. Bounding boxes for humans are colored red, and bounding boxes for objects are colored blue. Each image is displayed with the predicted action+object class followed by the probability computed by our model.</figDesc><table><row><cell>fly-airplane (61.35)</cell><cell>sit_at-dining_table (76.02)</cell><cell>hold-bird (96.17)</cell><cell>sit_on-couch (80.48)</cell><cell>carry-bottle (64.25)</cell></row><row><cell>drive-bus (67.74)</cell><cell>ride-boat (74.93)</cell><cell cols="2">lie_on-couch (74.00)</cell><cell>straddle-bicycle (61.44)</cell></row><row><cell>watch-bird (62.41 ? 97.90)</cell><cell>hold-potted_plant (49.98 ? 90.07)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>walk-dog (60.04 ? 24.53)</cell><cell>hold-horse (75.16 ? 42.95)</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The triplet ground truth labels H gt , O gt , and A gt are straightforward to determine from the HOI ground truth label Y gt .</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work was supported by the Institute for Information &amp; Communications Technology Promotion (2017-0-01772) grant funded by the Korea government.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In So Kweon received the BS and MS degrees in mechanical design and production engineering from Seoul National University, Seoul, South Korea, in 1981 and 1983, respectively, and the PhD degree in robotics from the Robotics Institute, Carnegie Mellon University, Pittsburgh, Pennsylvania, in 1990. He is an professor with the Electrical Engineering Department, KAIST, South Korea. He worked for the Toshiba R&amp;D Center, Japan, and joined the Department of Automation and Design Engineering, KAIST, Seoul, South Korea, in 1992, where he is currently a professor with the Department of Electrical Engineering. He is a recipient of the Best Student Paper Runner-up Award at the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 09). His research interests include camera and 3D sensor fusion, color modeling and analysis, visual tracking, and visual SLAM. He was the program co-chair for the Asian Conference on Computer Vision (ACCV 07) and was the general chair for the ACCV 12. He is also on the editorial board of the International Journal of Computer Vision. He is a member of the IEEE and KROS.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Detecting human-object interactions via functional generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankan</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Sai Saketh Rambhatla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Object level visual reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to detect human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xieyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huayi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hico: A benchmark for recognizing human-object interactions in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yugeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dealing with missing modalities in the visual question answer-difference prediction task through knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Won</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Jin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjae</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Detecting visual relationships with deep relational networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scene semantics from long-term observation of people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Delaitre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recognizing human actions in still images: a study of bag-of-features and part-based representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Delaitre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning person-object interactions for action recognition in still images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Delaitre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Largescale object classification using label relation graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartmut</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Object categorization using co-occurrence, location and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolina</forename><surname>Galleguillos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Drg: Dual relation graph for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ican: Instance-centric attention network for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The ecological approach to visual perception: classic edition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gibson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Psychology Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Detecting and recognizing human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">What makes a chair a chair</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Scene graph generation with external knowledge and image reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiuxiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Handong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Objects in action: An approach for combining action understanding and object perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Larry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04474</idno>
		<title level="m">Visual semantic role labeling</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">No-Frills Pytorch Github</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmay</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<ptr target="https://github.com/BigRedT/nofrillshoidet" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">No-frills human-object interaction detection: Factorization, layout encodings, and training techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmay</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning with side information through modality hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Significance of softmax-based features in comparison to distance metric learningbased features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shota</forename><surname>Horiguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Ikami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Visual compositional learning for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Harnessing deep neural networks with logic rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sharing features between objects and their attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Clevr: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Compositional learning for human object interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keizo</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Uniondet: Union-level detector towards real-time human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeho</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunwoo J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dense relational captioning: Triple-stream networks for relationship-based captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Jin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsoo</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Tae-Hyun Oh, and In So Kweon</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Image captioning with very scarce supervised data: Adversarial semisupervised learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Jin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Hyun</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Youngjin Yoon, and In So Kweon. Disjoint multi-task learning between heterogeneous humancentric tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Jin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Hyun</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Jin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Hyun</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.03855</idno>
		<title level="m">Dense relational image captioning via multi-task triple-stream networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Detecting human-object interactions with action co-occurrence priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Jin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Distribution aligning refinery of pseudolabel for imbalanced semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehyung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngbum</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sejun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunho</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Graph cut based inference with co-occurrence statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubor</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Inference methods for crfs with co-occurrence statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>L&amp;apos;ubor Ladick?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="213" to="225" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Superpixel-based object class segmentation using conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hichem</forename><surname>Sahbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">VIP-CNN: A visual phrase reasoning convolutional neural network for visual relationship detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Factorizable net: An efficient subgraph-based framework for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Scene graph generation from objects, phrases and region captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Detailed 2d-3d joint representation for humanobject interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Transferable interactiveness knowledge for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xijie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Ppdm: Parallel point detection and matching for real-time human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Amplifying key cues for humanobject-interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Visual relationship detection with language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Semantic hierarchies for visual object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications of the ACM</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="39" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Gyu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Whan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Distributionaware semantics-oriented pseudo-label for imbalanced semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngtaek</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Jin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05682</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Detecting unseen visual relations using analogies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Peyre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Phrase localization and visual relationship detection with comprehensive linguistic cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lazebnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Attentive relational networks for mapping images to scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengshi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning human-object interactions by graph parsing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxiong</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Scaling human-object interaction recognition through zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Achieving generalized object recognition through reasoning about association of function to structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louise</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Bowyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1097" to="1104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Superpixelenhanced pairwise conditional random field for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Sulimowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishfaq</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Aved</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Explicit spatiotemporal joint relation learning for tracking human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuankang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Cascaded hand pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">An introduction to conditional random fields for relational learning. Introduction to statistical relational learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="93" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Unbiased scene graph generation from biased training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Vsgnet: Spatial attention network for detecting human object interactions using graph convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oytun</forename><surname>Ulutan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Iftekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Modeling label dependencies in kernel learning for image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Phong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hichem</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sahbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Pose-aware multi-level feature network for human object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Contextual heterogeneous graph network for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Yingbiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Deep contextual attention for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">Haris</forename><surname>Anwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorma</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Laaksonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Learning human-object interaction detection using interaction points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Exploring context and visual pattern of relationship for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Nonlocal neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Donghyeon Cho, and In So Kweon. Linknet: Relational embedding for scene graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahun</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Interact as you intend: Intention-driven human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingjie</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongkang</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Learning to detect human-object interactions with knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingjie</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongkang</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Scene graph generation by iterative message passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Hd-cnn: hierarchical deep convolutional neural networks for large scale visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robinson</forename><surname>Piramuthu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Jagadeesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><surname>Decoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Graph r-cnn for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Shuffle-then-assemble: learning object-agnostic visual relationship features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Grouplet: A structured image representation for recognizing human and object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangpeng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Modeling mutual context of object and human pose in human-object interaction activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangpeng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Human action recognition by learning bases of action attributes and parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangpeng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoye</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><forename type="middle">Lai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Describing the scene as a whole: Joint object detection, scene classification and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Exploring visual relationship for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Zoom-net: Mining deep feature interactions for visual relationship recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guojun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Visual relationship detection with internal and external linguistic knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruichi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vlad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Neural motifs: Scene graph parsing with global context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">On exploring undetermined relationships for visual relationship detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Visual translation embedding network for visual relation detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zawlin</forename><surname>Kyaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Cooccurrent features in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Relationship proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Elgammal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Polysemy deciphering network for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xubin</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxing</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Relation parsing neural network for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Penghao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingmin</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Towards context-aware interaction recognition for visual relationship detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Hcvrd: a benchmark for large-scale human-centered visual relationship detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">He was awarded a silver prize from Samsung Humantech paper awards and Qualcomm Innovation awards. His research interests include high-level computer vision such as language and vision and human behavior understanding</title>
	</analytic>
	<monogr>
		<title level="m">respectively. He was a research intern in the Visual Computing Group, Microsoft Research Asia (MSRA)</title>
		<editor>Dong-Jin Kim received the B.S. degree, M.S. degree, and Ph.D.</editor>
		<meeting><address><addrLine>Daejeon, South Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>degree in Electrical Engineering from Korea Advanced Institute of Science and Technology (KAIST)</orgName>
		</respStmt>
	</monogr>
	<note>He is a student member of the IEEE</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
