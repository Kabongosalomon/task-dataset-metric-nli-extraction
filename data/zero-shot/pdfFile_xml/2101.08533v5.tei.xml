<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Eliminate Deviation with Deviation for Data Augmentation and a General Multi-modal Data Learning Method</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Gong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer and Cyber Security</orgName>
								<orgName type="institution">Fujian Normal University</orgName>
								<address>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqing</forename><surname>Huang</surname></persName>
							<email>lqhuang@fjnu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer and Cyber Security</orgName>
								<orgName type="institution">Fujian Normal University</orgName>
								<address>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifei</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer and Cyber Security</orgName>
								<orgName type="institution">Fujian Normal University</orgName>
								<address>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Eliminate Deviation with Deviation for Data Augmentation and a General Multi-modal Data Learning Method</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>One of the challenges of computer vision is that it needs to adapt to color deviations in changeable environments. Therefore, minimizing the adverse effects of color deviation on the prediction is one of the main goals of vision task. Current solutions focus on using generative models to augment training data to enhance the invariance of input variation. However, such methods often introduce new noise, which limits the gain from generated data. To this end, this paper proposes a strategy eliminate deviation with deviation, which is named Random Color Dropout (RCD). Our hypothesis is that if there are color deviation between the query image and the gallery image, the retrieval results of some examples will be better after ignoring the color information. Specifically, this strategy balances the weights between color features and color-independent features in the neural network by dropouting partial color information in the training data, so as to overcome the effect of color devitaion. The proposed RCD can be combined with various existing ReID models without changing the learning strategy, and can be applied to other computer vision fields, such as object detection. Experiments on several ReID baselines and three common large-scale datasets such as Mar-ket1501, DukeMTMC, and MSMT17 have verified the effectiveness of this method. Experiments on Cross-domain tests have shown that this strategy is significant eliminating the domain gap. Furthermore, in order to understand the working mechanism of RCD, we analyzed the effectiveness of this strategy from the perspective of classification, which reveals that it may be better to utilize many instead of all of color information in visual tasks with strong domain variations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Person re-identification (ReID) is to match the same person across diferent cameras and scenes <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref>. This technology have been widely applied to video surveillance <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>, image retrieval <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, criminal investigation <ref type="bibr" target="#b7">[8]</ref>, target  <ref type="bibr" target="#b0">[1]</ref> are displayed. It shows that the color deviation between the query image and gallery image will affect the retrieval results, and the retrieval results of some samples will be better after ignoring the color information. The numbers on the images indicate the rank of similarity in the retrieval results, the red and green numbers denote the wrong and correct results, respectively. tracking <ref type="bibr" target="#b9">[10]</ref> and others. The challenge of this task is that images captured by different cameras often contain significant intra-class variation caused by variations in viewpoint, human pose changes, occlusions, and color deviation under variable camera conditions, etc. As a result, the appearance of the same pedestrian image with great changes, making intra-class (the same pedestrian) metric distance larger than inter-class (different pedestrians). ReID usually combines representation learning <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref> with metric learning <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref>, and combines classification loss <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16]</ref> with triplet loss <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> in the training stage to optimize the neural network. In the inference stage, it is only necessary to use Cosine distance or Euclidean distance to measure the similarity between the query image and the gallery image, and then rank the gallery images according to the similarity, and finally use the re-ranking technique <ref type="bibr" target="#b23">[24]</ref> to further refine the search results.</p><p>The complexity of the inherent challenge of ReID means that its demand for data has the same complexity, and the complex data demand is difficult to meet and balance by the training set, which is also accompanied by the potential problem that the model overfits the only training data and lacks robustness. The dataset is hard to cover different camera environments and all their variations at different times, so the trained models tend to overfit the given training set and lack robustness to additional scenarios. It is no doubt that color features are important discriminative features, but color features instead limit the model to make correct predictions in some cases. For example, because white and gray, black and dark blue, and brown and yellow are similar under some lighting conditions, it is difficult for the model to make correct predictions for negative samples that are similar to target after overfitting the color deviation variations. As shown in (a) to (c) in <ref type="figure" target="#fig_0">Figure 1</ref>, the prediction results made by the model trained with grayscale images in this case are better after discarding the color bias.</p><p>Realistically, color deviations cause domain gaps exist both between datasets and within datasets <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>. These color biases are practically inexhaustible. Instead of generating a variety of data to let the model "see" these variations (especially intra-class variations) <ref type="bibr" target="#b24">[25]</ref> during training to enhance the robustness to input variations, it is better to balance the weight between color features and other important discriminant features implicitly. Aiming at the inherent color deviation problem that the images obtained under different shooting conditions, this paper proposes a strategy to eliminate deviation with deviation based on the assumption that the retrieval results of some samples will be better when discarding color information, which is named random color dropout (RCD). RCD balances the weight between color features and other important discriminant features in neural network by discarding part of the color information in the training data, so as to overcome the influence of color deviation. This strategy exists in various forms. For example, color deviation can be overcome with biased grayscale information or sketch information (or contour information). Taking grayscale as an example, it can be randomly selected a rectangular area in the RGB image and replace its pixels with the same rectangular area in the corresponding grayscale image , thus it generates a training image with different areas of biases during ReID model training. Compared with existing methods based on generative adversarial networks (GANs) <ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref>, the proposed method is more lightweight and effective because it not only does not introduce new noise but also saves a large amount of computational resources. At the same time, this strategy enables the model to naturally have cross-modal retrieval <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref> capabilities. For example, when taking the contour information as the intermediary to overcome the color deviation, the cross-modal retrieval between sketch and RGB visible image can be realized.</p><p>In addition, this paper analyzes the relationship between RCD and the generalization ability of neural networks from the perspective of classification, and reveals the intrinsic reasons that networks trained with RCD may outperform ordinary networks. Experiments show that the proposed method not only increases the robustness of the model to color deviation but also bridges the domain gap between different datasets, which has significant advantages over the existing state-of-the-art method. Taking the grayscale as an example, the RCD strategy proposed in this paper includes global grayscale transformation, local grayscale transformation, and a combination of these two. The method has the following advantages:</p><p>It is a lightweight approach which does not require any additional parameter learning or memory consumption. It can be combined with various CNN models without changing the learning strategy. It is a complementary approach to existing data augmentation.</p><p>The main contributions of this paper are summarized as follows:</p><p>? This paper proposes a learning strategy which against color deviation with information deviation, which decreases the overfitting and increases generalization ability of the model.</p><p>? A simple and effective cross-modal retrieval method is proposed, which does not need complex network design.</p><p>? This paper proves that the network trained with RCD may be better than the ordinary network from the perspective of classification.</p><p>? The strategy proposed in this paper is proved to be effective in improving ReID performance through extensive experiments and analysis. The effectiveness of the proposed method is verified on several baselines and representative datasets.</p><p>This work was previously published as a preprint on Arxiv and extended on the basis of it, including related demonstration and cross-modal retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The complexity of the inherent challenge of ReID means that its demand for data has the same complexity, and the failure to fully meet the complex demand of the data is the source of the problem of overfitting and insufficient generalization of the model to the training data. Improving generalization ability is the focus of research in convolutional neural networks (CNNs). Therefore, data augmentation is effective in improving the generalization ability of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Classic Data Augmentation</head><p>Many data augmentation <ref type="bibr" target="#b35">[36]</ref> methods have been proposed, such as random cropping <ref type="bibr" target="#b35">[36]</ref>, flipping <ref type="bibr" target="#b36">[37]</ref>, which are well known to play an important role in classification, detection and ReID. CutMix <ref type="bibr" target="#b37">[38]</ref> replaces one patch of an image with a patch from another image. Random erasing or cutout <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40]</ref> adds noise block to the image to regularize the network, while it helps to solve the occlusion problem in the ReID. The above methods are regarded as indispensable methods, and they are applied to various baselines <ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref>. These techniques have been proved to be effective in improving the prediction accuracy, and they are complementary to each other <ref type="bibr" target="#b40">[41]</ref>.</p><p>In solving the problem of color deviation, the early work <ref type="bibr" target="#b44">[45]</ref> used the filter and the maximum grouping layer to learn the illumination transformation, divided the pedestrian image into more small pieces to calculate the similarity, and uniformly handled the problems of misalignment, occlusion and illumination variation under the deep neural network; <ref type="bibr" target="#b45">[46]</ref> performed pre-processing before feature extraction and used multiscale Retinex algorithm to enhance the color information of light shaded regions to improve the color changes caused by lighting condition changes.</p><p>With the increasing maturity of GANs, GANs-based approaches for data augmentation have become an active research field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Data Augmentation Based on GANs</head><p>The goal of these methods is to mitigate the effect of color deviation or human-pose variation, and to improve the robustness of the model by learning the invariant features from the variation of the input. The appearance details and the emphases generated by different GANs-based methods are also different, but their goal is all to compensate for the difference between the source and target domains. For example, CamStyle <ref type="bibr" target="#b51">[52]</ref> generates new data for transferring different camera styles to learn invariant features between different cameras to increase the robustness of the model to camera style changes; CycleGAN <ref type="bibr" target="#b46">[47]</ref> was applied in <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b47">48]</ref> to transfer pedestrian image styles from one dataset to another; StarGAN <ref type="bibr" target="#b48">[49]</ref> was used by <ref type="bibr" target="#b49">[50]</ref> to generate pedestrian images with different camera styles.</p><p>Wei et al. <ref type="bibr" target="#b25">[26]</ref> proposed PTGAN to achieve pedestrian image transfer across different ReID datasets. It uses semantic segmentation to extract foreground masks to assist style transfer, and converts the background into the desired style of the dataset while keeping the foreground unchanged. different from global style transfer, DGNet <ref type="bibr" target="#b24">[25]</ref> utilizes GANs to transfer clothing among different pedestrians by manipulating appearance and structural details to generate more diverse data to reduce the impact of color changes on the model, which effectively improves the generalization ability of the model. In addition, <ref type="bibr" target="#b50">[51]</ref> uses 3D engine and environment rendering technology to build a virtual pedestrian data set with multiple lighting conditions, which is combined with other large real data sets to jointly pre-train a model.</p><p>The method proposed in this paper has been partially validated in other works. <ref type="bibr" target="#b51">[52]</ref> proved that the lack of robustness to color deviation is one of the main reasons why the model is vulnerable to adversarial metric attacks <ref type="bibr" target="#b52">[53]</ref><ref type="bibr" target="#b53">[54]</ref><ref type="bibr" target="#b54">[55]</ref>, and enhanced the model's adversarial defense using the method proposed in this paper; It is adopted in the new baseline proposed in <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b56">57]</ref> to help increase the generalization of the model. In addition, <ref type="bibr" target="#b57">[58]</ref> showed that the method proposed in this paper is also suitable for object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Methods</head><p>The RCD strategy proposed in this paper includes global transformation, local transformation, and a combination of the two. Taking grayscale as an example, it includes global grayscale transformation, local grayscale transformation, and combinations of the two. At the end of this subsection, we give the corresponding analysis of the proposed method. The framework of this method is showed in <ref type="figure" target="#fig_1">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Global Graycale Transformation</head><p>Input : Input image I; Graycale transformation probability p;</p><p>. Output: Grayscale images I * .</p><formula xml:id="formula_0">Initialization: p 1 ? Rand (0, 1). if p 1 ? p then I * ? I; return I * . else I * ? t(I); return I * . end</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Global Grayscale Transformation</head><p>In the data loading, it randomly samples K identities and M images of per person to constitute a training batch which size equals to B = K ? M . The set is denoted as Taking the grayscale as an example, this method randomly performs global grayscale transformation on the training batch with a probability, and then inputs into the model for training. This process can be defined as:</p><formula xml:id="formula_1">x v = {x v i |i = 1, 2, ..., M ? K}, where x v i = {x v i |y i }</formula><formula xml:id="formula_2">I g = t(R, G, B)<label>(1)</label></formula><p>where t(?) is the grayscale image conversion function, which is implemented by performing pixel-by-pixel accumulation calculations on the R, G, and B channels of the original visible RGB image; y is the label of the sample, the converted grayscale image label of x g are the same as the original ones:</p><formula xml:id="formula_3">(x g |y) = (x v |y)<label>(2)</label></formula><p>the procedure of LGT is shown in Algorithm.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Local Grayscale Transformation</head><p>In addition to transforming the data globally, we also consider transforming the data locally so that the model adapt better to the significantly varying bias due to color dropout from the local variation.</p><p>The local grayscale transformation (LGT) for each visible image x v can be achieved by the following equations: </p><formula xml:id="formula_4">x g = t(x v ),<label>(3)</label></formula><formula xml:id="formula_5">rect = RandP osition(x v ),<label>(4)</label></formula><formula xml:id="formula_6">x lg = LGT (x v , x g , rect)<label>(5)</label></formula><formula xml:id="formula_7">Initialization: p 1 ? Rand (0, 1). if p 1 ? p then I * ? I; return I * . else while True do S t ? Rand (s l , s h )?S; r t ? Rand (r 1 , r 2 ); H t ? ? S t ? r t , W t ? St rt ; x t ? Rand (0, W ), y t ? Rand (0, H); if x t + W t ? W and y t + H t ? H then P osition ? (x t , y t , x t + W t , y t + H t ); I(P osition) ? t(P osition); I * ? I; return I * . end end end and (x lg |y) = (x v |y)<label>(6)</label></formula><p>where x g is the grayscale images, and t(?) is the grayscale transformation funtion; RandP osition(?) is used to generate a random rectangle in the image, and the function of</p><p>LGT (?) is to give the pixels in the rectangle corresponding to the x g image to the x v image; x lg is the sample after local grayscale transformation, and y is the label of the transformed image.</p><p>In the process of model training, we conduct LGT randomly transformation on the training batch with a probability. For an image I in a batch, denote the probability of it undergoing LGT be p r , and the probability of it being kept unchanged be 1 ? p r . In this process, it randomly selects a rectangular region in the image and replaces it with the pixels of the same rectangular region in the corresponding grayscale image. Thus, training images which include regions with different levels of grayscale are generated. Among them, s l and s h are the minimum and maximum values of the ratio of the image to the randomly generated rectangle area, and the S t of the rectangle area limited between the minimum and maximum ratio is obtained by S t ? Rand(s l , s h ) ? S, r t is a coefficient used to determine the shape of the rectangle. It is limited to the interval (r 1 , r 2</p><p>). x t and y t are randomly generated by coordinates of the upper left corner of the rectangle. If the coordinates of the rectangle exceed the scope of the image, the area and position coordinates of the rectangle are re-determined. When a rectangle that meets the above requirements is found, the pixel values of the selected region are replaced by the corresponding rectangular region on the grayscale image converted from RGB image. As a result, training images which include regions with different levels of grayscale are generated, and the object structure is not damaged. The procedure of LGT is shown in Algorithm.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Loss function</head><p>ReD usually combines classification loss and triplet loss to train the model <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b59">60]</ref>. Here, we use x v i to denote the i-th RGB image in a training batch, and x g i to denote the image obtained after GGT or LGT conversion. Thenthe features of x v i and x g i can be expressed as:</p><formula xml:id="formula_8">f v i = f (x v i ) f g i = f (f g i ) (7)</formula><p>The Euclidean distance between two samples x g i and x g j is denoted as</p><formula xml:id="formula_9">D(x g i , x g j ),</formula><p>where The subscript {i, j} denotes the image index in the training batch. Formally, let x g i be the anchor sample, the triple {x g i , x g j , x g k } is selected in the following way:</p><formula xml:id="formula_10">P g i,j = max ?yi=yj D(x g i , x g j )<label>(8)</label></formula><formula xml:id="formula_11">N g i,k = min ?yi =yj D(x g i , x g k )<label>(9)</label></formula><p>For each anchor point x g i , the above strategy selects the positive sample pair with the same pedestrian class label and the farthest the most distant positive sample pair with the same pedestrian category label and the nearest negative sample pairs, forming a triplet {x g i , x g + j , x g ? k } for mining grayscale information. In general, the use of The boundary parameter ? is used to control the spacing of the positive and negative sample pairs. In summary, we can define the following triadic loss for training:</p><formula xml:id="formula_12">L g = 1 n n i=1 max[? + D(x g i , x g + j ) + D(x g i , x g ? k )]<label>(10)</label></formula><p>In addition, x v and x g are trained using a shared identity classifier ?. The predicted probability of identity labels y i is define as p(y i |x g i ; ?). The ID loss is represented as follows:</p><formula xml:id="formula_13">L ide = ? 1 n n i=1 log(p(y i |x g i ; ?))<label>(11)</label></formula><p>Therefore, the overall loss during random grayscale transformation is:</p><formula xml:id="formula_14">L total = L g + L ide<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Analysis of Random Color Dropping Policy</head><p>Here suppose there are m instances, the expected output, i.e. D = [d 1 , d 2 , . . . , d m ] T where d j denotes the expected output on the j-th instance, and the actual output of the i-th component neural network, i.e. F i = [f i1 , f i2 , . . . , f im ] T where f ij denotes the actual output of the i-th component network on the j-th instance. D and F i satisfy that d j ? {?1, +1}(j = 1, 2, . . . , m) and f ij ? {?1, +1}(i = 1, 2, . . . , N ; j = 1, 2, . . . , m) respectively. It is obvious that if the actual output of the i-th component network on the jth instance is correct according to the expected output then f ij d j = +1, otherwise f ij d j = ?1. Thus the generalization error of the i-th component neural network on those m instances is:</p><formula xml:id="formula_15">E i = 1 m m j=1 Error(f ij d j )<label>(13)</label></formula><p>where Error(x) is a function defined as:</p><formula xml:id="formula_16">Error(x) = ? ? ? 1, if x = ?1 0.5, if x = 0 0, if x = 1<label>(14)</label></formula><p>Here we introduce a vector Sum = [Sum 1 , Sum 2 , . . . , Sum m ] T where Sum j denotes the sum of the actual output of all the component neural networks on the j-th instance, i.e.</p><formula xml:id="formula_17">Sum j = N i=1 f ij<label>(15)</label></formula><p>Then the output of the neural network ensemble on the j-th instance is:f</p><formula xml:id="formula_18">j = Sgn(Sum j )<label>(16)</label></formula><p>where Sgn(x) is a function defined as:</p><formula xml:id="formula_19">Sgn(x) = ? ? ? 1, if x &gt; 0 0, if x = 0 ?1, if x &lt; 0<label>(17)</label></formula><p>It is obvious thatf j ? {?1, 0, +1}(j = 1, 2, . . . , m) . If the actual output of the ensemble on the j-th instance is correct according to the expected output thenf j d j = +1; if it is wrong thenf j d j = ?1; otherwisef j d j = 0, which means that there is a tie on the j-th instance, e.g. three component networks vote for +1 while other three networks vote for -1. Thus the generalization error of the ensemble is: Here suppose that the k-th component neural network is trained using grayscale images. Then the output of the new set on the j-th instance is:</p><formula xml:id="formula_20">E = 1 m m j=1 Error(f j d j )<label>(18)</label></formula><formula xml:id="formula_21">f j = Sgn(Sum j(j =k) ? f kj )<label>(19)</label></formula><p>and the generalization error of the new ensemble is:</p><formula xml:id="formula_22">E = 1 m m j=1 Error(f j d j )<label>(20)</label></formula><p>It is assumed that a certain number of networks with deviations will not affect the performance of the overall neural network, and the retrieval results of some examples will be better after ignoring the color information.  <ref type="bibr" target="#b60">[61]</ref> is a large-scale multitarget, multi-camera tracking dataset, a HD video dataset recorded by 8 synchronous cameras, with more than 2,700 individual pedestrians. The above two datasets are widely used in ReID studies. MSMT17 <ref type="bibr" target="#b25">[26]</ref>, created in winter, was presented in 2018 as a new, larger dataset closer to real-life scenes, containing a total of 4,101 individuals and covering multiple scenes and time periods.</p><formula xml:id="formula_23">Error(f j d j ) Error(f j d j )<label>(21)</label></formula><p>These three datasets are currently the largest datasets of ReID, and they are also the most representative because they collectively contain multi-season, multi-time, HD, and low-definition cameras with rich scenes and backgrounds as well as complex lighting variations.</p><p>Sketch ReID dataset <ref type="bibr" target="#b7">[8]</ref> contains 200 persons, each of which has one sketch and two photos. Photos of each person were captured during daytime by two cross-view cameras. It cropped the raw images (or video frames) manually to make sure that every photo contains the one specific person. It have a total of 5 artists to draw all persons'sketches and every artist has his own painting style.</p><p>Evaluation criteria. Following existing works <ref type="bibr" target="#b0">[1]</ref>, Rank-k precision and mean Average Precision (mAP) are adapted as evaluation metrics. Rank-1 denotes the average accuracy of the first return result corresponding to each query image. mAP denotes the mean of average accuracy, the query results are sorted according to the similarity, the closer the correct result is to the top of the list, the higher the score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Hyper-Parameter Setting</head><p>During CNN training, two hyper-parameters need to be evaluated. One of them is GGT probability p. Firstly, we take the hyper-parameter p as 0.01, 0.03, 0.05, 0.07, 0.1, 0.2, 0.3,..., 1 for the GGT experiments. Then we take the value of each parameter for three independent repetitions of the experiments. Finally, we calculate the average of the final result. The results of different p are shown in <ref type="figure" target="#fig_3">Fig.3</ref>. We can see that when p = 0.05, the performance of the model reaches the maximum value in Rank-1 and mAP. If we do not specify, the hyper-parameter is set p = 0.05 in the next experiments.</p><p>Another hyper-parameter is LGT probability p r . We take the hyper-parameter p r as the same as above for the LGT experiments, whose selection process is similar to the above p. The results of different p r are shown in <ref type="figure" target="#fig_4">Fig.4</ref>.</p><p>Obviously, when p r = 0.4 or p r = 0.7, the model achieves better performance. And the best performance is achieved when p r = 0.4. If we do not specify, the hyperparameter is set p r = 0.4 in the subsequent experiments.</p><p>Evaluation of GGT and LGT. Compared with the best results of GGT on baseline <ref type="bibr" target="#b43">[44]</ref>, the accuracy of LGT is improved by 0.5% and 1.4% on Rank-1 and mAP, respectively. Under the same conditions using re-Ranking <ref type="bibr" target="#b23">[24]</ref>, the accuracy of Rank-1 and mAP is improved by 0% and 0.4%, respectively. Therefore, the advantages of LGT are more obvious when re-Ranking is not used. However, <ref type="figure" target="#fig_4">Fig.4</ref> also shows that the performance improvement brought by</p><p>LGT is not stable enough because of the obvious fluctuation in LGT, while the performance improvement brought by GGT is very stable. Therefore, we improve the stability of the method by combining GGT with LGT.</p><p>Evaluation by Combining GGT with LGT. First, we fix the hyper-parameter value of GGT to p = 0.05, then keep the control variable unchanged to further determine the hyper-parameter of LGT. Finally, we take the hyperparameter pr of LGT to be 0.1, 0.2, ???, 0.7 to conduct combination experiments of GGT and LGT, and conduct 3 independent repeated experiments for each parameter p r to get the average value. The result is shown in 5. It can be seen that the performance improvement brought by the combination of GGT and LGT is more stable and with less fluctuation, and the comprehensive performance of the model is the best when the hyper-parameter value of LGT is p r = 0.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison Experiments</head><p>Performance comparison and analysis. We first evaluate baseline <ref type="bibr" target="#b43">[44]</ref> on the Market-1501 dataset <ref type="bibr" target="#b0">[1]</ref>. To be consistent with recent works, we follow the new training/testing protocol to conduct our experiments by k-reciprocal reranking (RK) <ref type="bibr" target="#b23">[24]</ref>. As can be seen from <ref type="figure" target="#fig_3">Fig.3 and Fig.4</ref>, our method improves by 1.2% on Rank-1 and 3.3% on mAP on  the baseline, and 1.5% on Rank-1 and 2.1% on mAP above baseline in the same conditions using the re-Ranking <ref type="bibr" target="#b23">[24]</ref>. Secondly, we further test the method in this paper on the baselines <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b59">60]</ref> with better performance. As we can see from <ref type="table" target="#tab_2">Table.1 to Table.</ref>3, the best results of our method improve by 0.6% and 1.3% on the Rank-1 and mAP on the strong baseline <ref type="bibr" target="#b58">[59]</ref>, respectively, and 0.8% and 0.5% Rank-1 and mAP above baseline under the same conditions using the re-Ranking <ref type="bibr" target="#b23">[24]</ref>, respectively. On fastReID <ref type="bibr" target="#b59">[60]</ref>, our method is 0.2% higher and 0.9% than baseline in Rank-1 and mAP, respectively, and higher 0.1% and 0.3% than baseline under using re-Ranking.</p><p>The default configuration on the Strong Baseline <ref type="bibr" target="#b58">[59]</ref> and FastReID <ref type="bibr" target="#b59">[60]</ref> uses data augmentation such as random flipping <ref type="bibr" target="#b36">[37]</ref>, cropping <ref type="bibr" target="#b35">[36]</ref>, and erasing <ref type="bibr" target="#b38">[39]</ref>. The method proposed in this paper further improves the model accuracy on the basis of using them, which shows that our method can be combined with other data augmentation methods.  Cross-domain tests.</p><p>Cross-domain person reidentification aims at adapting the model trained on a labeled source domain dataset to another target domain dataset without any annotation. It is pointed out by <ref type="bibr" target="#b58">[59]</ref> that the higher accuracy of the model does not mean that it has better generalization capacity. In response to the above potential problems, we use cross-domain tests to verify the robustness of the model. Experiments show that the proposed method effectively enhances the generalization capacity of the model, and the <ref type="table" target="#tab_3">Table 2</ref> shows the cross-domain experiments of the proposed method between two datasets, Market-1501 <ref type="bibr" target="#b0">[1]</ref> and DukeMTMC <ref type="bibr" target="#b60">[61]</ref>. In order to further explore the effectiveness of the proposed method in cross-domain experiments, we use GGT to conduct the following cross-domain experiments on strong baseline <ref type="bibr" target="#b58">[59]</ref>. The experiments are shown in <ref type="table">Table.</ref>4.</p><p>In <ref type="table">Table 4</ref>, +REA means that the trick of Random Erasing is used in model training, -REA means turning it off. Experimental results show that random erasing <ref type="bibr" target="#b38">[39]</ref> can also significantly improve the performance of the ReID model, but it will cause a significant drop in cross-domain performance. The proposed method can not only significantly im- <ref type="table">Table 4</ref>. The performance of different models is evaluated on cross-domain dataset. M?D means that we train the model on Market1501 <ref type="bibr" target="#b0">[1]</ref> and evaluate it on DukeMTMC <ref type="bibr" target="#b60">[61]</ref>. prove the cross-domain performance of the ReID model, but also be more robust because of learning more discriminative features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Domain</head><p>Comparison of state-of-the-arts. A comparison of the performance of our method with the state-of-the-art methods DGNet <ref type="bibr" target="#b24">[25]</ref> on Market1501 <ref type="bibr" target="#b0">[1]</ref> is shown in <ref type="table">Table.5 and  Table.</ref>6. As can be seen from Table.5, our method delivers a performance improvement that far exceeds that of DGNet, <ref type="figure">Figure 7</ref>. t-SNE <ref type="bibr" target="#b67">[68]</ref> visualization of six randomly selected images with different identities on Market1501 <ref type="bibr" target="#b0">[1]</ref>. Each image corresponds to the randomly generated images with color deviation. The same color means that they are obtained by transformation of the same image. Dots means original example.</p><p>the state-of-the-art GAN-based method, by more than 2.7 percentage points on mAP, which suggests that the proposed method is superior to existing data augmentation.</p><p>As can be seen from <ref type="table">Table.</ref>6, the generalization ability of the proposed method in cross-domain tests is improved by 1.9 percentage points in the Rank-1 compared with the baseline <ref type="bibr" target="#b43">[44]</ref>, which further shows that the proposed method is better than the existing data augmentation based on generative models. It is worth noting that when the data generated by DGNet is used for model training, the cross-domain performance of the model is poorly, which confirms the point of this paper that color deviation is is difficult to exhaust and that instead of enhancing robustness to input changes by generating a variety of data for the model to "see" during training, it is better to implicitly reduce the weight of the model in the discriminant feature of color information.</p><p>Cross-modal retrival. Another form of strategy proposed in this paper is to take sketch image as the intermediary of balancing weight. By applying the proposed global homogeneity transformation and local homogeneity transformation, the sketch image is transformed as a homogeneous image, as shown in <ref type="figure">Figure.</ref>6. It can not only improve the robustness of the model, but also realize the sketch-based ReID. We can see this clearly from <ref type="table">Table.</ref>.7 and <ref type="table">Table.</ref>. <ref type="bibr" target="#b7">8</ref>.</p><p>In terms of cross-modal retrieval <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b69">70]</ref>, in order to match images of different modalities, existing approaches usually achieve cross-modal retrieval with the help of attention mechanisms <ref type="bibr" target="#b68">[69]</ref>, multi-stream networks <ref type="bibr" target="#b69">[70]</ref>, and generative adversarial networks <ref type="bibr" target="#b7">[8]</ref>. Lu et al. proposed a cross-domain adversarial feature learning (AFL) method for sketch re-identification and contributed the sketch character re-identification dataset <ref type="bibr" target="#b7">[8]</ref>.</p><p>In order to make a fair comparison, as same as AFL <ref type="bibr" target="#b7">[8]</ref>, the method proposed in this paper is firstly trained on the Market-1501 dataset, and then fine tuned on sketch ReID dataset. In parameter setting, this paper set 5% Global Sketch Transformation and 70% Local Sketch Transformation. The experiment result shows that the performance im- <ref type="figure">Figure 8</ref>. Comparison of Grad-CAM <ref type="bibr" target="#b70">[71]</ref> activation map between normally trained model and our proactive defense model. provement in the Sketch Re-identification more than 8%. This experiment also shows the generality of the proposed method.</p><p>Visualization analysis. As the show in <ref type="figure">Figure 7</ref>, the model trained by DCR is robust to color variations. Therefore, we can observe that the features of examples with color deviation exhibit clustering effects better.</p><p>Grad-CAM <ref type="bibr" target="#b70">[71]</ref> uses the gradient information flowing into the last convolutional layer of the CNN to visualize the importance of each neuron in the output layer for the final prediction, by which it is possible to visualize which regions of the image have a significant impact on the prediction of a model. As shown in <ref type="figure">Figure 8</ref>, we can see that the the normally trained model activates irrelevant parts in the case of severe color deviation, while the model trianed with RCD is still effectively activating some important parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, a simple, effective and general strategy that can be applied in computer vision to overcome color deviation. Neither does the method require large scale training like GAN, nor introduces any noise. The method uses a random homogeneous transformation to realize the modeling of different modal relationships. The model balances the weights between color features and discriminative noncolor features by fitting differentiated homogeneous information in a mixed domain with information bias during the training process, thus reducing the negative impact of color deviation on ReID. In addition, this paper reveals the intrinsic reasons why networks trained with RCD outperform ordinary networks from a classification perspective. At the same time, experiments on several datasets and baselines show that the proposed method is effective and outperforms the state of the arts, and extra experiments show that the proposed strategy has natural cross modal properties.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The retrieval results of the model trained with visible (RGB) image and the model trained with grayscale image on Mar-ket1501</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Framework diagram of our Random Color Dropout (RCD): The application of global grayscale transformation and local grayscale transformation in the framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>represents the i-th sample image of the training batch, and y i represents the class label of the pedestrian.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Performance of GGT under different hyperparameters on Market1501.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Performance of LGT under different hyperparameters on Market1501.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Performance of combining GGT with LGT under different hyperparameters on Market1501.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Diagram of Global Sketch Transformation (GST) and Local Sketch Transformation (LST).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 2: Local Graycale Transformation Input : Input image I; Image size W and H; Area of image S; Transformation probability p; Area ratio range s l and s h ; Aspect ratio range r 1 and r 2 . Output: Transformed image I</figDesc><table /><note>* .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Performance comparison on Market1501 dataset.</figDesc><table><row><cell>Methods</cell><cell cols="2">Market1501 Rank-1(%) mAP(%)</cell></row><row><cell>IANet [62]</cell><cell>94.4</cell><cell>83.1</cell></row><row><cell>DGNet [25]</cell><cell>94.8</cell><cell>86.0</cell></row><row><cell>SCAL [63]</cell><cell>95.8</cell><cell>89.3</cell></row><row><cell>Circle Loss [64]</cell><cell>96.1</cell><cell>87.4</cell></row><row><cell>SB [59]</cell><cell>94.5</cell><cell>85.9</cell></row><row><cell>SB [59] + RK [24]</cell><cell>95.4</cell><cell>94.2</cell></row><row><cell>SB + GGT(ours)</cell><cell>94.6</cell><cell>85.7</cell></row><row><cell>SB + GGT+ RK(ours)</cell><cell>96.2</cell><cell>94.7</cell></row><row><cell>SB + LGT(ours)</cell><cell>95.1</cell><cell>87.2</cell></row><row><cell>SB + LGT + RK(ours)</cell><cell>95.9</cell><cell>94.4</cell></row><row><cell>FastReID [60]</cell><cell>96. 3</cell><cell>90.3</cell></row><row><cell>FastReID + RK</cell><cell>96.8</cell><cell>95.3</cell></row><row><cell>FastReID + GGT(ours)</cell><cell>96.5</cell><cell>91.2</cell></row><row><cell>FastReID + GGT + RK(ours)</cell><cell>96.9</cell><cell>95.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Performance comparison on DukeMTMC dataset.</figDesc><table><row><cell>Methods</cell><cell cols="2">DukeMTMC Rank-1(%) mAP(%)</cell></row><row><cell>IANet [62]</cell><cell>87.1</cell><cell>73.4</cell></row><row><cell>DGNet [25]</cell><cell>86.6</cell><cell>74.8</cell></row><row><cell>SCAL [63]</cell><cell>89.0</cell><cell>79.6</cell></row><row><cell>SB [59]</cell><cell>86.4</cell><cell>76.4</cell></row><row><cell>SB + RK [24]</cell><cell>90.3</cell><cell>89.1</cell></row><row><cell>SB + GGT(ours)</cell><cell>87.8</cell><cell>77.3</cell></row><row><cell>SB + GGT+ RK(ours)</cell><cell>90.9</cell><cell>89.2</cell></row><row><cell>SB + LGT(ours)</cell><cell>87.3</cell><cell>77.3</cell></row><row><cell>SB + LGT + RK(ours)</cell><cell>91</cell><cell>89.4</cell></row><row><cell>FastReID [60]</cell><cell>92.4</cell><cell>83.2</cell></row><row><cell>FastReID + RK</cell><cell>94.4</cell><cell>92.2</cell></row><row><cell>FastReID + LGT(ours)</cell><cell>92.8</cell><cell>84.2</cell></row><row><cell>FastReID + LGT + RK(ours)</cell><cell>94.3</cell><cell>92.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Performance comparison on MSMT17 dataset.</figDesc><table><row><cell>Methods</cell><cell cols="2">MSMT17 Rank-1(%) mAP(%)</cell></row><row><cell>IANet [62]</cell><cell>75.5</cell><cell>46.8</cell></row><row><cell>DGNet [25]</cell><cell>77.2</cell><cell>52.3</cell></row><row><cell>RGA-SC [65]</cell><cell>80.3</cell><cell>57.5</cell></row><row><cell>SCSN [66]</cell><cell>83.8</cell><cell>58.5</cell></row><row><cell>AdaptiveReID [67]</cell><cell>81.7</cell><cell>62.2</cell></row><row><cell>FastReID [60]</cell><cell>85.1</cell><cell>63.3</cell></row><row><cell>FastReID + GGT(ours)</cell><cell>86.2</cell><cell>65.3</cell></row><row><cell>FastReID + GGT&amp;LGT(ours)</cell><cell>86.2</cell><cell>65.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .Table 8 .</head><label>78</label><figDesc>Performance comparison on Market1501 dataset. Performance comparison between our RCD and Adversarial Feature Learning on Sketch ReID dataset.</figDesc><table><row><cell></cell><cell></cell><cell>M?D</cell><cell></cell><cell>D?M</cell></row><row><cell></cell><cell cols="4">Rank-1/mAP(%) Rank-1/mAP(%)</cell></row><row><cell>SB [59]+REA [39]+RK</cell><cell></cell><cell>33.6/24.3</cell><cell></cell><cell>51.6/32.3</cell></row><row><cell cols="2">SB+REA+GGT+RK(ours)</cell><cell>37.8/27.8</cell><cell></cell><cell>55.4/35.7</cell></row><row><cell>SB-REA+RK</cell><cell></cell><cell>45.5/37.0</cell><cell></cell><cell>58.2/37.8</cell></row><row><cell>SB-REA+GGT+RK(ours)</cell><cell></cell><cell>48.2/37.9</cell><cell></cell><cell>65.0/43.7</cell></row><row><cell cols="5">Table 5. Performance comparison between our LGT conversion</cell></row><row><cell cols="4">and DGNet data augmentation on Market1501.</cell></row><row><cell>Methods</cell><cell></cell><cell cols="3">Market1501 Rank-1 mAP(%)</cell></row><row><cell>Baseline [44]</cell><cell></cell><cell>88.8</cell><cell></cell><cell>71.6</cell></row><row><cell cols="2">Baseline + DGNet [25]</cell><cell>88.9</cell><cell></cell><cell>72.1</cell></row><row><cell cols="2">Baseline+ LGT(ours)</cell><cell>90.0</cell><cell></cell><cell>74.9</cell></row><row><cell cols="5">Table 6. Cross-domain performance comparison between our LGT</cell></row><row><cell>and DGNet on Market1501.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="4">Market1501?DukeMTMC Rank-1 mAP</cell></row><row><cell>Baseline [44]</cell><cell cols="2">37.8</cell><cell></cell><cell>27.0</cell></row><row><cell cols="3">Baseline + DGNet [25] 36.7</cell><cell></cell><cell>25.6</cell></row><row><cell>Baseline+ LGT(ours)</cell><cell cols="2">39.7</cell><cell></cell><cell>27.9</cell></row><row><cell>Methods</cell><cell></cell><cell cols="3">Market1501 Rank-1(%) mAP(%)</cell></row><row><cell>Baseline [44]</cell><cell></cell><cell></cell><cell>88.8</cell><cell>71.6</cell></row><row><cell cols="2">Baseline + RK [24]</cell><cell></cell><cell>90.5</cell><cell>85.2</cell></row><row><cell cols="3">Baseline+ GST+LST(ours)</cell><cell>88.9</cell><cell>72.6</cell></row><row><cell cols="3">Baseline+ GST+LST+RK(ours)</cell><cell>91.2</cell><cell>86.8</cell></row><row><cell cols="5">Sketch ReID dataset Rnak-1(%) Rnak-5(%) Rank-10(%)</cell></row><row><cell>AFL [8]</cell><cell>34.0</cell><cell cols="2">56.3</cell><cell>72.5</cell></row><row><cell>GST+LST(ours)</cell><cell>42.5</cell><cell cols="2">70.0</cell><cell>87.5</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep learning for person reidentification: A survey and outlook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaojie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="issue">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A discriminatively learned cnn embedding for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="728" to="739" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Diverse part discovery: Occluded person re-identification with part-aware transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2898" to="2907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bicnet-tks: Learning efficient spatialtemporal representation for video person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibing</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingpeng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Farewell to mutual information: Variational distillation for cross-modal person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyun</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhuang</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1522" to="1531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Watching you: Global-guided reciprocal learning for video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13334" to="13343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Cross-domain adversarial feature learning for sketch re-identification. 1, 2, 6</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaowei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Yizhe Song, and Shaogang Gong. Fine-grained sketch-based image retrieval by matching deformable part models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Towards a principled integration of multi-camera reidentification and tracking through optimal bayes filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Breuers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaly</forename><surname>Kurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A discriminatively learned cnn embedding for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM transactions on multimedia computing, communications, and applications (TOMM)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Person reidentification using cnn features learned from combination of attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tetsu</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Einoshin</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 23rd international conference on pattern recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2428" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spherereid: Deep hypersphere manifold embedding for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengjuan</forename><surname>Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="51" to="58" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving person re-identification by attribute and identity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutian</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenggang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="151" to="161" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Person re-identification: Past, present and future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02984</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep representation learning with part loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hantao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2860" to="2871" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ranked list loss for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elyor</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Garnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil M</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5207" to="5216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gated siamese convolutional neural network architecture for human re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrinal</forename><surname>Rahul Rama Varior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Haloi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="791" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A siamese long short-term memory architecture for human re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Rahul Rama Varior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="135" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">End-to-end comparative attention networks for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meibin</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3492" to="3506" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Person re-identification by multi-channel parts-based cnn with improved triplet loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanping</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the iEEE conference on computer vision and pattern recognition</title>
		<meeting>the iEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1335" to="1344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reranking person re-identification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Joint discriminative and generative learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2138" to="2147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Person transfer gan to bridge domain gap for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Camera style adaptation for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5157" to="5166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Image-image domain adaptation with preserved self-similarity and domain-dissimilarity for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pose transferrable person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4099" to="4108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Posenormalized image generation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelin</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="650" to="667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hetero-center loss for cross-modality person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanxin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">386</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="97" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.01735</idno>
		<title level="m">Kecheng Zheng, and Zheng-Jun Zha. Modality-adaptive mixup and invariant decomposition for rgb-infrared person re-identification</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Visible thermal person re-identification via dual-constrained top-ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyuan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pong C</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Infrared-visible cross-modal person re-identification with an x modality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diangang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="4610" to="4617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13001" to="13008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Bag of tricks and a strong baseline for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youzhi</forename><surname>Hao Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenqi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Fastreid: A pytorch toolbox for general instance re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.02631,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Vehiclenet: Learning robust visual representation for vehicle re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<idno>2020. 3</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction on Multimedia</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A discriminatively learned cnn embedding for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications, and Applications (TOMM)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2197" to="2206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Invariance matters: Exemplar memory for domain adaptive person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="598" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Stargan: Unified generative adversarial networks for multi-domain image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjey</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minje</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaegul</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8789" to="8797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Generalizing a person retrieval model hetero-and homogeneously</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="172" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Domain adaptation through synthesis for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slawomir</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Francois</forename><surname>Lalonde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="189" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Person reidentification method based on color attack and joint defence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifei</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="4313" to="4322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Vulnerability of person re-identification models to metric adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>Bouniot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romaric</forename><surname>Audigier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelique</forename><surname>Loesch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="794" to="795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Metric attack and defense for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Transferable, controllable, and inconspicuous adversarial attacks on person re-identification with deep misranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongjun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="342" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Flipreid: Closing the gap between training and inference in person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyang</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 9th European Workshop on Visual Information Processing (EUVIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Benchmarks for corruption invariant person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.00880</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Detection model of occluded object based on yolo using hard-example mining and augmentation policy optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyung-Yong</forename><surname>Seong-Eun Ryu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chung</surname></persName>
		</author>
		<idno>2021. 3</idno>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page">7093</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Bag of tricks and a strong baseline for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youzhi</forename><surname>Hao Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenqi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Fastreid: a pytorch toolbox for general instance re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.02631</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="3754" to="3762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Pyramidal person re-identification via multi-loss dynamic training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongqiao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="8514" to="8522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Mixed highorder attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binghui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="371" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Circle loss: A unified perspective of pair similarity optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changmao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongdao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6398" to="6407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Relation-aware global attention for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ieee/cvf conference on computer vision and pattern recognition</title>
		<meeting>the ieee/cvf conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3186" to="3195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Salience-guided cascaded suppression network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuesong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canmiao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3300" to="3310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Adaptive l2 regularization in person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyang</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heikki</forename><surname>Huttunen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 25th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9601" to="9607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2579" to="2605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Deep spatial-semantic attention for finegrained sketch-based image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5551" to="5560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">An efficient framework for visible-infrared cross modality person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emrah</forename><surname>Basaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhittin</forename><surname>G?kmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><forename type="middle">E</forename><surname>Kamasak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">115933</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
