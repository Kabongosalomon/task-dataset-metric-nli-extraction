<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MOVIE: REVISITING MODULATED CONVOLUTIONS FOR VISUAL COUNTING AND BEYOND</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duy-Kien</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vedanuj</forename><surname>Goswami</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MOVIE: REVISITING MODULATED CONVOLUTIONS FOR VISUAL COUNTING AND BEYOND</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T11:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper focuses on visual counting, which aims to predict the number of occurrences given a natural image and a query (e.g. a question or a category). Unlike most prior works that use explicit, symbolic models which can be computationally expensive and limited in generalization, we propose a simple and effective alternative by revisiting modulated convolutions that fuse the query and the image locally. Following the design of residual bottleneck, we call our method MoVie, short for Modulated conVolutional bottlenecks. Notably, MoVie reasons implicitly and holistically and only needs a single forward-pass during inference. Nevertheless, MoVie showcases strong performance for counting: 1) advancing the state-of-the-art on counting-specific VQA tasks while being more efficient; 2) outperforming prior-art on difficult benchmarks like COCO for common object counting; 3) helped us secure the first place of 2020 VQA challenge when integrated as a module for 'number' related questions in generic VQA models. Finally, we show evidence that modulated convolutions such as MoVie can serve as a general mechanism for reasoning tasks beyond counting.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>We focus on visual counting: given a natural image and a query, it aims to predict the correct number of occurrences in the image corresponding to that query. The query is generic, which can be a natural language question (e.g. 'how many kids are on the sofa') or a category name (e.g. 'car'). Since visual counting requires open-ended query grounding and multiple steps of visual reasoning , it is a unique testbed to evaluate a machine's ability to understand multi-modal data.</p><p>Mimicking how humans count, most existing counting modules <ref type="bibr" target="#b42">Trott et al. (2018)</ref> adopt an intuitiondriven reasoning procedure, which performs counting iteratively by mapping candidate image regions to symbols and count them explicitly based on relationships <ref type="bibr">(Fig. 1,</ref>. While interpretable, modeling regions and relations repeatedly can be expensive in computation <ref type="bibr" target="#b23">Jiang et al. (2020)</ref>. And more importantly, counting is merely a single visual reasoning task -if we consider the full spectrum of reasoning tasks (e.g. logical inference, spatial configuration), it is probably infeasible to manually design specialized modules for every one of them <ref type="figure">(Fig. 1, bottom-left)</ref>.</p><p>In this paper, we aim to establish a simple and effective alternative for visual counting without explicit, symbolic reasoning. Our work is built on two research frontiers. First, on the synthetic CLEVR dataset , it was shown that using queries to directly modulate convolutions can lead to major improvements in the ConvNet's reasoning power (e.g. achieving near-perfect 94% accuracy on counting) <ref type="bibr" target="#b37">Perez et al. (2018)</ref>. However, it was difficult to transfer this finding to natural images, partially due to the dominance of bottom-up attention features that represent images with sets of regions <ref type="bibr" target="#b1">Anderson et al. (2018)</ref>. Interestingly, recent analysis discovered that plain convolutional feature maps can be as powerful as region features <ref type="bibr" target="#b23">Jiang et al. (2020)</ref>, which becomes a second step-stone for our approach to compare fairly against region-based counting modules.</p><p>Motivated by fusing multi-modalities locally for counting, the central idea behind our approach is to revisit convolutions modulated by query representations. Following ResNet <ref type="bibr" target="#b17">He et al. (2016)</ref>, we choose bottleneck as our basic building block, with each bottleneck being modulated once. Multiple bottlenecks are stacked together to form our final module. Therefore, we call our method MoVie: Modulated conVolutional bottlenecks. Inference for MoVie is performed by a simple, feed-forward pass holistically on the feature map, and reasoning is done implicitly <ref type="figure">(Fig. 1, top-right)</ref>.   <ref type="figure">Figure 1</ref>: We study visual counting. Different from previous works that perform explicit, symbolic counting (left), we propose an implicit, holistic counter, MoVie, that directly modulates convolutions (right) and can outperform state-of-the-art methods on multiple benchmarks. Its simple design also allows potential generalization beyond counting to other visual reasoning tasks (bottom).</p><p>MoVie demonstrates strong performance. First, it improves the state-of-the-art on several VQAbased counting benchmarks (HowMany-QA <ref type="bibr" target="#b42">Trott et al. (2018)</ref> and TallyQA <ref type="bibr" target="#b0">Acharya et al. (2019)</ref>), while being more efficient. It also works well on counting common objects, significantly outperforming all previous approaches on challenging datasets like <ref type="bibr">COCO Lin et al. (2014)</ref>. Furthermore, we show MoVie can be easily plugged into generic VQA models and improve the 'number' category on VQA 2.0 <ref type="bibr" target="#b16">Goyal et al. (2017)</ref> -and with the help of MoVie, we won the first place of 2020 VQA challenge. To better understand this implicit model, we present detailed ablative analysis and visualizations of MoVie, and notably find it improves upon its predecessor FiLM <ref type="bibr" target="#b37">Perez et al. (2018)</ref> across all the counting benchmarks we experimented, with a similar computation cost.</p><p>Finally, we validate the feasibility of MoVie for reasoning tasks beyond counting <ref type="figure">(Fig. 1</ref>, bottomright) by its near-perfect accuracy on CLEVR and competitive results on GQA Hudson &amp; Manning (2019a). These evidences suggest that modulated convolutions such as MoVie can potentially serve as a general mechanism for visual reasoning. Code will be made available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Here we discuss related works to the counting module, see works related to the task in appendix.</p><p>Explicit counting/reasoning modules. <ref type="bibr" target="#b42">Trott et al. (2018)</ref> was among the first to treat counting differently from other types of questions, and cast the task as a sequential decision making problem optimized by reinforcement learning. A similar argument for distinction was presented in , which took a step further by showing their fully-differentiable method can be attached to generic VQA models as a module. However, the idea of modular design for VQA was not new -notably, several seminal works <ref type="bibr" target="#b2">Andreas et al. (2016)</ref>; <ref type="bibr" target="#b19">Hu et al. (2017)</ref> have described learnable procedures to construct networks for visual reasoning, with reusable modules optimized for particular capabilities (e.g. count, compare). Our work differs from such works in philosophy, as they put more emphasis (and likely bias) on interpretation whereas we seek data-driven, general-purpose components for visual reasoning.</p><p>Implicit reasoning modules. Besides modulated convolutions <ref type="bibr" target="#b37">Perez et al. (2018);</ref><ref type="bibr" target="#b10">De Vries et al. (2017)</ref>, another notable work is Relation Network <ref type="bibr" target="#b40">Santoro et al. (2017)</ref>, which learns to represent pair-wise relationships between features from different locations through simple MLPs, and showcases super-human performance on CLEVR. The counter from TallyQA <ref type="bibr" target="#b0">Acharya et al. (2019)</ref> followed this idea and built two such networks -one among foreground regions, and one between foreground and background. However, their counter is still based on regions, and neither generalization as a VQA module nor to other counting/reasoning tasks is shown.</p><p>Because existing VQA benchmarks like VQA 2.0 also include counting questions, generic VQA models <ref type="bibr" target="#b14">Fukui et al. (2016)</ref>; <ref type="bibr" target="#b49">Yu et al. (2019)</ref> without explicit counters also fall within the scope of 'implicit' ones when it comes to counting. However, a key distinction of MoVie is that we fuse multi-modal information locally, which will be covered in detail in Sec. 3.1 next. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">COUNTING WITH MODULATIONS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MODULATED CONVNET</head><p>Motivation. Why choosing modulated convolutions for counting? Apart from empirical evidence on synthetic dataset <ref type="bibr" target="#b37">Perez et al. (2018)</ref>, we believe the fundamental motivation lies in the convolution operation itself. Since ConvNets operate on feature maps with spatial dimensions (height and width), the extra modulation -in our case the query representation -is expected to be applied densely to all locations of the map in a fully-convolutional manner. This likely suits visual counting well for at least two reasons. First, counting (like object detection) is a translation-equivariant problem: given a fixed-sized local window, the outcome changes as the input location changes. Therefore, a local fusion scheme like modulated convolutions is more preferred compared to existing fusion schemes <ref type="bibr" target="#b14">Fukui et al. (2016)</ref>; <ref type="bibr" target="#b48">Yu et al. (2017)</ref>, which are typically applied after visual features are pooled into a single global vector. Second, counting requires exhaustive search over all possible locations, which puts the dominating bottom-up attention features <ref type="bibr" target="#b1">Anderson et al. (2018)</ref> that sparsely sample image regions at a disadvantage in recall, compared to convolutional features that output responses for each and every location.</p><p>Pipeline and module. In <ref type="figure" target="#fig_0">Fig. 2</ref> (a) we show the overall pipeline. The output convolutional features from a standard ConvNet (e.g. ResNet) are fed into the MoVie module at the top. <ref type="bibr">1</ref> The module consists of four modulated convolutional bottlenecks ( <ref type="figure" target="#fig_0">Fig. 2 (b)</ref>). Each bottleneck receives the query as an extra input to modulate the feature map and outputs another same-sized feature map. The final output after several stages of local fusion is average pooled and fed into a two-layer MLP classifier (with ReLU) to predict the answer. Note that we do not apply fusion between query and the global pooled vector: all the interactions between query and image occur in modulated bottlenecks locally.</p><p>Bottleneck. As depicted in <ref type="figure" target="#fig_0">Fig. 2</ref> (c), MoVie closely follows the original ResNet bottleneck design, which is both lightweight and effective for learning visual representations <ref type="bibr" target="#b17">He et al. (2016)</ref>. The only change is that: before the first 1?1 convolution layer, we insert a modulation block that takes the query representation as a side information to modulate the feature maps, which we detail next.</p><p>Modulation. We start with Feature-wise Linear Modulation (FiLM) <ref type="bibr" target="#b37">Perez et al. (2018)</ref> and introduce notations. Since the modulation operation is the same across all feature vectors, we simplify by just focusing on one single vector v?R C on the feature map, where C is the channel size. FiLM modulates v with linear transformations per channel and the outputv FiLM is:</p><formula xml:id="formula_0">v FiLM = (v ? ?) ? ?,<label>(1)</label></formula><p>where ? is element-wise multiplication, ? is element-wise addition (same as normal vector addition). Intuitively, ??R C scales the feature vector and ??R C does shifting. Both ? and ? are conditioned on the query representation q?R D through FC weights {W ? , W ? }?R D?C .</p><p>One crucial detail that stabilizes FiLM training is to predict the difference ?? rather than ? itself, where ?=1??? and 1 is an all-one vector. This essentially creates a residual connection, as:</p><formula xml:id="formula_1">v FiLM = [v ? (1 ? ??)] ? ? = v ? [(v ? ??) ? ?] .</formula><p>We can then view F FiLM (v???)?? as a residual function for modulation, conditioned jointly on v and q and will be added back to v. This perspective creates opportunities for us to explore other, potentially better forms of F(v, q) for counting.</p><p>The modulation block for MoVie is shown in <ref type="figure" target="#fig_0">Fig. 2 (d)</ref>. The modulation function is defined as:</p><formula xml:id="formula_2">F MoVie W T (v???)</formula><p>where W?R C?C is a learnable weight matrix that can be easily converted to a 1?1 convolution in the network. Intuitively, instead of using the output of v??? directly, this weight matrix learns to output C inner products between v and ?? weighted individually by each column in W. Such increased richness allows the model to potentially capture more intricate relationships. Our final formulation for MoVie is:</p><formula xml:id="formula_3">v MoVie = v ? W T (v ? ??).</formula><p>(2)</p><p>Note that we also removed ? and thus saved the need for W ? . Compared to FiLM, the parameter count of MoVie can be fewer, depending on the relative size of channel C and query dimension D.</p><p>Scale robustness. One concern on using convolutional feature maps directly for counting is on its sensitiveness to input image scales, as ConvNets are fundamentally not scale-invariant. Regionbased counting models <ref type="bibr" target="#b42">Trott et al. (2018)</ref>;  tend to be more robust to scale changes, as their features are computed on fixed-shaped (e.g. 7?7) features regardless of the sizes of the corresponding bounding boxes in pixels <ref type="bibr" target="#b39">Ren et al. (2015)</ref>. We find two implementation details helpful to remedy this issue. First is to always keep the input size fixed. Given an image, we resize and pad it to a global maximum size, rather than maximum size within the batch (which can vary based on aspect-ratios of sampled images). Second, we employ multi-scale training, by uniformly sampling the target image size from a pre-defined set (e.g. shorter size 800 pixels). Note that input size and image size can be decoupled, and the gap is filled by zero-padding.</p><p>Query representation. We consider two types of queries: questions <ref type="bibr" target="#b42">Trott et al. (2018)</ref> or class names <ref type="bibr" target="#b6">Chattopadhyay et al. (2017)</ref>. We map either into a single vector q, see appendix for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MOVIE AS A COUNTING MODULE</head><p>An important property of a question-based visual counter is to see if it can be integrated as a module for generic VQA models . At a high-level, state-of-the-art VQA models <ref type="bibr" target="#b24">Jiang et al. (2018)</ref>; <ref type="bibr" target="#b49">Yu et al. (2019)</ref> follow a common paradigm: Obtain an image representation i?R C and a question representation q?R D , then apply a fusion scheme (e.g. <ref type="bibr" target="#b14">Fukui et al. (2016)</ref>) to produce the final answer a. A VQA loss L against ground truth? is computed to train the model ( <ref type="figure">Fig. 3 (a)</ref>).</p><p>While a na?ve approach that directly appends pooled features v?R C from MoVie to i ( <ref type="figure">Fig. 3 (b</ref>   . We also experimented other variants ofv MoVie by switching on/off ? and the residual connection (?). The results indicate that: 1) The added linear mapping (W) is helpful for counting -all MoVie variants outperform original FiLM; 2) the residual connection also plays a role and benefits accuracy; 3) With W and ?, ? is less essential for counting performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of bottlenecks.</head><p>We then varied the number of modulated bottlenecks, with results shown in <ref type="table" target="#tab_2">Table 1b</ref>. We find the performance saturates around 4, but stacking multiple bottlenecks is useful compared to using a single one. We observe the same trend with FiLM but with weaker performance.  <ref type="formula" target="#formula_0">(2017)</ref>, we can surpass all the previous models by a large margin e.g. ?4% in absolute accuracy on HowMany-QA. The same also holds on TallyQA, where we achieve better performance for both simple and complex questions. Note we also have lower parameter counts and FLOPs.</p><p>Visualization. We visualize the activation maps produced by the last modulated bottleneck for several complex questions in TallyQA <ref type="figure">(Fig. 4)</ref>. Specifically, we compute the normalized 2 -norm map per-location on the feature map <ref type="bibr" target="#b32">Malinowski et al. (2018)</ref>. The attention on the question is also visualized by using attention weights Nguyen &amp; Okatani (2018) from the question encoder. First two rows give successful examples. On the left, MoVie is able to focus on the relevant portions of the image to produce the correct count number, and can extract key words from the question. We further modified the questions to be more general (right), and even with a larger number to count, MoVie can give the right answers. Four failure cases are shown in the last two rows, where the model either fails to predict the correct answer, or produces wrong attention maps to begin with.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">BEYOND COUNTING</head><p>Finally, to explore the capability of our model beyond counting, we evaluate MoVie on the CLEVR dataset . We train our model with a ImageNet pre-trained ResNet-101 for 45 epochs, and report a (near-perfect) test set accuracy of 97.42% -similar to the observation made in FiLM. This suggests it is the general idea of modulated convolutions that helped achieving strong performance on CLEVR, rather than specific forms presented in <ref type="bibr" target="#b37">Perez et al. (2018)</ref>.</p><p>Since CLEVR is synthetic, we also initiate an exploration of MoVie on the recent natural-image reasoning dataset, GQA Hudson &amp; Manning (2019a). We use ResNeXt-101 from VG and report competitive results in Tab. 5 without using extra supervisions like scene-graph <ref type="bibr" target="#b28">Krishna et al. (2017)</ref>. Despite simpler architecture compared to models like MAC Hudson &amp; Manning (2018), we demonstrate better overall accuracy with a larger gain on open questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we propose a simple and effective model named MoVie for visual counting by revisiting modulated convolutions that fuse queries to images locally. Different from previous works that perform explicit, symbolic reasoning, counting is done implicitly and holistically in MoVie and only needs a single forward-pass during inference. We significantly outperform state-of-the-arts on three major benchmarks in visual counting, namely HowMany-QA, Tally-QA and COCO; and show that MoVie can be easily incorporated as a module for general VQA models like MCAN to improve accuracy on 'number' related questions on VQA 2.0. The strong performance helped us secure the first place in the 2020 VQA challenge. Finally, we show MoVie can be directly extended to perform well on datasets like CLEVR and GQA, suggesting modulated convolutions as a general mechanism can be useful for other reasoning tasks beyond counting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A RELATED WORK FOR COUNTING TASKS</head><p>Specialized counting. Counting for specialized objects has a number of practical applications <ref type="bibr" target="#b33">Marsden et al. (2018)</ref>, including but are not limited to cell counting <ref type="bibr" target="#b47">Xie et al. (2018)</ref>, crowd counting Sindagi &amp; Patel (2017), vehicle counting <ref type="bibr" target="#b35">Onoro-Rubio &amp; L?pez-Sastre (2016)</ref>, wild-life counting <ref type="bibr" target="#b4">Arteta et al. (2016)</ref>, etc. While less general, they are important computer vision applications to respective domains, e.g. medical and surveillance. Standard convolution filters are used extensively in state-of-the-art models <ref type="bibr" target="#b7">Cheng et al. (2019)</ref> to produce density maps that approximate the target count number in a local neighborhood. However, such models are designed to deal exclusively with a single category of interest, and usually require point-level supervision <ref type="bibr" target="#b41">Sindagi &amp; Patel (2017)</ref> in addition to the ground-truth overall count number for training.</p><p>Another line of work on specialized counting is psychology-inspired <ref type="bibr" target="#b9">Cutini &amp; Bonato (2012)</ref>, which focuses on the phenomenon coined 'subitizing' <ref type="bibr" target="#b26">Kaufman et al. (1949)</ref>, that humans and animals can immediately tell the number of salient objects in a scene using holistic cues <ref type="bibr" target="#b50">Zhang et al. (2015)</ref>. It is specialized because the number of objects is usually limited to be small (e.g. up to 4 <ref type="bibr" target="#b50">Zhang et al. (2015)</ref>).</p><p>General visual counting. Lifting the restrictions of counting one category or a few items at a time, more general task settings for counting have been introduced. Generalizing to multiple semantic classes and more instances, common object counting <ref type="bibr" target="#b6">Chattopadhyay et al. (2017)</ref> as a task has been explored with a variety of strategies such as detection <ref type="bibr" target="#b39">Ren et al. (2015)</ref>, ensembling <ref type="bibr" target="#b15">Galton (1907</ref><ref type="bibr">), or segmentation Laradji et al. (2018</ref>. The most recent development in this direction <ref type="bibr" target="#b8">Cholakkal et al. (2019)</ref> also adopts a density map based approach, achieving state-of-the-art with weak, image-level supervision alone. Even more general is the setting of open-ended counting, where the counting target is expressed in natural language questions <ref type="bibr" target="#b0">Acharya et al. (2019)</ref>. This allows more advanced 'reasoning' task to be formulated involving objects, attributes, relationships, and more. Our module is designed for these general counting tasks, with the modulation coming from either a question or a class embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B IMPLEMENTATION DETAILS</head><p>In our experiments, query representations q?R D have two types: questions and categories.</p><p>Question representation. We use LSTM and Self Attention (SA) layers <ref type="bibr" target="#b43">Vaswani et al. (2017)</ref> for question encoding <ref type="bibr" target="#b49">Yu et al. (2019)</ref>. It was shown in Natural Language Processing (NLP) research that adding SA layers helps to produce informative and discriminative language representations <ref type="bibr" target="#b12">Devlin et al. (2019)</ref>; and we also empirically observe better results (0.7% improvement in accuracy and 0.1 reduction in RMSE according to our analysis on HowMany-QA <ref type="bibr" target="#b42">Trott et al. (2018)</ref> val set). Specifically, a question (or sentence in NLP) consisting of N words is first converted into a sequence Q 0 ={w 0 1 , . . . , w 0 N } of N 300-dim GloVe word embeddings <ref type="bibr" target="#b36">Pennington et al. (2014)</ref>, which are then fed into a one-directional LSTM followed by a stacked of L=4 layers of self attention:</p><formula xml:id="formula_4">Q 1 = ? ??? ? LSTM(Q 0 ),<label>(3)</label></formula><formula xml:id="formula_5">Q l = SA l (Q l?1 ),<label>(4)</label></formula><p>where Q l ={w l 1 , . . . , w l N }, l?{2, . . . , L + 1} is the D=512 dimensional embedding for each word in the question after the (l?1)-th SA layer. We cap all the questions to the same length N as in common practice and pad all-zero vectors to shorter questions Nguyen &amp; Okatani (2018).</p><p>Our design for the self-attention layer closely follows <ref type="bibr" target="#b43">Vaswani et al. (2017)</ref> and uses multi-head attentions (h=8 heads) with each head having D k =D/h dimensions and attend with a separate set of keys, queries, and values. Layer normalization and feed-forward network are included without position embeddings.</p><p>Given the final Q L+1 , to get the conditional vector q, we resort to a summary attention mechanism <ref type="bibr" target="#b34">Nguyen &amp; Okatani (2018)</ref>. A two-layer 512-dim MLP with ReLU non-linearity is applied to compute an attention score s n for each word representation w L+1 N . We normalize all scores by softmax to derive attention weights ? n and then compute an aggregated representation q via a weighted summation over Q L+1 .  Object detection based counting. We train a Faster R-CNN <ref type="bibr" target="#b39">Ren et al. (2015)</ref> with feature pyramid networks <ref type="bibr" target="#b31">Lin et al. (2017)</ref> using the latest implementation on Detectron2 . For fair comparison, we also use a ResNet-50 backbone <ref type="bibr" target="#b17">He et al. (2016)</ref> pre-trained on ImageNet, the same for our counting module. The detector is trained on the train2014 split of COCO images, which is referred as the train set for common object counting <ref type="bibr" target="#b6">Chattopadhyay et al. (2017)</ref> </p><formula xml:id="formula_6">RMSE = 1 M M i=1 (? i ? c i ) 2 ,<label>(5)</label></formula><p>where? i is ground-truth, c i is prediction, and N is the number of examples. Focusing more on non-zero counts, RMSE-nz tries to evaluate a model's counting ability on harder examples where the answer is at least one:</p><formula xml:id="formula_7">RMSE-nz = 1 M nz i?{i|?i&gt;0} (? i ? c i ) 2 ,<label>(6)</label></formula><p>where M nz is the number of examples where ground-truth is non-zero. To penalize the mistakes when the count number is small (as making a mistake of 1 when the ground-truth is 2 is more serious than when the ground-truth is 100), rel-RMSE is proposed as:   Specifically, suppose a fused representation in the joint branch is denoted as o=f (i+v), where i is from the VQA model, v is from MoVie, and f (?) is the function consisting of layers applied after the features are summed up. We can compute two variants of this representation o: one without v: o ?v =f (i), and one without i: o ?i =f (v). The similarity score is then computed between two pairs via dot-product: p i =o T o ?v and p v =o T o ?i . Given one question, we assign a score of 1 to MoVie if p i &gt;p v , and otherwise 0. The scores within each question type are then averaged, and produces the probability of how MoVie is chosen over the base VQA model for that particular question type.</p><formula xml:id="formula_8">rel-RMSE = 1 M M i=1 (? i ? c i ) 2 c i + 1 .<label>(7)</label></formula><p>We take two models as examples. One is MCAN-Small Yu et al. (2019) + MoVie (three-branch), and the other one replaces MCAN-Small with Pythia <ref type="bibr" target="#b24">Jiang et al. (2018)</ref>. The visualizations are shown in <ref type="figure">Fig. 5 and Fig. 6</ref>, respectively. We sort the question types based on how much MoVie has contributed, i.e. the 'probability'. Some observations: <ref type="figure">Figure 5</ref>: Visualization of where MoVie helps MCAN for different question types on VQA 2.0 val set. We compute the probability by assigning each question to MoVie based on similarity scores (see Sec. F for detailed explanations). The top contributed question types are counting related, confirming that state-of-the-art VQA models that perform global fusion are not ideally designed for counting, and the value of MoVie with local fusion. Best viewed on a computer screen with zoom. <ref type="figure">Figure 6</ref>: Similar to <ref type="figure">Fig. 5</ref> but with Pythia. Best viewed on a computer screen with zoom.</p><p>? MoVie shows significant contribution in the counting questions for both MCAN and Pythia: the top three question types are consistently 'how many people are in', 'how many people are', and 'how many', this evidence strongly suggests that existing models that fuse features globally between vision and language are not well suited for counting questions, and confirms the value of incorporating MoVie (that performs fusion locally) as a counting module for generic VQA models;</p><p>? The 'Yes/No' questions are likely benefited from MoVie as well, since the contribution of MoVie spreads in several question types belong to that category (e.g. 'are', 'are they', 'do', etc.) -this maybe because counting also includes 'verification-of-existing' questions such as 'are there people wearing hats in the image';</p><p>? For Pythia, we also find it likely helps 'color' related questions (e.g. 'what color are the', 'what color', etc.) and some other types -this strengthens our exploration that our model contributes beyond counting capability.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Question 2 :</head><label>2</label><figDesc>[spatial reasoning]   what is the animal in the front doing?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Overview of MoVie which can be placed on top of any convolutional features. The module consists of several modulated convolutional bottlenecks. Each bottleneck is a simple modified version of the residual bottleneck, with an additional modulation block before the first convolution. The output of the module is average pooled and fed into a two-layer MLP for the final answer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Question 1:[counting]   how many rhinos are there?</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>implicit</cell></row><row><cell></cell><cell></cell><cell></cell><cell>holistic counter</cell><cell>2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>[this work]</cell></row><row><cell></cell><cell></cell><cell cols="2">convolutional feature map</cell></row><row><cell>Image</cell><cell>region relations</cell><cell>Image</cell></row><row><cell></cell><cell>transfer?</cell><cell></cell><cell>transfer?</cell></row><row><cell></cell><cell></cell><cell>Question 2: [spatial reasoning] what is the animal in the front doing?</cell><cell>eating</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>)) likely suffers from feature co-adaptation<ref type="bibr" target="#b18">Hinton et al. (2012)</ref>, we follow a three-branch training scheme<ref type="bibr" target="#b44">Wang et al. (2019);</ref><ref type="bibr" target="#b38">Qi et al. (2020)</ref> by adding two auxiliary, training-only branches: One trains the original VQA model just with i, and the other trains a normal MoVie with v and MLP. All three branches are assigned an equal loss weight. The fusion parameters for i and the joint branch (i+v) are not shared. This setup forces the network to learn powerful representations within i and v, as they have to separately minimize the VQA loss. During testing, we only use the joint branch, leading to significant improvements especially on 'number' related questions for VQA without sacrificing inference speed(Fig. 3 (c)).MoVie as a counting module for VQA. (a) A high-level overview of the current VQA systems, image i and question q are fused to predict the answer a. (b) A na?ve approach to include MoVie as a counting module: directly add pooled features v (with one FC to match dimensions) to i. (c) Our final design to train with two auxiliary losses on i and v, while during testing only using the joint branch a j . Shaded areas are used for both train and test; white areas are train-only. MoVie vs. FiLM. ?: residual connection. Fixed vs. batch-dependent input size. Multi-scale vs. single-scale training.</figDesc><table><row><cell>, %</cell><cell>, %</cell><cell>, %</cell><cell>, %</cell><cell>, %</cell></row><row><cell>fuse</cell><cell>fuse</cell><cell>fuse</cell><cell>fuse</cell><cell>MLP</cell></row><row><cell></cell><cell>+</cell><cell></cell><cell>+</cell><cell></cell></row><row><cell>(a)</cell><cell>(b)</cell><cell></cell><cell>(c)</cell><cell></cell></row><row><cell cols="5">Figure 3: Design vFiLM vMoVie vMoVie variants (a) # bottlenecks ? ? ACC ? RMSE ? 57.1 2.67 58.4 2.60 58.2 2.63 57.9 2.64 58.5 2.63 ACC ? RMSE ? 1 57.2 2.65 2 58.0 2.63 3 58.2 2.62 4 58.4 2.60 5 58.5 2.63 (b) Number of modulated bottlenecks.</cell></row><row><cell cols="3">Fixed input size Test size ACC ? RMSE ? 400 22.1 4.70 600 36.0 3.16 800 56.2 2.68 400 53.9 2.92 600 57.3 2.69 800 58.4 2.60 (c) Train size 800 {400,600,800} (d)</cell><cell cols="2">Test size ACC ? RMSE ? 400 53.9 2.92 600 57.3 2.69 800 58.4 2.60 400 56.5 2.78 600 58.8 2.66 800 58.8 2.59</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Ablative analysis on HowMany-QA val set. MoVie modulation design outperforms FiLM under fair comparisons; both fixing input size and multi-scale training improve robustness to scales.4 EXPERIMENTSWe conduct a series of experiments to validate the effectiveness of MoVie. By default, we use Adam Kingma &amp; Ba (2015) optimizer, with batch size 128 and base learning rate 1e ?4 ; momentum 0.9 and 0.98. We start training by linearly warming up learning rate from 2.5e ?5 for 3 epochs<ref type="bibr" target="#b49">Yu et al. (2019)</ref>. The rate is decayed by 0.1? after 10 epochs and we finish training after 13 epochs.4.1 VISUAL COUNTINGFor visual counting, we have two tasks: open-ended counting with question queries where we ablated our design choices, and counting common object with class queries.Open-ended counting benchmarks. Two datasets are used to counting with question queries. First is HowMany-QA Trott et al. (2018)  where the train set questions are extracted from VQA 2.0 train and Visual Genome (VG)<ref type="bibr" target="#b28">Krishna et al. (2017)</ref>. The val and test sets are taken from VQA 2.RMSE ? ACC ? RMSE ? ACC ? RMSE ?</figDesc><table><row><cell cols="10">Method ACC ? MUTAN (2017) #params FLOPs HowMany-QA TallyQA-Simple TallyQA-Complex Backbone (M) (G) R-152 60.2 -45.5 2.93 56.5 1.51 49.1 1.59</cell></row><row><cell>Count module (2018)</cell><cell>R-101</cell><cell>44.6</cell><cell>-</cell><cell>54.7</cell><cell>2.59</cell><cell>70.5</cell><cell>1.15</cell><cell>50.9</cell><cell>1.58</cell></row><row><cell>IRLC (2018)</cell><cell>R-101</cell><cell>44.6</cell><cell>-</cell><cell>56.1</cell><cell>2.45</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TallyQA (2019)</cell><cell cols="4">R-101+152 104.8 1883.5 60.3</cell><cell>2.35</cell><cell>71.8</cell><cell>1.13</cell><cell>56.2</cell><cell>1.43</cell></row><row><cell>TallyQA (FG-Only)</cell><cell>R-101</cell><cell cols="2">44.6 1790.9</cell><cell>-</cell><cell>-</cell><cell>69.4</cell><cell>1.18</cell><cell>51.8</cell><cell>1.50</cell></row><row><cell>MoVie</cell><cell>R-50</cell><cell>25.6</cell><cell cols="2">176.1 61.2</cell><cell>2.36</cell><cell>70.8</cell><cell>1.09</cell><cell>54.1</cell><cell>1.52</cell></row><row><cell>MoVie</cell><cell>X-101</cell><cell>88.8</cell><cell cols="2">706.3 64.0</cell><cell>2.30</cell><cell>74.9</cell><cell>1.00</cell><cell>56.8</cell><cell>1.43</cell></row></table><note>0 val set. Each ground-truth answer is a number between 0 to 20. Extending HowMany-QA, the TallyQA Acharya et al. (2019) dataset augments the train set by adding synthetic counting questions automatically generated from COCO annotations. They also split the test set into two parts: test- simple and test-complex, based on whether the question requires advanced reasoning capability. The answers range between 0 and 15. For both datasets, accuracy (ACC, higher-better) and standard RMSE (lower-better) Acharya et al. (2019) are metrics used for evaluation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Open-ended counting on Howmany-QA and TallyQA test set. MoVie outperforms prior arts with lower parameter counts and FLOPs. X: ResNeXt<ref type="bibr" target="#b46">Xie et al. (2017)</ref>.</figDesc><table /><note>We first conduct ablative analysis on HowMany-QA for important design choices in MoVie. Here, we use fixed ResNet-50 features pre-trained on VG Jiang et al. (2020), and train MoVie on HowMany-QA train, evaluate on val. The results are summarized in Tab. 1. Modulation design. In Tab. 1a, we compare MoVie design with FiLM Perez et al. (2018): All other settings (e.g. the number of bottlenecks) are fixed and onlyv MoVie is replaced withv FiLM in our module (see Eq. 1 and 2)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Visualizations of attention maps on images and questions for several complex examples in TallyQA. First two rows show successful cases, and last two shows four failure ones. Best viewed in color on a computer screen. See text for detailed explanations.</figDesc><table><row><cell cols="3">How many red trains are in the picture</cell><cell cols="3">How many trains are in the picture</cell></row><row><cell cols="2">Pred: 1, Ans: 1</cell><cell></cell><cell></cell><cell>Pred: 2, Ans: 2</cell></row><row><cell cols="3">How many animals are laying down</cell><cell></cell><cell cols="2">How many animals are there</cell></row><row><cell cols="2">Pred: 1, Ans: 1</cell><cell></cell><cell></cell><cell>Pred: 8, Ans: 8</cell></row><row><cell cols="3">How many of the birds are in the air</cell><cell cols="3">How many road sign poles are there</cell></row><row><cell cols="2">Pred: 2, Ans: 1</cell><cell></cell><cell></cell><cell>Pred: 2, Ans: 3</cell></row><row><cell cols="2">How many of the pillows are gray</cell><cell></cell><cell cols="3">How many seals are on the bench</cell></row><row><cell cols="2">Pred: 2, Ans: 0</cell><cell></cell><cell></cell><cell>Pred: 0, Ans: 1</cell></row><row><cell>Figure 4: Method</cell><cell>Instance supervision</cell><cell cols="4">RMSE ? RMSE-nz ? rel-RMSE ? rel-RMSE-nz ?</cell></row><row><cell>LC-ResFCN (2018)</cell><cell></cell><cell>0.38</cell><cell>2.20</cell><cell>0.19</cell><cell>0.99</cell></row><row><cell>glance-noft-2L (2017)</cell><cell></cell><cell>0.42</cell><cell>2.25</cell><cell>0.23</cell><cell>0.91</cell></row><row><cell>CountSeg (2019)</cell><cell></cell><cell>0.34</cell><cell>1.89</cell><cell>0.18</cell><cell>0.84</cell></row><row><cell>Faster R-CNN (2019)</cell><cell></cell><cell>0.35</cell><cell>1.88</cell><cell>0.18</cell><cell>0.80</cell></row><row><cell>MoVie</cell><cell></cell><cell>0.30</cell><cell>1.49</cell><cell>0.19</cell><cell>0.67</cell></row></table><note>Common object counting. Second, we apply MoVie to common object counting, where the query is an object category. Following standard practice, we choose ResNet-50 pre-trained on Ima- geNet Deng et al. (2009) as backbone which is also fine-tuned. Due to the skewed distribution between zero and non-zero answers, we perform balanced sampling during training.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Common object counting on COCO test with various RMSE metrics. MoVie outperforms prior state-of-the-arts without instance-level supervision.Results onCOCO Lin et al. (2014)  are summarized in Tab. 3, where we compared against not only the state-of-the-art counting approach CountSeg<ref type="bibr" target="#b8">Cholakkal et al. (2019)</ref>, but also the latest improved version of Faster R-CNN. In addition to RMSE, three variants are introduced in<ref type="bibr" target="#b6">Chattopadhyay et al. (2017)</ref> with different focuses. MoVie outperforms all the methods on three metrics and comes as a close second for the remaining one without using any instance-level supervision. For more details and results on Pascal VOC<ref type="bibr" target="#b13">Everingham et al. (2015)</ref>, see appendix.</figDesc><table><row><cell>MoVie</cell><cell></cell><cell>82.48</cell><cell>49.26</cell><cell>54.77</cell><cell>64.46</cell></row><row><cell>MCAN-Small (2019) MCAN-Small + MoVie (na?ve fusion)</cell><cell>val</cell><cell>83.59 83.25</cell><cell>46.71 49.36</cell><cell>57.34 57.18</cell><cell>65.81 65.95</cell></row><row><cell>MCAN-Small + MoVie</cell><cell></cell><cell>84.01</cell><cell>50.45</cell><cell>57.87</cell><cell>66.72</cell></row><row><cell>MCAN-Large + X-101 (2020) MCAN-Large + X-101 + MoVie</cell><cell>test-dev</cell><cell>88.46 88.39</cell><cell>55.68 57.05</cell><cell>62.85 63.28</cell><cell>72.59 72.91</cell></row></table><note>4.2 VISUAL QUESTION ANSWERING Next, we explore MoVie as a counting module for generic VQA models. We use choose MCAN Yu et al. (2019), the 2019 VQA challenge winner, as our target model (see appendix for Pythia Jiang et al. (2018), the 2018 winner). For fair comparisons, we only use single-scale training, and adopt the learning rate schedule from MCAN to train MoVie using fixed ResNet-50 features.Method Test set Yes/No ? Number ? Other ? Overall ?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Top: We study different ways to incorporate MoVie as a counting module for generic VQA models. Bottom: we show consistent improvement especially for 'number' questions on test-dev.</figDesc><table><row><cell cols="7">Method CNN+LSTM BottomUp (2018) MAC (2018) NSM* (2019b) MoVie Humans</cell></row><row><cell>Overall ?</cell><cell>46.6</cell><cell>49.7</cell><cell>54.1</cell><cell>63.2</cell><cell>57.1</cell><cell>89.3</cell></row><row><cell>Binary ?</cell><cell>63.3</cell><cell>66.6</cell><cell>71.2</cell><cell>78.9</cell><cell>73.5</cell><cell>91.2</cell></row><row><cell>Open ?</cell><cell>31.8</cell><cell>34.8</cell><cell>38.9</cell><cell>49.3</cell><cell>42.7</cell><cell>87.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Reasoning on GQA test set to show the generalization of MoVie beyond counting (*: uses scene-graph supervision<ref type="bibr" target="#b28">Krishna et al. (2017)</ref>). We also report 97.42% on CLEVR test set. the bottom section of Tab. 4, we further verified the effectiveness of MoVie and the fusion scheme on the test-dev split of VQA 2.0, switching the base VQA model to MCAN-Large, the backbone to ResNeXt-101, and using more data (VQA 2.0 train+val and VG) for training. We find MoVie consistently strengthens the counting ability. Thanks to MoVie, our entry won the first place of 2020 VQA challenge, achieving a test-std score of 76.29 and test-challenge 76.36.</figDesc><table><row><cell>In</cell></row></table><note>The top section of Tab. 4 shows our analysis of different designs to incorporate MoVie into MCAN- Small (Sec. 3.2). We train all models on VQA 2.0 train and report the breakdown scores on val. Trained individually, MoVie outperforms MCAN in 'number' questions by a decent margin, but lags behind in other questions. Directly adding features from MoVie (na?ve fusion) shows limited im- provement, while our final three-branch fusion scheme is much more effective: increasing accuracy on all types with a strong emphasis on 'number' while keeping added cost minimum (see appendix).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Common object counting on VOC test set with various RMSE metrics.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>And finally, rel-RMSE-nz is used to calculate the relative RMSE for non-zero examples -both challenging and aligned with human perception.D COMMON OBJECTS COUNTING ON VOCAs mentioned in Sec. 4.1 of the main paper, we present the performance of MoVie on test split of Pascal VOC counting dataset in Tab. 6. Different from COCO, the VOC dataset is much smaller with 20 object categories<ref type="bibr" target="#b13">Everingham et al. (2015)</ref>. We can see that MoVie achieves comparable results to the state-of-the-art method, CountSeg<ref type="bibr" target="#b8">Cholakkal et al. (2019)</ref> in two relative metrics (rel-RMSE and rel-RMSE-nz) and falls behind in RMSE and RMSE-nz. In contrast, as shown in Tab. 3,</figDesc><table><row><cell></cell><cell cols="4"># Train params (M) # Test params (M) Train mem (G) Train speed (s/iter)</cell></row><row><cell>MCAN-Large (2019)</cell><cell>218.2</cell><cell>218.2</cell><cell>10.5</cell><cell>0.84</cell></row><row><cell>MCAN-Large + MoVie</cell><cell>260.2</cell><cell>241.2</cell><cell>11.7</cell><cell>0.89</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>Adding MoVie as a module to MCAN. Training speed is ?5% slower, and the additional parameters during testing is minimal (?10%) as it uses the joint branch only.</figDesc><table><row><cell>Method</cell><cell>Test set</cell><cell>Yes/No ?</cell><cell>Number ?</cell><cell>Other ?</cell><cell>Overall ?</cell></row><row><cell>MCAN-Large + X-101 (2020) MCAN-Large + X-101 + MoVie</cell><cell>test-dev</cell><cell>88.46 88.39</cell><cell>55.68 57.05</cell><cell>62.85 63.28</cell><cell>72.59 72.91</cell></row><row><cell>Pythia + X-101 (2018) Pythia + X-101 + MoVie</cell><cell>test-dev</cell><cell>84.13 85.15</cell><cell>45.98 53.25</cell><cell>58.76 59.31</cell><cell>67.76 69.26</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>VQA accuracy of Pythia with and without MoVie on VQA 2.0 test-dev set.MoVie outperforms CountSeg on COCO with a significant margin on three RMSE metrics. The performance difference in two datasets suggests that MoVie scales better than CountSeg in terms of dataset size and number of categories. Moreover, the maintained advantage on relative metrics indicates the output of MoVie is better aligned with human perception<ref type="bibr" target="#b6">Chattopadhyay et al. (2017)</ref>.E COUNTING MODULE FOR VQA ARCHITECTURESAs mentioned in 3.2, we integrate MoVie into VQA models as a counting module. Naturally, such an integration leads to changes in model size, training speed etc. We report the added computational costs in Tab. 7 for our three-branch fusion scheme into MCAN. We see the training speed is only ?5% slower, and the additional parameters used during testing is kept minimal (?10%). Note that since the integration of MoVie mainly benefits 'number' questions, it is different from general model size increase.Similar to MCAN, we also conducted experiments incorporating MoVie to Pythia<ref type="bibr" target="#b24">Jiang et al. (2018)</ref>, the 2018 VQA challenge winner, where we trained using the VQA 2.0 train+val, and evaluated on test-dev using the server. We observe even more significant improvements on Pythia for 'number' related questions (Tab. 8). MoVie also improves the performance of the network in all other categories, verifying that our MoVie generalizes to multiple VQA architectures.F VISUALIZATION OF WHERE MOVIE HELPSWhen MoVie is used as a counting module for generic VQA tasks, we fuse features pooled from MoVie and the features used by state-of-the-art VQA models (e.g.MCAN Yu et al. (2019)) to jointly predict the answer. Then a natural question arise: where does MoVie help? To answer this question, we want visualize how important MoVie and the original VQA features contribute to the final answer produced by the joint model. We conduct this study for each of the 55 question types listed in the VQA 2.0 dataset<ref type="bibr" target="#b3">Antol et al. (2015)</ref> for better insights.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In fact, the module can be placed in earlier stages, or even be split across different stages. However, we didn't find it help much but incurs extra computation overhead that computes convolutional features twice if used as a module for generic VQA models. That's why we stick to the top.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://en.wikipedia.org/wiki/Root-mean-square_deviation</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tallyqa: Answering complex counting questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kushal</forename><surname>Kafle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Counting in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Arteta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Mutan: Multimodal tucker fusion for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Ben-Younes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Counting everyday objects in everyday scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithvijit</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramprasaath</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning spatial awareness to improve crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Qi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Xiu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Object counting and instance segmentation with image-level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisham</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Fahad Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Subitizing and visual short-term memory in human and nonhuman species: a common shared system?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Cutini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Bonato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Modulating early visual processing by language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Harm De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?r?mie</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daylen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01847</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">One vote, one value</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Galton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<date type="published" when="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan R</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to reason: End-to-end module networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Compositional attention networks for machine reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03067</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gqa: A new dataset for real-world visual reasoning and compositional question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning by abstraction: The neural state machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.03615</idno>
		<title level="m">defense of grid features for visual question answering</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Pythia v0.1: the winning entry to the vqa challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.09956</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Clevr: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The discrimination of visual number. The American journal of psychology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Edna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Miles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Whelan Reese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Volkmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Where are the blobs: Counting by localization with point supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Issam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Negar</forename><surname>Laradji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">O</forename><surname>Rostamzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge J. Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning visual question answering by bootstrapping hard attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">People, penguins and petri dishes: Adapting object counting models to new visual domains and object types without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Marsden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Mcguinness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suzanne</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciara</forename><forename type="middle">E</forename><surname>Keogh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel E O&amp;apos;</forename><surname>Connor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Improved fusion of visual and language representations by dense symmetric co-attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takayuki</forename><surname>Duy-Kien Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Okatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Towards perspective-free object counting with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Onoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Rubio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto J L?pez-Sastre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Film: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harm De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.10692</idno>
		<title level="m">Imvotenet: Boosting 3d object detection in point clouds with image votes</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">A survey of recent advances in cnn-based single image crowd counting and density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vishwanath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>PR Letters</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Interpretable counting for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Trott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">What makes training multi-modal networks hard?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12681</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Microscopy cell counting and detection with fully convolutional regression networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alison</forename><surname>Noble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer methods in biomechanics and biomedical engineering: Imaging &amp; Visualization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Multi-modal factorized bilinear pooling with co-attention learning for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep modular co-attention networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Salient object subitizing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shugao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sameki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margrit</forename><surname>Betke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Brian Price, and Radomir Mech</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning to count objects in natural images for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><forename type="middle">S</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam Pr?gel-</forename><surname>Bennett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
