<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Update for Object Tracking with Recurrent Meta-learner</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bi</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
						</author>
						<title level="a" type="main">Learning to Update for Object Tracking with Recurrent Meta-learner</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Model update lies at the heart of object tracking. Generally, model update is formulated as an online learning problem where a target model is learned over the online training set. Our key innovation is to formulate the model update problem in the meta-learning framework and learn the online learning algorithm itself using large numbers of offline videos, i.e., learning to update. The learned updater takes as input the online training set and outputs an updated target model. As a first attempt, we design the learned updater based on recurrent neural networks (RNNs) and demonstrate its application in a template-based tracker and a correlation filter-based tracker. Our learned updater consistently improves the base trackers and runs faster than realtime on GPU while requiring small memory footprint during testing. Experiments on standard benchmarks demonstrate that our learned updater outperforms commonly used update baselines including the efficient exponential moving average (EMA)-based update and the well-designed stochastic gradient descent (SGD)-based update. Equipped with our learned updater, the template-based tracker achieves state-of-the-art performance among realtime trackers on GPU. Index Terms-model update, meta-learning, recurrent neural network, object tracking.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>O BJECT tracking is a crucial task in computer vision that deals with the problem of localizing one arbitrary target object in a video, given only the target position in the first frame. Typically, bounding boxes are used for representing the target position. Arbitrary target object implies that a dedicated target model 1 is needed for each target during testing. This is typically accomplished through online learning where the online training set 2 is extracted from the test video and a target model is initialized and updated on the fly.</p><p>In this work, we tackle the problem of model update: after an initial target model is built using reliable supervision in the first frame, how to exploit information in subsequent frames and update the initial model along with tracking? Model update is challenging because of unreliable and highly correlated online training set.  <ref type="bibr" target="#b4">1</ref> In this work, we only consider scoring-based target model that outputs the confidence of an image patch being the target. Note there are also positionregression based target models that, given an image patch, directly regress the target position. <ref type="bibr" target="#b5">2</ref> The framework of learning to update contains an offline phase and an online phase. The offline phase is referred to as training the updater with offline videos, whereas the online phase refers to tracking a new sequence. The term "(online) training set" means the set of image patches collected for updating the target model (during the online phase).</p><p>for building a target model is the information in the first frame. After that, the online training set is collected based on predicted target position, which is not always reliable. When small errors in the training samples accumulate, model update can cause the drifting problem. Moreover, the extracted online training set is highly correlated since most of the training samples are simply the translated and scaled version of a base sample. Correlated samples are easy to fit and do not help much with the generalization to hard samples.</p><p>Recent works <ref type="bibr" target="#b4">[1]</ref>, <ref type="bibr" target="#b5">[2]</ref> have investigated the possibility of no model update at all, and achieved remarkable tracking performance. These approaches can be interpreted as learning an invariant and discriminative feature extractor such that the target remains stable in the feature space and is separable from the background. However, learning a representation that is both invariant and discriminative for a long time is intrinsically difficult, as with time evolves, features that once are discriminative may become irrelevant and vice versa. Consider that when a red car drives into a dark tunnel, the red color becomes irrelevant, although it is discriminative before entering the tunnel. Instead of striving to construct a perfect model at the first frame, model update tries to keep up with the current target appearance along with tracking by constantly incorporating the new target information, and therefore eases the burden of feature representation. Moreover, by gradually adapting to the current video context, the tracking problem can be considerably simplified <ref type="bibr" target="#b6">[3]</ref>. In scenarios where target exhibits multi-modality, model update is indispensable.</p><p>Generally, model update can be formulated as an online learning problem with two stages. First, an online training set is collected along with tracking. Then, the target model is learned on the training set using algorithms like stochastic gradient descent (SGD). Existing update methods typically suffer from the problem of large training set and slow convergence thus being too slow for practical use. Moreover, due to unreliable training data, regularizations and rules are carefully designed based on expertise in the field to avoid model drifting.</p><p>In this work, we advocate the paradigm for object tracking that eases the heavy burdens of online learning by offline learning. Offline learning is performed before the actual tracking takes place and the learned model is shared among all test videos. Online learning, in contrast, is conducted during tracking and the learned model is specific to each test video. Our key innovation is to formulate the model update problem in the meta-learning framework and learn the online learning algorithm itself using large numbers of offline videos, i.e., learning to update. we call the learned updater, takes in the online training set and outputs the updated target model. Please refer to <ref type="figure">Fig. 1</ref> for visual illustrations. The benefit of learning to update is threefold: 1) After seeing all kinds of target variations in the offline training phase, the learned updater is able to capture target variation patterns among videos. These learned patterns are implicitly used during testing to avoid unlikely update (e.g., update to background) and thus can be seen as a form of regularization, which enables our learned updater to handle the unreliable online training set. 2) The learned updater is able to update the target model based on not only the online training set, but also rules learned from the offline dataset. Therefore, the learned updater is able to see beyond the highly correlated online training set and makes the updated model capable of generalizing to more challenging scenarios. 3) The learned patterns enable fast inference of the learned updater. As a result, our learned updater improves the performance of base trackers while running faster than realtime on GPU with a single forward pass of the neural network per frame.</p><p>In this paper, we formulate model update as a metalearning problem (a.k.a learning to learn) <ref type="bibr" target="#b7">[4]</ref>, <ref type="bibr" target="#b8">[5]</ref> and learn a model updater. Specifically, our learned updater is embodied as a RNN, which is well known for its ability to model sequential/temporal variations. Previous efforts to model target variations based on RNNs mostly fail to deliver satisfactory tracking performance due to inadequate offline training videos. In this work, we contribute several techniques to overcome data deficiencies and train RNNs effectively. With a properly trained updater, our tracker achieves state-of-the-art performance among realtime trackers.</p><p>As a first attempt of learning to update for object tracking, we demonstrate its application on two base trackers: a template-based tracker for its simplicity and a correlation filter-based tracker for its wide adoption. Our learned updater considerably improves the base trackers and outperforms relevant model update baselines including the exponential moving average (EMA)-and the SGD-based update method.</p><p>In summary, our contributions are threefold: 1) We propose a novel model update method for object tracking that (i) is formulated as a meta-learning problem, capable of learning target variation patterns and facilitating effective tracking, and (ii) runs faster than realtime (82 fps with SiamFC tracker and 70 fps with CFNet tracker) while requiring small memory footprint, thus being suitable for practical applications; 2) We propose several techniques to train our RNN-based updater effectively; 3) We validate our method in common object tracking benchmarks and show that it (i) consistently outperforms relevant model update baselines, and (ii) obtains state-of-the-art performance among realtime trackers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Handcrafted Update Methods. In general, target variation can be decomposed as short-and long-term variation. <ref type="bibr" target="#b9">[6]</ref> proposes a probabilistic mixture model, which has a stable component to account for the long-term variation, a wandering component and a loss component for the short-term variation. Inspired by the Atkinson-Shiffrin Memory Model, <ref type="bibr" target="#b10">[7]</ref> uses short-and long-term memory to handle target variations. Correlation filter and keypoint matching are employed for short-and long-term memory, respectively. Instead of using two separate components, we design a single component based on RNN and learn to process short-and long-term information in a data-driven manner.</p><p>One critical problem in model update is related to the stability-plasticity dilemma <ref type="bibr" target="#b11">[8]</ref>. On one hand, model update should be stable to avoid the drifting problem where small errors accumulate and the model gets adapted to other objects. On the other hand, it also needs plasticity to effectively assimilate new information derived during tracking. <ref type="bibr" target="#b12">[9]</ref> coined it the template update problem. They adopt a conservative update strategy which keeps the target model in the first frame (i.e., initial model), and updates the latest model only if its predicted locations are close to those of the initial model. Similar techniques are adopted in <ref type="bibr" target="#b13">[10]</ref> which learns a ridge regression based on the first and the last target model. Inspired by this, we design an anchor loss that uses the first target model as an anchor point. We find it particularly useful in our learning based updater and will elaborate on it in Section IV.</p><p>Another strategy to handle the model drifting problem is being more careful about the derived training samples. Instead of making hard decisions about labeling training samples as target or background, <ref type="bibr" target="#b6">[3]</ref> proposes a semi-supervised approach where only samples from the first frame are labeled and training samples from subsequent frames remain unlabeled. <ref type="bibr" target="#b14">[11]</ref> proposes an occlusion detector and updates the model only if the occlusion level is low. <ref type="bibr" target="#b15">[12]</ref> uses multiple-instance learning to update model with bags of samples, where positive bag contains at least one positive sample (without knowing which one), and negative bag contains only negative samples. <ref type="bibr" target="#b16">[13]</ref> estimates the training sample qualities by optimizing the target model loss with respect to both the target model and the sample qualities. We facilitate training information selection by adopting a gating mechanism in our learned updater.</p><p>A popular model update method for correlation filter-based trackers <ref type="bibr" target="#b17">[14]</ref>, <ref type="bibr" target="#b18">[15]</ref> is exponential moving average (EMA), which performs linear interpolation from the newly trained model (using only training samples in the current frame) and the previous target model. This method is attractive because 1) the update process is highly efficient without iterative optimization and 2) training samples are processed on the fly without the need to be stored. However, it is unlikely that linear combination can capture all of the complex target variations. Our model update method outperforms EMA-based update while preserving all the practical benefits mentioned above.</p><p>Meta-learning. In this work, model update is formulated as a meta-learning problem. Essentially, meta-learning models a learning problem in two scales: learning for specific tasks, and learning for general patterns that rule specific tasks. In our case, updating target model for a specific target (e.g., an airplane) is a specific task. We aim to learn a model update method that is applicable to any specific tasks. <ref type="bibr" target="#b7">[4]</ref>, <ref type="bibr" target="#b8">[5]</ref> learn to solve the optimization problem of neural networks based on RNNs. Their methods mainly focus on fast convergence and are not readily applicable for model update due to the unreliable training samples.</p><p>RNN-based trackers. RNNs are well known for their ability to model temporal variations. Given the importance of modeling temporal variations of target in object tracking, it is natural to consider taking advantage of RNNs. <ref type="bibr" target="#b19">[16]</ref> is among the first to use RNN for object tracking, but has only shown to work on simple synthetic datasets. RATM <ref type="bibr" target="#b20">[17]</ref> and HART <ref type="bibr" target="#b21">[18]</ref> develop attention mechanisms based on RNNs and demonstrate success on natural image datasets KTH <ref type="bibr" target="#b22">[19]</ref> and KITTI <ref type="bibr" target="#b23">[20]</ref>. Re3 <ref type="bibr" target="#b24">[21]</ref> models both appearance and motion variations using RNNs and achieves comparable results on several object tracking benchmarks <ref type="bibr" target="#b25">[22]</ref>- <ref type="bibr" target="#b27">[24]</ref>. However, Re3 <ref type="bibr" target="#b24">[21]</ref> only models short-term variations and requires manual resetting of RNN states every 32 frames. RFL <ref type="bibr" target="#b28">[25]</ref> proposes a filter generation method based on RNNs and resembles our method applied to the template-based tracker. Nevertheless, we tackle a more general model update problem and formulate our method in the meta-learning framework. RFL fails to deliver satisfactory tracking performance due to aggressive updating which is common in RNN-based trackers. By formulating the model update in the meta-learning framework, we focus on the generalization ability of the online-learned target model. With the proposed anchor loss, our approach outperforms RFL by a large margin. To the best of our knowledge, we propose the first RNN-based tracker that achieves state-of-the-art tracking performance among GPU-based realtime trackers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. BASE TRACKERS AND BASE UPDATE METHODS</head><p>In this work, we focus on the update of linear target model due to its simplicity and wide adoption.</p><p>Basically, a target model is a scoring function that outputs the confidence of an input image patch being the target. Importantly, the target model should have parameters ? that is updatable. By model update, we mean updating ? such that it accommodates the target variations during tracking.</p><p>For a linear target model, the confidence c of an image patch with feature x is the inner product between ? and x, i.e., c = ? T x.</p><p>During tracking, a tracker gathers a training set T for updating the target model parameters ?. Since the dataset is gathered online, it is called the online training set. Let T t be the online training set at time t, the target model is updated by an update function u, i.e., ? t = u(T t ). This work is about learning an update function u ? (?) with parameters ? using large numbers of offline videos.</p><p>Before diving into the proposed learned updater, we introduce two base trackers with linear target model as well as two baseline model update methods in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Template-Based Tracker: SiamFC</head><p>A template-based tracker simply uses the target feature (i.e., the feature of the target) as its target model. Intuitively, it means that the confidence of the test patch being the target is high when it is similar to the target feature and low otherwise. Due to the simplicity of the target model and the learning algorithm (i.e., an identity function), the performance of template-based tracker depends heavily on its features.</p><p>SiamFC <ref type="bibr" target="#b5">[2]</ref> is a template-based tracker which achieves good performance with an offline learned feature extractor. The feature extractor is learned under a two branch structure with millions of image pairs sampled from videos. Each image pair contains a target patch and a test patch. The feature of the target patch is computed as the linear target model. The learning objective is that the confidence of a test patch containing the target is high while the confidence of background being low.</p><p>Interestingly, SiamFC does not have a model update module. In other words, given the target feature in the first framex 1 (we will usex for target feature and x for the feature of any image patch), the online training set contains only the target feature T t = {x 1 } and the learning algorithm simply takes the target feature as the target model ? t+1 = u(T t ) =x 1 .</p><p>The detection process of SiamFC is equivalent to computing the similarity by sliding the target model ? over the search feature z and then outputting the position with the largest response. To facilitate fast detection, the feature extractor is designed to be fully convolutional and the similarity metric is simply inner product. Therefore, the detection process can be readily implemented via cross-correlation (or convolution in the neural network literature):</p><formula xml:id="formula_0">p = arg max p ? z .</formula><p>(1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Correlation Filter-Based Tracker: CFNet</head><p>Correlation filters are one representative example of linear target model that have shown superior performance and gained a lot of popularity. The key factor behind the success of correlation filters is an efficient learning algorithm that is able to handle tens of thousands of circulant training samples. Correlation filter-based trackers have been improved in various aspects since the seminal work of <ref type="bibr" target="#b17">[14]</ref>. For simplicity, we use the basic formulation and follow the setup in CFNet <ref type="bibr" target="#b29">[26]</ref>.</p><p>In correlation filter-based tracker, given one base sample of the target featurex, various virtual samples are obtained by cyclic shifts. The label of the base sample is 1 while the labels of the virtual samples follow a Gaussian function depending on the shifted distance. Given the samples and the corresponding labels, a ridge regression problem is solved efficiently in the Fourier domain making use of the property of circulant matrix <ref type="bibr" target="#b30">[27]</ref>. In this work, we will simply use CF(?) to denote the algorithm of learning the correlation filter from one base sample with multiple feature channels. Please refer to <ref type="bibr" target="#b17">[14]</ref>, <ref type="bibr" target="#b18">[15]</ref> for more details about CF(?).</p><p>The detection of correlation filter-based tracker is typically conducted in the Fourier domain. However, we convert the learned filter back to the spatial domain following CFNet. In this way, the detection process is the same as the SiamFC tracker. Moreover, it frees the learned updater from dealing with complex values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Two Baseline Model Update Methods</head><p>For linear target model based trackers, there are two commonly used model update methods. We briefly introduce these two baseline methods in this subsection.</p><p>EMA-based model update. Let g : X ? ? be the algorithm of learning a linear target model using a single target feature. For template-based tracker, it is the identity function. For correlation filter-based tracker, it is CF(?). Typically, the online training set contains the target feature at each frame, i.e.,</p><formula xml:id="formula_1">T t = {x 1 ,x 2 , ...,x t }. It is clear that T t = T t?1 ? {x t }.</formula><p>Given the online training set T t , the updated target model via EMA is</p><formula xml:id="formula_2">? t+1 = u(T t ) (2a) = (1 ? ?)u(T t?1 ) + ?g(x t ) (2b) = (1 ? ?)? t + ?? t (2c)</formula><p>where ? is the learning rate that controls the rate of adaptation,</p><formula xml:id="formula_3">? t = g(x t )</formula><p>is the candidate target model at time t. Note that this update method only requires the last target model ? t and the current target featurex t . Hence, there is no need to explicitly collect a large online training set. EMA-based update is widely adopted in correlation filterbased trackers because it is easy to implement and efficient in terms of both memory consumption and computational complexity. Moreover, EMA-based update is the update rules of correlation filters derived for the multiple base samples with single channel case <ref type="bibr" target="#b17">[14]</ref> and can be quite effective even for samples with multiple channels. However, in general, EMAbased update is ad hoc and only marginally improves the template-based tracker as shown in our experiments (Section V). SGD-based model update. Besides EMA, another update method is to collect the online training set using samples from the search space of detection, i.e.,  </p><formula xml:id="formula_4">T t = {(z 1 , y 1 ), (z 2 , y 2 ), ..., (z t , y t )} where z t is the feature of the</formula><formula xml:id="formula_5">? t+1 = ? t ? ? ?l T b t (?) ??<label>(3)</label></formula><p>where l T b t (?) is a differentiable objective function with variables ? over a batch of samples T b t from the online training set, and ? is the learning rate.</p><p>Typically, a fixed size of online training set is maintained by dropping the earliest samples when the capacity is exceeded. However, it still contains thousands of samples. Moreover, we show only one iteration of SGD update for brevity while it generally requires dozens of iterations to take effects. These computational burdens of the SGD-based update method hinder its practical usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. LEARNING TO UPDATE</head><p>In this section, we first formulate the model update problem in the meta-learning framework and then introduce the proposed model updater.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Model Update as Meta-Learning</head><p>Meta-learning is about learning from large numbers of tasks during meta-training to quickly learn a new task at meta-test time.  takes as input the training set of a task and outputs a learner which should perform well on the corresponding test set. The aim of meta-learning is to learn a meta-learner during metatraining such that the meta-learner can quickly learn a good learner at meta-test time, so meta-learning is also known as learning to learn. The uniqueness of meta-learning is that, at meta-test time, the meta-learner should quickly learn a new concept using examples from the training set of the concept. An example is to learn a classifier to differentiate "Apple" and "Pear" based on examples of each category where these two categories never appear during meta-training. This resembles model update for object tracking since the tracker should quickly update the target model to accommodate an object that does not appear during offline training <ref type="bibr" target="#b6">3</ref> .</p><p>In the context of meta-learning for target model update, we aim to learn a meta-learner (model updater) from large numbers of offline videos during meta-training (offline training). Each task is to learn the learner (target model) from the training set to locate the target at test set. The correspondence between learning to update and meta-learning is summarized at <ref type="table" target="#tab_3">Table I.</ref> During offline training, given the training set T and the test set Q of a task constructed from the offline videos (T, Q) ? V, the model updater computes target model ? = u ? (T ). Let l(?, Q) be the loss of the target model on the test set Q of a task. The model updater ? is learned by minimizing the <ref type="bibr" target="#b6">3</ref> In fact, during offline training, the "human" category never appears whereas humans appear often in the tracking benchmarks.</p><p>following loss:</p><formula xml:id="formula_6">L(?) = (T,Q)?V l(u ? (T ), Q) .<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Training Set and the Test Set</head><p>We now describe how to construct the training set and the test set of a task for learning to update.</p><p>Given a video from the offline videos and the corresponding target positions (width, height, center position) at each frame, we first normalize the scale variations of targets by scaling the image with factor s such that s(w + 2p) ? s(h + 2p) = A where w, h are the width and height of the target in the image, p = w+h 4 is the context margin and A = 127 ? 127 is the desired target size after scaling.</p><p>A subset of N image frames are sampled from the scaled images while keeping the temporal order. The first N ? 1 frames are cropped at the center of the target, with size 127 ? 127, and used as the training set of the target. The last frame is also cropped at the center of the target, with size 255 ? 255, and used as the test set. Note that we use a larger image patch in the test set since both of our base trackers are translation equivalent and the 255 ? 255 image can be seen as a set of 127 ? 127 images. Please refer to <ref type="figure">Fig. 2</ref> for several examples of training set and test set.</p><p>Cropped images are then embedded by the feature extractor. Finally, we have the training set T = {x 1 ,x 2 , ...,x N ?1 } wherex ? R m?m?d is the feature of the target. The test set Q = {(z, y)} where z ? R n?n?d is the feature of the search image and y ? {?1, +1} (n?m+1)?(n?m+1) is the corresponding label map. Features have spatial size m or n and channel size d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Instantiation of The Learned Updater</head><p>A model updater takes as input a training set T = {x 1 ,x 2 , ...,x N ?1 } and outputs the updated target model ?, i.e., ? = u(T ). The design of the updater includes several preferable properties: 1) supporting training set with variable size; 2) incremental update, i.e., during tracking, the target model is updated based on existing values instead of learning from scratch; 3) memory and computational efficiency.</p><p>In this work, we propose a RNN-based updater that satisfies all these properties. Concretely, our updater follows a threestep procedure.</p><p>Step 1: Project from feature space to model space. We first project each target feature in the training set into the model space by ? = g(x), where ? ? R m?m?d is the candidate target model and g(?) is the algorithm of learning a linear target model using a single target feature. In particular, g(?) is the identity function for SiamFC and is the CF(?) function followed by a center cropping function for CFNet.</p><p>Step 2: Aggregate target information. We use RNN to summarize the training set into a single tensor. For simplicity, gated recurrent unit (GRU) is adopted <ref type="bibr" target="#b31">[28]</ref>. We find GRU achieves better performance than the Long-short term memory (LSTM <ref type="bibr" target="#b32">[29]</ref>) in the ablation study. To preserve the spatial dimension, we extend the original GRU formulation to Convolutional GRU (ConvGRU) by replacing all matrix multiplications with convolutions.</p><p>Step 3: Generate target model. Given the last hidden state of the ConvGRU, one convolutional layer is used to generate the target model.</p><p>By adopting RNN, our updater is able to handle training set with variable size. Moreover, the target model is updated in an incremental manner. To make things clear, during tracking, denote T t?1 = {x 1 ,x 2 , ...,x t?1 } as the online training set at time t ? 1. After model update, we obtain the hidden state h t?1 . At time t, the online training set is updated with a new examplex t , i.e., T t = T t?1 ? {x t }. Since the first t ? 1 examples are unchanged, we can simply reuse h t?1 and get h t = ConvGRU(h t?1 ,x t ). Moreover, with incremental update, we can avoid explicitly storing and manipulating a large online training set during tracking since our updater only needs h t?1 to generate h t which saves a large amount of memory.</p><p>Until now, the updater is restricted to use N ? 1 target features as the training set and one search image feature as the test set. Note that given a sequence of N ? 1 target features, our updater can readily compute N ? 1 target models at each time step. Therefore, given a video with N images, we can construct various training sets with length 1, 2, ..., N ? 1 and the computation for model update can be shared.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. The Learning Objective</head><p>Given the updated target model ?, we need a "goodness" measurement of the target model which in turn indicates how good the updater is and thus enables optimization.</p><p>Classification Loss. Following the meta-learning framework, the updated model is evaluated on the test set Q = {(z, y)}. Using the normalized logistic loss for classification, we have l c (?; Q) = y(? ln ?(? z)) + (1 ? y)(? ln(1 ? ?(? z))) (n ? m + 1) ? (n ? m + 1) .</p><p>(5) Anchor Loss. At a first glance, it would seem that classification loss is all we need to train the updater. However, model update faces the intrinsic problem: the stability-plasticity dilemma, i.e., model update should be stable with respect to noise and flexible to assimilate new information. With only classification loss, since z is close tox N ?1 , the updater will adopt an aggressive update strategy and store new information brought byx N ?1 as much as possible. The problem is that x N ?1 is not always reliable during tracking and thus the updater trained with only classification loss is prone to small errors.</p><p>One effective method that is validated by the literature is to use the target model at the first frame as an anchor point <ref type="bibr" target="#b12">[9]</ref>, <ref type="bibr" target="#b13">[10]</ref>. Given inadequate training data, such an anchor point is hard to learn without regularization. Therefore, we design an anchor loss, which penalizes the updater when the updated target model drifts away from the initial target model:</p><formula xml:id="formula_7">l a (?; ? 1 ) = 1 m ? m ? d ||? ? ? 1 || 2 2 .<label>(6)</label></formula><p>where the loss is normalized by the number of target model parameters m ? m ? d. Total Loss. Classification loss and anchor loss are linearly combined for measuring a target model:</p><formula xml:id="formula_8">l(?; Q) = (1 ? ?)l c (?; Q) + ?l a (?; ? 1 ) ,<label>(7)</label></formula><p>where ? is the combination factor. Successful learning of the updater should maintain a good balance between the classification loss and the anchor loss. The total loss of the updater u ? (?) with learnable parameters ? is then the loss of the target model in the offline training set by inserting Eq. 7 into Eq. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Practical Techniques for Effective Learning</head><p>Our learned updater collects target information based on RNNs, which are well known for modeling sequential/temporal variations. However, the problem of limited offline training videos has to be addressed before it unleashes the power. We describe several techniques based on the nature of model update that turn out to be effective.</p><p>Modeling long-term variation by truncated backpropagation. Typically, a subset of N frames are sampled from the original video for RNN training. For convenience, we define maximum modeling length of an algorithm to be the largest length of all sampled sequences during training, where the length of a sampled sequence stands for the distance counted by #(frames) in the original video between the first and the last sampled frame. Since videos have hundreds of frames to track, it is desirable for RNN-based models to learn long-term dependency with a large maximum modeling length. However, training with long sequences is computationally demanding and may incur the vanishing gradient problem. To avoid such a problem, existing RNN-based trackers sample relatively few frames sparsely from the original video. For example, <ref type="bibr" target="#b28">[25]</ref> samples training sequences with 10 frames and large frame interval (30 frames on average). Such a sparse sampling strategy, however, enlarges the target variations between sampled frames, which is more difficult to learn. We conjecture that these trackers are disadvantaged by such limitations.</p><p>Contrarily, to train our learned updater, we sample training sequences with as many as 150 frames and small frame interval (? 2 frames) <ref type="bibr" target="#b7">4</ref> . To handle the aforementioned problem of training with long sequences, instead of backpropagating all the way to the first frame, we adopt truncated backpropagation. This method processes training frames one timestep at a time, and every H timesteps, it runs backpropagation through time (BPTT) for H timesteps. H is called the unroll length.</p><p>Matching training and testing behavior by estimated target position. During offline training of the learned updater, training set T of the training video needs to be generated. In our case, the online training set contains the target features at each frame T t = {x 1 ,x 2 , ...,x t }. The extraction of the target features depends on the target position which can only be estimated during tracking. RNNs often take as input the groundtruth during training, which is known as teacher forcing. However, as noted in <ref type="bibr" target="#b33">[30]</ref>, this causes discrepancy between training and testing and hampers the performance of RNNs. In this work, we always use the target position during training that is inferred by the target model to keep in line with testing.</p><p>Reducing overfitting by interval update. As noted in <ref type="bibr" target="#b34">[31]</ref>, instead of updating the target model in every frame, it is beneficial to apply a sparser update scheme. We adopt this simple strategy by updating the target model every M frames. Note that the hidden state of RNN is updated in every frame though. The reason for the improved performance is that by updating after a certain timesteps, the learned updater can make more informed update decision, and therefore reduces the risk of overfitting to current training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS A. Implementation Details</head><p>Training data. The feature extractor and learned updater are trained offline on the ILSVRC 2015 Object Detection from Video dataset (Imagenet VID) <ref type="bibr" target="#b35">[32]</ref>. Imagenet VID contains 4417 videos and each video has about 2 object tracks on average, adding up to 9220 tracks. Tracks are annotated with bounding boxes in each frame and contain about 230 frames on average. This differs from another large-scale video dataset, namely Youtube-BB <ref type="bibr" target="#b36">[33]</ref> where objects are annotated every 30 frames and each track has about 15 annotated frames. We find that small frame interval is important for training the learned updater, and thus, we use Imagenet VID instead of the much larger Youtube-BB. However, it is worth investigating effective ways to make use of Youtube-BB for object tracking.</p><p>Image sequences are sampled from tracks as training samples for the learned updater. We use bucketing <ref type="bibr" target="#b37">[34]</ref> (i.e., an RNN training technique which batch together sequences of similar lengths for efficiency) to handle sequences of different length. Multiples of the RNN unroll length are used as the bucket sizes. For example, bucket sizes of 25, 50, ..., 125, 150 are used for fast experimentation where 25 is the RNN unroll length. For tracks that are longer than the largest bucket size (e.g., 150), we sample a portion of the tracks with small frame interval (e.g., 1 or 2). For short tracks, we lengthen these tracks by duplicating frames or simply drop these tracks according to probabilities that are proportional to the track length. Images are preprocessed according to its base tracker SiamFC <ref type="bibr" target="#b5">[2]</ref> and CFNet <ref type="bibr" target="#b29">[26]</ref>. Particularly, these two base trackers use the same preprocessing procedures to crop and resize images such that targets are at the image center and take up 127 x 127 pixels together with context.</p><p>Architecture. For template-based tracker, we use the same modified Alexnet architecture in <ref type="bibr" target="#b5">[2]</ref> for the feature extractor. For correlation filter-based tracker, we use the 3 layer CNN feature extractor in <ref type="bibr" target="#b29">[26]</ref>, which is trained following the procedures in <ref type="bibr" target="#b5">[2]</ref>. All of our experiments stack two convolutional GRU layers, where convolution operations have kernel size 3 with zero padding to preserve spatial dimension. For template-based tracker, each convolutional GRU layer has 192 units while for correlation filter-based tracker, each has 64 units 5 . One convolutional layer with kernel size 3 is used to generate the updated target model based on the hidden states, which takes as input the concatenated states of the two convolutional GRU layers and outputs corresponding target models. Dropout <ref type="bibr" target="#b38">[35]</ref> and layer normalization <ref type="bibr" target="#b39">[36]</ref> are added in each convolutional GRU layer to avoid overfitting.</p><p>Optimization. Learned updaters are trained over 60 epochs, each epoch consists of 8309 image sequences. Gradients are computed using mini-batches of size 8, which are used by the Adam optimizer <ref type="bibr" target="#b40">[37]</ref>. Learning rate is fixed to be 1e-4. Weight decay is 5e-4.</p><p>Hardware and software specifications. The speed measurements of our trackers are performed on a computer with an Intel Core i7-5930K Haswell-E 6-Core 3.5GHz CPU and a GeForce GTX 1080 GPU. Our trackers are implemented in TensorFlow <ref type="bibr" target="#b41">[38]</ref>, which is compiled with CUDA 8.0 and cuDNN 6.0.</p><p>Postprocessing. We adopt the same strategy as our base trackers for penalizing large displacement and handling scale variations. Specifically, a cosine window is added to the response map to penalize the large displacement. For scale estimation, three search patches with different scales are extracted and the current scale is calculated by interpolating the newly predicted scale with a damping factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Benchmarks and Evaluation Protocols</head><p>OTB. The OTB benchmark contains three subdatasets: OTB-2013, OTB-50 and OTB-100, each of which consists of 51, 50 and 100 natural image sequences, respectively. The standard evaluation metric on OTB is the area under curve (AUC) of the threshold-success rate curve which represents the success rates at different thresholds. For each frame, the overlap (intersection over union) between the predicted target bounding box and groundtruth is computed. The success rate at a given threshold corresponds to the fraction of frames that has overlap no less than the given threshold.</p><p>VOT. The VOT benchmarks are a collection of tracking challenges held on a yearly basis starting from 2013. We use three recent benchmarks: VOT-2015, VOT-2016 and VOT-2017. Unlike OTB, which lets the tracker run until the end of the image sequence, VOT focuses on short-term tracking (no redetection is required) and resets the tracker once it drifts away from the target. The primary measure is the expected average overlap (EAO), which reflects the similar property as AUC. VOT-2017 also introduces a new "realtime challenge", where a tracker is constantly receiving images in realtime speed and if the tracker does not respond after a new frame becomes available, the last bounding box predicted by the tracker is reported for the current frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Study</head><p>We validate the effectiveness of various designs of our learned updater based on the template-based tracker. OTB-2013 is used for ablation study. All experiments use the same configurations except the components that are examined. Although larger unroll length typically gets better results, in the ablation study section, we use unroll length 25 for fast experimentation which is also the default configuration unless larger unroll length is needed. Moreover, only color images are adopted during offline training to prevent benchmark-specific choices <ref type="bibr" target="#b9">6</ref> . All models are trained using our implementation including SiamFC. Results are summarized in <ref type="table" target="#tab_3">Table II</ref>.</p><p>Anchor loss is crucial for successful learning. To overcome the stability-plasticity dilemma, we propose the anchor loss which penalizes large variations of the updated target model. To minimize the anchor loss, one straightforward strategy would be no update at all. However, besides the anchor loss, the learned updater is also constrained by the classification loss which encourages the update of the target model to keep up with target variations. It is of interest to investigate how the interplay between these two losses affects the performance of the learned updater.</p><p>Our learned updaters trained with different combination factors ? are shown in <ref type="table" target="#tab_3">Table II</ref> <ref type="bibr">(d)</ref>. We also include the results of the no-update baseline (i.e., the setup in SiamFC <ref type="bibr" target="#b5">[2]</ref>) for reference. When ? = 0 (i.e., no anchor loss), our learned updater is not constrained to generate target models that are consistent with the initial target model and can quickly drift away. The performance is even worse than not updating at all. As ? increases, the learned updater gets better and reaches the peak at 0.2. Further increasing the anchor loss weight diminishes the possible target model choices of the learned updater and the performance drops. Note that even without the classification loss (i.e., ? = 1), our learned updater outperforms the baseline with no update. The reason is that the learned updater is given the initial target model only once. After that, it constantly receives new target model information and the hidden states of RNN inevitably stores new information due to the soft store operations. It is this new information that helps tracking.</p><p>Learned updater is orthogonal to feature extractor. Our template-based tracker is based on <ref type="bibr" target="#b5">[2]</ref>, which aims to learn an invariant and discriminative feature extractor such that model update is not necessary. Two interesting questions are: 1) how much variation the Siamese network is able to learn, and 2) how the learned feature extractor affects our learned updater. We answer these questions from the perspective of training samples of feature extractor. Every training sample of the Siamese network contains two image patches from two frames of a video. These two patches are both centered on the target and at most K frames apart, and therefore the maximum modeling length is K according to the definition in Section IV-E. We investigate the effects of different K, and train an updater based on each feature extractor. Results are shown in <ref type="table" target="#tab_3">Table II(e).</ref> As can be seen, 1) Siamese network is able to capture the variations of target within 100 frames, but has difficulties learning beyond this limit. 2) Although the feature extractor is trained with the objective of invariance and does not require model update, our learned updater consistently improves the base trackers. Moreover, the improvement is positively correlated with the performance of the feature extractor.</p><p>Train longer, generalize better. Model update is a process that typically spans hundreds of frames. We investigate the effects of training with long image sequences (large maximum modeling length). Table II(f) shows the tracking results of the learned updater trained with different maximum modeling length and unroll lengths. By increasing the maximum modeling length from 100 to 300, the learned updater monotonically gets better results. Since modeling long term dependencies is still a challenge for RNN, the performance degenerates when it reaches 400. In conclusion, 1) large maximum modeling length helps the learned updater to generalize better if it is within the modeling capabilities of RNN; 2) large unroll length is still helpful under truncated backpropagation to model long term dependencies.</p><p>Moreover, it is worth mentioning that the performance of the Siamese network decays as the maximum modeling length is over 100 (as shown in Table II(e)). We have tried to increase the number of neurons in Siamese network, but it does not help. Contrarily, our learned updater can handle longer sequences (e.g., 300 frames). It can be inferred that it is difficult to extract invariant features to handle long-term target variation, whereas learning an updater to adapt the target model gradually is relatively easier.</p><p>Practical techniques are helpful. We demonstrate the effectiveness of the various practical techniques introduced in Section IV-E by removing one component at a time. Note that small unroll length is used by default instead of large unroll length for fast experimentation. The results are summarized in <ref type="table" target="#tab_3">Table II</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AUC(%) on OTB-2013</head><p>SiamFC-lu-unroll50 SiamFC-lu-unroll25 these techniques, interval update plays such an important role that the performance drops about 2% once removed. This is in accordance with the findings in <ref type="bibr" target="#b34">[31]</ref>.</p><p>Ablation on the RNN. Table II(b) shows the results of RNNs with different configurations of cell unit (ConvGRU or ConvLSTM), number of stacked RNN layers and the hidden state size. As can be seen, ConvGRU consistently outperforms ConvLSTM which may be related to the observation that the best performing cell unit is task-dependent <ref type="bibr" target="#b42">[39]</ref>. We also observe that the AUC largely increases from 62.1% to 63.6% by stacking two layers of ConvGRU. The performance is not sensitive to the size of the hidden state as long as it is in a reasonable range (192?256).</p><p>Joint training is difficult. The whole system (including feature extractor and model updater) can be trained jointly. We have tried several schemes for joint training, of which the results are summarized in <ref type="table" target="#tab_3">Table II(c)</ref>. Interestingly, it is best to train feature extractor and model updater separately -first train the feature extractor without model updater, and then fix the feature extractor and train the model updater. Scheme 3 jointly trains feature extractor and model updater after separately learning these two components. However, the performance still degrades. The reason is arguably that the ability of the feature extractor for modeling appearance invariance is hampered during joint training since the frame interval between selected images is small.  with configurations validated in the ablation study. As it turns out, the publicly available feature extractor of SiamFC trained with both color and gray images improves the tracker performance from 0.644 to 0.657 on OTB-2013. This is in line with our observations that the performance of a tracker equipped with our learned updater is positively correlated to that of the feature extractor. The results are summarized in <ref type="table" target="#tab_3">Table III</ref>. Qualitative comparisons are presented in <ref type="figure">Fig. 4</ref>.</p><p>Observations: 1) Our learned updater significantly outperforms the no update and EMA update baselines. 2) A somewhat surprising yet encouraging result is that our learned updater achieves better performance than the heavily designed and tuned SGD update method. 3) It is worth noting that for correlation filter-based tracker, EMA update is still a strong baseline. 4) Noticeably, the improvement on SiamFC is consistently larger than that on CFNet. As noted in <ref type="bibr" target="#b29">[26]</ref>, the CF(?) may impose priors that become overly restrictive when enough modeling capacity and data are available. 5) Our learned updater outperforms the SGD-based update method while enjoying comparable efficiency as the EMA-based update method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. State-of-the-art Comparison</head><p>We compare trackers equipped with our learned updaters against 10 state-of-the-art trackers: ECO <ref type="bibr" target="#b34">[31]</ref>, MDNet <ref type="bibr" target="#b43">[40]</ref>, LSART <ref type="bibr" target="#b44">[41]</ref>, CSRDCF <ref type="bibr" target="#b45">[42]</ref>, CREST <ref type="bibr" target="#b46">[43]</ref>, RFL <ref type="bibr" target="#b28">[25]</ref>, EAST <ref type="bibr" target="#b47">[44]</ref>, DSiam <ref type="bibr" target="#b13">[10]</ref>, CFNet <ref type="bibr" target="#b29">[26]</ref> and SiamFC <ref type="bibr" target="#b5">[2]</ref>. The results are summarized in Table IV(a).</p><p>Observations: 1) SiamFC-lu achieves state-of-the-art performance among realtime trackers. 2) SiamFC-lu outperforms DSiam and RFL, which also focus on improving the update mechanism of SiamFC.</p><p>To evaluate the practicability of different tracking methods, VOT-2017 introduces a new "realtime challenge" that only allows the tracker to respond in realtime; otherwise, the last predicted target position will be used for the current frame. We compare our method with the state-of-the-art trackers in this setting: CSRDCF++ (a C++ implementation of the CSRDCF <ref type="bibr" target="#b45">[42]</ref> tracker), SiamFC <ref type="bibr" target="#b5">[2]</ref>, ECO-HC (a lightweight version of ECO <ref type="bibr" target="#b34">[31]</ref> that uses HOG feature), LSART <ref type="bibr" target="#b44">[41]</ref> and CFCF <ref type="bibr" target="#b48">[45]</ref>. The results are shown in Table IV(b). It can be observed that our approach achieves state-of-the-art results in the realtime setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. DISCUSSION</head><p>One interesting question would be why learning to update actually works? We try to answer this question in three different perspectives. 1) From the high-level perspective, fast model update is viable because videos are intrinsically structured (e.g., temporal dependencies between target features, target variation patterns). Our learned updater captures these structures in a data driven manner. 2) As for the functionality, our learned updater can be seen as a learnable extension of the EMA-based update. The difference is that, instead of linearly interpolating the last target model and the current candidate target model, we adopt the gating mechanism. As a result, the learned updater inherits the efficiency of EMA-based update and the effectiveness of learning based method. 3) Empirically speaking, as shown in <ref type="figure">Fig. 4</ref>, after being trained under the classification loss and anchor loss, our learned updater is able to reliably absorb target variations while resisting the distractors.</p><p>As a first attempt, however, there are still many interesting problems left uninvestigated. One issue of RNN-based updater is that, the convolutional GRU requires large amount of GPU memory during offline training and thus being difficult to apply to target models with large numbers of parameters. How to scale to large target models would be an interesting research problem. Moreover, in this work, only linear target models are considered, how to extend to the non-linear cases such as MDNet is valuable future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>We propose a learning based framework to tackle the problem of model update during tracking. As a first attempt, only the update of linear target model is considered. The learned updater is parameterized based on RNN and successfully learned with several techniques proposed in this work. Our learned updater outperforms two common model update baselines including the efficient EMA-based update and the well-designed SGD-based update. After offline training, our learned updater can run efficiently during testing; therefore, our learned updater is able to consistently improve the base trackers without sacrificing the speed. Notably, the SiamFC tracker has been improved by nearly 40% in terms of the EAO on VOT-2017 while running at the speed of 82fps, which achieves state-of-the-art performance among realtime counterparts. In the future, we plan to extend the learning to update paradigm to non-linear target models. <ref type="figure">Fig. 4</ref>: Qualitative results of our learned updater compared with common model update baselines. Bolt: Our learned updater correctly adapts to the target while others are attracted by distractors. SGD-based update method adapts to part of the target instead. CarScale: Other methods keep tracking part of the target since only the front of the car is shown in the first frame. Contrarily, our learned updater gradually adapts to the whole target including both the front and the tail of the car. Matrix: our learned updater is able to perform equally well compared with the SGD-based update in this challenging sequence. Suv, Woman: While all other methods fail, our learned updater still successfully tracks the target.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 :</head><label>3</label><figDesc>The framework of learning to update during offline training. Given the training set T with image patches of a car, the target model ? is updated by the recurrent model updater. The target model is tested on the test set Q to obtain the classification loss. An anchor loss is also added to improve generalization. The model updater is learned by optimizing the anchor loss and the classification loss. During tracking, the model updater is fixed. An online training set is gathered along with tracking as T and the target model is updated by the model updater and applied to subsequent frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Anchor loss: Tracker accuracy as the combination factor ? varies during updater training. "lu" is our learned updater. Maximum modeling length of feature extractor: Tracker accuracy as the maximum modeling length of the feature extractor varies during offline training, with and without the learned updater. Maximum modeling length of model updater: Tracker accuracy with varied maximum modeling length and unroll length of the model updater during offline training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The only reliable supervision B. Li and W. Liu are with the School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan 430074, China. (e-mail: libi@hust.edu.cn; liuwy@hust.edu.cn) W. Xie and W. Zeng are with the Microsoft Research Asia, Beijing 100080, China.(e-mail: wenxie@microsoft.com; wezeng@microsoft.com)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>updater Target model Online training set Online Offline Learning Learned updater Learning Online training set Offline training tasks Manually design Target model</head><label></label><figDesc>The offline-learned update method, which</figDesc><table><row><cell>Learned</cell></row></table><note>Fig. 1: A general introduction of learning to update. Top: Typ- ically, a learning algorithm (e.g., SGD) is manually designed for online learning of the target model. Bottom: Contrarily, a learned updater is adopted for online learning, which is offline- trained using large numbers of videos. All figures are best viewed in color.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Tasks for learning the model updater during offline training. Each task consists of a training set and a test set. The model updater should learn from the training set and generate a target model which is tested on the test set.</figDesc><table><row><cell>Task 1 {</cell><cell>{</cell><cell>{</cell><cell>{</cell></row><row><cell>Training set</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Test set</cell><cell></cell></row><row><cell>Task 2 {</cell><cell>{</cell><cell>{</cell><cell>{</cell></row><row><cell>Training set</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Test set</cell><cell></cell></row><row><cell>Task 3 {</cell><cell>{</cell><cell>{</cell><cell>{</cell></row><row><cell>Training set</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Test set</cell><cell></cell></row><row><cell>Fig. 2:</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I :</head><label>I</label><figDesc>Correspondence between the meta-learning and learning to update. Best viewed by zooming in the electronic version.</figDesc><table><row><cell cols="2">meta-learning learn to update</cell><cell>Explanation</cell></row><row><cell>meta-training</cell><cell>offline training</cell><cell>The process of learning a meta-learner (model updater)</cell></row><row><cell>meta-test</cell><cell>online training</cell><cell>The process of applying the meta-learner (model updater)</cell></row><row><cell>meta-learner</cell><cell>model updater</cell><cell>The model that takes as input an training set and</cell></row><row><cell></cell><cell></cell><cell>outputs a learner (target model)</cell></row><row><cell>a task</cell><cell>a task</cell><cell>An example used for meta-training (offline training)</cell></row><row><cell></cell><cell></cell><cell>the meta-learner (model updater),</cell></row><row><cell></cell><cell></cell><cell>consisting of a training set and a test set</cell></row><row><cell>training set</cell><cell>training set</cell><cell>The dataset for learning a learner (target model)</cell></row><row><cell>test set</cell><cell>test set</cell><cell>The dataset for testing the learner (target model)</cell></row><row><cell>learner</cell><cell>target model</cell><cell>The model learned from the training set</cell></row></table><note>search image at frame t and y t is the corresponding label map. The update process based on SGD is</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Each task consists of a training set and a test set. The meta-learner (i.e. the model to be obtained via meta-training)</figDesc><table><row><cell></cell><cell></cell><cell>Target</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>model 1</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Anchor loss</cell><cell></cell><cell></cell></row><row><cell>Model updater</cell><cell>Conv</cell><cell>Target model</cell><cell>?</cell><cell>Response</cell></row><row><cell>ConvGRU</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Search</cell><cell></cell></row><row><cell>Candidate Target model</cell><cell></cell><cell></cell><cell>feature</cell><cell>Classification loss</cell></row><row><cell>(?)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CNN</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>{</cell><cell>{</cell><cell>{</cell><cell></cell><cell>{</cell></row><row><cell>Training set</cell><cell></cell><cell></cell><cell>Test set</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>(a). As shown in the table, every component has contributed to the final performance. Among</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE II :</head><label>II</label><figDesc>Ablations on the OTB-2013 dataset using template-based tracker. Only color images are adopted during offline training. RNNs are unrolled 25 steps by default.(a) Effective techniques: Cross mark means the technique is removed. RNNs are unrolled 25 steps by default unless large unroll length (50) is adopted. Image sequence are sampled by default with small frame interval (? 2). By removing the technique, we use large frame interval (? 10).</figDesc><table><row><cell></cell><cell></cell><cell cols="2">(b) RNN configurations: For exam-</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">ple, ConvGRU-192x2 denotes stack-</cell><cell cols="3">(c) Joint training: F: feature extractor, M:</cell></row><row><cell></cell><cell></cell><cell cols="2">ing 2 layers of ConvGRU, each with</cell><cell cols="3">model updater, F + M: joint training of feature</cell></row><row><cell></cell><cell></cell><cell>hidden size 192.</cell><cell></cell><cell cols="2">extractor and model updater.</cell></row><row><cell>interval update?</cell><cell></cell><cell>configuration</cell><cell>AUC (%)</cell><cell></cell><cell>Description</cell><cell>AUC (%)</cell></row><row><cell>estimated target position?</cell><cell></cell><cell>ConvGRU-192x1</cell><cell>62.1</cell><cell cols="2">scheme 1 stage 1: F + M</cell><cell>51.5</cell></row><row><cell>small frame interval? large unroll length?</cell><cell></cell><cell>ConvGRU-192x2 ConvGRU-192x3</cell><cell>63.6 63.3</cell><cell>scheme 2</cell><cell>stage 1: F stage 2: F + M</cell><cell>56.2</cell></row><row><cell>AUC (%)</cell><cell>64.4 63.6 63.0 62.6 61.7</cell><cell>ConvGRU-128x2</cell><cell>62.8</cell><cell></cell><cell>stage 1: F</cell></row><row><cell></cell><cell></cell><cell>ConvGRU-192x2</cell><cell>63.6</cell><cell>scheme 3</cell><cell>stage 2: M</cell><cell>63.1</cell></row><row><cell></cell><cell></cell><cell>ConvGRU-256x2</cell><cell>63.4</cell><cell></cell><cell>stage 3: F + M</cell></row><row><cell></cell><cell></cell><cell>ConvLSTM-128x2 ConvLSTM-192x2</cell><cell>62.2 62.8</cell><cell>Ours</cell><cell>stage 1: F</cell><cell>63.6</cell></row><row><cell></cell><cell></cell><cell>ConvLSTM-256x2</cell><cell>62.6</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE III :</head><label>III</label><figDesc>Comparisons with three representative baselines: no update, EMA-based update and SGD-based update. The AUC and EAO metric (higher is better) are reported for OTB and VOT, respectively. For OTB only, the feature extractors are trained with both color and grayscale images.We consider three relevant baselines: no update, EMA-based update and SGD-based update. Please refer to Section III for an introduction of the EMA-based and SGD-based update. No update: the target model is initialized in the first frame and then remains fixed. EMA-based update: the last target model and the current candidate target model are linearly interpolated. The learning rate ? is searched on OTB-2013 from 0.01 to 0.2 with step size 0.01.</figDesc><table><row><cell></cell><cell>OTB-2013</cell><cell cols="6">OTB-100 OTB-50 VOT-2015 VOT-2016 VOT-2017 Speed(FPS)</cell></row><row><cell>SiamFC-no-update</cell><cell>0.608</cell><cell>0.582</cell><cell>0.516</cell><cell>0.290</cell><cell>0.235</cell><cell>0.188</cell><cell>117</cell></row><row><cell>SiamFC-ema</cell><cell>0.618</cell><cell>0.597</cell><cell>0.538</cell><cell>0.286</cell><cell>0.259</cell><cell>0.216</cell><cell>91</cell></row><row><cell>SiamFC-sgd</cell><cell>0.644</cell><cell>0.614</cell><cell>0.563</cell><cell>0.306</cell><cell>0.278</cell><cell>0.248</cell><cell>13</cell></row><row><cell>SiamFC-lu (Ours)</cell><cell>0.657</cell><cell>0.620</cell><cell>0.577</cell><cell>0.318</cell><cell>0.295</cell><cell>0.263</cell><cell>82</cell></row><row><cell>CFNet-no-update</cell><cell>0.568</cell><cell>0.541</cell><cell>0.501</cell><cell>0.219</cell><cell>0.201</cell><cell>0.173</cell><cell>134</cell></row><row><cell>CFNet-ema</cell><cell>0.608</cell><cell>0.580</cell><cell>0.550</cell><cell>0.237</cell><cell>0.229</cell><cell>0.189</cell><cell>79</cell></row><row><cell>CFNet-sgd</cell><cell>0.613</cell><cell>0.590</cell><cell>0.555</cell><cell>0.235</cell><cell>0.212</cell><cell>0.182</cell><cell>9</cell></row><row><cell>CFNet-lu (Ours)</cell><cell>0.621</cell><cell>0.599</cell><cell>0.565</cell><cell>0.242</cell><cell>0.230</cell><cell>0.208</cell><cell>70</cell></row><row><cell cols="3">D. Baseline Comparisons</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>??? SGD-based update: we adopt the short-term update and long-term update following MDNet 7 [40]. Short-term update is triggered when the tracker has low confidence</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE IV :</head><label>IV</label><figDesc>Comparisons with state-of-the-art trackers.(a) OTB and VOT: Trackers are split into two groups: realtime trackers and non-realtime trackers. The AUC and EAO metric (higher is better) are reported for OTB and VOT, respectively. Red and blue fonts indicate 1st and 2nd best performance of each group, respectively.</figDesc><table><row><cell></cell><cell></cell><cell cols="8">OTB-2013 OTB-100 OTB-50 VOT-2015 VOT-2016 VOT-2017 Speed(FPS)</cell></row><row><cell>ECO</cell><cell></cell><cell>0.709</cell><cell>0.694</cell><cell>0.643</cell><cell>-</cell><cell cols="2">0.374</cell><cell>0.280</cell><cell>6</cell></row><row><cell>MDNet</cell><cell></cell><cell>0.708</cell><cell>0.678</cell><cell>0.645</cell><cell>0.38</cell><cell>-</cell><cell></cell><cell>-</cell><cell>1</cell></row><row><cell>LSART</cell><cell></cell><cell>0.677</cell><cell>0.672</cell><cell>-</cell><cell>-</cell><cell cols="2">0.324</cell><cell>0.323</cell><cell>1</cell></row><row><cell>CSRDCF</cell><cell></cell><cell>-</cell><cell>0.587</cell><cell>-</cell><cell>0.320</cell><cell cols="2">0.338</cell><cell>0.222</cell><cell>13</cell></row><row><cell>CREST</cell><cell></cell><cell>0.673</cell><cell>0.623</cell><cell>-</cell><cell>-</cell><cell cols="2">0.283</cell><cell>-</cell><cell>1</cell></row><row><cell>RFL</cell><cell></cell><cell>-</cell><cell>0.581</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>0.222</cell><cell>15</cell></row><row><cell cols="2">SiamFC-lu (Ours)</cell><cell>0.657</cell><cell>0.620</cell><cell>0.577</cell><cell>0.318</cell><cell cols="2">0.295</cell><cell>0.263</cell><cell>82</cell></row><row><cell>EAST</cell><cell></cell><cell>0.638</cell><cell>0.629</cell><cell>-</cell><cell>0.34</cell><cell>-</cell><cell></cell><cell>-</cell><cell>159</cell></row><row><cell>DSiam</cell><cell></cell><cell>0.656</cell><cell>-</cell><cell>-</cell><cell>0.293</cell><cell>-</cell><cell></cell><cell>-</cell><cell>25</cell></row><row><cell cols="2">CFNet-lu (Ours)</cell><cell>0.621</cell><cell>0.599</cell><cell>0.565</cell><cell>0.242</cell><cell cols="2">0.230</cell><cell>0.208</cell><cell>70</cell></row><row><cell>CFNet</cell><cell></cell><cell>0.610</cell><cell>0.589</cell><cell>0.538</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>79</cell></row><row><cell>SiamFC</cell><cell></cell><cell>0.608</cell><cell>0.582</cell><cell>0.516</cell><cell>0.290</cell><cell cols="2">0.235</cell><cell>0.188</cell><cell>117</cell></row><row><cell cols="10">(b) VOT-2017 realtime challenge: Results of top-performing trackers and</cell></row><row><cell cols="10">our trackers on the VOT-2017 realtime and non-realtime (also called baseline)</cell></row><row><cell cols="8">challenge. The EAO metric (higher is better) is reported.</cell><cell></cell></row><row><cell>challenge</cell><cell cols="5">SiamFC-lu (Ours) CSRDCF++ CFNet-lu (Ours)</cell><cell cols="3">SiamFC ECO-HC LSART</cell><cell>CFCF</cell><cell>ECO</cell></row><row><cell>realtime</cell><cell></cell><cell>0.258</cell><cell>0.212</cell><cell>0.200</cell><cell></cell><cell>0.182</cell><cell>0.177</cell><cell>0.055</cell><cell>0.059</cell><cell>0.078</cell></row><row><cell>non-realtime</cell><cell></cell><cell>0.263</cell><cell>0.229</cell><cell>0.208</cell><cell></cell><cell>0.188</cell><cell>0.238</cell><cell>0.323</cell><cell>0.286</cell><cell>0.280</cell></row><row><cell cols="10">while long-term update is conducted every 10 frames. The</cell></row><row><cell cols="10">hyperparameters of the SGD-based update (e.g., online</cell></row><row><cell cols="10">training set size 1000, batch size 8, learning rate 10,</cell></row><row><cell cols="10">number of iterations, 500 for long-term, 200 for short-</cell></row><row><cell cols="7">term, etc.) are searched on OTB-2013.</cell><cell></cell><cell></cell></row><row><cell cols="10">For fair comparison, we retrain the updater using publicly</cell></row><row><cell cols="10">available feature extractors (SiamFC and CFNet are open-</cell></row><row><cell>sourced)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">In this sense, the maximum modeling length of our method here is 300 frames.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">The number of hidden units are set by searching from 32 to 384 with step size 32. We empirically find that setting the number of hidden units close to the channel size of the target model performs well.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">OTB-2013 contains both color and grayscale videos. Therefore, models trained with both color and grayscale images typically perform better in this benchmark.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">Unlike MDNet which updates the last 3 layers, we only update the linear target model, i.e., the last layer for meaningful comparison.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENT This work was supported in part by National Natural Science Foundation of China (grant No. 61733007, 61572207).</p><p>The authors would like to thank Chong Luo and Anfeng He for helpful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*+!</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-*%</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">/</forename></persName>
		</author>
		<imprint>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>2&amp;amp;!</surname></persName>
		</author>
		<idno>&amp;34&apos;53!.&amp;&quot; #6789!:&amp;34&apos;53!.&amp;</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<title level="m">/&lt;89!:&amp;34&apos;53!.&amp; =*4&apos;53!.&amp; 6&quot;*&apos;,3</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Siamese instance search for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1420" to="1429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Fully-convolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<editor>ECCVw</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="850" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semi-supervised on-line boosting for robust tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="234" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to learn by gradient descent by gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust online appearance models for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Jepson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>El-Maraghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1296" to="1311" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-store tracker (muster): A cognitive psychology inspired approach to object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">V</forename><surname>Prokhorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="749" to="758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Competitive learning: From interactive activation to adaptive resonance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Grossberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="23" to="63" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The template update problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ishikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="810" to="815" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning dynamic siamese network for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning occlusion with likelihoods for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1551" to="1558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Robust object tracking with online multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1619" to="1632" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adaptive decontamination of the training set: A unified formulation for discriminative visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>H?ger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1430" to="1438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visual object tracking using adaptive correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Bolme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Draper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Lui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2544" to="2550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">High-speed tracking with kernelized correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="583" to="596" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">First step toward modelfree, anonymous object tracking with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06425</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Ratm: recurrent attentive tracking model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
		<editor>CVPRw</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hierarchical attentive recurrent tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recognizing human actions: A local svm approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schuldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">I. J. Robotics Res</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1231" to="1237" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Re3: Real-time recurrent regression networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06368</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The visual object tracking vot2014 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Pflugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cehovin</surname></persName>
		</author>
		<editor>ECCVw</editor>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="191" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The visual object tracking vot2016 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Pflugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cehovin</surname></persName>
		</author>
		<editor>ECCVw</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="777" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Online object tracking: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2411" to="2418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Recurrent filter learning for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2010" to="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">End-to-end representation learning for correlation filter based tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2805" to="2813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Toeplitz and circulant matrices: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations and Trends R in Communications and Information Theory</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="155" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1171" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Eco: Efficient convolution operators for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6638" to="6646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Youtubeboundingboxes: A large high-precision human-annotated data set for object detection in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mazzocchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5296" to="5305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Accelerating recurrent neural network training using sequence bucketing and multi-gpu data parallelization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Khomenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Shyshkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Radyvonenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bokhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Stream Mining &amp; Processing (DSMP), IEEE First International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="100" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<title level="m">Recurrent neural network regularization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Man?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Vi?gas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>software available from tensorflow.org. [Online</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning multi-domain convolutional neural networks for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4293" to="4302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Learning spatial-aware regressions for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Discriminative correlation filter with channel and spatial reliability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lukezic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Voj?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Zajc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4847" to="4856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Crest: Convolutional residual learning for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2574" to="2583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Learning policies for adaptive tracking with deep feature cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="105" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Good features to correlate for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gundogdu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Alatan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2526" to="2540" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<title level="m">2014, and is currently a Ph.D. student at the media and communication lab, HUST, supervised by Prof. Wenyu Liu. His research interests include meta-learning, few-shot learning and object tracking</title>
		<meeting><address><addrLine>Wuhan, China</addrLine></address></meeting>
		<imprint/>
	</monogr>
	<note>Bi Li received the B.Sc. degree from Huazhong University of Science and Technology (HUST)</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<title level="m">2015. He has been working as an associate researcher in Microsoft Research Asia since 2015. His research interests include computer vision and machine learning</title>
		<meeting><address><addrLine>Nanjing, China; Beijing, China</addrLine></address></meeting>
		<imprint/>
		<respStmt>
			<orgName>Wenxuan Xie received the B.Sc. degree from Nanjing University</orgName>
		</respStmt>
	</monogr>
	<note>2010, and the Ph.D. degree from Peking University</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">He has been leading the video analytics research empowering the Microsoft Cognitive Services and Azure Media Analytics Services since 2014. He was with Univ. of Missouri (MU) from 2003 to 2016, most recently as a Full Professor. Prior to that, he had worked for PacketVideo Corp</title>
	</analytic>
	<monogr>
		<title level="m">Wenjun (Kevin) Zeng (M&apos;97-SM&apos;03-F&apos;12) is a Principal Research Manager and a member of the senior leadership team (SLT) of Microsoft Research Asia</title>
		<imprint/>
		<respStmt>
			<orgName>Sharp Labs of America, Bell Labs, and Panasonic Technology</orgName>
		</respStmt>
	</monogr>
	<note>Wenjun has contributed significantly to the development of international standards (ISO MPEG, JPEG2000, and OMA</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">His current research interest includes mobile-cloud media computing, computer vision, social network/media analysis, and multimedia communications and security. He was an Associate Editor-in-Chief of IEEE Multimedia Magazine, and was an AE of IEEE Trans. on Circuits &amp; Systems for Video Technology (TCSVT)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ph</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TMM, ACM TOMCCAP, TCSVT, and IEEE Communications Magazine. He was on the Steering Committee of IEEE Trans. on Mobile Computing and IEEE TMM. He served as the Steering Committee Chair of IEEE ICME in 2010 and 2011, and is serving or has served as the General Chair or TPC Chair for several IEEE conferences</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
		<respStmt>
			<orgName>degrees from Tsinghua Univ., the Univ. of Notre Dame, and Princeton Univ., respectively</orgName>
		</respStmt>
	</monogr>
	<note>ICME. He was the recipient of several best paper awards. He is a Fellow of the IEEE</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">He is now a professor and associate dean of the School of Electronic Information and Communications, HUST. His current research areas include computer vision, multimedia, and machine learning</title>
	</analytic>
	<monogr>
		<title level="m">Wenyu Liu (M&apos;08-SM&apos;15) received the B.S. degree in Computer Science from Tsinghua University, Beijing, China, in 1986, and the M.S. and Ph.D. degrees, both in Electronics and Information Engineering</title>
		<meeting><address><addrLine>Wuhan, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
		<respStmt>
			<orgName>from Huazhong University of Science &amp; Technology (HUST)</orgName>
		</respStmt>
	</monogr>
	<note>He is a senior member of IEEE</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
