<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Supervised 3D Human Pose Estimation via Part Guided Novel Image Synthesis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jogendra</forename><surname>Nath</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Science</orgName>
								<address>
									<settlement>Bangalore</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kundu</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Seth</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Science</orgName>
								<address>
									<settlement>Bangalore</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mugalodi</forename><surname>Rakesh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Science</orgName>
								<address>
									<settlement>Bangalore</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesh Babu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Science</orgName>
								<address>
									<settlement>Bangalore</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Chakraborty</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Science</orgName>
								<address>
									<settlement>Bangalore</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Supervised 3D Human Pose Estimation via Part Guided Novel Image Synthesis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Camera captured human pose is an outcome of several sources of variation. Performance of supervised 3D pose estimation approaches comes at the cost of dispensing with variations, such as shape and appearance, that may be useful for solving other related tasks. As a result, the learned model not only inculcates task-bias but also dataset-bias because of its strong reliance on the annotated samples, which also holds true for weakly-supervised models. Acknowledging this, we propose a self-supervised learning framework 1 to disentangle such variations from unlabeled video frames. We leverage the prior knowledge on human skeleton and poses in the form of a single part based 2D puppet model, human pose articulation constraints, and a set of unpaired 3D poses. Our differentiable formalization, bridging the representation gap between the 3D pose and spatial part maps, not only facilitates discovery of interpretable pose disentanglement, but also allows us to operate on videos with diverse camera movements. Qualitative results on unseen in-the-wild datasets establish our superior generalization across multiple tasks beyond the primary tasks of 3D pose estimation and part segmentation. Furthermore, we demonstrate state-of-the-art weaklysupervised 3D pose estimation performance on both Hu-man3.6M and MPI-INF-3DHP datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Analyzing humans takes a central role in computer vision systems. Automatic estimation of 3D pose and 2D partarrangements of highly deformable humans from monocular RGB images remains an important, challenging and unsolved problem. This ill-posed classical inverse problem has diverse applications in human-robot interaction <ref type="bibr" target="#b52">[53]</ref>, augmented reality <ref type="bibr" target="#b14">[15]</ref>, gaming industry, etc.</p><p>In a fully-supervised setting <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b9">10]</ref>, the advances * Equal contribution. <ref type="bibr" target="#b0">1</ref>   in this area are mostly driven by recent deep learning architectures and the collection of large-scale annotated samples. However, unlike 2D landmark annotations, it is very difficult to manually annotate 3D human pose on 2D images. A usual way of obtaining 3D ground-truth (GT) pose annotations is through a well-calibrated in-studio multi-camera setup <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b49">50]</ref>, which is difficult to configure in outdoor environments. This results in a limited diversity in the available 3D pose datasets, which greatly limits the generalization of supervised 3D pose estimation models.</p><p>To facilitate better generalization, several recent works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b45">46]</ref> leverage weakly-supervised learning techniques that reduce the need for 3D GT pose annotations. Most of these works use an auxiliary task such as multiview 2D pose estimation to train a 3D pose estimator <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b26">27]</ref>. Instead of using 3D pose GT for supervision, a 3D pose network is supervised with loss functions on multiview projected 2D poses. To this end, several of these works still require considerable annotations in terms of paired 2D pose GT <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b27">28]</ref>, multi-view images <ref type="bibr" target="#b26">[27]</ref> and known camera parameters <ref type="bibr" target="#b45">[46]</ref>. Dataset bias still remains a challenge in these techniques as they use paired image and 2D pose GT datasets which have limited diversity. Given the ever-changing human fashion and evolving culture, the visual appearance of humans keeps varying and we need to keep updating the 2D pose datasets accordingly.</p><p>In this work, we propose a differentiable and modular self-supervised learning framework for monocular 3D human pose estimation along with the discovery of 2D part segments. Specifically, our encoder network takes an image as input and outputs 3 disentangled representations: 1. view-invariant 3D human pose in canonical co-ordinate system, 2. camera parameters and 3. a latent code representing foreground (FG) human appearance. Then, a decoder network takes the above encoded representations, projects them onto 2D and synthesizes FG human image while also producing 2D part segmentation. Here, a major challenge is to disentangle the representations for 3D pose, camera, and appearance. We achieve this disentanglement by training on video frame pairs depicting the same person, but in varied poses. We self-supervise our network with consistency constraints across different network outputs and across image pairs. Compared to recent self-supervised approaches that either rely on videos with static background <ref type="bibr" target="#b44">[45]</ref> or work with the assumption that temporally close frames have similar background <ref type="bibr" target="#b21">[22]</ref>, our framework is robust enough to learn from large-scale in-the-wild videos, even in the presence of camera movements. We also leverage the prior knowledge on human skeleton and poses in the form of a single part-based 2D puppet model, human pose articulation constraints, and a set of unpaired 3D poses. <ref type="figure" target="#fig_1">Fig. 1</ref> illustrates the overview of our self-supervised learning framework.</p><p>Self-supervised learning from in-the-wild videos is challenging due to diversity in human poses and backgrounds in a given pair of frames which may be further complicated due to missing body parts. We achieve the ability to learn on these wild video frames with a pose-anchored deformation of puppet model that bridges the representation gap between the 3D pose and the 2D part maps in a fully differentiable manner. In addition, the part-conditioned appearance decoding allows us to reconstruct only the FG human appearance resulting in robustness to changing backgrounds.</p><p>Another distinguishing factor of our technique is the use of well-established pose prior constraints. In our selfsupervised framework, we explicitly model 3D rigid and non-rigid pose transformations by adopting a differentiable parent-relative local limb kinematic model, thereby reducing ambiguities in the learned representations. In addition, for the predicted poses to follow the real-world pose distribution, we make use of an unpaired 3D pose dataset. We interchangeably use predicted 3D pose representations and sampled real 3D poses during training to guide the model towards a plausible 3D pose distribution.</p><p>Our network also produces useful part segmentations. With the learned 3D pose and camera representations, we model depth-aware inter-part occlusions resulting in robust part segmentation. To further improve the segmentation beyond what is estimated with pose cues, we use a novel differentiable shape uncertainty map that enables extraction of limb shapes from the FG appearance representation.</p><p>We make the following main contributions:</p><p>? We present techniques to explicitly constrain the 3D pose by modeling it at its most fundamental form of rigid and non-rigid transformations. This results in interpretable 3D pose predictions, even in the absence of any auxiliary 3D cues such as multi-view or depth. ? We propose a differentiable part-based representation which enables us to selectively attend to foreground human appearance which in-turn makes it possible to learn on in-the-wild videos with changing backgrounds in a self-supervised manner. ? We demonstrate generalizability of our self-supervised framework on unseen in-the-wild datasets, such as LSP <ref type="bibr" target="#b22">[23]</ref> and YouTube. Moreover, we achieve stateof-the-art weakly-supervised 3D pose estimation performance on both Human3.6M <ref type="bibr" target="#b19">[20]</ref> and MPI-INF-3DHP <ref type="bibr" target="#b35">[36]</ref> datasets against the existing approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Human 3D pose estimation is a well established problem in computer vision, specifically in fully supervised paradigm. Earlier approaches <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b8">9]</ref> proposed to infer the underlying graphical model for articulated pose estimation. However, the recent CNN based approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b36">37]</ref> focus on regressing spatial keypoint heat-maps, without explicitly accounting for the underlying limb connectivity information. However, the performance of such models heavily relies on a large set of paired 2D or 3D pose annotations. As a different approach, <ref type="bibr" target="#b25">[26]</ref> proposed to regress latent representation of a trained 3D pose autoencoder to indirectly endorse a plausibility bound on the output predictions. Recently, several weakly supervised approaches utilize varied set of auxiliary supervision other than the direct 3D pose supervision (see <ref type="table" target="#tab_0">Table 1</ref>). In this paper, we address a more challenging scenario where we consider access to only a set of unaligned 2D pose data to facilitate the learning of a plausible 2D pose prior (see <ref type="table" target="#tab_0">Table 1</ref>).</p><p>In literature, while several supervised shape and appearance disentangling techniques <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b48">49]</ref> exist, the available unsupervised pose estimation works (i.e. in the absence of multi-view or camera extrinsic supervision), are mostly limited to 2D landmark estimation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22]</ref> for rigid or mildly deformable structures, such as facial landmark detection, constrained torso pose recovery etc. The general idea <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b53">54]</ref> is to utilize the relative transformation between a pair of images depicting a consistent appearance with varied pose. Such image pairs are usually sampled from a video satisfying appearance invariance <ref type="bibr" target="#b21">[22]</ref> or synthetically generated deformations <ref type="bibr" target="#b46">[47]</ref>. Beyond landmarks, object parts <ref type="bibr" target="#b31">[32]</ref> can infer shape alongside the pose. Part representations are best suited for 3D articulated objects as a result of its occlusion-aware property as opposed to simple landmarks. In general, the available unsupervised part learning techniques <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b18">19]</ref> are mostly limited to segmentation based discriminative tasks. On the other hand, <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b40">41]</ref> explicitly leverage the consistency between geometry and the semantic part segments. However, the kinematic articulation constraints are well defined in 3D rather than in 2D <ref type="bibr" target="#b1">[2]</ref>. Motivated by this, we aim to leverage the advantages of both non-spatial 3D pose <ref type="bibr" target="#b25">[26]</ref> and spatial part-based representation <ref type="bibr" target="#b31">[32]</ref> by proposing a novel 2D pose-anchored part deformation model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>We develop a differentiable framework for selfsupervised disentanglement of 3D pose and foreground appearance from in-the-wild video frames of human activity.</p><p>Our self-supervised framework builds on the conventional encoder-decoder architecture (Sec. 3.2). Here, the encoder produces a set of local 3D vectors from an input RGB image. This is then processed through a series of 3D transformations, adhering to the 3D pose articulation constraints to obtain a set of 2D coordinates (camera projected, non-spatial 2D pose). In Sec. 3.1, we define a set of part based representations followed by carefully designed differentiable transformations required to bridge the representation gap between the non-spatial 2D pose and the spatial part maps. This serves three important purposes. First, their spatial nature facilitates compatible input pose conditioning for the fully-convolutional decoder architecture. Second, it enables the decoder to selectively synthesize only FG human appearance ignoring the variations in the background. Third, it facilitates a novel way to encode the 2D joint and part association using a single template puppet model. Finally, Sec. 3.3 describes the proposed selfsupervised paradigm which makes use of the pose-aware spatial part maps for simultaneous discovery of 3D pose and part segmentation using image pairs from wild videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Joint-anchored spatial part representation</head><p>One of the major challenges in unsupervised pose or landmark detection is to map the model-discovered landmarks to the standard landmark conventions. This is essential to facilitate the subsequent task-specific pipelines, which expect the input pose to follow a certain convention. Prior works <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b29">30]</ref> rely on paired supervision to learn this mapping. In absence of such supervision, we aim to encode this convention in a canonical part dictionary where the association of 2D joints with respect to the body parts is extracted from a single manually annotated puppet template ( <ref type="figure" target="#fig_3">Fig. 2C, top panel)</ref>. This can be interpreted as a 2D human puppet model, which can approximate any human pose deformation via independent spatial transformation of body parts while keeping intact the anchored joint associations.</p><p>Canonical maps. We extract canonical part maps, {? (l) c } L l=1 (here, l: limb index and L: total number of limbs or parts), where we perform erosion followed by Gaussian blurring of binary part segments to account for the associated shape uncertainty (i.e. body shape or apparel shape variations). We represent ?</p><formula xml:id="formula_0">(l) c : U ? [0, 1], where U ? N 2</formula><p>is the space of spatial indices. In addition, we also extract canonical shape uncertainty maps {? </p><formula xml:id="formula_1">p = S (l) ? ? (l)</formula><p>c . Here, S (l) represents an affine transformation of the spatial indices u ? U, whose rotation, scale, and translation parameters are obtained as a function of (q l(j1) , q l(j2) , r l(j1) c , r l(j2) c ), where q l(j1) , q l(j2) denote the joint locations associated with the limb l in pose q. Similarly, we also compute the part-wise shape uncertainty maps as ?</p><formula xml:id="formula_2">(l) p = S (l) ? ? (l) c . Note that, {? (l) p } L l=1 and {? (l) p } L l=1</formula><p>are unaware of inter-part occlusion in the absence of limb-depth information. Following this, we obtain single-channel maps (see <ref type="figure" target="#fig_3">Fig. 2D</ref>), i.e. a) shape uncertainty map as w unc = max l ? (l)</p><formula xml:id="formula_3">p , and b) single-channel FG-BG map as w f g = max l ? (l) p .</formula><p>The above formalization bridges the representation gap between the raw joint locations, q and the output spatial maps ? p , w f g , and w unc , thereby facilitating them to be used as differentiable spatial maps for the subsequent selfsupervised learning.</p><p>Depth-aware part segmentation. For 3D deformable objects, a reliable 2D part segmentation can be obtained with the help of following attributes, i.e. a) 2D skeletal . We obtain a scalar depth value associated with each limb l as,</p><formula xml:id="formula_4">d (l) = (q l(j1) d + q l(j2) d</formula><p>)/2. We use these depth values to alter the strength of depth-unaware part-wise pose maps, ? (l) p at each spatial location, u ? U by modulating the strength of part map intensity as being inversely proportional to the depth values. This is realized in the following steps:</p><formula xml:id="formula_5">a) ? (l) d (u) = softmax L l=1 (? (l) p (u)/d (l) ), b) ? (L+1) d (u) = 1 ? max L l=1 ? (l) d (u), and c)? (l) d (u) = softmax L+1 l=1 (? (l) d (u)</formula><p>). Here, (L + 1) indicates the spatial-channel dedicated for the background. Additionally, a non-differentiable 2D partsegmentation map (see <ref type="figure" target="#fig_3">Fig. 2E</ref>) is obtained as,</p><formula xml:id="formula_6">y p (u, l) = 1(l = argmax L+1 l=1? (l) d (u)).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Self-supervised pose network</head><p>The architecture for self-supervised pose and appearance disentanglement consists of a series of pre-defined differentiable transformations facilitating discovery of a constrained latent pose representation. As opposed to imposing learning based constraints <ref type="bibr" target="#b13">[14]</ref>, we devise a way around where the 3D pose articulation constraints (i.e. knowledge of joint connectivity and bone-length) are directly applied via structural means, implying guaranteed constraint imposition. a) Encoder network. As shown in <ref type="figure" target="#fig_3">Fig. 2A</ref>, the encoder E takes an input image I and outputs three disentangled factors, a) a set of local 3D vectors: v 3D ? R 3J , b) camera parameters: c, and c) a FG appearance: a ? R H?W ?Ch .</p><p>As compared to spatial 2D geometry <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b31">32]</ref>, discovering the inherent 3D human pose is a highly challenging task considering the extent of associated non-rigid deformation, and rigid camera variations <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b28">29]</ref>. To this end, we define a canonical coordinate system C, where face-vector of the skeleton is canonically aligned along the +ve X-axis, thus making it completely view-invariant. Here, the face-vector is defined as the perpendicular direction of the plane spanning the neck, left-hip and right-hip joints. As shown in <ref type="figure" target="#fig_3">Fig. 2B</ref>, in v 3D , except the pelvis, neck, left-hip and righthip, all other joints are defined at their respective parent relative local coordinate systems (i.e. parent joint as the origin with axis directions obtained by performing Gram-Schmidt orthogonalization of the parent-limb vector and the facevector). Accordingly, we define a recursive forward kinematic transformation T f k to obtain the canonical 3D pose from the local limb vectors, i.e. p 3D = T f k (v 3D ), which accesses a constant array of limb length magnitudes <ref type="bibr" target="#b67">[68]</ref>.</p><p>Here, the camera extrinsics, c (3 rotation angles and 3 restricted translations ensuring that the camera-view captures all the skeleton joints in p 3D ) is obtained at the encoder output, whereas a fixed perspective camera projection is applied to obtain the final 2D pose representation, i.e. q = T c (p 3D ). A part based deformation operation on this 2D pose q (Sec. 3.1) is shown as p = T s (q, d (l) ), where T s : {S (l) } L l=1 and p : {?  <ref type="figure">Figure 3</ref>. An overview of data-flow pipelines for the proposed self-supervised objectives. Images close to the output heads show the network output for the given set of exemplar input tuple. Here, the color of transformation blocks are consistent with <ref type="figure" target="#fig_3">Fig. 2A.</ref> and ii) a predicted part segmentation map? via a bifurcated CNN decoder (see <ref type="figure">Fig. 3A</ref>). The common decoder branch, D consists of a series of up-convolutional layers conditioned on the spatial pose map p at each layer's input (i.e. multi-scale pose conditioning). Whereas, D I and D seg follow up-convolutional layers to their respective outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Self-supervised training objectives</head><p>The prime design principle of our self-supervised framework is to leverage the interplay between the pose and appearance information by forming paired input images of either consistent pose or appearance.</p><p>Given a pair of source and target image, (I s , I t ), sampled from the same video i.e. with consistent FG appearance, the shared encoder extracts their respective pose and appearance as (p s , p t ) and (a s , a t ) (see <ref type="figure">Fig. 3A</ref>). We denote the decoder outputs as (? pt as ,? pt as ) while the decoder takes in pose, p t with appearance, a s (this notation is consistent in later sections). Here,? pt as is expected to depict the person in the target pose p t . Such a cross-pose transfer setup is essential to restrict leakage of pose information through the appearance. Whereas, the low dimensional bottleneck of v 3D followed by the series of differentiable transformations prevents leakage of appearance through pose <ref type="bibr" target="#b21">[22]</ref>.</p><p>To effectively operate on wild video frames (i.e. beyond the in-studio fixed camera setup <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b21">22]</ref>), we aim to utilize the pose-aware, spatial part representations as a means to disentangle the FG from BG. Thus, we plan to reconstruct I pt as with a constant BG color BG c and segmented FG appearance (see <ref type="figure">Fig. 3A</ref>). Our idea stems from the concept of co-saliency detection <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b17">18]</ref>, where the prime goal is to discover the common or salient FG from a given set of two or more images. Here, the part appearances belonging to the model predicted part regions have to be consistent across I s and I t for a successful self-supervised pose discovery. Access to unpaired 3D/2D pose samples. We denote p 3D z and q z = T c (p 3D z ) as a 3D pose and its projection (via random camera), sampled from an unpaired 3D pose dataset D z , respectively. Such samples can be easily collected without worrying about BG or FG diversity in the corresponding camera feed (i.e. a single person activity). We use these samples to further constrain the latent space towards realizing a plausible 3D pose distribution. a) Image reconstruction objective. Unlike <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b21">22]</ref>, we do not have access to the corresponding ground-truth representation for the predicted? pt as , giving rise to an increased possibility of producing degenerate solutions or mode-collapse. In such cases, the model focuses on fixed background regions as the common region between the two images, specifically for in-studio datasets with limited BG variations. One way to avoid such scenarios is to select image pairs with completely diverse BG (i.e. select image pairs with high L2 distance in a sample video-clip).</p><p>To explicitly restrict the model from inculcating such BG bias, we incorporate content-based regularization. An uncertain pseudo FG mask is used to establish a one-to-one correspondence between the reconstructed image? pt as and the target image I t . This is realized through a spatial-mask, m sal which highlights the salient regions (applicable for any image frame) or regions with diverse motion cues (applicable for frames captured in a fixed camera). We formulate a pseudo (uncertain) reconstruction objective as,</p><formula xml:id="formula_7">L u I = (1 ?? pt as (L + 1) + ?m It sal ) ? |? pt as ? I t |</formula><p>Here, ? denotes pixel-wise weighing and ? is a balancing hyperparameter. Note that, the final loss is computed as average over all the spatial locations, u ? U. This loss enforces a self-supervised consistency between the poseaware part maps,? pt as and the salient common FG to facilitate a reliable pose estimate.</p><p>As a novel direction, we utilize q z ? D z to form a pair of image predictions (? pz as ,? pz at ) following simultaneous appearance invariance and pose equivariance (see <ref type="figure">Fig. 3B</ref>). Here, a certain reconstruction objective is defined as,</p><formula xml:id="formula_8">L c I = w pz f g ? |? pz as ?? pz at | + (1 ? w pz f g ) ? |? pz as ? BG c | b) Part-segmentation objective.</formula><p>Aiming to form a consistency between the true pose p z and the corresponding part segmentation output, we formulate,</p><formula xml:id="formula_9">L seg = (1 ? w unc ) ? CE(? pz as , y pz ) + w unc ? SE(? pz as )</formula><p>Here, CE, and SE denote the pixel-wise cross-entropy and self-entropy respectively. Moreover, we confidently enforce segmentation loss with respect to one-hot map y pz (Sec. 3.1) only at the certain regions, while minimizing the Shanon's entropy for the regions associated with shape uncertainty as captured in w unc . Here, the limb depth required to compute y pz is obtained fromp 3D z = E p (? pz as ) <ref type="figure">(Fig. 3C</ref>). In summary, the above self-supervised objectives form a consistency among p,?, and?; a) L u I enforces consistency between? and?, b) L c I enforces consistency between p (via w f g ) and?, c) L seg enforces consistency between p (via y pz ) and?. However, the model inculcates a discrepancy between the predicted pose and the true pose distributions. It is essential to bridge this discrepancy as L c I and L seg rely on true pose q z = T c (p 3D z ), whereas L u I relies on the predicted pose q t . Thus, we employ an adaptation strategy to guide the model towards realizing a plausible pose prediction. c) Adaptation via energy minimization. Instead of employing an ad-hoc adversarial discriminator <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b61">62]</ref>, we devise a simpler yet effective decoupled energy minimization strategy <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21]</ref>. We avoid a direct encoder-decoder interaction during gradient back-propagation, by updating the encoder parameters, while freezing the decoder parameters and vice-versa. However, this is performed while enforcing a reconstruction loss at the output of the secondary encoder in a cyclic auto-encoding scenario (see <ref type="figure">Fig. 3C</ref>). The two energy functions are formulated as</p><formula xml:id="formula_10">L p 3D z = |p 3D z ?p 3D z | and L as = |a s ?? s |, wherep 3D z = E p (? pz as ) and? s = E a (? pz as )</formula><p>. The decoder parameters are updated to realize a faith-ful? pz as , as the frozen encoder expects? pz as to match its input distribution of real images (i.e. I s ) for an effective energy minimization. Here, the encoder can be perceived as a frozen energy network as used in energy-based GAN <ref type="bibr" target="#b65">[66]</ref>. A similar analogy applies while updating the encoder parameters with gradients from the frozen decoder. Each alternate energy minimization step is preceded by an overall optimization of the above consistency objectives, where both encoder and decoder parameters are updated simultaneously (see Algo. 1). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We perform a thorough experimental analysis to establish the effectiveness of our proposed framework on 3D pose estimation, part segmentation and novel image synthesis tasks, across several datasets beyond the in-studio setup. Implementation details. We employ an ImageNet trained Resnet-50 architecture <ref type="bibr" target="#b16">[17]</ref> as the base CNN for the encoder E. We first bifurcate it into two CNN branches dedicated to pose and appearance, then the pose branch is further bifurcated into two multi-layer fully-connected networks to obtain the local pose vectors v 3D and the camera parameters c. While training, we use separate AdaGrad optimizers <ref type="bibr" target="#b11">[12]</ref> for each loss term at alternate training iterations. We perform appearance (color-jittering) and pose augmentations (mirror flip and inplane rotation) selectively for I t and I s conceding their invariance effect on p t and a s respectively.</p><p>Datasets. We train the base-model on image pairs sampled from a mixed set of video datasets i.e. Human3.6M <ref type="bibr" target="#b19">[20]</ref> (H3.6M) and an in-house collection of in-the-wild YouTube videos (YTube). As opposed to the in-studio H3.6M images, the YTube dataset constitutes a substantial diversity in apparels, action categories (dance forms, parkour stunts, etc.), background variations, and camera movements. The raw video frames are pruned to form the suitable image pairs after passing them through an off-the-shelf persondetector <ref type="bibr" target="#b43">[44]</ref>. We utilize an unsupervised saliency detection method <ref type="bibr" target="#b69">[70]</ref> to obtain m sal for the wild YTube frames, whereas for samples from H3.6M m sal is obtained directly through the BG estimate <ref type="bibr" target="#b44">[45]</ref>. Further, LSP <ref type="bibr" target="#b30">[31]</ref> and MPI-INF-3DHP <ref type="bibr" target="#b35">[36]</ref> (3DHP) datasets are used to evaluate generalizability of our framework. We collect the unpaired 3D pose samples, q z from MADS <ref type="bibr" target="#b64">[65]</ref> and CMU-MoCap <ref type="bibr" target="#b0">[1]</ref> dataset keeping a clear domain gap with respect to the standard datasets chosen for benchmarking our performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluation on Human3.6M</head><p>Inline with the prior arts <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b44">45]</ref>, we evaluate our 3D pose estimation performance in the standard protocol-II setting (i.e. with scaling and rigid alignment). We experimented on 4 different variants of the proposed framework with increasing degrees of supervision levels. The base model in absence of any paired supervision is regarded as Ours(unsup). In presence of multi-view information (with camera extrinsics), we finetune the model by enforcing consistent canonical pose p 3D and camera shift for multi-view image pairs, termed as Ours(multi-view-sup). Similarly, finetuning in presence of a direct supervision on the corresponding 2D pose GT is regarded as Ours(weakly-sup). Lastly, finetuning in presence of a direct 3D pose supervision on 10% of the full training set is referred to as Ours(semi-sup). <ref type="table" target="#tab_1">Table 2</ref> depicts our superior performance against the prior arts in their respective supervision levels. <ref type="table" target="#tab_1">Table 2</ref>. Comparison of 3D pose estimation results on Hu-man3.6M. Comparable metrics of fully-supervised methods are included for reference. Our approach achieves state-of-the art performance while brought to the same supervision-level (divided by horizontal lines) of Full-2D (row no. <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref> or Mutli-view (row no. 9-10). Moreover, Ours(semi-sup) achieves comparable performance against the prior fully supervised approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>No.</head><p>Protocol-II Supervision Avg. MPJPE(?) 1.</p><p>Zhou et al. <ref type="bibr" target="#b68">[69]</ref> Full-3D 106.7 2.</p><p>Chen et al. <ref type="bibr" target="#b5">[6]</ref> Full-3D 82.7 3.</p><p>Martinez et al. <ref type="bibr" target="#b34">[35]</ref> Full-3D 52.1 4.</p><p>Wu et al. <ref type="bibr" target="#b59">[60]</ref> Full-2D 98.4 5.</p><p>Tung et al. <ref type="bibr" target="#b56">[57]</ref> Full-2D 97.2 6.</p><p>Chen et al. <ref type="bibr" target="#b6">[7]</ref> Full-2D 68.0 7.</p><p>Wandt et al. <ref type="bibr" target="#b58">[59]</ref> Full-2D 65.1 8.</p><p>Ours(weakly-sup) Full-2D 62.4 9.</p><p>Rhodin et al. <ref type="bibr" target="#b44">[45]</ref> Multi-view 98.2 10.</p><p>Ours(multi-view-sup) Multi-view 85.8 11.</p><p>Ours(unsup) No sup. 99.2 12.</p><p>Ours(semi-sup) 10%-3D 50.8 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation on MPI-INF-3DHP</head><p>With our framework, we also demonstrate a higher level of cross-dataset generalization, thereby minimizing the need for finetuning on novel unseen datasets. The carefully devised constraints at the intermediate pose representation are expected to restrict the model from producing implausible poses even when tested in unseen environments. To evaluate this, we directly pass samples of MPI-INF-3DHP <ref type="bibr" target="#b35">[36]</ref> (3DHP) test-set through Ours(weakly-sup) model trained on YTube+H3.6M and refer it as unsupervised transfer, denoted as -3DHP in <ref type="table" target="#tab_1">Table 4</ref>. Further, we finetune the Ours(weakly-sup) on 3DHP dataset at three supervision levels, a) no supervision, b) full 2D pose supervision, and c) 10% 3D pose supervision as reported in <ref type="table" target="#tab_1">Table 4</ref>, at rows 10, 7, and 11 respectively. The reported metrics clearly highlight our superiority against the prior arts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation study</head><p>We evaluate the effectiveness of the proposed local vector representation followed by the forward kinematic transformations, against a direct estimation of the 3D joints in camera coordinate system in the presence of appropriate bone length constraints <ref type="bibr" target="#b6">[7]</ref>. As reported in <ref type="table" target="#tab_1">Table 3</ref>, our disentanglement of camera from the view-invariant canonical 2.5k (5% S1) 5k (10% S1) 25k (100% S1) 49k (S1) 129k (S1+S5) 179k (S1+S5+S6)  pose shows a clear superiority as a result of using the 3D pose articulation constraints in the most fundamental form. Besides this, we also perform ablations by removing q z or m sal from the unsupervised training pipeline. As shown in <ref type="figure">Fig. 5B</ref>, without q z the model predicts implausible part arrangements even while maintaining a roughly consistent FG silhouette segmentation. However, without m sal , the model renders a plausible pose on the BG area common between the image pairs, as a degenerate solution.</p><p>As an ablation of the semi-supervised setting, (see <ref type="figure" target="#fig_6">Fig. 4</ref>), we train the proposed framework on progressively increasing amount of 3D pose supervision alongside the unsupervised learning objective. Further, we perform the same for the Encoder network without the unsupervised objectives (thus, discarding the decoder networks) and term it as Encoder(Only-3D-sup). The plots in <ref type="figure" target="#fig_6">Fig. 4</ref> clearly highlight our reduced dependency on the supervised data implying graceful and faster transferability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation of part segmentation</head><p>For evaluation of part-segmentation, we standardize the ground-truth part conventions across both LSP <ref type="bibr" target="#b22">[23]</ref> and H3.6M datasets via SMPL model fitting <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b30">31]</ref>. This convention roughly aligns with the part-puppet model used in <ref type="figure" target="#fig_3">Fig. 2C</ref>, thereby maintaining a consistent part to joint association. Note that, w unc is supposed to account for the ambiguity between the puppet-based shape-unaware segmen-View syn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.</head><p>A. H36M, in-studio dataset B.   tation against the image dependent shape-aware segmentation. In <ref type="figure">Fig. 5A</ref>, we show the effectiveness of this design choice, where the shape-unaware segmentation is obtained at y p after depth-based part ordering, and the corresponding shape-aware segmentation is obtained at? output. Further, quantitative comparison of part segmentation is reported in <ref type="table" target="#tab_1">Table 5</ref>. We achieve comparable results against the prior arts, in absence of additional supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Qualitative results</head><p>To evaluate the effectiveness of the disentangled factors beyond the intended primary task of 3D pose estimation and part segmentation, we manipulate them to analyze their effect on the decoder synthesized output image. In posetransfer, pose obtained from an image is transferred to the appearance of another. However, in view-syn., we randomly vary the camera extrinsic values in c. The results shown in <ref type="figure">Fig. 5A</ref> are obtained from Ours(unsup) model, which is trained on the mixed YTube+H3.6 dataset. This demonstrates the clear disentanglement of pose and appearance. <ref type="figure">Fig. 6</ref> depicts qualitative results for the primary 3D pose estimation and part segmentation tasks using Ours(weaklysup) model as introduced in Sec. 4.1. In <ref type="figure">Fig. 6B</ref>, we show results on the unseen LSP dataset, where the model has not seen this dataset even during self-supervised training. A consistent performance on such unseen dataset further establishes generalizability of the proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed a self-supervised 3D pose estimation method that disentangles the inherent factors of variations via part guided human image synthesis. Our framework has two prominent traits. First, effective imposition of both human 3D pose articulation and joint-part association constraint via structural means. Second, usage of depth-aware part based representation to specifically attend to the FG human resulting in robustness to changing backgrounds. However, extending such a framework for multi-person or partially visible human scenarios remains an open challenge.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Project page: http://val.cds.iisc.ac.in/pgp-human/</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Our self-supervised framework not only produces 3D pose and part segmentation but also enables novel image synthesis via interpretable latent manipulation of the disentangled factors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>highlight only the uncertain regions(Fig. 2C, bottom panel). The two anchored joint locations for each limb l and its corresponding part map ? except for the torso which is represented using 4 joints.Part deformation model. For a given 2D pose q ? R 2J with J being the total number of joints, part-wise pose maps are obtained as independent spatial-transformations of the canonical part maps, i.e. ?(l)    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>A. An overview of the full encoder module. B. Transforming the parent-relative local 3D vectors v3D to camera projected 2D pose q. C. The template 2D puppet model. D. Puppet imitates the pose in q. E. Image independent depth-aware part segmentation. pose, b) part-shape information, and c) knowledge of interpart occlusion. Here, the 2D skeletal pose and the knowledge of inter-part occlusion can be extracted by accessing camera transformation of the corresponding 3D pose representation. Let, the depth of the 2D joints in q with respect to the camera be denoted as q l(j1) d and q l(j2) d</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Finally, T denotes the entire series of differentiable transformations, i.e. T s ? T c ? T f k , as shown in Fig. 2A. Here, ? denotes composition operation. b) Decoder network. The decoder takes a concatenated representation of the FG appearance, a and pose, p as input to obtain two output maps, i) a reconstructed image?, A. Masked image recon. as content regularization B. Appearance invariance and pose equivariance shared C. Adaptation via decoupled energy minimization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Ours(Semi-sup.) Encoder(Only-3D-sup)Amount of data with 3D supervision 1% is 3700</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>3D pose estimation on H3.6M as a function of the amount of training supervision. Ours(semi-sup) shows faster transferability as compared to the fully supervised counterpart.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>A. Novel image synthesis via latent manipulation of a, p and c. It also shows the effect of independent non-rigid (pose-transfer) and rigid (view-syn.) variations as a result of explicit disentanglement. Notice the corresponding shape-unaware (puppet deformation) and shape-aware part-segmentation results. B. Qualitative analysis of ablations showing importance of qz and m sal (refer Sec. 4.3). A. Results on H36M dataset (in-studio) C. Results on LSP dataset (unseen, in-the-wild samples) E. Results on 3DHP D. Results on YTube dataset (in-the-wild) B. Results on 3DPW dataset (unseen, in-the-wild samples) Qualitative results on 5 different datasets. Failure cases are highlighted in magenta which specifically occur in presence of multi-level inter-limb occlusion (see LSP failure case) and very rare, athletic poses (see YTube failure case). However, the model faithfully attends to single-level occlusions, enabled by the depth-aware part representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Characteristic comparison of our approach against prior weakly-supervised human 3D pose estimation works, in terms of access to direct (paired) or indirect (unpaired) supervision levels.</figDesc><table><row><cell>Methods</cell><cell>Paired sup. (MV: muti-view) MV Cam. 2D pair extrin. pose</cell><cell>Unpaireed 2D/3D pose Supervision</cell><cell>Sup. for latent to 3D pose mapping</cell></row><row><cell>Rhodin et al. [45]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Kocabas et al. [27]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Chen et al. [8]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Wandt et al. [59]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Chen et al. [7]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>? E :</head><label>E</label><figDesc>Trainable parameters of the Encoder E ? D : Trainable parameters of the Decoder (includes D, D I , and D seg ) for iter &lt; MaxIter do if iter (mod 2) = 0 then Update ? E by optimizing L p 3D z and L as in separate Adagrad optimizers on frozen ? D . else Update ? D by optimizing L p 3D z and L as in separate Adagrad optimizers on frozen ? E . end Update (? E , ? D ) by optimizing L u I , L c I , and L seg in separate Adagrad optimizers.</figDesc><table><row><cell>end</cell></row><row><cell>Algorithm 1: Self-supervised learning with the proposed</cell></row><row><cell>adaptation via decoupled energy minimization.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table><row><cell cols="3">Ablation analysis, highlighting importance of various</cell></row><row><cell cols="3">constraints and regularization in the proposed self-supervised 3D</cell></row><row><cell cols="3">pose estimation framework. (Qualitative results in Fig. 5B)</cell></row><row><cell>Method</cell><cell>MPJPE(?) on</cell><cell>3DPCK(?) on</cell></row><row><cell>(unsup.)</cell><cell>Human3.6M</cell><cell>MPI-3DHP</cell></row><row><cell>Ours(unsup) w/o T f k ? T c</cell><cell>126.8</cell><cell>51.7</cell></row><row><cell>Ours(unsup) w/o q z ? D z</cell><cell>178.9</cell><cell>40.3</cell></row><row><cell>Ours(unsup) w/o m sal</cell><cell>189.4</cell><cell>35.7</cell></row><row><cell>Ours(unsup)</cell><cell>99.2</cell><cell>77.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>3D pose estimation on 3DHP. Here, 2nd column denotes whether the approach uses 3DHP samples (paired or unpaired) while training. And the 3rd column specifies the supervision level.</figDesc><table><row><cell>No. Method</cell><cell cols="5">Trainset 3DHP Sup. PCK (?) AUC (?) MPJPE (?)</cell></row><row><cell cols="3">1. Mehta et al. [38] +3DHP Full-3D</cell><cell>76.6</cell><cell>40.4</cell><cell>124.7</cell></row><row><cell cols="3">2. Rogez et al. [48] +3DHP Full-3D</cell><cell>59.6</cell><cell>27.6</cell><cell>158.4</cell></row><row><cell cols="3">3. Zhou et al. [67] +3DHP Full-2D</cell><cell>69.2</cell><cell>32.5</cell><cell>137.1</cell></row><row><cell>4. HMR [24]</cell><cell cols="2">+3DHP Full-2D</cell><cell>77.1</cell><cell>40.7</cell><cell>113.2</cell></row><row><cell cols="3">5. Yang et al. [62] +3DHP Full-2D</cell><cell>69.0</cell><cell>32.0</cell><cell>-</cell></row><row><cell>6. Chen et al. [7]</cell><cell cols="2">+3DHP Full-2D</cell><cell>71.7</cell><cell>36.3</cell><cell>-</cell></row><row><cell cols="3">7. Ours(weakly-sup) +3DHP Full-2D</cell><cell>84.6</cell><cell>60.8</cell><cell>93.9</cell></row><row><cell>8. Chen et al. [7]</cell><cell>-3DHP</cell><cell>-</cell><cell>64.3</cell><cell>31.6</cell><cell>-</cell></row><row><cell cols="2">9. Ours(weakly-sup) -3DHP</cell><cell>-</cell><cell>82.1</cell><cell>56.3</cell><cell>103.8</cell></row><row><cell cols="3">10. Ours(weakly-sup) +3DHP No sup.</cell><cell>83.2</cell><cell>58.7</cell><cell>97.6</cell></row><row><cell cols="3">11. Ours(semi-sup) +3DHP 10%-3D</cell><cell>86.3</cell><cell>62.0</cell><cell>74.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>370k</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(all)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>On YTube, in-the-wild dataset</figDesc><table><row><cell>A.</cell><cell>Shape-unaware</cell><cell>Shape-aware</cell><cell>Pose transfer</cell><cell>View syn.</cell><cell>Shape-unaware</cell><cell>Shape-aware</cell><cell>Pose transfer</cell><cell>w/o q z w/ m sal</cell><cell>w/ q z w/o m sal</cell><cell>w/ q z w/ m sal</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Segmentation results on F1 metric (?) for LSP dataset.</figDesc><table><row><cell>Method</cell><cell>Pose Sup.</cell><cell cols="2">FG vs BG FG Parts</cell></row><row><cell>SMPLify [4]</cell><cell>Full-2D + SMPL-fitting</cell><cell>0.88</cell><cell>0.64</cell></row><row><cell>HMR [24]</cell><cell>Full-2D + SMPL-fitting</cell><cell>0.86</cell><cell>0.59</cell></row><row><cell>Ours(weakly-sup)</cell><cell>Full-2D (no SMPL)</cell><cell>0.84</cell><cell>0.56</cell></row><row><cell>Ours(unsup)</cell><cell>No sup. (no SMPL)</cell><cell>0.78</cell><cell>0.47</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">CMU graphics lab motion capture database</title>
		<ptr target="http://mocap.cs.cmu.edu/.6" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pose-conditioned joint angle limits for 3D human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ijaz</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Synthesizing images of humans in unseen poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guha</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><forename type="middle">V</forename><surname>Dalca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredo</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Guttag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Keep it smpl: Automatic estimation of 3D human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Human pose estimation via convolutional part heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">3D human pose es-timation= 2d pose estimation+ matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Hang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised 3D pose estimation with geometric selfsupervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Hang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Drover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename><surname>Rohith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Stojanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Weakly-supervised discovery of geometry-aware representation for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwan-Yee</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Articulated pose estimation by a graphical model with image dependent pairwise relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning 3D human pose from structure and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Mundhada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uday</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Safeer</forename><surname>Afaque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised learning of disentangled representations from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Emily L Denton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A variational u-net for conditional appearance and shape generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Sutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">In the wild human pose estimation using explicit 2d features and intermediate 3D representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikhsanul</forename><surname>Habibie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Shape recognition and pose estimation for mobile augmented reality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nate</forename><surname>Hagbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriel</forename><surname>Bergig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihad</forename><surname>El-Sana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Billinghurst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Coteaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised cnn-based cosaliency detection with graphical optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Jui</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Chi</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoning</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Yu</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">SCOPS: Self-supervised co-part segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sifei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3D human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Decoupled neural interfaces using synthetic gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><forename type="middle">Marian</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised learning of object landmarks through conditional image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Jakab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Warpnet: Weakly supervised matching for singleview reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning latent representations of 3D human pose with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isinsu</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bugra</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Selfsupervised learning of 3D human pose using multi-view geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salih</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Akbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep non-rigid structure from motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning of human actions as trajectories in pose embedding manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jogendra</forename><surname>Nath Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maharshi</forename><surname>Gor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Phani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R Venkatesh</forename><surname>Uppala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Kinematic-structure-preserved representation for unsupervised 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jogendra</forename><surname>Nath Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Seth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M V</forename><surname>Rahul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakesh</forename><surname>Mugalodi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Venkatesh Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chakraborty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unite the people: Closing the loop between 3D and 2d human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised part-based disentangling of object shape and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonard</forename><surname>Bereska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Milbich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Tinne Tuytelaars, and Luc Van Gool. Pose guided person image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Disentangled person image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Monocular 3D human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Vnect: Real-time 3D human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Vnect: Real-time 3D human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">3D human pose estimation from a single image via distance matrix regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Anchornet: A weakly supervised network to learn geometrysensitive features for semantic matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Novotny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">C3DPO: Canonical 3D pose networks for non-rigid structure from motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Novotny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhila</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pose machines: Articulated pose estimation via inference machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">Andrew</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unsupervised geometry-aware representation for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning monocular 3D human pose estimation from multi-view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rg</forename><surname>Sp?rri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isinsu</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?d?ric</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Convolutional neural network architecture for geometric matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Lcr-net: Localization-classification-regression for human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deformable gans for pose-based human image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksandr</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enver</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Lathuili?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Humaneva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Alexandru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Unsupervised discovery of mid-level discriminative patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Pose estimation and adaptive robot behaviour for human-robot interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Svenstrup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?ren</forename><surname>Tranberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hans J?rgen Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Unsupervised learning of object frames by dense equivariant image labelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Thewlis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Unsupervised learning of object landmarks by factorized spatial embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Thewlis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jonathan J Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bregler</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Adversarial inverse graphics networks: Learning 2D-to-3D lifting and image-to-image translation from unpaired supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Yu Fish</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Seto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning from synthetic humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gul</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Repnet: Weakly supervised training of an adversarial reprojection network for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Single image 3D interpreter network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">End-to-end learning of deformable mixture of parts and deep convolutional neural networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">3D human pose estimation in the wild by adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Articulated pose estimation with flexible mixtures-of-parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Co-saliency detection via a self-paced multiple-instance learning framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Martial arts, dancing and sports dataset: A challenging stereo and multi-view dataset for 3D human pose estimation. Image and Vision Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Howard</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Energybased generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Towards 3D human pose estimation in the wild: a weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Deep kinematic pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Sparse representation for 3D shape estimation: A convex relaxation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spyridon Leonardos, and Kostas Daniilidis</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Saliency optimization from robust background detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangjiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
