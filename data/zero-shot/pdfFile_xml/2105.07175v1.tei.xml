<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross-Modal Progressive Comprehension for Referring Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrui</forename><surname>Hui</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaofei</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
						</author>
						<title level="a" type="main">Cross-Modal Progressive Comprehension for Referring Segmentation</title>
					</analytic>
					<monogr>
						<title level="j" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE</title>
						<meeting> <address><addrLine>X, X 20XX 1</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="volume">XX</biblScope>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Referring Segmentation</term>
					<term>Progressive Comprehension</term>
					<term>Graph Reasoning</term>
					<term>Multimodal Feature Fusion !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Given a natural language expression and an image/video, the goal of referring segmentation is to produce the pixellevel masks of the entities described by the subject of the expression. Previous approaches tackle this problem by implicit feature interaction and fusion between visual and linguistic modalities in a one-stage manner. However, human tends to solve the referring problem in a progressive manner based on informative words in the expression, i.e., first roughly locating candidate entities and then distinguishing the target one. In this paper, we propose a Cross-Modal Progressive Comprehension (CMPC) scheme to effectively mimic human behaviors and implement it as a CMPC-I (Image) module and a CMPC-V (Video) module to improve referring image and video segmentation models. For image data, our CMPC-I module first employs entity and attribute words to perceive all the related entities that might be considered by the expression. Then, the relational words are adopted to highlight the target entity as well as suppress other irrelevant ones by spatial graph reasoning. For video data, our CMPC-V module further exploits action words based on CMPC-I to highlight the correct entity matched with the action cues by temporal graph reasoning. In addition to the CMPC, we also introduce a simple yet effective Text-Guided Feature Exchange (TGFE) module to integrate the reasoned multimodal features corresponding to different levels in the visual backbone under the guidance of textual information. In this way, multi-level features can communicate with each other and be mutually refined based on the textual context. Combining CMPC-I or CMPC-V with TGFE can form our image or video version referring segmentation frameworks and our frameworks achieve new state-of-the-art performances on four referring image segmentation benchmarks and three referring video segmentation benchmarks respectively. Our code is available at https://github.com/spyflying/CMPC-Refseg.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In this paper, we target at an emerging task called referring segmentation <ref type="bibr" target="#b19">[20]</ref>[28] <ref type="bibr" target="#b3">[4]</ref>. Given a natural language expression and an image/video as inputs, the goal of referring segmentation is to segment the entities referred by the subject of the input expression. Traditional semantic segmentation methods <ref type="bibr" target="#b6">[7]</ref>[62] <ref type="bibr" target="#b12">[13]</ref> aim to classify each pixel as one of a fixed set of categories denoted by short words (e.g., "person", "cell phone"). Referring segmentation can be regarded as a generalized semantic segmentation task where the categories belong to an open set denoted by expressions with various grammars and diverse contents, such as entities, attributes, relationships and actions, etc. Combining rich visual and linguistic information, referring segmentation has a wide range of potential applications such as languagebased robot controlling <ref type="bibr" target="#b46">[47]</ref> <ref type="bibr" target="#b13">[14]</ref>, interactive image editing <ref type="bibr" target="#b4">[5]</ref>, etc.</p><p>Previous works tackle the referring image or video segmentation task using a straightforward <ref type="bibr">Si</ref>  concatenation-and-convolution scheme <ref type="bibr" target="#b19">[20]</ref> involving dynamic filters <ref type="bibr" target="#b36">[37]</ref> <ref type="bibr" target="#b14">[15]</ref>, convolutional RNNs <ref type="bibr" target="#b33">[34]</ref> <ref type="bibr" target="#b27">[28]</ref> or cross-modal attention mechanism <ref type="bibr" target="#b41">[42]</ref>[4] <ref type="bibr" target="#b55">[56]</ref> <ref type="bibr" target="#b43">[44]</ref> to fuse visual and linguistic features. Instead of the above one-stage implicit approaches, human tends to comprehend the referring expression in a progressive way <ref type="bibr" target="#b42">[43]</ref>. As human reads the expression, different nouns and adjectives will first be located in the image/video to find all the candidate entities. Then, relational and action words like prepositions and verbs are extracted to reason the relationships among different entities, where the disturbing entities are excluded and the target one is found out.</p><p>To mimic the more natural processing way of human beings, we introduce a Cross-Modal Progressive Comprehension (CMPC) scheme to solve the referring segmentation task in multiple stages. We define the entity referred by the expression as the referent. For referring image segmentation, we illustrate the CMPC scheme in the left part of <ref type="figure" target="#fig_0">Fig. 1</ref>. If the referent is described by "The man holding a white frisbee", the referring process is divided into two progressive stages. First, the model can utilize entity words and attribute words, e.g., "man" and "white frisbee", to perceive all the possible entities mentioned in the expression. Second, as one image may contain multiple entities of the same category, for example, the three men in <ref type="figure" target="#fig_0">Fig. 1  (b</ref> f) Our model first perceives all the entities described in the expression based on entity words and attribute words, e.g., "man", "white frisbee" and "girl" (orange masks and blue outline).</p><p>(c)(f) After finding out all the candidate entities that may be referred by the input expression, our model exploits relational word, e.g., "holding" and "in", to highlight the entity involved with the relationship (green arrow) and suppress the others which are not involved. (g) For video data, our model further utilizes action word "bouncing" to aggregate temporal context information from neighbor frames for better localizing the referent in a changing video. (d)(h) Based on the progressive comprehension results, our model can finally determine the referent as output (purple mask). (Best viewed in color).</p><p>ing relationships among entities. In <ref type="figure" target="#fig_0">Fig. 1 (c)</ref>, under the guidance of the relational word "holding" which associates "man" with "white frisbee", the model can focus on the referent who holds a white frisbee rather than the other two men. After comprehending multimodal information progressively, the model can make correct prediction as shown in <ref type="figure" target="#fig_0">Fig. 1 (d)</ref>.</p><p>Different from image, expressions containing action descriptions <ref type="bibr" target="#b14">[15]</ref> <ref type="bibr" target="#b43">[44]</ref> are often used to refer entities in a video. In the right part of <ref type="figure" target="#fig_0">Fig. 1</ref>, we illustrate the video version of our CMPC scheme. If the entity in a video is described by "The girl in white shirt bouncing on the ball", the referring process is divided into three stages. The first entity perception stage and the second relation-aware reasoning stage are conducted on the center frame of a video snippet, which is almost same as those of the image model. Then in the third stage, action words (e.g., "bouncing") are exploited by the model to capture temporal cues among frames in the video to further highlight the referent conducting the described action, as shown in <ref type="figure" target="#fig_0">Fig. 1 (g)</ref>. Finally, multimodal spatio-temporal information is comprehended by the model to make correct predictions along the video as shown in <ref type="figure" target="#fig_0">Fig. 1 (h)</ref>.</p><p>To tackle image and video inputs respectively, we develop two versions of CMPC scheme. For image data, we propose a CMPC-I (Image) module which progressively exploits different types of words in the expression to segment the referent in an image. Concretely, our CMPC-I module consists of two stages. First, we extract linguistic features of entity words and attribute words (e.g., "man" and "white frisbee") from the expression and then fuse them with visual features extracted from the image to build multimodal features. During this process, all the entities that may be referred by the expression are perceived. Second, we construct a fully-connected spatial graph where each image region is regarded as a vertex and multimodal information of the entity is contained in each vertex. Appropriate edges are required for vertexes to communicate with each other. Naive edges which connect all the vertexes equally will introduce abundant information and hinder the identification of the referent. Thus, our CMPC-I module employs relational words (e.g., "holding") of the expression as a group of routers to build adaptive edges to connect spatial vertexes, i.e., entities, which are involved with the relationship described in the expression. Particularly, spatial vertexes (e.g., "man") yielding strong responses to the relational words (e.g., "holding") will exchange information with other vertexes (e.g., "frisbee") that also highly correlate with the relational words. Meanwhile, less interaction will occur among spatial vertexes yielding weak responses to the relational words. After relation-aware reasoning on the multimodal graph, our CMPC-I module can highlight feature of the referent while suppressing those of the irrelevant entities, which assists in generating accurate segmentation. For video data, we further extend our CMPC-I module to CMPC-V (Video) module with an additional action-aware reasoning stage based on action words to exploit temporal informa-tion. Concretely, our CMPC-V module extracts global multimodal features of all the frames in the video snippet based on action words (e.g., "bouncing") after the same entity perception and relation-aware reasoning stages. A fully-connected temporal graph is constructed using each frame as a vertex where frame feature is served as vertex feature. Information propagation among temporal vertexes is performed to extract temporal multimodal context among frames. Finally, features of the temporal graph are aggregated with feature of annotated frame to supplement temporal multimodal context for better segmentation.</p><p>As prior works <ref type="bibr" target="#b27">[28]</ref>[56] <ref type="bibr" target="#b3">[4]</ref> show multiple levels of visual features can complement each other, we also propose a Text-Guided Feature Exchange (TGFE) module to exploit information of multimodal features refined by our CMPC modules from different levels. Combining CMPC-I or CMPC-V module with TGFE module forms our image or video version referring segmentation framework. For each level of multimodal features, our TGFE module utilizes linguistic features as guidance to select useful feature channels from other levels to enable information communication. After multiple rounds of communication, Our TGFE further fuses multi-level features by Con-vLSTM <ref type="bibr" target="#b47">[48]</ref> to comprehensively integrate low-level visual details and high-level semantics for precise segmentation results.</p><p>The main contributions of our paper are summarized as follows:</p><p>? We introduce a Cross-Modal Progressive Comprehension (CMPC) scheme to align multimodal features in multiple stages based on informative words in the expression, which provides a general solution to referring segmentation task and is robust to different visual modalities. ? We instantiate the CMPC scheme by proposing a CMPC-I module containing entity perception and relation-aware reasoning stages for referring image segmentation. We further propose a CMPC-V module by extending CMPC-I with an action-aware reasoning stage for referring video segmentation. ? We also propose a TGFE module to aggregate multi-level multimodal features to enhance feature of the referent for better segmentation. ? Combining CMPC-I or CMPC-V with TGFE, our image and video version frameworks achieves current state-of-the-art performances on four referring image segmentation benchmarks and three referring video segmentation benchmarks, respectively.</p><p>This paper is an extension of our previous conference version <ref type="bibr" target="#b20">[21]</ref>. The current work adds to the initial version with significant aspects. First, we extend our CMPC scheme from referring image segmentation to referring video segmentation by introducing an additional action-aware reasoning stage, which effectively extracts temporal multimodal context to enhance feature of the referent. Second, we improve our initial CMPC-I module by changing the way of multimodal feature concatenation to better highlight feature of the referent. Third, we add considerable new experimental results including ablation study, model setting and visualization analysis. Our image model in this paper also obtains better performance than our conference version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Semantic Segmentation</head><p>Based on Fully Convolutional Networks (FCN) <ref type="bibr" target="#b34">[35]</ref>, semantic segmentation has made a huge progress in recent years. FCN uses convolution layers to replace all the fully-connected layers in original classification networks and becomes the most popular architecture in the semantic segmentation community. DeepLab series <ref type="bibr" target="#b5">[6]</ref>[7] <ref type="bibr" target="#b7">[8]</ref> incorporates FCN with dilated convolutions with different dilation rates, which enlarges the receptive field of filters to aggregate multi-scale visual context. PSPNet <ref type="bibr" target="#b61">[62]</ref> proposes similar pyramid pooling operations to extract multi-scale context as well. Later works such as DANet <ref type="bibr" target="#b12">[13]</ref> and CFNet <ref type="bibr" target="#b59">[60]</ref> exploit self-attention mechanism <ref type="bibr" target="#b44">[45]</ref> to capture long-range dependencies among image positions and achieve notable performance. In this paper, we target at the more challenging semantic segmentation problem whose semantic categories are specified by diverse natural language sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Referring Expression Grounding</head><p>Given a natural language expression, referring expression grounding aims to localize the entities matched with the expression in the given image or video. Many works conduct localization in bounding box level. Liao et al. <ref type="bibr" target="#b31">[32]</ref> performs cross-modality correlation filtering to match features from different modalities in real time. Graph models involving attention mechanism are explored in <ref type="bibr" target="#b50">[51]</ref>[52] <ref type="bibr" target="#b52">[53]</ref>[54] to find the most related objects for the expression. Yu et al. <ref type="bibr" target="#b56">[57]</ref> propose modular networks to decompose the referring expression into subject, location and relationship in order to finely compute the matching score. Most box-based methods are two-stage where a pretrained detector is utilized to first generate RoI proposals for later grounding. This design paradigm achieves high localization performance but lacks global context information and heavily relies on the quality of proposal candidates. In addition, it can only ground a single object and cannot locate the stuff or multiple objects.</p><p>Beyond bounding box, the referred object can also be localized more precisely with segmentation mask. Hu et al. <ref type="bibr" target="#b19">[20]</ref> first proposes the referring image segmentation problem and directly concatenates and fuses multimodal features from CNN and LSTM <ref type="bibr" target="#b17">[18]</ref> to generate the segmentation mask. In <ref type="bibr" target="#b33">[34]</ref> and <ref type="bibr" target="#b36">[37]</ref>, multimodal LSTM is employed to sequentially fuse visual and linguistic features in multiple time steps. Multi-level feature fusion is explored in <ref type="bibr" target="#b27">[28]</ref> to recurrently refine the local details of segmentation mask. As context information is critical to segmentation task, recent works employ cross-modal attention <ref type="bibr" target="#b41">[42]</ref>[4] <ref type="bibr" target="#b54">[55]</ref> and self-attention <ref type="bibr" target="#b55">[56]</ref> to extract multimodal context between image regions and referring words. Cycleconsistency learning <ref type="bibr" target="#b8">[9]</ref> and adversarial training <ref type="bibr" target="#b39">[40]</ref> are also investigated to boost the segmentation performance. Gavrilyuk et al. <ref type="bibr" target="#b14">[15]</ref> further introduce referring segmentation task into video data in which sentences contain action descriptions of the actors in the videos. They utilize dynamic filters and multiresolution decoder to generate mask of the referent. Based on <ref type="bibr" target="#b14">[15]</ref>, Wang et al. <ref type="bibr" target="#b43">[44]</ref> propose asymmetric cross-guided attention between visual and linguistic modalities to segment the referent more precisely. Different from box-based methods, most mask-based methods are one-stage where FCN <ref type="bibr" target="#b34">[35]</ref> is utilized to directly generate the mask of the referent. This design paradigm can extract rich global context information and does not rely on pretrained detectors. However, generating masks based on monolithic representations of referring expressions and visual contents may be difficult to distinguish between different instances, thus producing false positive masks and harming localization performance. In this paper, we tackle the above issue of one-stage methods by proposing to progressively highlight the referent via entity perception, relation-aware reasoning and action-aware reasoning, which effectively distinguishes different instances and reasons the referent on both image and video data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Graph-Based Reasoning</head><p>Recently, graph-based models have shown their effectiveness in context reasoning for many tasks. Graph Convolution Networks (GCN) <ref type="bibr" target="#b2">[3]</ref> becomes popular for its superiority on semi-supervised classification. Wang et al. <ref type="bibr" target="#b45">[46]</ref> uses RoI proposals as vertexes to construct a spatial-temporal graph and conduct context reasoning with GCN, which improve performance on video recognition task. Chen et al. <ref type="bibr" target="#b9">[10]</ref> propose a global reasoning module which projects visual feature into an interactive space and performs graph convolution for global context reasoning. Then they reproject the reasoned global context back to the coordinate space to enhance original visual feature. There are several concurrent works <ref type="bibr" target="#b28">[29]</ref>[31] <ref type="bibr" target="#b60">[61]</ref> sharing the same spirit with <ref type="bibr" target="#b9">[10]</ref> while using different implementations. Graph-based reasoning has also been widely applied in vision and language tasks such as Hu et al. <ref type="bibr" target="#b18">[19]</ref> and Yu et al. <ref type="bibr" target="#b58">[59]</ref>. Hu et al. <ref type="bibr" target="#b18">[19]</ref> build a fully-connected visual graph where each node corresponds to an object proposal generated by a pre-trained detector and formulates the message passing among graph nodes as a recurrent process. Yu et al. <ref type="bibr" target="#b58">[59]</ref> build two heterogeneous graphs where the primal vision-to-answer graph utilizes object proposals and answer words as graph nodes, while the dual question-to-answer graph utilizes query words and answer words as graph nodes. They conduct message passing between visual nodes and answer nodes in the primal graph, while between query nodes and answer nodes in the dual graph. Different from above two methods, we propose to regard image regions and video frames as vertexes to build spatial and temporal graphs for effectively reasoning multimodal context based on informative words in the expression which is more sutible for the segmentation task. Besides, we exploit the relational words as routers to connect each pair of visual nodes on the feature map and message passing among all visual nodes is guided by relational words in a more effective way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>In this section, we elaborate the instantiations of our introduced CMPC scheme to tackle the referring segmentation task on image and video data respectively. The proposed modules are denoted as CMPC-I (Image) and CMPC-V (Video) in the rest of our paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">CMPC on Image</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Image and Words Feature Extraction</head><p>As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, our image model takes an image and a sentence as inputs. We use a CNN backbone to extract multi-level image features V I and fuse them with an 8-D spatial coordinate feature O ? R H?W ?8 respectively using a 1 ? 1 convolution layer following prior works <ref type="bibr" target="#b33">[34]</ref> <ref type="bibr" target="#b55">[56]</ref>. After the convolution, each level of image features are transformed to the same size of R H?W ?Cv , with H, W and C v being the height, width and channel dimension of the image features, respectively. We denote the transformed image features by {X 3 I , X 4 I , X 5 I } which correspond to the output of the 3rd, 4th and 5th stages of CNN backbone (e.g., ResNet-101 <ref type="bibr" target="#b16">[17]</ref>). For ease of presentation, we denote a single level of image features as X I ? R H?W ?Cv . The words features L = {l 1 , l 2 , ..., l T } is extracted with a language encoder (e.g., LSTM <ref type="bibr" target="#b17">[18]</ref>), where T is the length of the sentence and l i ? R C l (i ? {1, 2, ..., T }) denotes feature of the i-th word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Entity Perception</head><p>Since the input image may contain many entities, it is natural to progressively narrow down the candidate set from all the entities to the actual referent. The first stage of our CMPC-I (Image) module is entity perception. We associate linguistic features of entity words and attribute words with the correlated visual features of spatial regions using bilinear fusion <ref type="bibr" target="#b0">[1]</ref> to perceive all the candidate entities. Concretely, we classify the words into 4 types, i.e., entity, attribute, relation and unnecessary word following <ref type="bibr" target="#b50">[51]</ref>. Since there is no annotation for word types of referring expressions in the datasets we used, we do not have direct supervision on learning the word classification. Thus, we supervise the word classification using the binary cross-entropy segmentation loss at the end of our model, which is also adopted by <ref type="bibr" target="#b50">[51]</ref>. Detailed experimental analyses and visualized results about word classification are presented in Section 4.4. For each word, a 4-D vector is predicted to indicate the probability that it belongs to one of these Next, we adopt a simplified bilinear fusion strategy <ref type="bibr" target="#b0">[1]</ref> to associate q e with the image feature X I on each spatial region to obtain the multimodal feature M I ? R H?W ?Cm as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Modal Progressive Comprehension</head><formula xml:id="formula_0">M i I = (q e W 3i ) (X I W 4i ),<label>(3)</label></formula><formula xml:id="formula_1">M I = r i=1 M i I (4)</formula><p>where W 3i ? R C l ?Cm and W 4i ? R Cv?Cm are learnable parameters, denotes element-wise product and r is a hyper-parameter. By integrating both visual and linguistic context into the multimodal features, all the entities that may be matched with the expression are accordingly perceived.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Relation-Aware Reasoning</head><p>After perceiving all the possible entities in the image, the second stage of our CMPC-I module is relationaware reasoning. We construct a fully-connected multimodal graph based on M I using relational words as a group of routers to connect vertexes. Each vertex of the graph represents a spatial region on M I . By reasoning among vertexes of the multimodal graph, our model can highlight the responses of the referent which are involved with the relationship cue while suppressing those of the non-referred ones.</p><p>Concretely, the multimodal graph is defined as</p><formula xml:id="formula_2">G S = (V, E,M I , A)</formula><p>where V and E are the sets of vertexes and edges,M I = {m I j } N j=1 ? R N ?Cm is the set of vertex features, A ? R N ?N is the adjacency matrix and N is number of vertexes.</p><p>Details of relation-aware reasoning is illustrated in the right part of <ref type="figure">Fig. 3</ref>. Since each location on M I represents a spatial region on the original image, each region is regarded as a vertex of the graph. The multimodal graph consists of N = H ? W vertexes in total. A linear layer is applied to M I to transform it into the features of vertexesM I after the reshaping operation. We use the affinities between vertexes and relational words in the referring expression to determine the edge weights. Features of relational words A multimodal fully-connected graph G S is constructed with each vertex corresponds to an image region on M . The adjacency matrix of G S is defined as the product of the matching degrees between vertexes and relational words in the expression. Graph convolution is utilized to reason among vertexes so that the referent could be highlighted during the interaction with correlated vertexes.</p><formula xml:id="formula_3">R = {r t } T t=1 ? R T ?C l are calculated by: r t = p rel t l t , t = 1, 2, ..., T.<label>(5)</label></formula><p>As shown in <ref type="figure">Fig. 3</ref>, to obtain adjacency matrix A, we first conduct cross-modal attention mechanism by matrix product betweenM I and R with necessary dimension transformation:</p><formula xml:id="formula_4">B = (M I W 5 )(RW 6 ) T ,<label>(6)</label></formula><p>where B ? R N ?T measures the feature relevance between each vertex ofM I and each word of R. Then, we apply softmax function on the T dimension of B and obtain B1 ? R N ?T :</p><formula xml:id="formula_5">B 1 = sof tmax(B),<label>(7)</label></formula><p>where each row of B1 means the normalized T attention weights between each vertex and T words. Accordingly, another softmax on the N dimension of</p><formula xml:id="formula_6">B T ? R T ?N obtains B2 ? R T ?N : B 2 = sof tmax(B T ),<label>(8)</label></formula><p>where each row of B2 means the normalized N attention weights between each word and N vertexes. The meaning of softmax operation is normalizing the attention weights to control the amount of information propagation on the graph within a reasonable interval. Afterwards, we perform matrix product between B1 ? R N ?T and B2 ? R T ?N and obtain normalized adjacency matrix A ? R N ?N :</p><formula xml:id="formula_7">A = B 1 B 2 .<label>(9)</label></formula><p>Each element A ij of A represents the normalized magnitude of information flow from the spatial region i to the region j, which depends on their affinities with relational words in the expression. If a vertex has high attention weight with a certain word, and this word also has high attention weight with another vertex, then these two vertexes will have high attention weight with each other in a relational context. In this way, adaptive edges connecting spatial vertexes can be built by leveraging relational words of the expression as a group of routers.</p><p>After building the multimodal graph G S , we conduct graph convolution <ref type="bibr" target="#b25">[26]</ref> over it as follow:</p><formula xml:id="formula_8">M I = (A + I)M I W 7 ,<label>(10)</label></formula><p>where W 7 ? R Cm?Cm is a learnable weight matrix, I is the identity matrix serving as a residual connection to ease optimization. The graph convolution performs reasoning among vertexes, i.e., image regions, which selectively highlights the referent according to the relationship cues while suppressing other irrelevant ones. As a result, more discriminative features can be generated to improve referring segmentation. Afterwards, we conduct reshaping operation to obtain the image-format enhanced multimodal features M I ? R H?W ?Cm . To exploit the textual information, we first aggregate features of all necessary words into a vector s ? R C l by weighted sum using the predefined probability vectors as weights:</p><formula xml:id="formula_9">s = T t=0 (p ent t + p attr t + p rel t )l t .<label>(11)</label></formula><p>Then, we repeat s for H ? W times and concatenate it with M I andM I along channel dimension. In our conference version <ref type="bibr" target="#b20">[21]</ref>, we concatenate repeated s with image feature X I andM I . Since the multimodal feature M I contains richer context information about entities in the image, we suppose concatenating M I can provide more sufficient guidance than pure image feature X I , which is also proved empirically in <ref type="table">Table 1</ref> and <ref type="table" target="#tab_9">Table 5</ref>. We further apply a 1 ? 1 convolution on the concatenated features to obtain the output feature Y I ? R H?W ?Cm , which embodies multimodal context for the referent.  <ref type="figure">Fig. 4</ref>. Illustration of the action-aware reasoning stage of our CMPC-V module. We ignore previous EP and RAR stages for clarity. We first conduct dot-product attention between video features M V and action sentence feature q a to obtain the dense attention maps D V on all the frames of the video snippet. Then, D V is applied on M V to aggregate global temporal featuresP V of all the frames. We construct a fully-connected temporal graph G T based onP V and perform graph convolution on it to reason temporal context. Finally, reasoned temporal context are projected back to the feature of the center frameM ctr V to yield the image-format temporal contextP V .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">CMPC on Video</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Video and Sentence Feature Extraction</head><p>Our video model takes a video clip and a sentence as input. The length of the video clip is K frames and the center frame is annotated with pixel-wise mask. We reshape the video clip as a batch of images and feed them into the CNN backbone as we do in the CMPC-I model. The output multi-level video clip features are denoted as V F . We follow the same protocol in the image model to fuse each level of V F with 8-D coordinate feature O ? R H?W ?8 respectively using a 1 ? 1 convolution. After the convolution, each level of video clip features are transformed to the same size of R K?H?W ?C f , with K, H, W and C f being the frame numbers, height, width and channel dimension of the video clip features. We denote the transformed video clip features by {X 3 F , X 4 F , X 5 F } which correspond to the output of the 3rd, 4th and 5th stages of CNN backbone (e.g., ResNet-101 <ref type="bibr" target="#b16">[17]</ref>) and denote a single level of video clip features as X F for ease of presentation. The extraction of words features L = {l 1 , l 2 , ..., l T } ? R T ?C l is same as our image model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Action-Aware Reasoning</head><p>As video data usually contains temporal information, reasoning only static relationship between entities are not enough to identify the referent in a video. Therefore, we further introduce an action-aware reasoning stage to highlight entities matched with the temporal action cues and fuse them with entities matched with the static relationship cues to distinguish the referent.</p><p>Concretely, we first use a similar approach to the Section 3.1.2 to classify the words into 5 types, i.e., entity, attribute, relation, action and unnecessary word. We denote the probability vector of word t as p t = [p ent t , p attr t , p rel t , p act t , p un t ] ? R 5 , which denotes the probability that each word belongs to one of the 5 categories. We calculate the global linguistic feature of actions q a ? R C l by weighted combination of the all the words in the expression:</p><formula xml:id="formula_10">q a = T t=1 p act t l t .<label>(12)</label></formula><p>Then, the video clip features X F are first fed into our entity perception stage to perceive all the possible entities in each frame independently and produce multimodal feature M V ? R K?H?W ?Cm . Since the static relationships between entities in each frame are almost identical, we only select the annotated center frame of the video clip to conduct relation-aware reasoning stage to reduce the computational budgets. The reasoned feature of the center frame is denoted asM ctr V ? R H?W ?Cm . As shown in <ref type="figure">Fig. 4</ref>, in order to highlight features of entities which are matched with the temporal action cues in the expression, we conduct cross-modal attention between M V and q a with necessary reshaping operations:D</p><formula xml:id="formula_11">V = (M V W 8 )(q a W 9 ),<label>(13)</label></formula><formula xml:id="formula_12">D V = sof tmax(D V ? C m ),<label>(14)</label></formula><formula xml:id="formula_13">P V = D V M V ,<label>(15)</label></formula><p>where D V ? R K?1?HW , M V ? R K?HW ?Cm after reshaping. We combine K with the batch axis and perform a batch matrix-matrix product to obtain P V ? R K?1?Cm . Afterwards, we reshape P V toP V ? R K?Cm and useP V as feature of vertexes to build a temporal graph G T with K frames as vertexes. Each vertex saves the global feature of entities matched with the action cues in each frame. Then, we conduct matrix product between different transformations of P V ? R K?Cm to obtain feature relevances among vertexes as the adjacency matrix A V ? R K?K of G T :</p><formula xml:id="formula_14">A V = (P V W 12 )(P V W 13 ),<label>(16)</label></formula><formula xml:id="formula_15">A V = sof tmax(? V ? C m ).<label>(17)</label></formula><p>We further conduct graph convolution <ref type="bibr" target="#b25">[26]</ref> among temporal vertexes as follows:</p><formula xml:id="formula_16">P V = (A V + I)P V W 14 ,<label>(18)</label></formula><p>The reasoned contextP V ? R K?Cm is then projected to each spatial location ofM ctr V ? R H?W ?Cm . We first conduct cross-modal attention betweenP V andM ctr V :</p><formula xml:id="formula_17">E V = (P V W 14 )(M ctr V W 15 ),<label>(19)</label></formula><formula xml:id="formula_18">E V = sof tmax(? V ? C m ),<label>(20)</label></formula><p>where E V ? R K?HW measures the feature relevances betweenP V andM ctr V . Then, we perform another matrix product between E V andP V to obtain the image-format temporal contextP V ? R H?W ?Cm with necessary reshaping operations:</p><formula xml:id="formula_19">P V = E V TP V .<label>(21)</label></formula><p>Finally, we concatenate and fuse M ctr V ,M ctr V ,P V and repeated s to produce feature Y V ? R H?W ?Cm for the referent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Text-Guided Feature Exchange</head><p>As previous works <ref type="bibr" target="#b27">[28]</ref>[56] <ref type="bibr" target="#b3">[4]</ref> demonstrate the importance of multi-level feature fusion in referring segmentation, we further introduce a Text-Guided Feature Exchange (TGFE) module which exploits visual and language context to communicate information among multi-level features. Our TGFE module takes visual features Y 3 , Y 4 , Y 5 (subscripts are omitted) and necessary sentence feature s as inputs. After n rounds of feature exchange, Y 3 n , Y 4 n , Y 5 n are produced as outputs. For level i at round k, TGFE module aims to exchange feature information from Y j k?1 , j ? {3, 4, 5}\{i} to Y i k?1 and output aggregated feature Y i k . Concretely, we first conduct cross-modal attention between sentence feature s ? R C l and Y i k?1 ? R H?W ?Cm with necessary reshaping and dimension transforming operations:</p><formula xml:id="formula_20">? i k?1 = (sW 10 )(Y i k?1 W 11 ) T ,<label>(22)</label></formula><p>where ? i k?1 ? R 1?HW measures the feature relevance between the whole sentence and each spatial location on Y i k?1 . Then, we utilize ? i k?1 as a weighted global pooling matrix to aggregate global context vector</p><formula xml:id="formula_21">g i k?1 ? R Cm from Y i k?1 ? R H?W ?Cm : g i k?1 = ? i k?1 Y i k?1 .<label>(23)</label></formula><p>Then we fuse s and g i k?1 with a fully connected layer to obtain a context vector c i k?1 ? R Cm which contains multimodal context of Y i k?1 with more textual information. Afterwards, we exploit c i k?1 to select features relevant to Y i k?1 from the other two levels of features Y j k?1 , j ? {3, 4, 5}\{i} by channel attention. The original feature Y i k?1 of level i at round k ? 1 will be added with selected features from the other two levels at round k ? 1 to obtain Y i k of level i at round k as follows:</p><formula xml:id="formula_22">Y i k = ? ? ? ? ? ? ? Y i k?1 + j?{3,4,5}\{i} ?(c i k?1 ) Y j k?1 , k ? 1 Y i , k = 0<label>(24)</label></formula><p>where ?(?) denotes sigmoid function. At each round, feature of each level i ? {3, 4, 5} will select its relevant features from the other two levels under the guidance of textual information. After n rounds of exchange, the output features Y 3 n , Y 4 n and Y 5 n are further fused with ConvLSTM <ref type="bibr" target="#b47">[48]</ref> to produce the final mask prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Datasets</head><p>We conduct extensive experiments on four referring image segmentation benchmarks including UNC <ref type="bibr" target="#b57">[58]</ref>, UNC+ <ref type="bibr" target="#b57">[58]</ref>, G-Ref <ref type="bibr" target="#b35">[36]</ref> and ReferIt <ref type="bibr" target="#b23">[24]</ref>, and also on three referring video segmentation benchmarks including A2D Sentences <ref type="bibr" target="#b14">[15]</ref>, J-HMDB Sentences <ref type="bibr" target="#b14">[15]</ref> and Refer-Youtube-VOS <ref type="bibr" target="#b40">[41]</ref>.</p><p>UNC, UNC+ and G-Ref are all collected on MS-COCO <ref type="bibr" target="#b32">[33]</ref>. They contain 19, 994, 19, 992 and 26, 711 images with 142, 209, 141, 564 and 104, 560 referring expressions for over 50, 000 objects, respectively. Expressions in UNC+ contain no location words while those in G-Ref have much longer length than others. ReferIt is collected on IAPR TC-12 <ref type="bibr" target="#b10">[11]</ref> and contains 19, 894 images with 130, 525 expressions for 96, 654 objects (including stuff). A2D Sentences is extended from the Actor-Action Dataset <ref type="bibr" target="#b48">[49]</ref> by providing textual descriptions for each video. It contains 3, 782 videos annotated with 8 action classes performed by 7 actor classes. J-HMDB sentences is extended from the J-HMDB dataset <ref type="bibr" target="#b21">[22]</ref> which contains 21 different actions, 928 videos and corresponding 928 sentences. All the actors in JHMDB dataset are humans and one natural language query is annotated to describe the action performed by each actor. Refer-Youtube-VOS is a large-scale referring video segmentation dataset extended from Youtube-VOS dataset <ref type="bibr" target="#b49">[50]</ref> which contains 3975 videos, 7451 objects and 27899 expressions with both first-frame expression and full-video expression annotated. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 1</head><p>Comparison with state-of-the-art methods on four datasets for referring image segmentation. Overall IoU is adopted as the metric. "n/a" denotes MAttNet does not use the same split as other methods. 3 rounds of feature exchange are adopted in TGFE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Evaluation Metrics</head><p>We adopt Prec@X and overall Intersection-over-Union (overall IoU) as metrics to evaluate our image model. Prec@X measures the percentage of test samples whose IoU with ground-truth masks are higher than the threshold X ? {0.5, 0.6, 0.7, 0.8, 0.9}. Overall IoU accumulates the total intersection regions over total union regions of all the test samples. For video model, we additionally use mean Average Precision (mAP) and mean IoU as metrics in addition to Prec@X and overall IoU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Implementation Details</head><p>We adopt DeepLab-ResNet101 <ref type="bibr" target="#b6">[7]</ref> which is pretrained on the PASCAL-VOC dataset <ref type="bibr" target="#b11">[12]</ref> as the CNN backbone to extract visual features for the input image and video. The output of Res3, Res4 and Res5 are used for multi-level feature fusion. Input images and video frames are resized to 320 ? 320. The input video clip contains 5 frames for CMPC-v model. Besides, for video segmentation, we further adopt I3D <ref type="bibr" target="#b1">[2]</ref> pretrained on Kinetics dataset <ref type="bibr" target="#b22">[23]</ref> as backbone of CMPC-v model to compare fairly with previous methods <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b37">[38]</ref>. Due to the temporal downsamping operation in I3D, the input video clip of our model contains 8 frames following PRPE to retain temporal information throughout the network. The output of last three stages of I3D are used for multi-level feature fusion. Channel dimensions of features are set as C v = C l = C m = C h = 1000 and the cell size of ConvLSTM <ref type="bibr" target="#b47">[48]</ref> is set to 500. When comparing with other methods, the hyper-parameter r of bilinear fusion is set to 5 and the number of feature exchange rounds n is set to 3. GloVe word embeddings <ref type="bibr" target="#b38">[39]</ref> pretrained on Common Crawl 840B tokens are adopted following <ref type="bibr" target="#b3">[4]</ref>. Number of graph convolution layers is set as 2 on G-Ref dataset and 1 on others. We train the network using Adam optimizer <ref type="bibr" target="#b24">[25]</ref> with the initial learning rate of 2.5e ?4 and weight decay of 5e ?4 . Parameters of CNN backbone are fixed during training. Binary cross-entropy loss averaged over all pixels is used for training. DenseCRF <ref type="bibr" target="#b26">[27]</ref> is adopted to refine the segmentation masks for fair comparison with prior works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with State-of-the-arts</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Referring Image Segmentation</head><p>To demonstrate the superiority of our method for referring image segmentation, we evaluate it on four benchmark datasets. As illustrated in <ref type="table">Table 1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Referring Video Segmentation</head><p>Comparisons with state-of-the-art methods on A2D Sentences dataset are summarized in <ref type="table" target="#tab_5">Table 2</ref>. Since prior works <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b37">[38]</ref>     backbone to encode video features, we also build a 3D-version of our CMPC-V network for fair comparison. We present results of both 2D backbone and 3D backbone in <ref type="table" target="#tab_5">Table 2</ref>, which are denoted as 'Ours-R2D' and 'Ours-I3D' respectively. Our I3D-based model achieves notable improvements comparing with the R2D-based model, indicating that 3D backbone extracts more temporal information. Our method also outperforms previous state-of-the-arts, PRPE <ref type="bibr" target="#b37">[38]</ref>, on most evaluation metrics except Overall IoU, where our model achieves comparable result with PRPE.</p><p>To further demonstrate the generalization ability of our video model, we conduct experiments on the J-HMDB Sentences dataset <ref type="bibr" target="#b14">[15]</ref> and the Refer-Youtube-VOS dataset <ref type="bibr" target="#b40">[41]</ref>. We follow prior works <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b37">[38]</ref> to use the best model pretrained on A2D Sentences dataset to directly evaluate all the test samples of J-HMDB Sentences dataset without finetuning. As shown in <ref type="table" target="#tab_6">Table 3</ref>, our video model achieves notable performance gain over previous methods for most evaluation metrics (4.0% Overall IoU, 5.7% Prec@0.5, etc.), indicating that our method obtains stronger generalization ability. Please note that all the methods including ours produce 0.0% or 0.1% on Prec@0.9, which is probably because without training on J-HMDB Sentences, models cannot generate particularly fine masks on unseen samples.</p><p>For Refer-Youtube-VOS dataset, we train our video model for 200, 000 iterations with 5e ?4 as initial learning rate (divided by 10 at 160, 000th iteration). As shown in <ref type="table" target="#tab_7">Table 4</ref>, Our video model outperforms URVOS <ref type="bibr" target="#b40">[41]</ref> on most metrics except Prec@0.5 without further refining segmentation masks using memory attention between frames. The comparison shows that our model is able to recognize the referred objects without too much interactions among frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Studies</head><p>We perform ablation studies on UNC val set, G-Ref val set and A2D Sentence test set to testify the effectiveness of each proposed module for referring image and video segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Components of CMPC-I Module</head><p>We first explore the effectiveness of each component of our proposed CMPC-I module. Experimental results are summarized in <ref type="table" target="#tab_9">Table 5</ref>. EP and RAR denotes the entity perception and relation-aware reasoning stages in CMPC-I module respectively. GloVe means EP RAR TGFE CLSTM GloVe CMF Prec@0.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 6</head><p>Ablation studies of our CMPC-V modules on A2D Sentence test set. EP, RAR and AAR indicate entity perception, relation-aware reasoning and action-aware reasoning stages in our CMPC-V module. GloVe and CMF are adopted in the baseline by default for clarity of presentation. TGFE* here means TGFE together with ConvLSTM.</p><p>using GloVe word embeddings <ref type="bibr" target="#b38">[39]</ref> to initialize the parameters of embedding layer. CMF means concatenating multimodal feature from EP instead of pure visual feature to produce the output of CMPC-I, as mentioned in the last paragraph of Section 3.1.3. Results in rows 1 to 6 are all based on single-level visual feature, i.e. Res5. Our baseline (row 1) simply concatenates the visual feature from DeepLab-101 and linguistic feature from LSTM and makes predictions on the fusion of them.</p><p>As shown in row 2 of <ref type="table" target="#tab_9">Table 5</ref>, including EP brings 1.70% IoU improvement over the baseline, indicating the perception of candidate entities can help model to eliminate noisy backgrounds. In row 3, RAR alone brings 6.04% IoU improvement over baseline, demonstrating that the referent can be effectively highlighted by leveraging relational words as routers to reason among spatial regions, thus boosting the performance notably. Combining EP with RAR, our CMPC-I module can achieve 55.38% IoU with single level visual feature, enlarging the performance margin to 8.02% IoU. This indicates that our model can accurately identify the referent by progressively comprehending the input image and expression. Integrated with GloVe word embeddings, the IoU gain achieves 8.64% with the aid of large-scale corpus. As shown in row 13 ad 14, our proposed TGFE has significant influence on the performance while ConvLSTM only yields marginal improvements, demonstrating the effectiveness of TGFE. Particularly, CMF further boosts the performance gain to 9.76%, which shows concatenating multimodal feature can provide richer context than pure visual feature.</p><p>We further conduct ablation studies based on multilevel visual features as shown in rows 7 to 13 of <ref type="table" target="#tab_9">Table 5</ref>. Row 7 is the multi-level version of row 1 using ConvLSTM to fuse multi-level features. The TGFE module in rows 7 to 11 adopts single round of feature exchange. As shown in <ref type="table" target="#tab_9">Table 5</ref>, our model yields consistent performance gains with the singlelevel version, which demonstrates the effectiveness of our CMPC-I module under multi-level situation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Components of CMPC-V Module</head><p>We build the CMPC-V module based on CMPC-I module by introducing an additional action-aware reasoning (AAR) stage to exploit temporal information for identifying the referent. Our video-version baseline contains GloVe and CMF by default and we evaluate TGFE, EP, RAR and AAR respectively for clarity. The experimental results are summarized in <ref type="table">Table 6</ref>. We can observe that TGFE can bring notable gains on most of metrics benefited from multi-level visual features. Combining EP and RAR stages can improve the performance slightly by utilizing only spatial information. It should be noticed that incorporating our proposed AAR stage is able to further obtain large performance gain over our strong baseline using TGFE, EP and RAR stages, demonstrating that temporal context information is critical to the referring video segmentation task.</p><p>We also tried to use action words as routers to obtain the adjacency matrix of AAR as in RAR, and the results are shown in <ref type="table">Table 7</ref>. AR and DR represent "Adaptive Relevance" in RAR and "Direct Relevance" in original AAR respectively. Adaptive relevance yields inferior performance than direct relevance, indicating that direct relevance is more suitable to propagate information among different frames. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 7</head><p>Ablation studies of different ways to obtain adjacency matrix in AAR on A2D dataset. AR means "Adaptive Relevance", which denotes the adjacency matrix is obtained in the similar way of RAR. DR means "Direct Relevance", which denotes the adjacency matrix is obtained with direct feature relevance in our original implementation. We also tried to remove the additional necessary words extracting process in TGFE stage and the results are shown in <ref type="table" target="#tab_14">Table 9</ref>. Results shows that extracting necessary words features can yield slight improvements. It indicates the words extraction is not redundant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">TGFE module</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Number of Graph Convolution Layer</head><p>In <ref type="table" target="#tab_15">Table 10</ref>, we explore the number of graph convolution layers in CMPC-I module based on single-    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Visualization Analyses</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Accuracy of Word Classification</head><p>As mentioned in Section 3.1.2, we supervise the learning of word classification by the final segmentation loss due to the lack of annotations for word types. The 4 types of words, i.e., entity, attribute, relation and unnecessary, are implicitly defined according to the role each word plays in the whole expression and it is hard to quantitatively evaluate words classification accuracy. Thus we show the visualization of the words classification probabilities which are predicted by our model in <ref type="figure">Fig. 5 (d)</ref>. Among the three blue blocks, from left to right are the probabilities of the word being entity, attribute and relation types where darker color denotes larger probability. In the first example of expression "back right top donut", words "back", "right" and "top" have largest probabilities of being relation type, while word "donut" has largest probability of being entity type. This indicates our model can well recognize the type of each word in a soft manner and further utilize these words to highlight features of the referent by our CMPC module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Correlations of Word Classification and Segmentation</head><p>To demonstrate our model can learn meaningful word classification results, we randomly set the classification probabilities for words in the expression during testing. Experimental results are summarized in <ref type="table" target="#tab_16">Table 11</ref>. Assigning random classification probabilities to each word leads to notable performance degradation, which shows the implicit learning of word classification is able to guide the progressive comprehension of referring expressions. We also visualized the segmentation results with random word classification probabilities in the expressions during testing in <ref type="figure">Fig. 6</ref>. The segmentation results show that with randomly assigned word categories, the model cannot identify the correct object described in the expression. Taking the first row as an example, after modifying the word categories, the model mis-recognizes the man in the middle of the image as referred object, while model with original word categories could make correct prediction. These results show that our model can learn meaningful word classification results without direct supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Qualitative Results</head><p>In <ref type="figure">Fig. 7</ref>, we presents qualitative comparison between the multi-level baseline model (row 7 in <ref type="table" target="#tab_9">Table 5</ref>) and our full model (row 13 in <ref type="table" target="#tab_9">Table 5</ref>) for referring image segmentation. From the top-left example we can observe that the baseline model fails to make clear judgement between the two girls, while our model is able to distinguish the correct girl involving the relationship with the phone. Similar result is shown in the top-right example of <ref type="figure">Fig. 7</ref>. As illustrated in the bottom row of <ref type="figure">Fig. 7</ref>, attributes and location relationship can also be well handled by our model, demonstrating its effectiveness.</p><p>We also provide qualitative results of our full video model (row 5 in <ref type="table">Table 6</ref>) and baseline model (row 1  <ref type="table" target="#tab_9">Table 5</ref>). (c) Results of our model (row 12 in <ref type="table" target="#tab_9">Table 5</ref>). (d) Ground-truth.</p><p>in <ref type="table">Table 6</ref>) in <ref type="figure" target="#fig_4">Fig. 8</ref>. Colors of expressions correspond to masks of instances. As shown in <ref type="figure" target="#fig_4">Fig. 8 (c)</ref> and (d), our full video model well segments the baby and dog with coherent masks while the baseline model fails to distinguish different actors, indicating the effectiveness of our proposed modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.4">Visualization of Affinity Maps</head><p>In <ref type="figure">Fig. 9</ref>, we visualize the affinity maps between multimodal feature and the first word of the expression in our CMPC-I module. As shown in (b) and (c), our model can progressively produce more concentrated responses on the referent as the expression becomes more informative from only entity words to the full sentence. It should be noticed that when we manually modify the expression to refer to other entities in the image, our model can still comprehend the new expression and correctly identify the referent. For example, in the third row of <ref type="figure">Fig. 9(e)</ref>, when the expression changes from "Donut at the bottom" to "Donut at the left", high response area shifts from bottom donut to the left donut accordingly. It indicates that our model can adapt to new expressions flexibly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.5">Effectiveness of Relation-Aware Reasoning and Action-Aware Reasoning</head><p>To verify the effectiveness of relation-aware reasoning and action-aware reasoning, we present the comparison with and without them in <ref type="figure" target="#fig_0">Fig. 10</ref>. First, we show the comparison of with relationaware reasoning (EP + RAR) and without relationaware reasoning (EP) in the top rows of <ref type="figure" target="#fig_0">Fig. 10</ref>. It is shown that comparing with EP, EP + RAR is able to discriminate the right referent from others. Taking the "baby crawling in the corridor" "dog on the left crawling" "bird walking on the grass" "guy in blue shirt is walking towards a three point line" "player in blue shirt passing a basketball on left" "man in blue shirt shooting the basketball on the right"  <ref type="table">Table 6</ref>) (b)(d)(f) Results of our full video model (row 5 in <ref type="table">Table 6</ref>). Colors of masks correspond to different expressions.</p><p>2nd row as an example, there are several boys in the images and EP only fails to recognize the right most boy. While EP + RAR makes the right prediction.</p><p>In addition, we also show the comparison of with action-aware reasoning (EP + RAR + AAR) and without action-aware reasoning (EP + RAR) in the bottom of <ref type="figure" target="#fig_0">Fig. 10</ref>. In the first row, EP + RAR fails to discriminate the man who is moving his head up and down from others who also stand behind camera. With AAR to recognize action in the video, EP + RAR + AAR is able to locate the correct man. The visualization results demonstrate the necessity of action reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION AND FUTURE WORK</head><p>To address the referring segmentation problem for image and video, we propose a CMPC scheme which first perceives candidate entities which might be considered by the expression using entity and attribute words, then conduct graph-based reasoning with the aid of relational words and action words to further highlight the referent while suppressing others. We implement CMPC scheme as two modules, namely CMPC-I and CMPC-V for image and video inputs. We also propose a TGFE module which exploits textual information to selectively integrate multi-level features to refine the mask prediction. Our model consistently outperforms previous state-of-the-art methods on four referring image segmentation benchmarks and three referring video segmentation benchmarks, demonstrating its effectiveness. In the future, we plan to analyze the linguistic information more structurally and explore more compact graph formulation. Our code is available at https://github.com/spyflying/ CMPC-Refseg.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>), the model needs to further highlight the referent matched with the relationship cue in the expression while suppressing other mismatched ones by reason-arXiv:2105.07175v1 [cs.CV] 15 May 2021 "The girl in white shirt bouncing on the ball" "The girl in white shirt bouncing on the ball" Entity Perception &amp; Relational-Aware Reasoning "The girl in white shirt bouncing on the ball" Illustration of our progressive referring segmentation method on image and video data. (a)(e) Input referring expression, image and video. (b)(</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Overview of our proposed method using referring image segmentation as an example. Visual features and linguistic features are first progressively aligned by our CMPC-I module. Then multi-level multimodal features are fed into our TGFE module for information communication across different levels. Finally, multi-level features are fused with ConvLSTM for final prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .Fig. 6 .Fig. 7 .</head><label>567</label><figDesc>Back right top donut Query: Man in blue shirt Query: Baseball player holding bat Query: Right sky below cloud Visualization of words classification probabilities on four benchmark datasets. (a) Original image. (b) Ground-truth. (c) Prediction of our model. (d) Word classification probabilities of our model. Types include Entity, Attribute (Attr) and Relation (Rel). Darker color denotes larger probability.Expression: "A cat is sitting on the back" Expression: "A man in black is standing on the left side" Expression: "Boy eating on the left" (a) Original Images (b) Ground truth (c) Correct word classes (d) Random word classes Segmentation results of random word type assignment. (a) Original images. (b) Ground truth masks for referred objects. (c) Segmentation results of our model with correct word categories. (d) Segmentation results of our model with randomly assigned word categories. "girl on phone" "big green suitcase" "stander in darker pants" "left cup" Qualitative results of referring image segmentation. (a) Original image. (b) Results of the multi-level baseline model (row 7 in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 .</head><label>8</label><figDesc>Qualitative results of referring video segmentation. (a)(c)(e) Results of baseline model (row 1 in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 .Fig. 10 .</head><label>910</label><figDesc>Visualization of affinity maps between images and expressions in our CMPC-I module. (a) Original image. (b)(c) Affinity maps of only entity words and full expressions in the test samples. (d) Ground-truth. (e) Affinity maps of expressions manually modified by us. Expression: "A car is flying in the air" Expression: "Right most boy jumping up and down" (a) Original Images (b) Ground truth (c) EP only (d) EP + RAR Expression: "man behind the camera moving his head up and down" Expression: "man in black talking to the camera" (a) Original Images (b) Ground truth (c) EP + RAR (d) EP + RAR + AAR Top: Segmentation results w/ and w/o relationaware reasoning. Bottom: Segmentation results w/ and w/o action-aware reasoning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Liu and Bo Li are with Institute of Artificial Intelligence, Beihang University, Email: {liusi, boli}@buaa.edu.cn. Tianrui Hui and Shaofei Huang are with Institute of Information Engineering, Chinese Academy of Sciences, and also with School of Cyber Security, University of Chinese Academy of Sciences, E-mail: {huitianrui, huangshaofei}@iie.ac.cn. Yunchao Wei is with Institute of Information Science, Beijing Jiaotong University, Email: wychao1987@gmail.com. Guanbin Li is with School of Computer Science and Engineering, Sun Yat-sen University, and also with Pazhou Lab, E-mail: liguanbin@mail.sysu.edu.cn. Corresponding author: Guanbin Li.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Fig. 3. Illustration of our CMPC-I module which consists of two stages. First, visual features X I are bilinearly fused with linguistic features q of entity words and attribute words for Entity Perception (EP) stage. Second, multimodal features M I from EP stage are fed into Relation-Aware Reasoning (RAR) stage for feature enhancement.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>?</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Linear</cell><cell>?</cell><cell></cell><cell>Vertex ?</cell><cell></cell><cell></cell></row><row><cell>Fusion Bilinear</cell><cell>Linear</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell><cell>Graph Convolution</cell><cell>?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Edge</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Linear</cell><cell>?</cell><cell>?</cell><cell></cell><cell></cell><cell cols="2">: Vertexes Number</cell></row><row><cell>The man holding a white frisbee</cell><cell>The man holding a white frisbee</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">: Words Number , ? : Feature Dimension</cell></row><row><cell>Entity Perception</cell><cell></cell><cell></cell><cell cols="2">Relation-Aware Reasoning</cell><cell>Entity:</cell><cell>Attribute:</cell><cell>Relation:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 2</head><label>2</label><figDesc>Comparison with state-of-the-art methods on A2D Sentences for referring video segmentation. Our method significantly outperforms state-of-the-arts using only RGB input.</figDesc><table><row><cell>Method</cell><cell>Prec@0.5</cell><cell>Prec@0.6</cell><cell>Precision Prec@0.7</cell><cell>Prec@0.8</cell><cell>Prec@0.9</cell><cell>mAP 0.5:0.95</cell><cell>IoU Overall</cell><cell>Mean</cell></row><row><cell>Gavrilyuk et al. [15] (RGB+Flow)</cell><cell>69.9</cell><cell>46.0</cell><cell>17.3</cell><cell>1.4</cell><cell>0.0</cell><cell>23.3</cell><cell>54.1</cell><cell>54.2</cell></row><row><cell>ACGANet [44] (RGB)</cell><cell>75.6</cell><cell>56.4</cell><cell>28.7</cell><cell>3.4</cell><cell>0.0</cell><cell>28.9</cell><cell>57.6</cell><cell>58.4</cell></row><row><cell>PRPE [38]</cell><cell>69.0</cell><cell>57.2</cell><cell>31.9</cell><cell>6.0</cell><cell>0.1</cell><cell>29.4</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours (RGB)</cell><cell>81.3</cell><cell>65.7</cell><cell>37.1</cell><cell>7.0</cell><cell>0.0</cell><cell>34.2</cell><cell>61.6</cell><cell>61.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 3</head><label>3</label><figDesc>Comparison with state-of-the-art methods on JHMDB Sentences for referring video segmentation. Our method significantly outperforms previous ones using only RGB input.</figDesc><table><row><cell>Method</cell><cell>J</cell><cell>F</cell><cell>Prec@0.5</cell><cell>Prec@0.6</cell><cell>Precision Prec@0.7</cell><cell>Prec@0.8</cell><cell>Prec@0.9</cell></row><row><cell>URVOS w/o memory attention [41]</cell><cell>39.38</cell><cell>41.78</cell><cell>46.26</cell><cell>40.98</cell><cell>34.81</cell><cell>25.42</cell><cell>10.86</cell></row><row><cell>URVOS [41]</cell><cell>45.27</cell><cell>49.19</cell><cell>52.19</cell><cell>46.77</cell><cell>40.16</cell><cell>27.67</cell><cell>14.11</cell></row><row><cell>Ours</cell><cell>45.64</cell><cell>49.32</cell><cell>51.65</cell><cell>47.16</cell><cell>40.96</cell><cell>31.21</cell><cell>16.52</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 4</head><label>4</label><figDesc>Comparison with state-of-the-art methods on Refer-Youtube-VOS dataset. Our method outpreforms previous ones without memory attention across frames.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 5</head><label>5</label><figDesc>Ablation studies of CMPC-I and TGFE modules on UNC val set. EP and RAR indicate entity perception and relation-aware reasoning stages. CMF denotes concatenating multimodal feature from EP instead of pure visual feature for the output of CMPC-I. Here TGFE adopts one round of feature exchange.</figDesc><table><row><cell></cell><cell>TGFE*</cell><cell>EP</cell><cell>RAR</cell><cell>AAR</cell><cell>Prec@0.5</cell><cell>Prec@0.6</cell><cell>Precision Prec@0.7</cell><cell>Prec@0.8</cell><cell>Prec@0.9</cell><cell>mAP 0.5:0.95</cell><cell>IoU Overall</cell><cell>Mean</cell></row><row><cell>1 2 3 4 5</cell><cell>? ? ? ?</cell><cell>? ? ?</cell><cell>? ?</cell><cell>?</cell><cell>52.8 56.7 56.5 57.6 59.0</cell><cell>47.2 49.7 50.3 50.2 52.7</cell><cell>38.7 40.3 40.7 40.2 43.4</cell><cell>25.4 25.6 26.9 26.1 28.4</cell><cell>5.9 5.8 6.2 5.8 6.8</cell><cell>31.3 32.7 33.1 33.1 35.1</cell><cell>62.3 63.2 63.9 63.2 64.9</cell><cell>46.6 49.3 49.8 49.7 51.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8</head><label>8</label><figDesc>presents the ablation results of TGFE module for referring image segmentation. n is the number of feature exchange rounds. Our experiments are conducted upon CMPC-I module without CMF. Results show that only one round of feature exchange in TGFE could improve the IoU from 59.85% to 60.72%. The IoU performance increases as the number of feature exchange rounds increases, which well proves the effectiveness of our TGFE module. We further directly incorporate TGFE module with the baseline model and results are shown in row 7 and row 8 ofTable 5. TGFE with single round of feature exchange boosts the IoU from 56.38% to 58.81%, indicating our TGFE module can effectively utilize rich contexts in multi-level features.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 8</head><label>8</label><figDesc>Overall IoUs of different numbers of feature exchange rounds in TGFE module on UNC val set. n denotes the number of feature exchange rounds. Remove the engineering technique CMF from CMPC-I module.</figDesc><table><row><cell>Method</cell><cell cols="3">Prec@0.5 Prec@0.7 Prec@0.9</cell><cell>Overall IoU</cell></row><row><cell>Full Model</cell><cell>72.08</cell><cell>55.65</cell><cell>12.80</cell><cell>61.80</cell></row><row><cell>w/o NW</cell><cell>71.57</cell><cell>56.11</cell><cell>12.51</cell><cell>61.23</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE 9</head><label>9</label><figDesc></figDesc><table><row><cell cols="5">Ablation studies of necessary words extracting (NW)</cell></row><row><cell></cell><cell cols="3">on UNC val set.</cell><cell></cell></row><row><cell cols="5">level feature without CMF. g denotes the number</cell></row><row><cell cols="5">of graph convolution layers in CMPC-I. Results on</cell></row><row><cell cols="5">UNC val set show that more graph convolution layers</cell></row><row><cell cols="5">lead to performance degradation. However, on G-</cell></row><row><cell cols="5">Ref val set, 2 layers of graph convolution in CMPC-I</cell></row><row><cell cols="5">obtains better performance than 1 layer while 3 layers</cell></row><row><cell cols="5">decreasing the performance. Since G-Ref has much</cell></row><row><cell cols="5">longer expressions (average length of 8.4 words) than</cell></row><row><cell cols="5">UNC (average length &lt; 4 words), we suppose that</cell></row><row><cell cols="5">stacking more graph convolution layers in CMPC-</cell></row><row><cell cols="5">I can appropriately improve the reasoning ability</cell></row><row><cell cols="5">for longer referring expressions. However, too many</cell></row><row><cell cols="5">graph convolution layers may introduce noises and</cell></row><row><cell cols="2">harm the performance.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell>g = 0</cell><cell cols="2">CMPC-I g = 1 g = 2</cell><cell>g = 3</cell></row><row><cell>UNC val</cell><cell>49.06</cell><cell>55.38</cell><cell>51.57</cell><cell>50.70</cell></row><row><cell>G-Ref val</cell><cell>36.50</cell><cell>38.19</cell><cell>40.12</cell><cell>38.96</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE 10</head><label>10</label><figDesc>Experiments of graph convolution on UNC val set and G-Ref val set in terms of overall IoU. g denotes the number of graph convolution layers. Experiments are all conducted on single level feature w/o CMF.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>TABLE 11</head><label>11</label><figDesc>Comparison with random word type assignment on 4 datasets for referring image segmentation. Overall IoU is adopted as the metric.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">categories. We denote the probability vector of word t as p t = [p ent t , p attr t , p rel t , p un t ] ? R 4 and calculate it by:p t = sof tmax(W 2 ?(W 1 l t + b 1 ) + b 2 ),(1)whereW 1 ? R Cn?C l , W 2 ? R 4?Cn , b 1 ? R Cn and b 2 ? R 4 are learnable parameters, ?(?) is sigmoid function, p ent t , p attr t , p rel tand p un t denote the probabilities of word t being the entity, attribute, relation and unnecessary word respectively. Then, we calculate the global linguistic context of entities q ? R C l by weighted combination of the all the words in the expression:q e =T t=1 (p ent t + p attr t )l t .(2)</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mutan: Multimodal tucker fusion for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Ben-Younes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dense and low-rank gaussian crfs using deep embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">See-through-text grouping for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding-Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songhao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Chen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwann-Tzong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyng-Luh</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Language-based image editing with recurrent attentive models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7062</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Referring expression object segmentation with caption-aware consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Wen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiantian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.04748</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graph-based global reasoning networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Shuicheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The segmented and annotated iapr tc-12 benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><forename type="middle">A</forename><surname>Hugo Jair Escalante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesus</forename><forename type="middle">A</forename><surname>Hern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelio</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>L?pez-L?pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><forename type="middle">F</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Sucar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Villase?or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grubinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The pascal visual object classes (voc) challenge. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Room-and-object aware knowledge reasoning for remote embodied referring expression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Actor and action video segmentation from a sentence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirill</forename><surname>Gavrilyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Language-conditioned graph networks for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Segmentation from natural language expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Referring image segmentation via cross-modal progressive comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaofei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrui</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luoqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Towards understanding action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hueihan</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Referitgame: Referring to objects in photographs of natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahar</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Matten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Referring image segmentation via recurrent refinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaican</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Chun</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Beyond grids: Learning graph representations for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Tracking by natural language specification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Symbolic graph reasoning meets convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">A real-time cross-modality correlation filtering method for referring expression comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.07072</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Recurrent multimodal interaction for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Generation and comprehension of unambiguous object descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oana</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dynamic multimodal instance segmentation guided by natural language queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Margffoy-Tuay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilio</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Botero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arbel?ez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Polar relative positional encoding for video-language segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI, 2020</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Referring image segmentation by generative adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikui</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Urvos: Unified referring video object segmentation network with a large-scale benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonguk</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Key-word-aware network for referring expression image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengcan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanman</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingbo</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Integration of visual and linguistic information in spoken language comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tanenhaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><forename type="middle">M</forename><surname>Spivey-Knowlton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><forename type="middle">C</forename><surname>Eberhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sedivy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Asymmetric cross-guided attention network for actor and action video segmentation from natural language query</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Abhinav Gupta, and Kaiming He. Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhourong</forename><surname>Shi Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai-Kin</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chun</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Can humans fly? action understanding with multiple classes of actors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shao-Hang</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Youtube-vos: A large-scale video object segmentation benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingcheng</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.03327</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Cross-modal relationship inference for grounding referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sibei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Dynamic graph attention for referring expression comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sibei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Graph-structured referring expression reasoning in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sibei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Relationshipembedded representation learning for grounding referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sibei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Dual convolutional lstm network for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linwei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<idno>TMM, 2020. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Crossmodal self-attention network for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linwei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrigank</forename><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Mattnet: Modular attention network for referring expression comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Modeling context in referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Poirson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Heterogeneous graph learning for visual commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijiang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Xiao</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Co-occurrent features in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Latentgnn: Learning efficient non-local relations for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shipeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11634</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
