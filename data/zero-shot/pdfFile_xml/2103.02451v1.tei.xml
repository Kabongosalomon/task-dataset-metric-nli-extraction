<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Consistency and Awareness for Monocular Self-Supervised Depth Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>May 30 -June 5, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chawla</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varma</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Arani</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zonooz</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;quot;</forename><surname>Multimodal</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>@inproceedings{chawlavarma2021multimodal, author={H. {Chawla}</roleName><forename type="first">A</forename><surname>{varma}</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>{arani}</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>{zonooz}}</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hemang</forename><surname>Chawla</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnav</forename><surname>Varma</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elahe</forename><surname>Arani</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bahram</forename><surname>Zonooz</surname></persName>
						</author>
						<title level="a" type="main">Consistency and Awareness for Monocular Self-Supervised Depth Estimation</title>
					</analytic>
					<monogr>
						<title level="j" type="main">IEEE</title>
						<meeting> <address><addrLine>Xi&apos;an, China Scale; Xi&apos;an, China</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page">2021</biblScope>
							<date type="published">May 30 -June 5, 2021</date>
						</imprint>
					</monogr>
					<note>This paper has been accepted for publication in the proceedings booktitle={2021 IEEE International Conference on Robotics and Automation (ICRA)}, title={Multimodal Scale Consistency and Awareness for Monocular Self-Supervised Depth Estimation}, location={Xi&apos;an, China}, publisher={IEEE (in press)}, year={2021}}</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Dense depth estimation is essential to sceneunderstanding for autonomous driving. However, recent selfsupervised approaches on monocular videos suffer from scaleinconsistency across long sequences. Utilizing data from the ubiquitously copresent global positioning systems (GPS), we tackle this challenge by proposing a dynamically-weighted GPS-to-Scale (g2s) loss to complement the appearance-based losses. We emphasize that the GPS is needed only during the multimodal training, and not at inference. The relative distance between frames captured through the GPS provides a scale signal that is independent of the camera setup and scene distribution, resulting in richer learned feature representations. Through extensive evaluation on multiple datasets, we demonstrate scale-consistent and -aware depth estimation during inference, improving the performance even when training with low-frequency GPS data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Robots and autonomous driving systems require sceneunderstanding for planning and navigation. Therefore, spatial perception through depth estimation is essential for enabling complex behaviors in unconstrained environments. Even though sensors such as LiDARs can perceive depth at metricscale <ref type="bibr" target="#b0">[1]</ref>, their output is sparse and they are expensive to use. In contrast, monocular color cameras are compact, low-cost, and consume less energy. While traditional camera-based approaches rely upon hand-crafted features from multiple views <ref type="bibr" target="#b1">[2]</ref>, deep learning based approaches can predict depth from a single image. Among these, self-supervised methods that predict the ego-motion and depth simultaneously by view-synthesis of adjacent frames <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b4">[5]</ref> are preferred over supervised methods that require accurate ground truth labels for training <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b7">[8]</ref>.</p><p>However, monocular vision inherently suffers from scale ambiguity. Additionally, the self-supervised approaches introduce scale-inconsistency in estimated depth across different video snippets <ref type="bibr" target="#b8">[9]</ref>. Consequently, most of the existing methods scale the estimated relative depth using the LiDAR ground truth during evaluation. Recent methods tackling this problem utilize additional 3D geometric constraints to introduce scale-consistency <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, but require at least some depth or stereo supervision to predict at metric-scale <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. Nevertheless, obtaining metric scale predictions at low cost is necessary for practical deployment.</p><p>Since self-supervised learning allows training on large and varied data including crowdsourced data <ref type="bibr" target="#b13">[13]</ref>, <ref type="bibr" target="#b14">[14]</ref>, the ubiquitous GPS copresent with videos can be employed <ref type="bibr">*</ref>   <ref type="figure">Fig. 1</ref>. A schematic of our proposed multimodal self-supervised depth and ego-motion prediction network for monocular videos. We introduce a GPS-to-Scale (g2s) loss that leads to scale-consistent and -aware estimates during inference.</p><p>for multimodal training. Taking cues from how cross-modal learning leads to richer learned feature representations <ref type="bibr" target="#b15">[15]</ref>, <ref type="bibr" target="#b16">[16]</ref>, we hypothesize that the relative distance between image frames captured from the GPS can provide a scale signal that complements commonly used appearance-based losses to predict scale-consistent and -aware improved estimates. In this work, we propose a GPS-to-Scale (g2s) loss that utilizes the ratio of magnitudes of the relative translation measured by the GPS and the relative translation predicted by the pose network to enforce scale-consistency and -awareness on the depth predictions, linked together via the perspective projection model <ref type="bibr" target="#b4">[5]</ref>. Scale consistency implies that the standard deviation of the depth scale factors across the video is low. Scale awareness implies that the mean scale factor is close to 1. Note that this GPS information is only used during the training, while the inference is directly performed on the unlabeled monocular videos. Furthermore, we compare different weighting strategies for the proposed loss and demonstrate that exponentially increasing the weight on g2s over the epochs leads to the best performance. Experiments on the KITTI raw <ref type="bibr" target="#b17">[17]</ref> Eigen <ref type="bibr" target="#b18">[18]</ref> split as well as the improved KITTI depth benchmark <ref type="bibr" target="#b19">[19]</ref> show that adding the g2s loss improves performance and scale-consistency over state-of-the-art-methods, even with low-frequency planar GPS (without altitude). Finally, with experiments on outof-distribution Make3D <ref type="bibr" target="#b20">[20]</ref> and Cityscapes <ref type="bibr" target="#b21">[21]</ref> datasets, we show that the introduced scale-consistency and -awareness is present across domains in comparison with other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Estimating scene depth is a long-standing problem in computer vision. Traditional approaches solve this by utilizing disparity across multiple views within a non-linear optimization framework <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b22">[22]</ref>. Supervised methods that produce high-quality estimates have also been proposed [6]- <ref type="bibr" target="#b7">[8]</ref>, but necessitate the availability of accurate ground truth and cross-calibration of sensors for training. Instead, using view-synthesis as a signal, self-supervised methods produce accurate depth maps from stereo image pairs <ref type="bibr" target="#b23">[23]</ref>, <ref type="bibr" target="#b24">[24]</ref> or monocular video snippets <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b4">[5]</ref>. We focus on methods employing purely monocular setups, as they are more pervasive and do not depend upon prior knowledge of relative rotation and translation of the stereo camera pairs. However, most existing monocular approaches utilize only appearancebased losses with the assumption of brightness consistency that limits training on small video subsequences without any long sequence constraints. Hence, the depth and ego-motion estimates from these methods suffer from scale-inconsistency along with the global scale-ambiguity present in monocular vision. Therefore, ground truth LiDAR depth maps <ref type="bibr" target="#b3">[4]</ref> or camera height <ref type="bibr" target="#b25">[25]</ref> are used during inference to recover perimage scale.</p><p>Methods addressing this problem add 3D-geometry-based losses to introduce scale-consistency <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, yet utilize at least some depth or stereo supervision to introduce scaleawareness <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. Recently <ref type="bibr" target="#b26">[26]</ref> introduced a similar instantaneous velocity based multi-modal supervision. However, access to instantaneous velocity may require the use of inertial measurement units (IMU) that are less ubiquitous. In contrast, GPS is often copresent, such as in dashboard cameras albeit with lower frequency, allowing training on more data. In this work, we introduce a GPS-to-scale (g2s) loss that produces improved scale-consistent and -aware results even with low-frequency planar GPS without altitude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>Our objective is to simultaneously train depth and egomotion prediction networks that produce scale-consistent and -aware estimates from only a monocular color camera during inference. Here we describe the baseline network and appearance-based losses for self-supervised learning, followed by the motivation and description of our proposed dynamically-weighted GPS-to-Scale (g2s) loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>Given a set of n images from a video sequence, and m loosely corresponding GPS coordinates, the inputs to the networks are a sequence of temporally consecutive RGB image triplets {I ?1 , I 0 , I 1 } ? R H?W ?3 and the the synced GPS coordinates {G ?1 , G 0 , G 1 } ? R 3 , when available. The depth network, f D : R H?W ?3 ? R H?W , outputs dense depth (or disparity) for each pixel coordinate p of a single image. Simultaneously, the ego-motion network, f E :</p><formula xml:id="formula_0">R 2?H?W ?3 ? R 6 , outputs relative translation (t x , t y , t z )</formula><p>and rotation (r x , r y , r z ) forming the affine transformation RT 0 1 ? SE(3) between a pair of adjacent images. The predicted depthD and ego-motionT are linked together via the perspective projection model <ref type="bibr" target="#b4">[5]</ref>, that warps the source (s) images I s ? {I ?1 , I 1 } to the target (t) image I t ? {I 0 }, given the camera intrinsics K.</p><p>We establish a strong baseline by following the best practices of appearance-based learning from Monodepth2 <ref type="bibr" target="#b3">[4]</ref>. The networks are trained using the appearance-based photometric loss between the real and synthesized target images, as well as a smoothness loss for depth regularization in low texture scenes <ref type="bibr" target="#b3">[4]</ref>. Following <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b27">[27]</ref>, we use auto-masking (M) to disregard the temporally stationary pixels in the image triplets. The total appearance-based loss is calculated by upscaling the predicted depths from intermediate decoder layers to the input resolution.</p><p>Additionally, we introduce the dynamically-weighted g2s loss that enforces scale-consistency and -awareness using the ratio of the measured and estimated translation magnitudes. <ref type="figure">Fig. 1</ref> illustrates the complete architecture that uses the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. GPS-to-Scale (g2s) Loss</head><p>Appearance-based losses provide supervisory signals on short monocular subsequences. This leads to scaleinconsistency of the predictions across long videos. Approaches addressing this problem through 3D-geometrybased losses provide a signal that depends upon the camera setup and the scene distribution <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Therefore, we introduce the GPS-to-Scale (g2s) loss that provides an independent cross-modal signal leading to scale-consistent and -aware estimates.</p><p>Synced Local Coordinates: The GPS information, ubiquitously copresent with videos, consists of the latitude, longitude, and optionally the altitude of the vehicle. First, we convert these geodetic coordinates to local coordinates G = {x g , y g , z g } using the Mercator projection such that,</p><p>x g = cos ? ? lat 0 180 r e log tan ? ? (90 + lat) 360 (1)</p><formula xml:id="formula_1">y g = alt (2) z g = cos ? ? lat 0 180 r e ? ? lon 180<label>(3)</label></formula><p>where r e = 6 378 137 m is taken as the radius of earth. Since the GPS frequency may be different from the frame-rate of the captured video, we additionally sync these local coordinates with the images using their respective timestamps. Utilizing the ratio of the relative distance measured by the GPS and the relative distance predicted by the network, we additionally impose our proposed g2s loss given by,</p><formula xml:id="formula_2">L g2s = s,t G s?t 2 T s?t 2 ? 1 2<label>(4)</label></formula><p>where s ? {?1, 1} and t ? {0}. Following <ref type="bibr" target="#b4">[5]</ref> we remove static frames while training, thereby allowing the g2s loss to be differentiable for all plausible inputs.</p><p>GPS noise and bias: By forming this loss upon the translation magnitude instead of the individual components (t x , t y , t z ), we account for any noise or systemic bias that may be present in the GPS measurements <ref type="bibr" target="#b28">[28]</ref>. This loss encourages the ego-motion estimates to be closer to the common metric scale across the image triplets, thereby introducing the scale-consistency and -awareness which is extended to the depth estimates that are tied to the egomotion via the perspective projection model.</p><p>Note that CNNs tend to learn surface statistical regularities by exploiting superficial clues (or shortcuts) specific to the distribution being trained on <ref type="bibr" target="#b29">[29]</ref>, <ref type="bibr" target="#b30">[30]</ref>. Since the GPS signal does not depend upon the specific scene distribution or camera setup, we hypothesize that adding our proposed g2s loss in a multimodal context can help to disentangle intended higher-level abstractions <ref type="bibr" target="#b16">[16]</ref> from the shortcut features to improve the estimates and help in generalizing scaleconsistency to out-of-distribution (o.o.d.) datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Dynamic Weighting Strategy</head><p>The networks learn to synthesize more plausible views of the target images I t by improving their depth and egomotion predictions over the training epochs. Thus, heavily penalizing the networks for the incorrect scales during the early training can interfere with the learning of individual translations, rotations, and pixel-wise depths. Hence, we dynamically weigh the g2s loss in an exponential manner to provide a scale signal that is low in the beginning and increases as the training progresses. The weight w to the g2s loss L g2s is given by,</p><formula xml:id="formula_3">w = exp (epoch ? epoch max ).<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Final Training Loss</head><p>The final loss combining the appearance-based losses <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> with Eqs. 4 and 5 is given by,</p><formula xml:id="formula_4">L = L appearance + w ? L g2s ,<label>(6)</label></formula><p>which is averaged over each batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>For all our experiments, we follow the setup of Mon-odepth2 <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Depth Estimation</head><p>Following the established protocols, we compare our depth predictions on the Eigen Split <ref type="bibr" target="#b18">[18]</ref> of KITTI <ref type="bibr" target="#b17">[17]</ref> raw dataset as shown in <ref type="table">Tables I and II</ref>. This contains 39, 810 training and 697 test images respectively. The depth is evaluated using metrics from <ref type="bibr" target="#b18">[18]</ref> up to the fixed range of 80 m, unless specified otherwise. We also evaluate against the Improved ground truth depth <ref type="bibr" target="#b19">[19]</ref> which contains 652 (93%) of the 697 Original test images. Best results for each metric are in bold. The second best results are underlined. * denotes results when trained on Cityscapes along with KITTI.</p><p>1) Performance and Scale-Consistency: For evaluating the performance and scale-consistency of depth estimation, we follow the standard procedure of scaling the per-image estimated depthsD with individual scale factors given by the ratio of the median ground truth depths from LiDAR and the median predicted depths <ref type="bibr" target="#b4">[5]</ref>. A lower standard deviation of the scale factors corresponds to a higher scale-consistency.</p><p>As shown in <ref type="table">Table I</ref>, we outperform existing depth estimation methods on the KITTI Original as well as Improved ground truths for the Eigen split. This improvement can be attributed to the richer learned feature representations as explained in Sec III-B. Furthermore, <ref type="figure">Fig. 2</ref> validates our results visually, and demonstrates that the learning of richer feature representation with our proposed multi-modal training leads to sharper depth estimates with improved structure preservation. As discussed earlier in Sec III-B, this can be explained by the disentangling of the intended higherlevel abstractions from the shortcut features <ref type="bibr" target="#b16">[16]</ref>, <ref type="bibr" target="#b29">[29]</ref>, <ref type="bibr" target="#b30">[30]</ref>.</p><p>We also compare the variation of the scale factor for different methods as shown in <ref type="figure" target="#fig_0">Fig. 3</ref>. Note that the standard deviation of depth scale factors is the lowest for our method at 0.07. Unlike previous methods that measure scale-consistency by the standard deviation of the scales normalized by the median scale, we report un-normalized standard deviation. This shows that the network is able to estimate scale-consistent depths with the use of our proposed g2s loss during training.</p><p>2) Scale-Awareness: We also compare the unscaled depth estimates in <ref type="table">Table II</ref> (LR and HR denote methods trained on low and high resolution images respectively. pp <ref type="bibr" target="#b3">[4]</ref> denotes post-processing during inference). As shown, most stateof-the-art monocular self-supervised methods produce poor estimates without the per-image scaling based on the LiDAR ground truth depths. However, our unscaled estimates are close to that from Table I. As shown in <ref type="figure" target="#fig_0">Fig. 3</ref>, our mean depth scale factor is ? 1 (specifically 1.03), establishing the scale-awareness introduced by our method for monocular depth estimation. We outperform Roussel et al. <ref type="bibr" target="#b11">[12]</ref> which uses stereo pre-training on CityScapes to predict scaleaware monocular depth for KITTI. We also show comparable performance against Packnet-SfM <ref type="bibr" target="#b26">[26]</ref> which uses a much heavier depth-estimation-dedicated architecture unlike the ResNet family based methods such as ours. Moreover, while our method has an inference time of 40 ms on an NVIDIA 1080Ti GPU, <ref type="bibr" target="#b26">[26]</ref> has is slower with an inference time of 60 ms even on a Titan V100 GPU.</p><p>Hence, through experiments in Sec IV-A.1 and IV-A.2, we demonstrate that the G2S loss provides a scale-signal based on relative distance between image frames resulting in scaleconsistent and -aware estimates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. KITTI Depth Prediction Benchmark</head><p>We also measure the performance of our method on the KITTI Depth Prediction Benchmark using the metrics from <ref type="bibr" target="#b19">[19]</ref>. We train our method with a ResNet50 encoder on an image size of 1024 ? 320 for 30 epochs, and evaluate it Input CC SC-SfMLearner Monodepth2 Ours <ref type="figure">Fig. 2</ref>. Single-image depth estimates on the KITTI Eigen split. Our method produces sharper, high-quality predictions that preserve more structure when compared against existing methods. using the online KITTI benchmark server. <ref type="bibr" target="#b0">1</ref> Results, ordered based on their rank, are shown in <ref type="table" target="#tab_1">Table III</ref> (D, M, and S represent supervised training with ground truth depths, monocular sequences, and stereo pairs, respectively. Seg represents additional supervised semantic segmentation training. G represents the use of GPS for multi-modal selfsupervision). We outperform all self-supervised methods while also performing better than many supervised methods 1 http://www.cvlibs.net/datasets/kitti/eval_depth. php?benchmark=depth_prediction. See results under g2s which use ground truth depth maps during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Studies</head><p>To study the efficacy of the proposed g2s loss in detail, we perform ablation studies on the introduced weighting strategy, as well as the frequency and dimensionality of the GPS used in the multi-modal training.</p><p>1) Weighting Strategy: In <ref type="table" target="#tab_4">Table V</ref>, we compare our proposed weighting strategy (Eq. 5) against the alternative constant and linearly increasing weights for the g2s loss. The mean and standard deviation of scale factors as well as the corresponding metrics on scaled predictions are shown. We confirm that utilizing an exponential weighting strategy effectively leverages the scale signal to produce scaleconsistent and -aware depth estimates. As explained earlier in Section III-C, this is because penalizing the networks for the incorrect scales during the early training can interfere with the learning. Therefore, providing an increasing scale signal over the epochs, while allowing effective appearance-based learning in the early training, leads to the best results.</p><p>2) GPS Frequency and Dimensionality: While GPS is ubiquitously copresent with driving video sequences, crowdsourced data often consists of high frames-per-second (fps) videos but lower frequency GPS. Furthermore, while altitude can be trilaterated by the GPS receivers, it is often not measured by the low-cost setups. Therefore, we study the efficacy of the g2s loss over different GPS frequencies, and the impact of the lack of altitude/height measurements in two-dimensional GPS.</p><p>Note that the images in the KITTI dataset are captured at 10 fps. To simulate the GPS frequencies lower than 10 Hz  we randomly select the GPS data for f &lt; 10 frames for each 10-frame non-overlapping subsequences (? 1 s) in the training data. Thereafter, we apply our g2s loss as described in Eq. 4 on the adjacent image pairs that have corresponding GPS available. The results are visualized in <ref type="figure" target="#fig_1">Fig. 4</ref>.</p><p>We observe that our method is able to learn scale-aware depth estimation by using even the low-frequency GPS, thereby indicating the strength of the proposed g2s loss. Our method improves upon the baseline Monodepth2 <ref type="bibr" target="#b3">[4]</ref> even with a low-frequency scale-signal. We also note that our method performs equally well without the availability of the altitude information. Thus, we conclude that our method would be applicable in the case of datasets with 2dimensional or sparse GPS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Out-of-Distribution Performance</head><p>We also study the generalization capability of our method on o.o.d. <ref type="bibr" target="#b30">[30]</ref> datasets -Make3D (M3D) <ref type="bibr" target="#b20">[20]</ref> and Cityscapes (CS) <ref type="bibr" target="#b21">[21]</ref>. We evaluate our method (trained on the KITTI Eigen split) on the 2 : 1 center crop of o.o.d. test images. <ref type="table" target="#tab_3">Table IV</ref> shows the mean and standard deviation of the scale factors for the estimated depths, capped at 70 m. The standard deviation on the depth scale factor is the lowest for our method, indicating scale-consistency. This has also been visualized for the Make3D and Cityscapes test sets in Figs. 5 and 6. Also note that the mean of depth scale factors is significantly closer to 1 than for other methods, even though metric-scale is no longer maintained. Finally, the qualitative results on the Make3D and Cityscapes dataset as shown in <ref type="figure">Figs. 7 and 8</ref>, demonstrate that the proposed multi-modal   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>This work addresses the problem of estimating scaleconsistent and -aware monocular dense depths in a selfsupervised setting, a feature essential for many practical autonomous vehicle applications. Previously, only appearancebased losses were used, and hence it was necessary to scale the predictions using the LiDAR ground truth. In contrast, by utilizing the camera-setup-and scene-independent GPS information, we propose an exponentially-weighted GPSto-Scale (g2s) loss to predict metrically accurate singleimage depths within a multimodal self-supervised learning framework. No GPS information is used during the inference. Validating our approach on the KITTI dataset, we improve upon existing methods to predict sharper depths with finerdelineation of objects at scale. Through ablation studies, we Input SC-SfMLearner Monodepth2 Ours <ref type="figure">Fig. 7</ref>. Qualitative results on Make3D test set <ref type="bibr" target="#b20">[20]</ref>. All methods were trained on the monocular sequences from the KITTI Eigen split <ref type="bibr" target="#b18">[18]</ref>. Note that finer details are present in our predictions, such as building structures, silhouettes of flowers, and tree trunks. <ref type="figure">Fig. 8</ref>. Qualitative results on Cityscapes test set <ref type="bibr" target="#b21">[21]</ref>. All methods were trained on the monocular sequences from the KITTI Eigen split. Note that finer details are present in our predictions, such as vehicle details, silhouettes of humans, and traffic signs.</p><p>also demonstrate the efficacy of our proposed loss, even when training on low-frequency or sparse GPS without height information. Finally, we show that our method results in better scale-consistency and -awareness even on out-ofdistribution datasets. We posit that these improved results are a consequence of learning richer representations within a multimodal self-supervised framework. In the future, it seems promising to also study the impact of such at framework on the adversarial robustness of monocular depth estimation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 .</head><label>3</label><figDesc>Box-plot visualizing the mean and standard deviation of scale factors for per-image dense depth estimation on the test set of Eigen split<ref type="bibr" target="#b18">[18]</ref>. Existing methods scaled the estimated depth using the per-image ground truth during inference. Our method is scale-consistent and -aware and does not need ground truth during inference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>Ablation study on different GPS frequencies and dimensionality. Mean scale factor and performance of depth estimation indicated by the Abs Rel Error<ref type="bibr" target="#b18">[18]</ref> is shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Out-of-Distribution depth scale variation on the Make3D test set<ref type="bibr" target="#b20">[20]</ref>. Out-of-Distribution depth scale variation on the Cityscapes test set<ref type="bibr" target="#b21">[21]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I TABLE II Unscaled</head><label>III</label><figDesc>Per-image scaled DENSE DEPTH PREDICTION (WITHOUT POST-PROCESSING) ON KITTI Original [17] AND Improved [19]. DENSE DEPTH PREDICTION ON KITTI Original [17].</figDesc><table><row><cell>GT</cell><cell>Methods</cell><cell>Resolution</cell><cell>Abs Rel</cell><cell cols="3">Error? Sq Rel RMSE RMSE log</cell><cell cols="2">Accuracy? ? &lt; 1.25 ? &lt; 1.25 2</cell><cell>? &lt; 1.25 3</cell></row><row><cell></cell><cell>SfMLearner [5]</cell><cell>416?128</cell><cell>0.208</cell><cell>1.768</cell><cell>6.856</cell><cell>0.283</cell><cell>0.678</cell><cell>0.885</cell><cell>0.957</cell></row><row><cell></cell><cell>GeoNet [31]</cell><cell>416?128</cell><cell>0.155</cell><cell>1.296</cell><cell>5.857</cell><cell>0.233</cell><cell>0.793</cell><cell>0.931</cell><cell>0.973</cell></row><row><cell></cell><cell>Vid2Depth [10]</cell><cell>416?128</cell><cell>0.163</cell><cell>1.240</cell><cell>6.220</cell><cell>0.250</cell><cell>0.762</cell><cell>0.916</cell><cell>0.968</cell></row><row><cell>Original</cell><cell>Struct2Depth [3] VITW [14] Roussel et al. [12] CC [32]</cell><cell>416?128 416?128 416?128 832?256</cell><cell>0.141 0.128 0.179 0.140</cell><cell>1.026 0.959 1.545 1.070</cell><cell>5.291 5.230 6.765 5.326</cell><cell>0.215 0.212 0.268 0.217</cell><cell>0.816 0.845 0754 0.826</cell><cell>0.945 0.947 0916 0.941</cell><cell>0.979 0.976 0.966 0.975</cell></row><row><cell></cell><cell>SC-SfMLearner [9]</cell><cell>832?256</cell><cell>0.137</cell><cell>1.089</cell><cell>5.439</cell><cell>0.217</cell><cell>0.830</cell><cell>0.942</cell><cell>0.975</cell></row><row><cell></cell><cell>Monodepth2 [4]</cell><cell>640?192</cell><cell>0.115</cell><cell>0.903</cell><cell>4.863</cell><cell>0.193</cell><cell>0.877</cell><cell>0.959</cell><cell>0.981</cell></row><row><cell></cell><cell>SG Depth [33]</cell><cell>640?192</cell><cell>0.117</cell><cell>0.907</cell><cell>4.844</cell><cell>0.194</cell><cell>0.875</cell><cell>0.958</cell><cell>0.980</cell></row><row><cell></cell><cell>Ours</cell><cell>640?192</cell><cell>0.112</cell><cell>0.894</cell><cell>4.852</cell><cell>0.192</cell><cell>0.877</cell><cell>0.958</cell><cell>0.981</cell></row><row><cell>Improved</cell><cell>SfMLearner  *  [5] Geonet  *  [31] Vid2Depth  *  [10] Monodepth2 [14] Ours</cell><cell>416?128 416?128 416?128 640?192 640?192</cell><cell>0.176 0.132 0.134 0.090 0.088</cell><cell>1.532 0.994 0.983 0.545 0.554</cell><cell>6.129 5.240 5.501 3.942 3.968</cell><cell>0.244 0.193 0.203 0.137 0.137</cell><cell>0.758 0.883 0.827 0.914 0.913</cell><cell>0.921 0.953 0.944 0.983 0.981</cell><cell>0.971 0.985 0.981 0.995 0.995</cell></row><row><cell></cell><cell>Methods</cell><cell>Resolution</cell><cell>Abs Rel</cell><cell cols="2">Error? Sq Rel RMSE</cell><cell>RMSE log</cell><cell cols="2">Accuracy? ? &lt; 1.25 ? &lt; 1.25 2</cell><cell>? &lt; 1.25 3</cell></row><row><cell></cell><cell>SfMLearner [5]</cell><cell>416?128</cell><cell>0.977</cell><cell>15.161</cell><cell>19.189</cell><cell>3.832</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell></row><row><cell></cell><cell>Roussel et al. [12]</cell><cell>416?128</cell><cell>0.175</cell><cell>1.585</cell><cell>6.901</cell><cell>0.281</cell><cell>0.751</cell><cell>0.905</cell><cell>0.959</cell></row><row><cell></cell><cell>CC [32]</cell><cell>832?256</cell><cell>0.961</cell><cell>14.672</cell><cell>18.838</cell><cell>3.280</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell></row><row><cell>LR</cell><cell>SC-SfMLearner [9]</cell><cell>832?256</cell><cell>0.961</cell><cell>14.915</cell><cell>19.089</cell><cell>3.264</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell></row><row><cell></cell><cell>Monodepth2 [4]</cell><cell>640?192</cell><cell>0.969</cell><cell>15.126</cell><cell>19.199</cell><cell>3.489</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell></row><row><cell></cell><cell>Packnet-SfM [26]</cell><cell>640?192</cell><cell>0.111</cell><cell>0.829</cell><cell>4.788</cell><cell>0.199</cell><cell>0.864</cell><cell>0.954</cell><cell>0.980</cell></row><row><cell></cell><cell>Ours</cell><cell>640?192</cell><cell>0.111</cell><cell>0.900</cell><cell>4.935</cell><cell>0.200</cell><cell>0.863</cell><cell>0.953</cell><cell>0.979</cell></row><row><cell></cell><cell>Ours (pp)</cell><cell>640?192</cell><cell>0.109</cell><cell>0.860</cell><cell>4.855</cell><cell>0.198</cell><cell>0.865</cell><cell>0.954</cell><cell>0.980</cell></row><row><cell>HR</cell><cell>Packnet-SfM [26] Ours Ours (pp)</cell><cell>1280?384 1024?384 1024?384</cell><cell>0.107 0.109 0.109</cell><cell>0.803 0.844 0.809</cell><cell>4.566 4.774 4.705</cell><cell>0.197 0.194 0.193</cell><cell>0.876 0.869 0.869</cell><cell>0.957 0.958 0.959</cell><cell>0.979 0.981 0.982</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III QUANTITATIVE</head><label>III</label><figDesc>COMPARISON ON THE KITTI DEPTH PREDICTION BENCHMARK (ONLINE SERVER).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.4 1.5</cell><cell>GPS with height GPS with no height</cell><cell>0.150 0.175 0.200</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.2 1.3 Mean Scale</cell><cell>0.075 0.100 0.125 Abs Rel Err</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.1</cell><cell>0.025 0.050</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.0</cell><cell>1 2 3 4 5 6 7 8 9 10 GPS frequency</cell><cell>0.000</cell></row><row><cell>Method</cell><cell>Train</cell><cell>SILog</cell><cell>SqErrRel</cell><cell>AbsErrRel</cell><cell>iRMSE</cell></row><row><cell>DORN [6]</cell><cell>D</cell><cell>11.77</cell><cell>2.23</cell><cell>8.78</cell><cell>12.98</cell></row><row><cell>SORD [34]</cell><cell>D</cell><cell>12.39</cell><cell>2.49</cell><cell>10.10</cell><cell>13.48</cell></row><row><cell>VNL [35]</cell><cell>D</cell><cell>12.65</cell><cell>2.46</cell><cell>10.15</cell><cell>13.02</cell></row><row><cell>DS-SIDENet [36]</cell><cell>D</cell><cell>12.86</cell><cell>2.87</cell><cell>10.03</cell><cell>14.40</cell></row><row><cell>PAP [37]</cell><cell>D</cell><cell>13.08</cell><cell>2.72</cell><cell>10.27</cell><cell>13.95</cell></row><row><cell>Guo et al. [38]</cell><cell>D+S</cell><cell>13.41</cell><cell>2.86</cell><cell>10.60</cell><cell>15.06</cell></row><row><cell>Ours</cell><cell>M+G</cell><cell>14.16</cell><cell>3.65</cell><cell>11.40</cell><cell>15.53</cell></row><row><cell>Monodepth2 [4]</cell><cell>M+S</cell><cell>14.41</cell><cell>3.67</cell><cell>11.22</cell><cell>14.73</cell></row><row><cell>DABC [39]</cell><cell>D</cell><cell>14.49</cell><cell>4.08</cell><cell>12.72</cell><cell>15.53</cell></row><row><cell>SDNet [40]</cell><cell>D</cell><cell>14.68</cell><cell>3.90</cell><cell>12.31</cell><cell>15.96</cell></row><row><cell>APMoE [41]</cell><cell>D</cell><cell>14.74</cell><cell>3.88</cell><cell>11.74</cell><cell>15.63</cell></row><row><cell>CSWS [27]</cell><cell>D</cell><cell>14.85</cell><cell>348</cell><cell>11.84</cell><cell>16.38</cell></row><row><cell>HBC [42]</cell><cell>D</cell><cell>15.18</cell><cell>3.79</cell><cell>12.33</cell><cell>17.86</cell></row><row><cell>SGDepth [33]</cell><cell>M+Seg</cell><cell>15.30</cell><cell>5.00</cell><cell>13.29</cell><cell>15.80</cell></row><row><cell>DHGRL [43]</cell><cell>D</cell><cell>15.47</cell><cell>4.04</cell><cell>12.52</cell><cell>15.72</cell></row><row><cell>MultiDepth [44]</cell><cell>D</cell><cell>16.05</cell><cell>3.89</cell><cell>13.82</cell><cell>18.21</cell></row><row><cell>LSIM [45]</cell><cell>S</cell><cell>17.92</cell><cell>6.88</cell><cell>14.04</cell><cell>17.62</cell></row><row><cell>Monodepth [24]</cell><cell>S</cell><cell>22.02</cell><cell>20.58</cell><cell>17.79</cell><cell>21.84</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV SCALE</head><label>IV</label><figDesc>FACTORS ON OUT-OF-DISTRIBUTION DATASETS.</figDesc><table><row><cell></cell><cell>Method</cell><cell>? scale</cell><cell>? scale ?</cell></row><row><cell>M3D</cell><cell>SC-SfMLearner Monodepth2 Ours</cell><cell>40.62 76.02 2.81</cell><cell>17.24 24.40 0.85</cell></row><row><cell></cell><cell>SC-SfMLearner</cell><cell>60.99</cell><cell>22.44</cell></row><row><cell>CS</cell><cell>Monodepth2</cell><cell>118.61</cell><cell>36.49</cell></row><row><cell></cell><cell>Ours</cell><cell>4.01</cell><cell>1.22</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V ABLATION</head><label>V</label><figDesc>STUDY OF DIFFERENT WEIGHTING STRATEGIES ON THE KITTI EIGEN SPLIT<ref type="bibr" target="#b18">[18]</ref>. training improves the delineation of different objects in the depth estimation even for new scenes. These results can be explained by the learning of richer transferable discriminative features due to the scene and camera-setup independence of the GPS scale signal as explained in Sec III-B.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Weights</cell><cell></cell><cell>? scale</cell><cell>? scale ?</cell><cell cols="3">Error? Abs Rel Sq Rel RMSE</cell><cell>RMSE log</cell><cell>Accuracy? ? &lt; 1.25 ? &lt; 1.25 2</cell><cell>? &lt; 1.25 3</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Const. 1</cell><cell></cell><cell>0.776</cell><cell>0.126</cell><cell>1.280</cell><cell>53.454</cell><cell>21.915</cell><cell>0.934</cell><cell>0.217</cell><cell>0.427</cell><cell>0.604</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Const. 10 ?3</cell><cell>1.159</cell><cell>0.120</cell><cell>0.125</cell><cell>1.032</cell><cell>5.214</cell><cell>0.203</cell><cell>0.860</cell><cell>0.955</cell><cell>0.979</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Linear</cell><cell></cell><cell>0.124</cell><cell>0.020</cell><cell>0.443</cell><cell>4.757</cell><cell>12.083</cell><cell>0.588</cell><cell>0.303</cell><cell>0.561</cell><cell>0.766</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Ours (Eq. 5)</cell><cell>1.031</cell><cell>0.073</cell><cell>0.112</cell><cell>0.894</cell><cell>4.852</cell><cell>0.192</cell><cell>0.877</cell><cell>0.958</cell><cell>0.981</cell></row><row><cell>Scale</cell><cell>100 150</cell><cell></cell><cell cols="2">Monodepth2 SC-SfMLearner Ours Metric Scale</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1</cell><cell>0</cell><cell>25</cell><cell cols="2">50 Image idx 75</cell><cell>100</cell><cell>125</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to localize using a lidar intensity map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Barsan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pokrovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning (CoRL)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="605" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Depth extraction from video using non-parametric sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Karsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Depth prediction without the sensors: Leveraging structure for unsupervised learning from monocular videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Casser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Digging into self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Ba-net: Dense bundle adjustment network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.04807</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deeptam: Deep tracking and mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ummenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised scale-consistent depth and ego-motion learning from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from monocular video using 3d geometric constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Robust semisupervised monocular depth estimation with reprojected distances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ambrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning. PMLR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Monocular depth estimation in new environments with absolute scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Roussel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Eycken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Crowdsourced 3d mapping: A combined multi-view geometry and selfsupervised learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jukola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brouns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Arani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zonooz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/RSJ International Conference on Intelligent Robots and Systems(IROS)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Depth from videos in the wild: Unsupervised monocular depth learning from unknown cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jonschkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep multimodal learning: A survey on recent advances and trends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramachandram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sparsity invariant cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Make3d: Learning 3d scene structure from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Orb-slam: a versatile and accurate monocular slam system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mur-Artal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M M</forename><surname>Montiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Tardos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on robotics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1147" to="1163" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised cnn for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Bg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="740" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Toward hierarchical self-supervised monocular absolute depth estimation for autonomous driving applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Ang</surname><genName>Jr</genName></persName>
		</author>
		<idno>2020. 3</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/RSJ International Conference on Intelligent Robots and Systems(IROS)</title>
		<imprint/>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">3d packing for self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ambrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raventos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Monocular depth estimation with hierarchical fusion of dilated cnns and soft-weighted-sum inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An experimental study on relative and absolute pose graph fusion for vehicle localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dubbelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="630" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Measuring the tendency of cnns to learn surface statistical regularities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.11561</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.07780</idno>
		<title level="m">Shortcut learning in deep neural networks</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Geonet: Unsupervised learning of dense depth, optical flow and camera pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1983" to="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Competitive collaboration: Joint unsupervised learning of depth, camera motion, optical flow and motion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Balles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="240" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Self-Supervised Monocular Depth Estimation: Solving the Dynamic Object Problem by Semantic Guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-A</forename><surname>Term?hlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fingscheidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Soft labels for ordinal regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marathe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4738" to="4747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Enforcing geometric constraints of virtual normal for depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5684" to="5693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep robust single image depth estimation neural network using scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>El-Khamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="37" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Patternaffinitive propagation across depth, surface normal and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4106" to="4115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning monocular depth by distilling cross-domain stereo networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="484" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep attentionbased classification network for robust depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Sdnet: Semantically guided depth estimation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kretz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="288" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pixel-wise attentional gating for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1024" to="1033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Hierarchical binary classification for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Robotics and Biomimetics (ROBIO)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1975" to="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep hierarchical guidance and regularization learning for end-to-end depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="430" to="442" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multidepth: Single-image depth estimation via multi-task regression and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liebel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>K?rner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Intelligent Transportation Systems Conference (ITSC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1440" to="1447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learn stereo, infer mono: Siamese networks for self-supervised, monocular, depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
