<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Learning for Precipitation Nowcasting: A Benchmark and A New Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihan</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonard</forename><surname>Lausen</surname></persName>
							<email>lelausen@cse.ust.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
							<email>dyyeung@cse.ust.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai-Kin</forename><surname>Wong</surname></persName>
							<email>wkwong@hko.gov.hk</email>
							<affiliation key="aff1">
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chun</forename><surname>Woo</surname></persName>
							<email>wcwoo@hko.gov.hk</email>
							<affiliation key="aff1">
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Kong Observatory</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Learning for Precipitation Nowcasting: A Benchmark and A New Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the goal of making high-resolution forecasts of regional rainfall, precipitation nowcasting has become an important and fundamental technology underlying various public services ranging from rainstorm warnings to flight safety. Recently, the Convolutional LSTM (ConvLSTM) model has been shown to outperform traditional optical flow based methods for precipitation nowcasting, suggesting that deep learning models have a huge potential for solving the problem. However, the convolutional recurrence structure in ConvLSTM-based models is location-invariant while natural motion and transformation (e.g., rotation) are location-variant in general. Furthermore, since deep-learning-based precipitation nowcasting is a newly emerging area, clear evaluation protocols have not yet been established. To address these problems, we propose both a new model and a benchmark for precipitation nowcasting. Specifically, we go beyond ConvLSTM and propose the Trajectory GRU (TrajGRU) model that can actively learn the location-variant structure for recurrent connections. Besides, we provide a benchmark that includes a real-world large-scale dataset from the Hong Kong Observatory, a new training loss, and a comprehensive evaluation protocol to facilitate future research and gauge the state of the art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Precipitation nowcasting refers to the problem of providing very short range (e.g., 0-6 hours) forecast of the rainfall intensity in a local region based on radar echo maps 1 , rain gauge and other observation data as well as the Numerical Weather Prediction (NWP) models. It significantly impacts the daily lives of many and plays a vital role in many real-world applications. Among other possibilities, it helps to facilitate drivers by predicting road conditions, enhances flight safety by providing weather guidance for regional aviation, and avoids casualties by issuing citywide rainfall alerts. In addition to the inherent complexities of the atmosphere and relevant dynamical processes, the ever-growing need for real-time, large-scale, and fine-grained precipitation nowcasting poses extra challenges to the meteorological community and has aroused research interest in the machine learning community <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>The conventional approaches to precipitation nowcasting used by existing operational systems rely on optical flow <ref type="bibr" target="#b29">[30]</ref>. In a modern day nowcasting system, the convective cloud movements are first estimated from the observed radar echo maps by optical flow and are then used to predict the future radar echo maps using semi-Lagrangian advection. However, these methods are unsupervised from the machine learning point of view in that they do not take advantage of the vast amount of existing radar echo data. Recently, progress has been made by utilizing supervised deep learning <ref type="bibr" target="#b16">[17]</ref> techniques for precipitation nowcasting. Shi et al. <ref type="bibr" target="#b24">[25]</ref> formulated precipitation nowcasting as a spatiotemporal sequence forecasting problem and proposed the Convolutional Long Short-Term Memory (ConvLSTM) model, which extends the LSTM <ref type="bibr" target="#b7">[8]</ref> by having convolutional structures in both the input-to-state and state-to-state transitions, to solve the problem. Using the radar echo sequences for model training, the authors showed that ConvLSTM is better at capturing the spatiotemporal correlations than the fully-connected LSTM and gives more accurate predictions than the Real-time Optical flow by Variational methods for Echoes of Radar (ROVER) algorithm <ref type="bibr" target="#b29">[30]</ref> currently used by the Hong Kong Observatory (HKO).</p><p>However, despite their pioneering effort in this interesting direction, the paper has some deficiencies. First, the deep learning model is only evaluated on a relatively small dataset containing 97 rainy days and only the nowcasting skill score at the 0.5mm/h rain-rate threshold is compared. As real-world precipitation nowcasting systems need to pay additional attention to heavier rainfall events such as rainstorms which cause more threat to the society, the performance at the 0.5mm/h threshold (indicating raining or not) alone is not sufficient for demonstrating the algorithm's overall performance <ref type="bibr" target="#b29">[30]</ref>. In fact, as the area Deep Learning for Precipitation Nowcasting is still in its early stage, it is not clear how models should be evaluated to meet the need of real-world applications. Second, although the convolutional recurrence structure used in ConvLSTM is better than the fullyconnected recurrent structure in capturing spatiotemporal correlations, it is not optimal and leaves room for improvement. For motion patterns like rotation and scaling, the local correlation structure of consecutive frames will be different for different spatial locations and timestamps. It is thus inefficient to use convolution which uses a location-invariant filter to represent such location-variant relationship. Previous attempts have tried to solve the problem by revising the output of a recurrent neural network (RNN) from the raw prediction to be some location-variant transformation of the input, like optical flow or dynamic local filter <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b2">3]</ref>. However, not much research has been conducted to address the problem by revising the recurrent structure itself.</p><p>In this paper, we aim to address these two problems by proposing both a benchmark and a new model for precipitation nowcasting. For the new benchmark, we build the HKO-7 dataset which contains radar echo data from 2009 to 2015 near Hong Kong. Since the radar echo maps arrive in a stream in the real-world scenario, the nowcasting algorithms can adopt online learning to adapt to the newly emerging patterns dynamically. To take into account this setting, we use two testing protocols in our benchmark: the offline setting in which the algorithm can only use a fixed window of the previous radar echo maps and the online setting in which the algorithm is free to use all the historical data and any online learning algorithm. Another issue for the precipitation nowcasting task is that the proportions of rainfall events at different rain-rate thresholds are highly imbalanced. Heavier rainfall occurs less often but has a higher real-world impact. We thus propose the Balanced Mean Squared Error (B-MSE) and Balanced Mean Absolute Error (B-MAE) measures for training and evaluation, which assign more weights to heavier rainfalls in the calculation of MSE and MAE. We empirically find that the balanced variants of the loss functions are more consistent with the overall nowcasting performance at multiple rain-rate thresholds than the original loss functions. Moreover, our experiments show that training with the balanced loss functions is essential for deep learning models to achieve good performance at higher rain-rate thresholds. For the new model, we propose the Trajectory Gated Recurrent Unit (TrajGRU) model which uses a subnetwork to output the state-to-state connection structures before state transitions. TrajGRU allows the state to be aggregated along some learned trajectories and thus is more flexible than the Convolutional GRU (ConvGRU) <ref type="bibr" target="#b1">[2]</ref> whose connection structure is fixed. We show that TrajGRU outperforms ConvGRU, Dynamic Filter Network (DFN) <ref type="bibr" target="#b2">[3]</ref> as well as 2D and 3D Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b28">29]</ref> in both a synthetic MovingMNIST++ dataset and the HKO-7 dataset.</p><p>Using the new dataset, testing protocols, training loss and model, we provide extensive empirical evaluation of seven models, including a simple baseline model which always predicts the last frame, two optical flow based models (ROVER and its nonlinear variant), and four representative deep learning models (TrajGRU, ConvGRU, 2D CNN, and 3D CNN). We also provide a large-scale benchmark for precipitation nowcasting. Our experimental validation shows that (1) all the deep learning models outperform the optical flow based models, (2) TrajGRU attains the best overall performance among all the deep learning models, and (3) after applying online fine-tuning, the models tested in the online setting consistently outperform those in the offline setting. To the best of our knowledge, this is the first comprehensive benchmark of deep learning models for the precipitation nowcasting problem. Besides, since precipitation nowcasting can be viewed as a video prediction problem <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b28">29]</ref>, our work is the first to provide evidence and justification that online learning could potentially be helpful for video prediction in general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Deep learning for precipitation nowcasting and video prediction For the precipitation nowcasting problem, the reflectivity factors in radar echo maps are first transformed to grayscale images before being fed into the prediction algorithm <ref type="bibr" target="#b24">[25]</ref>. Thus, precipitation nowcasting can be viewed as a type of video prediction problem with a fixed "camera", which is the weather radar. Therefore, methods proposed for predicting future frames in natural videos are also applicable to precipitation nowcasting and are related to our paper. There are three types of general architecture for video prediction: RNN based models, 2D CNN based models, and 3D CNN based models. Ranzato et al. <ref type="bibr" target="#b23">[24]</ref> proposed the first RNN based model for video prediction, which uses a convolutional RNN with 1 ? 1 state-state kernel to encode the observed frames. Srivastava et al. <ref type="bibr" target="#b25">[26]</ref> proposed the LSTM encoder-decoder network which uses one LSTM to encode the input frames and another LSTM to predict multiple frames ahead. The model was generalized in <ref type="bibr" target="#b24">[25]</ref> by replacing the fully-connected LSTM with ConvLSTM to capture the spatiotemporal correlations better. Later, Finn et al. <ref type="bibr" target="#b4">[5]</ref> and De Brabandere et al. <ref type="bibr" target="#b2">[3]</ref> extended the model in <ref type="bibr" target="#b24">[25]</ref> by making the network predict the transformation of the input frame instead of directly predicting the raw pixels. Ruben et al. <ref type="bibr" target="#b27">[28]</ref> proposed to use both an RNN that captures the motion and a CNN that captures the content to generate the prediction. Along with RNN based models, 2D and 3D CNN based models were proposed in <ref type="bibr" target="#b21">[22]</ref> and <ref type="bibr" target="#b28">[29]</ref> respectively. Mathieu et al. <ref type="bibr" target="#b21">[22]</ref> treated the frame sequence as multiple channels and applied 2D CNN to generate the prediction while <ref type="bibr" target="#b28">[29]</ref> treated them as the depth and applied 3D CNN. Both papers show that Generative Adversarial Network (GAN) <ref type="bibr" target="#b5">[6]</ref> is helpful for generating sharp predictions.</p><p>Structured recurrent connection for spatiotemporal modeling From a higher-level perspective, precipitation nowcasting and video prediction are intrinsically spatiotemporal sequence forecasting problems in which both the input and output are spatiotemporal sequences <ref type="bibr" target="#b24">[25]</ref>. Recently, there is a trend of replacing the fully-connected structure in the recurrent connections of RNN with other topologies to enhance the network's ability to model the spatiotemporal relationship. Other than the ConvLSTM which replaces the full-connection with convolution and is designed for dense videos, the SocialLSTM [1] and the Structural-RNN (S-RNN) <ref type="bibr" target="#b12">[13]</ref> have been proposed sharing a similar notion. SocialLSTM defines the topology based on the distance between different people and is designed for human trajectory prediction while S-RNN defines the structure based on the given spatiotemporal graph. All these models are different from our TrajGRU in that our model actively learns the recurrent connection structure. Liang et al. <ref type="bibr" target="#b18">[19]</ref> have proposed the Structure-evolving LSTM, which also has the ability to learn the connection structure of RNNs. However, their model is designed for the semantic object parsing task and learns how to merge the graph nodes automatically. It is thus different from TrajGRU which aims at learning the local correlation structure for spatiotemporal data.</p><p>Benchmark for video tasks There exist benchmarks for several video tasks like online object tracking <ref type="bibr" target="#b30">[31]</ref> and video object segmentation <ref type="bibr" target="#b22">[23]</ref>. However, there is no benchmark for the precipitation nowcasting problem, which is also a video task but has its unique properties since radar echo map is a completely different type of data and the data is highly imbalanced (as mentioned in Section 1). The large-scale benchmark created as part of this work could help fill the gap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>In this section, we present our new model for precipitation nowcasting. We first introduce the general encoding-forecasting structure used in this paper. Then we review the ConvGRU model and present our new TrajGRU model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Encoding-forecasting Structure</head><p>We adopt a similar formulation of the precipitation nowcasting problem as in <ref type="bibr" target="#b24">[25]</ref>. Assume that the radar echo maps form a spatiotemporal sequence I 1 , I 2 , . . . . At a given timestamp t, our model generates the most likely K-step predictions,? t+1 ,? t+2 , . . . ,? t+K , based on the previous J observations including the current one: I t?J+1 , I t?J+2 , . . . , I t . Our encoding-forecasting network first encodes the observations into n layers of RNN states:</p><formula xml:id="formula_0">H 1 t , H 2 t , . . . , H n t = h(I t?J+1 , I t?J+2 , .</formula><p>. . , I t ), and then uses another n layers of RNNs to generate the predictions based on these encoded states:</p><formula xml:id="formula_1">I t+1 ,? t+2 , . . . ,? t+K = g(H 1 t , H 2 t , .</formula><p>. . , H n t ). <ref type="figure">Figure 1</ref> illustrates our encoding-forecasting structure for n = 3, J = 2, K = 2. We insert downsampling and upsampling layers between the RNNs, which are implemented by convolution and deconvolution with stride. The reason to reverse the order of the forecasting network is that the high-level states, which have captured the global spatiotemporal representation, could guide the update of the low-level states. Moreover, the low-level states could further influence the prediction. This structure is more reasonable than the previous structure <ref type="bibr" target="#b24">[25]</ref> which does not reverse the link of the forecasting network because we are free to plug in additional RNN layers on top and no skip-connection is required to aggregate the low-level information. One can choose any type of RNNs like ConvGRU or our newly proposed TrajGRU in this general encoding-forecasting structure as long as their states correspond to tensors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Convolutional GRU</head><p>The main formulas of the ConvGRU used in this paper are given as follows:</p><formula xml:id="formula_2">Z t = ?(W xz * X t + W hz * H t?1 ), R t = ?(W xr * X t + W hr * H t?1 )</formula><p>,</p><formula xml:id="formula_3">H t = f (W xh * X t + R t ? (W hh * H t?1 )), H t = (1 ? Z t ) ? H t + Z t ? H t?1 .<label>(1)</label></formula><p>The bias terms are omitted for notational simplicity. ' * ' is the convolution operation and '?' is the Hadamard product. Here, H t , R t , Z t , H t ? R C h ?H?W are the memory state, reset gate, update gate, and new information, respectively. X t ? R Ci?H?W is the input and f is the activation, which is chosen to be leaky ReLU with negative slope equals to 0.2 <ref type="bibr" target="#b19">[20]</ref> througout the paper. H, W are the height and width of the state and input tensors and C h , C i are the channel sizes of the state and input tensors, respectively. Every time a new input arrives, the reset gate will control whether to clear the previous state and the update gate will control how much the new information will be written to the state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Trajectory GRU</head><p>When used for capturing spatiotemporal correlations, the deficiency of ConvGRU and other ConvRNNs is that the connection structure and weights are fixed for all the locations. The convolution operation basically applies a location-invariant filter to the input. If the inputs are all zero and the reset gates are all one, we could rewrite the computation process of the new information at a specific location (i, j) at timestamp t, i.e, H t,:,i,j , as follows:</p><formula xml:id="formula_4">H t,:,i,j = f (W hh concat( H t?1,:,p,q | (p, q) ? N h i,j )) = f ( |N h i,j | l=1 W l hh H t?1,:,p l,i,j ,q l,i,j ). (2)</formula><p>Here, N h i,j is the ordered neighborhood set at location (i, j) defined by the hyperparameters of the state-to-state convolution such as kernel size, dilation and padding <ref type="bibr" target="#b31">[32]</ref>. (p l,i,j , q l,i,j ) is the lth neighborhood location of position (i, j). The concat(?) function concatenates the inner vectors in the set and W hh is the matrix representation of the state-to-state convolution weights.</p><p>As the hyperparameter of convolution is fixed, the neighborhood set N h i,j stays the same for all locations. However, most motion patterns have different neighborhood sets for different locations. For example, rotation and scaling generate flow fields with different angles pointing to different directions. It would thus be more reasonable to have a location-variant connection structure as  <ref type="figure">Figure 1</ref>: Example of the encoding-forecasting structure used in the paper. In the figure, we use three RNNs to predict two future frames?3,?4 given the two input frames I1, I2. The spatial coordinates G are concatenated to the input frame to ensure the network knows the observations are from different locations. The RNNs can be either ConvGRU or TrajGRU. Zeros are fed as input to the RNN if the input link is missing.</p><formula xml:id="formula_5">H t,:,i,j = f ( L l=1 W l hh H t?1,:,p l,i,j (?),q l,i,j (?) ),<label>(3)</label></formula><p>(a) For convolutional RNN, the recurrent connections are fixed over time.</p><p>(b) For trajectory RNN, the recurrent connections are dynamically determined. where L is the total number of local links, (p l,i,j (?), q l,i,j (?)) is the lth neighborhood parameterized by ?.</p><p>Based on this observation, we propose the TrajGRU, which uses the current input and previous state to generate the local neighborhood set for each location at each timestamp. Since the location indices are discrete and non-differentiable, we use a set of continuous optical flows to represent these "indices". The main formulas of TrajGRU are given as follows:</p><formula xml:id="formula_6">U t , V t = ?(X t , H t?1 ), Z t = ?(W xz * X t + L l=1 W l hz * warp(H t?1 , U t,l , V t,l )), R t = ?(W xr * X t + L l=1 W l hr * warp(H t?1 , U t,l , V t,l )), H t = f (W xh * X t + R t ? ( L l=1 W l hh * warp(H t?1 , U t,l , V t,l ))), H t = (1 ? Z t ) ? H t + Z t ? H t?1 .<label>(4)</label></formula><p>Here, L is the total number of allowed links. U t , V t ? R L?H?W are the flow fields that store the local connection structure generated by the structure generating network ?. The W l hz , W l hr , W l hh are the weights for projecting the channels, which are implemented by 1 ? 1 convolutions. The warp(H t?1 , U t,l , V t,l ) function selects the positions pointed out by U t,l , V t,l from H t?1 via the bilinear sampling kernel <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b9">10]</ref>. If we denote M = warp(I, U, V) where M, I ? R C?H?W and U, V ? R H?W , we have:</p><formula xml:id="formula_7">M c,i,j = H m=1 W n=1 I c,m,n max(0, 1 ? |i + V i,j ? m|) max(0, 1 ? |j + U i,j ? n|).<label>(5)</label></formula><p>The advantage of such a structure is that we could learn the connection topology by learning the parameters of the subnetwork ?. In our experiments, ? takes the concatenation of X t and H t?1 as the input and is fixed to be a one-hidden-layer convolutional neural network with 5 ? 5 kernel size and 32 feature maps. Thus, ? has only a small number of parameters and adds nearly no cost to the overall computation. Compared to a ConvGRU with K ? K state-to-state convolution, TrajGRU is able to learn a more efficient connection structure with L &lt; K 2 . For ConvGRU and TrajGRU, the number of model parameters is dominated by the size of the state-to-state weights, which is O(L ? C 2 h ) for TrajGRU and O(K 2 ? C 2 h ) for ConvGRU. If L is chosen to be smaller than K 2 , the number of parameters of TrajGRU can also be smaller than the ConvGRU and the TrajGRU model is able to use the parameters more efficiently. Illustration of the recurrent connection structures of ConvGRU and TrajGRU is given in <ref type="figure" target="#fig_1">Figure 2</ref>. Recently, Jeon &amp; Kim <ref type="bibr" target="#b13">[14]</ref> has used similar ideas to extend the convolution operations in CNN. However, their proposed Active Convolution Unit (ACU) focuses on the images where the need for location-variant filters is limited. Our TrajGRU focuses on videos where location-variant filters are crucial for handling motion patterns like rotations. Moreover, we are revising the structure of the recurrent connection and have tested different number of links while <ref type="bibr" target="#b13">[14]</ref> fixes the link number to 9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments on MovingMNIST++</head><p>Before evaluating our model on the more challenging precipitation nowcasting task, we first compare TrajGRU with ConvGRU, DFN and 2D/3D CNNs on a synthetic video prediction dataset to justify its effectiveness.</p><p>The previous MovingMNIST dataset <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b24">25]</ref> only moves the digits with a constant speed and is not suitable for evaluating different models' ability in capturing more complicated motion patterns. We thus design the MovingMNIST++ dataset by extending MovingMNIST to allow random rotations, scale changes, and illumination changes. Each frame is of size 64 ? 64 and contains three moving digits. We use 10 frames as input to predict the next 10 frames. As the frames have illumination changes, we use MSE instead of cross-entropy for training and evaluation 2 . We train all models using the Adam optimizer <ref type="bibr" target="#b15">[16]</ref> with learning rate equal to 10 ?4 and momentum equal to 0.5. For the RNN models, we use the encoding-forecasting structure introduced previously with three RNN layers. All RNNs are either ConvGRU or TrajGRU and all use the same set of hyperparameters. For TrajGRU, we initialize the weight of the output layer of the structure generating network to zero. The strides of the middle downsampling and upsampling layers are chosen to be 2. The numbers of filters for the three RNNs are 64, 96, 96 respectively. For the DFN model, we replace the output layer of ConvGRU with a 11 ? 11 local filter and transform the previous frame to get the prediction. For the RNN models, we train them for 200,000 iterations with norm clipping threshold equal to 10 and batch size equal to 4. For the CNN models, we train them for 100,000 iterations with norm clipping threshold equal to 50 and batch size equal to 32. The detailed experimental configuration of the models for the MovingMNIST++ experiment can be found in the appendix. We have also tried to use conditional GAN for the 2D and 3D models but have failed to get reasonable results. <ref type="table" target="#tab_0">Table 1</ref> gives the results of different models on the same test set that contains 10,000 sequences. We train all models using three different seeds to report the standard deviation. We can find that TrajGRU with only 5 links outperforms ConvGRU with state-to-state kernel size 3 ? 3 and dilation 2 ? 2 (9 links). Also, the performance of TrajGRU improves as the number of links increases. TrajGRU with L = 13 outperforms ConvGRU with 7 ? 7 state-to-state kernel and yet has fewer parameters. Another observation from the table is that DFN does not perform well in this synthetic dataset. This is because DFN uses softmax to enhance the sparsity of the learned local filters, which fails to model illumination change because the maximum value always gets smaller after convolving with a positive kernel whose weights sum up to 1. For DFN, when the pixel values get smaller, it is impossible for them to increase again. <ref type="figure">Figure 3</ref> visualizes the learned structures of TrajGRU. We can see that the network has learned reasonable local link patterns. <ref type="figure">Figure 3</ref>: Selected links of TrajGRU-L13 at different frames and layers. We choose one of the 13 links and plot an arrow starting from each pixel to the pixel that is referenced by the link. From left to right we display the learned structure at the first, second and third layer of the encoder. The links displayed here have learned behaviour for rotations. We sub-sample the displayed links for the first layer for better readability. We include animations for all layers and links in the supplementary material. (Best viewed when zoomed in.)</p><p>5 Benchmark for Precipitation Nowcasting</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">HKO-7 Dataset</head><p>The HKO-7 dataset used in the benchmark contains radar echo data from 2009 to 2015 collected by HKO. The radar CAPPI reflectivity images, which have resolution of 480 ? 480 pixels, are taken from an altitude of 2km and cover a 512km ? 512km area centered in Hong Kong. The data are recorded every 6 minutes and hence there are 240 frames per day. The raw logarithmic radar reflectivity factors are linearly transformed to pixel values via pixel = 255 ? dBZ+10 70 + 0.5 and are clipped to be between 0 and 255. The raw radar echo images generated by Doppler weather radar are noisy due to factors like ground clutter, sea clutter, anomalous propagation and electromagnetic interference <ref type="bibr" target="#b17">[18]</ref>. To alleviate the impact of noise in training and evaluation, we filter the noisy pixels in the dataset and generate the noise masks by a two-stage process described in the appendix.</p><p>As rainfall events occur sparsely, we select the rainy days based on the rain barrel information to form our final dataset, which has 812 days for training, 50 days for validation and 131 days for testing. Our current treatment is close to the real-life scenario as we are able to train an additional model that classifies whether or not it will rain on the next day and applies our precipitation nowcasting model if this coarser-level model predicts that it will be rainy. The radar reflectivity values are converted to rainfall intensity values (mm/h) using the Z-R relationship: dBZ = 10 log a + 10b log R where R is the rain-rate level, a = 58.53 and b = 1.56. The overall statistics and the average monthly rainfall distribution of the HKO-7 dataset are given in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation Methodology</head><p>As the radar echo maps arrive in a stream, nowcasting algorithms can apply online learning to adapt to the newly emerging spatiotemporal patterns. We propose two settings in our evaluation protocol: (1) the offline setting in which the algorithm always receives 5 frames as input and predicts 20 frames ahead, and (2) the online setting in which the algorithm receives segments of length 5 sequentially and predicts 20 frames ahead for each new segment received. The evaluation protocol is described more systematically in the appendix. The testing environment guarantees that the same set of sequences is tested in both the offline and online settings for fair comparison.</p><p>For both settings, we evaluate the skill scores for multiple thresholds that correspond to different rainfall levels to give an all-round evaluation of the algorithms' nowcasting performance. <ref type="table" target="#tab_1">Table 2</ref> shows the distribution of different rainfall levels in our dataset. We choose to use the thresholds 0.5, 2, 5, 10, 30 to calculate the CSI and Heidke Skill Score (HSS) <ref type="bibr" target="#b8">[9]</ref>. For calculating the skill score at a specific threshold ? , which is 0.5, 2, 5, 10 or 30, we first convert the pixel values in prediction and ground-truth to 0/1 by thresholding with ? . We then calculate the TP (prediction=1, truth=1), FN (prediction=0, truth=1), FP (prediction=1, truth=0), and TN (prediction=0, truth=0). The CSI score is calculated as TP TP+FN+FP and the HSS score is calculated as TP?TN?FN?FP (TP+FN)(FN+TN)+(TP+FP)(FP+TN) . During the computation, the masked points are ignored. As shown in <ref type="table" target="#tab_1">Table 2</ref>, the frequencies of different rainfall levels are highly imbalanced. We propose to use the weighted loss function to help solve this problem. Specifically, we assign a weight w(x) to each pixel according to its rainfall intensity x: 480 i=1 480 j=1 w n,i,j |x n,i,j ? x n,i,j |, where N is the total number of frames and w n,i,j is the weight corresponding to the (i, j)th pixel in the nth frame. For the conventional MSE and MAE measures, we simply set all the weights to 1 except the masked points.</p><formula xml:id="formula_8">w(x) = ? ? ? ? ? ? ? ? ? ? ? ? ? 1, x &lt; 2 2, 2 ? x &lt; 5 5, 5 ? x &lt;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluated Algorithms</head><p>We have evaluated seven nowcasting algorithms, including the simplest model which always predicts the last frame, two optical flow based methods (ROVER and its nonlinear variant), and four deep learning methods (TrajGRU, ConvGRU, 2D CNN, and 3D CNN). Specifically, we have evaluated the performance of deep learning models in the online setting by fine-tuning the algorithms using AdaGrad <ref type="bibr" target="#b3">[4]</ref> with learning rate equal to 10 ?4 . We optimize the sum of B-MSE and B-MAE during offline training and online fine-tuning. During the offline training process, all models are optimized by the Adam optimizer with learning rate equal to 10 ?4 and momentum equal to 0.5 and we train these models with early-stopping on the sum of B-MSE and B-MAE. For RNN models, the training batch size is set to 4. For the CNN models, the training batch size is set to 8. For TrajGRU and ConvGRU models, we use a 3-layer encoding-forecasting structure with the number of filters for the RNNs set to 64, 192, 192. We use kernel size equal to 5 ? 5, 5 ? 5, 3 ? 3 for the ConvGRU models while the number of links is set to 13, 13, 9 for the TrajGRU model. We also train the ConvGRU model with the original MSE and MAE loss, which is named "ConvGRU-nobal", to evaluate the improvement by training with the B-MSE and B-MAE loss. The other model configurations including ROVER, ROVER-nonlinear and deep models are included in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Evaluation Results</head><p>The overall evaluation results are summarized in <ref type="table">Table 3</ref>. In order to analyze the confidence interval of the results, we train 2D CNN, 3D CNN, ConvGRU and TrajGRU models using three different random seeds and report the standard deviation in <ref type="table" target="#tab_3">Table 4</ref>. We find that training with balanced loss functions is essential for good nowcasting performance of heavier rainfall. The ConvGRU model that is trained without balanced loss, which best represents the model in <ref type="bibr" target="#b24">[25]</ref>, has worse nowcasting score than the optical flow based methods at the 10mm/h and 30mm/h thresholds. Also, we find that all the deep learning models that are trained with the balanced loss outperform the optical flow based models. Among the deep learning models, TrajGRU performs the best and 3D CNN outperforms 2D CNN, which shows that an appropriate network structure is crucial to achieving good performance. The improvement of TrajGRU over the other models is statistically significant because the differences in B-MSE and B-MAE are larger than three times their standard deviation. Moreover, the performance with online fine-tuning enabled is consistently better than that without online fine-tuning, which verifies the effectiveness of online learning at least for this task. <ref type="table">Table 3</ref>: HKO-7 benchmark result. We mark the best result within a specific setting with bold face and the second best result by underlining. Each cell contains the mean score of the 20 predicted frames. In the online setting, all algorithms have used the online learning strategy described in the paper. '?' means that the score is higher the better while '?' means that the score is lower the better. 'r ? ? ' means the skill score at the ? mm/h rainfall threshold. For 2D CNN, 3D CNN, ConvGRU and TrajGRU models, we train the models with three different random seeds and report the mean scores.   Based on the evaluation results, we also compute the Kendall's ? coefficients <ref type="bibr" target="#b14">[15]</ref> between the MSE, MAE, B-MSE, B-MAE and the CSI, HSS at different thresholds. As shown in <ref type="table" target="#tab_4">Table 5</ref>, B-MSE and B-MAE have stronger correlation with the CSI and HSS in most cases.</p><formula xml:id="formula_9">Algorithms CSI ? HSS ? B-MSE ? B-MAE ? r ? 0.5 r ? 2 r ? 5 r ? 10 r ? 30 r ? 0.5 r ? 2 r ? 5 r ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this paper, we have provided the first large-scale benchmark for precipitation nowcasting and have proposed a new TrajGRU model with the ability of learning the recurrent connection structure. We have shown TrajGRU is more efficient in capturing the spatiotemporal correlations than ConvGRU. For future work, we plan to test if TrajGRU helps improve other spatiotemporal learning tasks like visual object tracking and video segmentation. We will also try to build an operational nowcasting system using the proposed algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Weight Initialization</head><p>The weights and biases of all models are initialized with the MSRA initializer <ref type="bibr" target="#b6">[7]</ref> except that the weights and biases of the structure generating network in TrajGRUs are initialized to be zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Structure Generating Network in TrajGRU</head><p>The structure generating network takes the concatenation of the state tensor and the input tensor as the input. We fix the network to have two convolution layers. The first convolution layer uses 5 ? 5 kernel size, 2 ? 2 padding size, 32 filters and uses the leaky ReLU activation. The second convolution layer uses 5 ? 5 kernel size, 2 ? 2 padding and 2L filters where L is the number of links.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Details about the MovingMNIST++ Experiment C.1 Generation Process</head><p>For each sequence, we choose three digits randomly from the MNIST dataset <ref type="bibr" target="#b2">3</ref> . Each digit will move, rotate, scale up or down at a randomly sampled speed. Also, we multiply the pixel values by an illumination factor every time to make the digits have time-varying appearances. The hyperparameters of the generation process are given in <ref type="table" target="#tab_5">Table 6</ref>. In our experiment, we always generate a length-20 sequence and use the first 10 frames to predict the last 10 frames. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Network Structures</head><p>The general structure of the 2D CNN, 3D CNN and the DFN model used in the paper are illustrated in <ref type="figure" target="#fig_5">Figure 4</ref>. We always use batch normalization <ref type="bibr" target="#b10">[11]</ref> in 2D and 3D CNNs.</p><p>The detailed network configurations of 2D CNN, 3D CNN, ConvGRU, DFN and TrajGRU are described in <ref type="table">Table 7</ref>, 8, 9, 10, 11. <ref type="table">Table 7</ref>: The details of the 2D CNN model. The two dimensions in kernel, stride, pad and other features represent for height and width. We set the base filter number c to 70. We derive the 2D model from the 3D model by multiplying the number of channels with the respective kernel size of the 3D model. The 10 channels in the input of 'enc1' and the output of 'vid5' correspond to the input and output frames, respectively.  (a) Illustration of the 2D/3D CNNs used in the paper. In this example, we use 4 convolution layers to get the representation of the 5 input frames, which is further used to forecast the 5 future frames. We use either 2D convolution or 3D convolution in the encoder and the forecaster.  Transform Transform</p><formula xml:id="formula_10">I 2 I ^ 3 (b)</formula><p>Illustration of the DFN model used in the paper. In this example, we use 2 frames to predict 2 frames. The?s are the predicted local filters, which are used to transform the last input frame or the previous predicted frame. We use ConvGRU as the RNN model in the experiment.     The overall statistics of the HKO-7 dataset is given in <ref type="figure" target="#fig_6">Figure 5</ref> and <ref type="table" target="#tab_0">Table 12</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Denoising Process</head><p>We first remove the ground clutter and sun spikes, which appear at a fixed position, by detecting the outlier locations in the image. For each in-boundary location i in the frame, we use the ratio of its pixel value equal to 1, 2, ..., 255 as the feature xi ? R 255 and estimate these features' sample mean? =</p><formula xml:id="formula_11">N i=1 x i N and covariance matrix? = N i=1 (x i ??)(x i ??) T N ?1</formula><p>. We then calculate the Mahalanobis distance DM (x) = (x ??) T? ? (x ??) 4 of these features using the estimated mean and covariance. Locations that have the Mahalanobis distances higher than the mean distance plus three times the standard deviation are classified as outliers. After out-lier detection, the 480 ? 480 locations in the image are divided into 177316 inliers, 2824 outliers and 50260 out-of-boundary points. The outlier detection process is illustrated in <ref type="figure" target="#fig_8">Figure 6</ref>. After out-lier detection, we further remove other types of noise, like sea clutter, by filtering out the pixels with value smaller than 71 and larger than 0. Two examples that compare the original radar echo sequence and the denoised sequence are included in the attached "denoising" folder.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Evaluation Protocol</head><p>We illustrate our evaluation protocol in Algorithm 1. We can choose the evaluation type to be 'offline' or 'online'. In the online setting, the model is able to store the previously seen sequences in a buffer and fine-tune the parameters using the sampled training batches from the buffer. For algorithms that are tested in the online setting in the paper, we sample the last 25 consecutive frames in the buffer to update the model if these frames are available. The buffer will be made empty once a new episode flag is received, which indicates that the newly observed 5-frame segment is not consecutive to the previous frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 Details of Optical Flow based Algorithms</head><p>For the ROVER algorithm, we use the same hyperparameters as <ref type="bibr" target="#b24">[25]</ref>. For the ROVER-nonlinear algorithm, we follow the implementation in <ref type="bibr" target="#b29">[30]</ref>. We first non-linearly transform the input frames and then calculate the optical flow based on the transformed frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5 Network Structures</head><p>We use the general structure for 2D and 3D CNNs illustrated in <ref type="figure" target="#fig_5">Figure 4a</ref>. The network configurations of the 2D CNN, 3D CNN, ConvGRU and TrajGRU models are described in <ref type="table" target="#tab_0">Table 13</ref>, 14, 15, 16. <ref type="table" target="#tab_0">Table 13</ref>: The details of the 2D CNN model. The two dimensions in kernel, stride, pad and other features represent for height and width. We set the base filter number c to 70. We derive the 2D model from the 3D model by multiplying the number of channels with the respective kernel size of the 3D model. The first 5 and last 20 channels respectively correspond to the in-and output frames.     <ref type="table" target="#tab_0">Table 11</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Comparison of the connection structures of convolutional RNN and trajectory RNN. Links with the same color share the same transition weights. (Best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>10 10 ,</head><label>10</label><figDesc>10 ? x &lt; 30 30, x ? 30 . Also, the masked pixels have weight 0. The resulting B-MSE and B-MAE scores are computed as B-MSE = n,i,j (x n,i,j ?x n,i,j ) 2 and B-MAE = 1 N N n=1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Illustration of the 2D CNN, 3D CNN and DFN models used in the paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Average rainfall intensity of different months in the HKO-7 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>( a )</head><label>a</label><figDesc>Mahalanobis distance of a random portion of 10000 in-lier locations and 157 out-lier locations. The threshold is chosen to be the mean distance plus three times the standard deviation. (Best viewed in color.) (b) Outlier locations that are excluded in learning and evaluation. The purple points are the out-of-boundary locations and the red points are the outliers. (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Illustration of the outlier detection process and the final outlier mask obtained in HKO-7 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of TrajGRU and the baseline models in the MovingMNIST++ dataset. 'Conv-K?-D?' refers to the ConvGRU with kernel size ? ? ? and dilation ? ? ?. 'Traj-L?' refers to the TrajGRU with ? links. We replace the output layer of the ConvGRU-K5-D1 model to get the DFN.</figDesc><table><row><cell></cell><cell cols="10">Conv-K3-D2 Conv-K5-D1 Conv-K7-D1 Traj-L5 Traj-L9 Traj-L13 TrajGRU-L17 DFN Conv2D Conv3D</cell></row><row><cell>#Parameters</cell><cell>2.84M</cell><cell>4.77M</cell><cell>8.01M</cell><cell>2.60M</cell><cell>3.42M</cell><cell>4.00M</cell><cell>4.77M</cell><cell cols="3">4.83M 29.06M 32.52M</cell></row><row><cell>Test MSE ?10 ?2</cell><cell>1.495</cell><cell>1.310</cell><cell>1.254</cell><cell>1.351</cell><cell>1.247</cell><cell>1.170</cell><cell>1.138</cell><cell>1.461</cell><cell>1.681</cell><cell>1.637</cell></row><row><cell>Standard Deviation ?10 ?2</cell><cell>0.003</cell><cell>0.004</cell><cell>0.006</cell><cell>0.020</cell><cell>0.015</cell><cell>0.022</cell><cell>0.019</cell><cell>0.002</cell><cell>0.001</cell><cell>0.002</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Rain rate statistics in the HKO-7 benchmark.</figDesc><table><row><cell>Rain Rate (mm/h)</cell><cell cols="2">Proportion (%) Rainfall Level</cell></row><row><cell>0 ? x &lt; 0.5</cell><cell>90.25</cell><cell>No / Hardly noticeable</cell></row><row><cell>0.5 ? x &lt; 2</cell><cell>4.38</cell><cell>Light</cell></row><row><cell>2 ? x &lt; 5</cell><cell>2.46</cell><cell>Light to moderate</cell></row><row><cell>5 ? x &lt; 10</cell><cell>1.35</cell><cell>Moderate</cell></row><row><cell>10 ? x &lt; 30</cell><cell>1.14</cell><cell>Moderate to heavy</cell></row><row><cell>30 ? x</cell><cell>0.42</cell><cell>Rainstorm warning</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>10 r ? 30</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Offline Setting</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Last Frame</cell><cell>0.4022</cell><cell>0.3266 0.2401 0.1574</cell><cell>0.0692</cell><cell>0.5207</cell><cell>0.4531 0.3582 0.2512</cell><cell>0.1193</cell><cell>15274</cell><cell>28042</cell></row><row><cell>ROVER + Linear</cell><cell>0.4762</cell><cell>0.4089 0.3151 0.2146</cell><cell>0.1067</cell><cell>0.6038</cell><cell>0.5473 0.4516 0.3301</cell><cell>0.1762</cell><cell>11651</cell><cell>23437</cell></row><row><cell cols="2">ROVER + Non-linear 0.4655</cell><cell>0.4074 0.3226 0.2164</cell><cell>0.0951</cell><cell>0.5896</cell><cell>0.5436 0.4590 0.3318</cell><cell>0.1576</cell><cell>10945</cell><cell>22857</cell></row><row><cell>2D CNN</cell><cell>0.5095</cell><cell>0.4396 0.3406 0.2392</cell><cell>0.1093</cell><cell>0.6366</cell><cell>0.5809 0.4851 0.3690</cell><cell>0.1885</cell><cell>7332</cell><cell>18091</cell></row><row><cell>3D CNN</cell><cell>0.5109</cell><cell>0.4411 0.3415 0.2424</cell><cell>0.1185</cell><cell>0.6334</cell><cell>0.5825 0.4862 0.3734</cell><cell>0.2034</cell><cell>7202</cell><cell>17593</cell></row><row><cell>ConvGRU-nobal</cell><cell>0.5476</cell><cell>0.4661 0.3526 0.2138</cell><cell>0.0712</cell><cell>0.6756</cell><cell>0.6094 0.4981 0.3286</cell><cell>0.1160</cell><cell>9087</cell><cell>19642</cell></row><row><cell>ConvGRU</cell><cell>0.5489</cell><cell>0.4731 0.3720 0.2789</cell><cell>0.1776</cell><cell>0.6701</cell><cell>0.6104 0.5163 0.4159</cell><cell>0.2893</cell><cell>5951</cell><cell>15000</cell></row><row><cell>TrajGRU</cell><cell>0.5528</cell><cell>0.4759 0.3751 0.2835</cell><cell>0.1856</cell><cell>0.6731</cell><cell>0.6126 0.5192 0.4207</cell><cell>0.2996</cell><cell>5816</cell><cell>14675</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Online Setting</cell><cell></cell><cell></cell><cell></cell></row><row><cell>2D CNN</cell><cell>0.5112</cell><cell>0.4363 0.3364 0.2435</cell><cell>0.1263</cell><cell>0.6365</cell><cell>0.5756 0.4790 0.3744</cell><cell>0.2162</cell><cell>6654</cell><cell>17071</cell></row><row><cell>3D CNN</cell><cell>0.5106</cell><cell>0.4344 0.3345 0.2427</cell><cell>0.1299</cell><cell>0.6355</cell><cell>0.5736 0.4766 0.3733</cell><cell>0.2220</cell><cell>6690</cell><cell>16903</cell></row><row><cell>ConvGRU</cell><cell>0.5511</cell><cell>0.4737 0.3742 0.2843</cell><cell>0.1837</cell><cell>0.6712</cell><cell>0.6105 0.5183 0.4226</cell><cell>0.2981</cell><cell>5724</cell><cell>14772</cell></row><row><cell>TrajGRU</cell><cell>0.5563</cell><cell>0.4798 0.3808 0.2914</cell><cell>0.1933</cell><cell>0.6760</cell><cell>0.6164 0.5253 0.4308</cell><cell>0.3111</cell><cell>5589</cell><cell>14465</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Confidence intervals of selected deep models in the HKO-7 benchmark. We train 2D CNN, 3D CNN, ConvGRU and TrajGRU using three different random seeds and report the standard deviation of the test scores. ? 2 r ? 5 r ? 10 r ? 30 r ? 0.5 r ? 2 r ? 5 r ? 10 r ? 30</figDesc><table><row><cell>Algorithms</cell><cell cols="5">CSI r ? 0.5 r Offline Setting</cell><cell>HSS</cell><cell></cell><cell cols="2">B-MSE B-MAE</cell></row><row><cell>2D CNN</cell><cell>0.0032</cell><cell>0.0023 0.0015 0.0001</cell><cell>0.0025</cell><cell>0.0032</cell><cell cols="2">0.0025 0.0018 0.0003</cell><cell>0.0043</cell><cell>90</cell><cell>95</cell></row><row><cell>3D CNN</cell><cell>0.0043</cell><cell>0.0027 0.0016 0.0024</cell><cell>0.0024</cell><cell>0.0042</cell><cell cols="2">0.0028 0.0018 0.0031</cell><cell>0.0041</cell><cell>44</cell><cell>26</cell></row><row><cell>ConvGRU</cell><cell>0.0022</cell><cell>0.0018 0.0031 0.0008</cell><cell>0.0022</cell><cell>0.0022</cell><cell cols="2">0.0021 0.0040 0.0010</cell><cell>0.0038</cell><cell>52</cell><cell>81</cell></row><row><cell>TrajGRU</cell><cell>0.0020</cell><cell>0.0024 0.0025 0.0031</cell><cell>0.0031</cell><cell>0.0019</cell><cell cols="2">0.0024 0.0028 0.0039</cell><cell>0.0045</cell><cell>18</cell><cell>32</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Online Setting</cell><cell></cell><cell></cell><cell></cell></row><row><cell>2D CNN</cell><cell>0.0002</cell><cell>0.0005 0.0002 0.0002</cell><cell>0.0012</cell><cell>0.0002</cell><cell cols="2">0.0005 0.0002 0.0003</cell><cell>0.0019</cell><cell>12</cell><cell>12</cell></row><row><cell>3D CNN</cell><cell>0.0004</cell><cell>0.0003 0.0002 0.0003</cell><cell>0.0008</cell><cell>0.0004</cell><cell cols="2">0.0004 0.0003 0.0004</cell><cell>0.0001</cell><cell>23</cell><cell>27</cell></row><row><cell>ConvGRU</cell><cell>0.0006</cell><cell>0.0012 0.0017 0.0019</cell><cell>0.0024</cell><cell>0.0006</cell><cell cols="2">0.0012 0.0019 0.0023</cell><cell>0.0031</cell><cell>30</cell><cell>69</cell></row><row><cell>TrajGRU</cell><cell>0.0008</cell><cell>0.0004 0.0002 0.0002</cell><cell>0.0002</cell><cell>0.0007</cell><cell cols="2">0.0004 0.0002 0.0002</cell><cell>0.0003</cell><cell>10</cell><cell>20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Kendall's ? coefficients between skill scores. Higher absolute value indicates stronger correlation. The numbers with the largest absolute values are shown in bold face. ? 2 r ? 5 r ? 10 r ? 30 r ? 0.5 r ? 2 r ? 5 r ? 10 r ? 30</figDesc><table><row><cell cols="3">CSI -0.39 -0.39 -0.07 r ? 0.5 r MSE Skill Scores -0.24</cell><cell>-0.01</cell><cell>-0.33</cell><cell>HSS -0.42 -0.39 -0.06</cell><cell>0.01</cell></row><row><cell>MAE</cell><cell>-0.41</cell><cell>-0.57 -0.55 -0.25</cell><cell>-0.27</cell><cell>-0.50</cell><cell>-0.60 -0.55 -0.24</cell><cell>-0.26</cell></row><row><cell>B-MSE</cell><cell>-0.70</cell><cell>-0.57 -0.61 -0.86</cell><cell>-0.84</cell><cell>-0.62</cell><cell>-0.55 -0.61 -0.86</cell><cell>-0.84</cell></row><row><cell>B-MAE</cell><cell>-0.74</cell><cell>-0.59 -0.58 -0.82</cell><cell>-0.92</cell><cell>-0.67</cell><cell>-0.57 -0.59 -0.83</cell><cell>-0.92</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Hyperparameters of the MovingMNIST++ dataset. We choose the velocity, scaling factor, rotation angle and illumination factor uniformly within the range listed in the table.</figDesc><table><row><cell>Hyper-parameter</cell><cell>Value</cell></row><row><cell>Number of digits</cell><cell>3</cell></row><row><cell>Frame size</cell><cell>64 ? 64</cell></row><row><cell>Velocity</cell><cell>[0, 3.6)</cell></row><row><cell cols="2">Scaling factor Rotation angle Illumination factor [0.6, 1.0) [ 1 1.1 , 1.1) [ ?? 12 , ? 12 )</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>The details of the 3D CNN model. The three dimensions in kernel, stride, pad and other features represent for depth, height and width. We set the base filter number c to 128.</figDesc><table><row><cell>Name</cell><cell>Kernel</cell><cell>Stride</cell><cell>Pad</cell><cell>Ch I/O</cell><cell>In Res</cell><cell>Out Res</cell><cell>Type</cell><cell>Input</cell></row><row><cell>enc1</cell><cell cols="3">4 ? 4 ? 4 2 ? 2 ? 2 1 ? 1 ? 1</cell><cell>1/c</cell><cell cols="2">10 ? 64 ? 64 5 ? 32 ? 32</cell><cell>Conv</cell><cell>in</cell></row><row><cell>enc2</cell><cell cols="3">4 ? 4 ? 4 2 ? 2 ? 2 1 ? 1 ? 1</cell><cell>c/2c</cell><cell>5 ? 32 ? 32</cell><cell>2 ? 16 ? 16</cell><cell>Conv</cell><cell>enc1</cell></row><row><cell>enc3</cell><cell cols="4">4 ? 4 ? 4 2 ? 2 ? 2 1 ? 1 ? 1 2c/3c</cell><cell>2 ? 16 ? 16</cell><cell>1 ? 8 ? 8</cell><cell>Conv</cell><cell>enc2</cell></row><row><cell>enc4</cell><cell cols="4">4 ? 4 ? 4 2 ? 2 ? 2 2 ? 1 ? 1 3c/4c</cell><cell>1 ? 8 ? 8</cell><cell>1 ? 4 ? 4</cell><cell>Conv</cell><cell>enc3</cell></row><row><cell>vid1</cell><cell cols="4">2 ? 1 ? 1 1 ? 1 ? 1 0 ? 0 ? 0 4c/8c</cell><cell>1 ? 4 ? 4</cell><cell>2 ? 4 ? 4</cell><cell cols="2">Deconv enc4</cell></row><row><cell>vid2</cell><cell cols="4">4 ? 4 ? 4 2 ? 2 ? 2 1 ? 1 ? 1 8c/4c</cell><cell>2 ? 4 ? 4</cell><cell>4 ? 8 ? 8</cell><cell>Deconv</cell><cell>vid1</cell></row><row><cell>vid3</cell><cell cols="4">4 ? 4 ? 4 2 ? 2 ? 2 2 ? 1 ? 1 4c/2c</cell><cell>4 ? 8 ? 8</cell><cell cols="2">6 ? 16 ? 16 Deconv</cell><cell>vid2</cell></row><row><cell>vid4</cell><cell cols="3">4 ? 4 ? 4 2 ? 2 ? 2 2 ? 1 ? 1</cell><cell>2c/c</cell><cell cols="3">6 ? 16 ? 16 10 ? 32 ? 32 Deconv</cell><cell>vid3</cell></row><row><cell>vid5</cell><cell cols="3">3 ? 4 ? 4 1 ? 2 ? 2 1 ? 1 ? 1</cell><cell>c/1</cell><cell cols="3">10 ? 32 ? 32 10 ? 64 ? 64 Deconv</cell><cell>vid4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>The details of the ConvGRU model. The 'In Kernel', 'In Stride' and 'In Pad' are the kernel, stride and padding in the input-to-state convolution. 'State Ker.' and 'State Dila.' are the kernel size and dilation size of the state-to-state convolution. We set k and d as stated in the paper. The 'In State' is the initial state of the RNN layer.</figDesc><table><row><cell>Name</cell><cell cols="6">In Kernel In Stride In Pad State Ker. State Dila. Ch I/O</cell><cell>In Res</cell><cell>Out Res</cell><cell>Type</cell><cell cols="2">In In State</cell></row><row><cell>econv1</cell><cell>3 ? 3</cell><cell>1 ? 1</cell><cell>1 ? 1</cell><cell>-</cell><cell>-</cell><cell>4/16</cell><cell cols="2">64 ? 64 64 ? 64</cell><cell>Conv</cell><cell>in</cell><cell>-</cell></row><row><cell>ernn1</cell><cell>3 ? 3</cell><cell>1 ? 1</cell><cell>1 ? 1</cell><cell>k ? k</cell><cell>d ? d</cell><cell cols="5">16/64 64 ? 64 64 ? 64 ConvGRU econv1</cell><cell>-</cell></row><row><cell>edown1</cell><cell>3 ? 3</cell><cell>2 ? 2</cell><cell>1 ? 1</cell><cell>-</cell><cell>-</cell><cell cols="3">64/64 64 ? 64 32 ? 32</cell><cell>Conv</cell><cell>ernn1</cell><cell>-</cell></row><row><cell>ernn2</cell><cell>3 ? 3</cell><cell>1 ? 1</cell><cell>1 ? 1</cell><cell>k ? k</cell><cell>d ? d</cell><cell cols="5">64/96 32 ? 32 32 ? 32 ConvGRU edown1</cell><cell>-</cell></row><row><cell>edown2</cell><cell>3 ? 3</cell><cell>2 ? 2</cell><cell>1 ? 1</cell><cell>-</cell><cell>-</cell><cell cols="3">96/96 32 ? 32 16 ? 16</cell><cell>Conv</cell><cell>ernn2</cell><cell>-</cell></row><row><cell>ernn3</cell><cell>3 ? 3</cell><cell>1 ? 1</cell><cell>1 ? 1</cell><cell>k ? k</cell><cell>d ? d</cell><cell cols="5">96/96 16 ? 16 16 ? 16 ConvGRU edown2</cell><cell>-</cell></row><row><cell>frnn1</cell><cell>3 ? 3</cell><cell>1 ? 1</cell><cell>1 ? 1</cell><cell>k ? k</cell><cell>d ? d</cell><cell cols="4">96/96 16 ? 16 16 ? 16 ConvGRU</cell><cell>-</cell><cell>ernn3</cell></row><row><cell>fup1</cell><cell>4 ? 4</cell><cell>2 ? 2</cell><cell>1 ? 1</cell><cell>-</cell><cell>-</cell><cell cols="3">96/96 16 ? 16 32 ? 32</cell><cell>Deconv</cell><cell>frnn1</cell><cell>-</cell></row><row><cell>frnn2</cell><cell>3 ? 3</cell><cell>1 ? 1</cell><cell>1 ? 1</cell><cell>k ? k</cell><cell>d ? d</cell><cell cols="4">96/96 32 ? 32 32 ? 32 ConvGRU</cell><cell>fup1</cell><cell>ernn2</cell></row><row><cell>fup2</cell><cell>4 ? 4</cell><cell>2 ? 2</cell><cell>1 ? 1</cell><cell>-</cell><cell>-</cell><cell cols="3">96/96 32 ? 32 64 ? 64</cell><cell>Deconv</cell><cell>frnn2</cell><cell>-</cell></row><row><cell>frnn3</cell><cell>3 ? 3</cell><cell>1 ? 1</cell><cell>1 ? 1</cell><cell>k ? k</cell><cell>d ? d</cell><cell cols="4">96/64 64 ? 64 64 ? 64 ConvGRU</cell><cell>fup2</cell><cell>ernn1</cell></row><row><cell>fconv4</cell><cell>3 ? 3</cell><cell>1 ? 1</cell><cell>1 ? 1</cell><cell>-</cell><cell>-</cell><cell cols="3">64/16 64 ? 64 64 ? 64</cell><cell>Conv</cell><cell>frnn3</cell><cell>-</cell></row><row><cell>fconv5</cell><cell>1 ? 1</cell><cell>1 ? 1</cell><cell>0 ? 0</cell><cell>-</cell><cell>-</cell><cell>16/1</cell><cell cols="2">64 ? 64 64 ? 64</cell><cell>Conv</cell><cell>fconv4</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>The details of the DFN model. The output of the 'fconv4' layer will be used to transform the previous prediction or the last input frame. All hyperparameters have the same meaning as inTable 9.</figDesc><table><row><cell>Name</cell><cell cols="6">In Kernel In Stride In Pad State Ker. State Dila. Ch I/O</cell><cell>In Res</cell><cell>Out Res</cell><cell>Type</cell><cell cols="2">In In State</cell></row><row><cell>econv1</cell><cell>3 ? 3</cell><cell>1 ? 1</cell><cell>1 ? 1</cell><cell>-</cell><cell>-</cell><cell>4/16</cell><cell cols="2">64 ? 64 64 ? 64</cell><cell>Conv</cell><cell>in</cell><cell>-</cell></row><row><cell>ernn1</cell><cell>3 ? 3</cell><cell>1 ? 1</cell><cell>1 ? 1</cell><cell>k ? k</cell><cell>d ? d</cell><cell cols="5">16/64 64 ? 64 64 ? 64 ConvGRU econv1</cell><cell>-</cell></row><row><cell>edown1</cell><cell>3 ? 3</cell><cell>2 ? 2</cell><cell>1 ? 1</cell><cell>-</cell><cell>-</cell><cell cols="3">64/64 64 ? 64 32 ? 32</cell><cell>Conv</cell><cell>ernn1</cell><cell>-</cell></row><row><cell>ernn2</cell><cell>3 ? 3</cell><cell>1 ? 1</cell><cell>1 ? 1</cell><cell>k ? k</cell><cell>d ? d</cell><cell cols="5">64/96 32 ? 32 32 ? 32 ConvGRU edown1</cell><cell>-</cell></row><row><cell>edown2</cell><cell>3 ? 3</cell><cell>2 ? 2</cell><cell>1 ? 1</cell><cell>-</cell><cell>-</cell><cell cols="3">96/96 32 ? 32 16 ? 16</cell><cell>Conv</cell><cell>ernn2</cell><cell>-</cell></row><row><cell>ernn3</cell><cell>3 ? 3</cell><cell>1 ? 1</cell><cell>1 ? 1</cell><cell>k ? k</cell><cell>d ? d</cell><cell cols="5">96/96 16 ? 16 16 ? 16 ConvGRU edown2</cell><cell>-</cell></row><row><cell>frnn1</cell><cell>3 ? 3</cell><cell>1 ? 1</cell><cell>1 ? 1</cell><cell>k ? k</cell><cell>d ? d</cell><cell cols="4">96/96 16 ? 16 16 ? 16 ConvGRU</cell><cell>-</cell><cell>ernn3</cell></row><row><cell>fup1</cell><cell>4 ? 4</cell><cell>2 ? 2</cell><cell>1 ? 1</cell><cell>-</cell><cell>-</cell><cell cols="3">96/96 16 ? 16 32 ? 32</cell><cell>Deconv</cell><cell>frnn1</cell><cell>-</cell></row><row><cell>frnn2</cell><cell>3 ? 3</cell><cell>1 ? 1</cell><cell>1 ? 1</cell><cell>k ? k</cell><cell>d ? d</cell><cell cols="4">96/96 32 ? 32 32 ? 32 ConvGRU</cell><cell>fup1</cell><cell>ernn2</cell></row><row><cell>fup2</cell><cell>4 ? 4</cell><cell>2 ? 2</cell><cell>1 ? 1</cell><cell>-</cell><cell>-</cell><cell cols="3">96/96 32 ? 32 64 ? 64</cell><cell>Deconv</cell><cell>frnn2</cell><cell>-</cell></row><row><cell>frnn3</cell><cell>3 ? 3</cell><cell>1 ? 1</cell><cell>1 ? 1</cell><cell>k ? k</cell><cell>d ? d</cell><cell cols="4">96/64 64 ? 64 64 ? 64 ConvGRU</cell><cell>fup2</cell><cell>ernn1</cell></row><row><cell>fconv4</cell><cell>3 ? 3</cell><cell>1 ? 1</cell><cell>1 ? 1</cell><cell>-</cell><cell>-</cell><cell cols="3">64/121 64 ? 64 64 ? 64</cell><cell>Conv</cell><cell>frnn3</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>The details of the TrajGRU model. 'L' is the number of links in the state-to-state transition. We set l as stated in the paper. All other hyperparameters have the same meaning as inTable 9.</figDesc><table><row><cell>Name</cell><cell cols="5">In Kernel In Stride In Pad L Ch I/O</cell><cell>In Res</cell><cell>Out Res</cell><cell>Type</cell><cell cols="2">In In State</cell></row><row><cell>econv1</cell><cell>3 ? 3</cell><cell>1 ? 1</cell><cell>1 ? 1</cell><cell>-</cell><cell>4/16</cell><cell cols="2">64 ? 64 64 ? 64</cell><cell>Conv</cell><cell>in</cell><cell>-</cell></row><row><cell>ernn1</cell><cell>3 ? 3</cell><cell>1 ? 1</cell><cell>1 ? 1</cell><cell cols="6">l 16/64 64 ? 64 64 ? 64 TrajGRU econv1</cell><cell>-</cell></row><row><cell>edown1</cell><cell>3 ? 3</cell><cell>2 ? 2</cell><cell>1 ? 1</cell><cell cols="4">-64/64 64 ? 64 32 ? 32</cell><cell>Conv</cell><cell>ernn1</cell><cell>-</cell></row><row><cell>ernn2</cell><cell>3 ? 3</cell><cell>1 ? 1</cell><cell>1 ? 1</cell><cell cols="6">l 64/96 32 ? 32 32 ? 32 TrajGRU edown1</cell><cell>-</cell></row><row><cell>edown2</cell><cell>3 ? 3</cell><cell>2 ? 2</cell><cell>1 ? 1</cell><cell cols="4">-96/96 32 ? 32 16 ? 16</cell><cell>Conv</cell><cell>ernn2</cell><cell>-</cell></row><row><cell>ernn3</cell><cell>3 ? 3</cell><cell>1 ? 1</cell><cell>1 ? 1</cell><cell cols="6">l 96/96 16 ? 16 16 ? 16 TrajGRU edown2</cell><cell>-</cell></row><row><cell>frnn1</cell><cell>3 ? 3</cell><cell>1 ? 1</cell><cell>1 ? 1</cell><cell cols="5">l 96/96 16 ? 16 16 ? 16 TrajGRU</cell><cell>-</cell><cell>ernn3</cell></row><row><cell>fup1</cell><cell>4 ? 4</cell><cell>2 ? 2</cell><cell>1 ? 1</cell><cell cols="4">-96/96 16 ? 16 32 ? 32</cell><cell>Deconv</cell><cell>frnn1</cell><cell>-</cell></row><row><cell>frnn2</cell><cell>3 ? 3</cell><cell>1 ? 1</cell><cell>1 ? 1</cell><cell cols="5">l 96/96 32 ? 32 32 ? 32 TrajGRU</cell><cell>fup1</cell><cell>ernn2</cell></row><row><cell>fup2</cell><cell>4 ? 4</cell><cell>2 ? 2</cell><cell>1 ? 1</cell><cell cols="4">-96/96 32 ? 32 64 ? 64</cell><cell>Deconv</cell><cell>frnn2</cell><cell>-</cell></row><row><cell>frnn3</cell><cell>3 ? 3</cell><cell>1 ? 1</cell><cell>1 ? 1</cell><cell cols="5">l 96/64 64 ? 64 64 ? 64 TrajGRU</cell><cell>fup2</cell><cell>ernn1</cell></row><row><cell>fconv4</cell><cell>3 ? 3</cell><cell>1 ? 1</cell><cell>1 ? 1</cell><cell cols="4">-64/16 64 ? 64 64 ? 64</cell><cell>Conv</cell><cell>frnn3</cell><cell>-</cell></row><row><cell>fconv5</cell><cell>1 ? 1</cell><cell>1 ? 1</cell><cell>0 ? 0</cell><cell>-</cell><cell>16/1</cell><cell cols="2">64 ? 64 64 ? 64</cell><cell>Conv</cell><cell>fconv4</cell><cell>-</cell></row><row><cell cols="7">D Details about the HKO-7 Benchmark</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">D.1 Overall Data Statistics</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 12 :</head><label>12</label><figDesc>Overall statistics of the HKO-7 dataset.</figDesc><table><row><cell></cell><cell>Train</cell><cell>Validate</cell><cell>Test</cell></row><row><cell>Years</cell><cell cols="2">2009-2014 2009-2014</cell><cell>2015</cell></row><row><cell>#Days</cell><cell>812</cell><cell>50</cell><cell>131</cell></row><row><cell>#Frames</cell><cell>192,168</cell><cell>11,736</cell><cell>31,350</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 14 :</head><label>14</label><figDesc>The details of the 3D CNN model. The three dimensions in kernel, stride, pad and other features represent for channel, height and width. We set the base filter number c to 128.</figDesc><table><row><cell>Name</cell><cell>Kernel</cell><cell>Stride</cell><cell>Pad</cell><cell>Ch I/O</cell><cell>In Res</cell><cell>Out Res</cell><cell>Type</cell><cell>Input</cell></row><row><cell>enc0</cell><cell cols="3">1 ? 7 ? 7 1 ? 5 ? 5 0 ? 1 ? 1</cell><cell>1/c</cell><cell>5 ? 480 ? 480</cell><cell>5 ? 96 ? 96</cell><cell>Conv</cell><cell>in</cell></row><row><cell>enc1</cell><cell cols="3">1 ? 4 ? 4 1 ? 3 ? 3 0 ? 1 ? 1</cell><cell>c/c</cell><cell>5 ? 96 ? 96</cell><cell>5 ? 32 ? 32</cell><cell>Conv</cell><cell>enc0</cell></row><row><cell>enc2</cell><cell cols="3">4 ? 4 ? 4 2 ? 2 ? 2 1 ? 1 ? 1</cell><cell>c/2c</cell><cell>5 ? 32 ? 32</cell><cell>2 ? 16 ? 16</cell><cell>Conv</cell><cell>enc1</cell></row><row><cell>enc3</cell><cell cols="3">4 ? 4 ? 4 2 ? 2 ? 2 1 ? 1 ? 1</cell><cell>2c/3c</cell><cell>2 ? 16 ? 16</cell><cell>1 ? 8 ? 8</cell><cell>Conv</cell><cell>enc2</cell></row><row><cell>enc4</cell><cell cols="3">4 ? 4 ? 4 2 ? 2 ? 2 2 ? 1 ? 1</cell><cell>3c/4c</cell><cell>1 ? 8 ? 8</cell><cell>1 ? 4 ? 4</cell><cell>Conv</cell><cell>enc3</cell></row><row><cell>vid1</cell><cell cols="4">2 ? 1 ? 1 1 ? 1 ? 1 0 ? 0 ? 0. 4c/8c</cell><cell>1 ? 4 ? 4</cell><cell>2 ? 4 ? 4</cell><cell cols="2">Deconv enc4</cell></row><row><cell>vid2</cell><cell cols="4">4 ? 4 ? 4 2 ? 2 ? 2 1 ? 1 ? 1. 8c/4c</cell><cell>2 ? 4 ? 4</cell><cell>4 ? 8 ? 8</cell><cell>Deconv</cell><cell>vid1</cell></row><row><cell>vid3</cell><cell cols="4">4 ? 4 ? 4 2 ? 2 ? 2 0 ? 1 ? 1. 4c/2c</cell><cell>4 ? 8 ? 8</cell><cell>10 ? 16 ? 16</cell><cell>Deconv</cell><cell>vid2</cell></row><row><cell>vid4</cell><cell cols="3">4 ? 4 ? 4 2 ? 2 ? 2 1 ? 1 ? 1.</cell><cell>2c/c</cell><cell>10 ? 16 ? 16</cell><cell>20 ? 32 ? 32</cell><cell>Deconv</cell><cell>vid3</cell></row><row><cell>vid5</cell><cell cols="3">3 ? 5 ? 5 1 ? 3 ? 3 1 ? 1 ? 1.</cell><cell>c/8</cell><cell>20 ? 32 ? 32</cell><cell>20 ? 96 ? 96</cell><cell>Deconv</cell><cell>vid4</cell></row><row><cell>vid6</cell><cell cols="3">3 ? 7 ? 7 1 ? 5 ? 5 1 ? 1 ? 1.</cell><cell>8/8</cell><cell>20 ? 96 ? 96</cell><cell cols="2">20 ? 480 ? 480 Deconv</cell><cell>vid5</cell></row><row><cell>vid7</cell><cell cols="3">3 ? 3 ? 3 1 ? 1 ? 1 1 ? 1 ? 1.</cell><cell>8/1</cell><cell cols="3">20 ? 480 ? 480 20 ? 480 ? 480 Deconv</cell><cell>vid6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 15 :</head><label>15</label><figDesc>The details of the ConvGRU model. All hyperparameters have the same meaning as inTable 9.</figDesc><table><row><cell>Name</cell><cell cols="5">In Kernel In Stride In Pad State Ker. State Dila.</cell><cell>Ch I/O</cell><cell>In Res</cell><cell>Out Res</cell><cell>Type</cell><cell cols="2">In In State</cell></row><row><cell>econv1</cell><cell>7 ? 7</cell><cell>5 ? 5</cell><cell>1 ? 1</cell><cell>-</cell><cell>-</cell><cell>4/8</cell><cell>480 ? 480</cell><cell>96 ? 96</cell><cell>Conv</cell><cell>in</cell><cell>-</cell></row><row><cell>ernn1</cell><cell>3 ? 3</cell><cell>1 ? 1</cell><cell>1 ? 1</cell><cell>5 ? 5</cell><cell>1 ? 1</cell><cell>8/64</cell><cell>96 ? 96</cell><cell>96 ? 96</cell><cell>ConvGRU</cell><cell>econv1</cell><cell>-</cell></row><row><cell>edown1</cell><cell>5 ? 5</cell><cell>3 ? 3</cell><cell>1 ? 1</cell><cell>-</cell><cell>-</cell><cell>64/64</cell><cell>96 ? 96</cell><cell>32 ? 32</cell><cell>Conv</cell><cell>ernn1</cell><cell>-</cell></row><row><cell>ernn2</cell><cell>3 ? 3</cell><cell>1 ? 1</cell><cell>1 ? 1</cell><cell>5 ? 5</cell><cell>1 ? 1</cell><cell>64/192</cell><cell>32 ? 32</cell><cell>32 ? 32</cell><cell>ConvGRU</cell><cell>edown1</cell><cell>-</cell></row><row><cell>edown2</cell><cell>3 ? 3</cell><cell>2 ? 2</cell><cell>1 ? 1</cell><cell>-</cell><cell>-</cell><cell>192/192</cell><cell>32 ? 32</cell><cell>16 ? 16</cell><cell>Conv</cell><cell>ernn2</cell><cell>-</cell></row><row><cell>ernn3</cell><cell>3 ? 3</cell><cell>1 ? 1</cell><cell>1 ? 1</cell><cell>3 ? 3</cell><cell>1 ? 1</cell><cell>192/192</cell><cell>16 ? 16</cell><cell>16 ? 16</cell><cell>ConvGRU</cell><cell>edown2</cell><cell>-</cell></row><row><cell>frnn1</cell><cell>3 ? 3</cell><cell>1 ? 1</cell><cell>1 ? 1</cell><cell>3 ? 3</cell><cell>1 ? 1</cell><cell>192/192</cell><cell>16 ? 16</cell><cell>16 ? 16</cell><cell>ConvGRU</cell><cell>-</cell><cell>ernn3</cell></row><row><cell>fup1</cell><cell>4 ? 4</cell><cell>2 ? 2</cell><cell>1 ? 1</cell><cell>-</cell><cell>-</cell><cell>192/192</cell><cell>16 ? 16</cell><cell>32 ? 32</cell><cell>Deconv</cell><cell>frnn1</cell><cell>-</cell></row><row><cell>frnn2</cell><cell>3 ? 3</cell><cell>1 ? 1</cell><cell>1 ? 1</cell><cell>5 ? 5</cell><cell>1 ? 1</cell><cell>192/192</cell><cell>32 ? 32</cell><cell>32 ? 32</cell><cell>ConvGRU</cell><cell>fup1</cell><cell>ernn2</cell></row><row><cell>fup2</cell><cell>5 ? 5</cell><cell>3 ? 3</cell><cell>1 ? 1</cell><cell>-</cell><cell>-</cell><cell>192/192</cell><cell>32 ? 32</cell><cell>96 ? 96</cell><cell>Deconv</cell><cell>frnn2</cell><cell>-</cell></row><row><cell>frnn3</cell><cell>3 ? 3</cell><cell>1 ? 1</cell><cell>1 ? 1</cell><cell>5 ? 5</cell><cell>1 ? 1</cell><cell>192/64</cell><cell>96 ? 96</cell><cell>96 ? 96</cell><cell>ConvGRU</cell><cell>fup2</cell><cell>ernn1</cell></row><row><cell>fdeconv4</cell><cell>7 ? 7</cell><cell>5 ? 5</cell><cell>1 ? 1</cell><cell>-</cell><cell>-</cell><cell>64/8</cell><cell>96 ? 96</cell><cell>480 ? 480</cell><cell>Deconv</cell><cell>frnn3</cell><cell>-</cell></row><row><cell>fconv5</cell><cell>1 ? 1</cell><cell>1 ? 1</cell><cell>0 ? 0</cell><cell>-</cell><cell>-</cell><cell>8/1</cell><cell cols="2">480 ? 480 480 ? 480</cell><cell>Conv</cell><cell>fdeconv4</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 16 :</head><label>16</label><figDesc>The details of the TrajGRU model. All hyperparameters have the same meaning as in</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The MSE for the MovingMNIST++ experiment is averaged by both the frame size and the length of the predicted sequence.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">MNIST dataset:http://yann.lecun.com/exdb/mnist/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We use Moore-Penrose pseudoinverse in the implementation.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research has been supported by General Research Fund 16207316 from the Research Grants Council and Innovation and Technology Fund ITS/205/15FP from the Innovation and Technology Commission in Hong Kong. The first author has also been supported by the Hong Kong PhD Fellowship.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Algorithm 1 Evaluation protocol in the HKO-7 benchmark 1: procedure HKO7TEST(model, type) <ref type="bibr">2:</ref> env ? GETENV(type) <ref type="bibr">3:</ref> while not env.end() do 4: </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Social LSTM: Human trajectory prediction in crowded spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kratarth</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Delving deeper into convolutional networks for learning video representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Bert De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised learning for physical interaction through video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Equitability revisited: Why the &quot;equitable threat score&quot; is not equitable</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Robin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">T</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David B</forename><surname>Jolliffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stephenson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Weather and Forecasting</publisher>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="710" to="726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Structural-RNN: Deep learning on spatio-temporal graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashesh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Active convolution: Learning the shape of convolution for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunho</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A new measure of rank correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Maurice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kendall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1/2</biblScope>
			<biblScope unit="page" from="81" to="93" />
			<date type="published" when="1938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ensemble classification for anomalous propagation echo detection with clustering-based subset-selection method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hansoo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungshin</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Atmosphere</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<title level="m">Interpretable structure-evolving LSTM. In CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Awni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The distribution of raindrops with size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Meteorology</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="165" to="166" />
			<date type="published" when="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Video (language) modeling: a baseline for generative models of natural videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcaurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6604</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Convolutional LSTM network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhourong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai-Kin</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chun</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Use of NWP for nowcasting convective precipitation: Recent progress and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzhen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isztar</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zawadzki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeanette</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Onvlee-Hooimeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Joe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping-Wah</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Golding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin of the American Meteorological Society</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="409" to="426" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Decomposing motion and content for natural video sequence prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Generating videos with scene dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai-Kin</forename><surname>Wong</surname></persName>
		</author>
		<title level="m">Operational application of optical flow techniques to radar-based rainfall nowcasting. Atmosphere</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">48</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Online object tracking: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwoo</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
