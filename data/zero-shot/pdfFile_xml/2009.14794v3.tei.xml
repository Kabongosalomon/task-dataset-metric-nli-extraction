<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2021 RETHINKING ATTENTION WITH PERFORMERS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerii</forename><surname>Likhosherstov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyou</forename><surname>Song</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreea</forename><surname>Gane</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamas</forename><surname>Sarlos</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Davis</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afroz</forename><surname>Mohiuddin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Colwell</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Weller</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Alan Turing Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2021 RETHINKING ATTENTION WITH PERFORMERS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attentionkernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can also be used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers. * Equal contribution. Correspondence to {kchoro,lcolwell}@google.com. Code for Transformer models on protein data can be found in github.com/google-research/ google-research/tree/master/protein_lm and Performer code can be found in github.com/ google-research/google-research/tree/master/performer. Google AI Blog: https:// ai.googleblog.com/2020/10/rethinking-attention-with-performers.html</p><p>Published as a conference paper at ICLR 2021 layers <ref type="bibr" target="#b6">(Child et al., 2019)</ref>. Unfortunately, there is a lack of rigorous guarantees for the representation power produced by such methods, and sometimes the validity of sparsity patterns can only be verified empirically through trial and error by constructing special GPU operations (e.g. either writing C++ CUDA kernels <ref type="bibr" target="#b6">(Child et al., 2019)</ref> or using TVMs <ref type="bibr" target="#b1">(Beltagy et al., 2020)</ref>). Other techniques which aim to reduce Transformers' space complexity include reversible residual layers allowing one-time activation storage in training <ref type="bibr" target="#b27">(Kitaev et al., 2020)</ref> and shared attention weights <ref type="bibr" target="#b54">(Xiao et al., 2019)</ref>. These constraints may impede application to long-sequence problems, where approximations of the attention mechanism are not sufficient. Approximations based on truncated back-propagation  are also unable to capture long-distance correlations since the gradients are only propagated inside a localized window. Other methods propose biased estimation of regular attention but only in the non-causal setting and with large mean squared error .</p><p>In response, we introduce the first Transformer architectures, Performers, capable of provably accurate and practical estimation of regular (softmax) full-rank attention, but of only linear space and time complexity and not relying on any priors such as sparsity or low-rankness. Performers use the Fast Attention Via positive Orthogonal Random features (FAVOR+) mechanism, leveraging new methods for approximating softmax and Gaussian kernels, which we propose. We believe these methods are of independent interest, contributing to the theory of scalable kernel methods. Consequently, Performers are the first linear architectures fully compatible (via small amounts of fine-tuning) with regular Transformers, providing strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and lower variance of the approximation.</p><p>FAVOR+ can be also applied to efficiently model other kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, that are beyond the reach of regular Transformers, and find for them optimal attention-kernels. FAVOR+ can also be applied beyond the Transformer scope as a more scalable replacement for regular attention, which itself has a wide variety of uses in computer vision <ref type="bibr" target="#b22">(Fu et al., 2019</ref>), reinforcement learning (Zambaldi et al., 2019, training with softmax cross entropy loss, and even combinatorial optimization <ref type="bibr" target="#b52">(Vinyals et al., 2015)</ref>.</p><p>We test Performers on a rich set of tasks ranging from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing the effectiveness of the novel attention-learning paradigm leveraged by Performers. We emphasize that in principle, FAVOR+ can also be combined with other techniques, such as reversible layers <ref type="bibr" target="#b27">(Kitaev et al., 2020)</ref> or cluster-based attention (Roy et al., 2020).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">FAVOR+ MECHANISM &amp; POSITIVE ORTHOGONAL RANDOM FEATURES</head><p>Below we describe in detail the FAVOR+ mechanism -the backbone of the Performer s architecture. We introduce a new method for estimating softmax (and Gaussian) kernels with positive orthogonal random features which FAVOR+ leverages for the robust and unbiased estimation of regular (softmax) attention and show how FAVOR+ can be applied for other attention-kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">PRELIMINARIES -REGULAR ATTENTION MECHANISM</head><p>Let L be the size of an input sequence of tokens. Then regular dot-product attention <ref type="bibr" target="#b47">(Vaswani et al., 2017)</ref> is a mapping which accepts matrices Q, K, V ? R L?d as input where d is the hidden dimension (dimension of the latent representation). Matrices Q, K, V are intermediate representations of the input and their rows can be interpreted as queries, keys and values of the continuous dictionary data structure respectively. Bidirectional (or non-directional (Devlin et al., 2018)) dot-product attention has the following form, where A ? R L?L is the so-called attention matrix:</p><p>(1) Here exp(?) is applied elementwise, 1 L is the all-ones vector of length L, and diag(?) is a diagonal matrix with the input vector as the diagonal. Time and space complexity of computing (1) are O(L 2 d) and O(L 2 + Ld) respectively, because A has to be stored explicitly. Hence, in principle, dot-product attention of type (1) is incompatible with end-to-end processing of long sequences. Bidirectional attention is applied in encoder self-attention and encoder-decoder attention in Seq2Seq architectures.</p><p>Another important type of attention is unidirectional dot-product attention which has the form:</p><p>Att ? (Q, K, V) = D ?1 AV, A = tril(A), D = diag( A1 L ),</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION AND RELATED WORK</head><p>Transformers <ref type="bibr" target="#b47">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b17">Dehghani et al., 2019)</ref> are powerful neural network architectures that have become SOTA in several areas of machine learning including natural language processing (NLP) (e.g. speech recognition <ref type="bibr" target="#b32">(Luo et al., 2020)</ref>), neural machine translation (NMT) <ref type="bibr" target="#b5">(Chen et al., 2018)</ref>, document generation/summarization, time series prediction, generative modeling (e.g. image generation ), music generation <ref type="bibr" target="#b24">(Huang et al., 2019)</ref>, and bioinformatics <ref type="bibr">(Rives et al., 2019;</ref><ref type="bibr" target="#b25">Ingraham et al., 2019;</ref><ref type="bibr" target="#b20">Elnaggar et al., 2019;</ref><ref type="bibr" target="#b19">Du et al., 2020)</ref>.</p><p>Transformers rely on a trainable attention mechanism that identifies complex dependencies between the elements of each input sequence. Unfortunately, the regular Transformer scales quadratically with the number of tokens L in the input sequence, which is prohibitively expensive for large L and precludes its usage in settings with limited computational resources even for moderate values of L. Several solutions have been proposed to address this issue <ref type="bibr" target="#b1">(Beltagy et al., 2020;</ref><ref type="bibr" target="#b23">Gulati et al., 2020;</ref><ref type="bibr" target="#b2">Chan et al., 2020;</ref><ref type="bibr" target="#b6">Child et al., 2019;</ref><ref type="bibr" target="#b0">Bello et al., 2019)</ref>. Most approaches restrict the attention mechanism to attend to local neighborhoods  or incorporate structural priors on attention such as sparsity <ref type="bibr" target="#b6">(Child et al., 2019)</ref>, pooling-based compression <ref type="bibr" target="#b37">(Rae et al., 2020)</ref> clustering/binning/convolution techniques (e.g. <ref type="bibr" target="#b41">(Roy et al., 2020)</ref> which applies k-means clustering to learn dynamic sparse attention regions, or <ref type="bibr" target="#b27">(Kitaev et al., 2020)</ref>, where locality sensitive hashing is used to group together tokens of similar embeddings), sliding windows <ref type="bibr" target="#b1">(Beltagy et al., 2020)</ref>, or truncated targeting <ref type="bibr" target="#b4">(Chelba et al., 2020)</ref>. There is also a long line of research on using dense attention matrices, but defined by low-rank kernels substituting softmax <ref type="bibr" target="#b26">(Katharopoulos et al., 2020;</ref><ref type="bibr" target="#b42">Shen et al., 2018)</ref>. Those methods critically rely on kernels admitting explicit representations as dot-products of finite positive-feature vectors.</p><p>The approaches above do not aim to approximate regular attention, but rather propose simpler and more tractable attention mechanisms, often by incorporating additional constraints (e.g. identical query and key sets as in <ref type="bibr" target="#b27">(Kitaev et al., 2020)</ref>), or by trading regular with sparse attention using more where tril(?) returns the lower-triangular part of the argument matrix including the diagonal. As discussed in <ref type="bibr" target="#b47">(Vaswani et al., 2017)</ref>, unidirectional attention is used for autoregressive generative modelling, e.g. as self-attention in generative Transformers as well as the decoder part of Seq2Seq Transformers.</p><p>We will show that attention matrix A can be approximated up to any precision in time O(Ld 2 log(d)). For comparison, popular methods leveraging sparsity via Locality-Sensitive Hashing (LSH) techniques <ref type="bibr" target="#b27">(Kitaev et al., 2020)</ref> have O(Ld 2 log L) time complexity. In the main body of the paper we will describe FAVOR+ for bidirectional attention. Completely analogous results can be obtained for the unidirectional variant via the mechanism of prefix-sums (all details in the Appendix B.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">GENERALIZED KERNELIZABLE ATTENTION</head><p>FAVOR+ works for attention blocks using matrices A ? R L?L of the form A(i, j) = K(q i , k j ), with q i /k j standing for the i th /j th query/key row-vector in Q/K and kernel K : R d ? R d ? R + defined for the (usually randomized) mapping: ? : R d ? R r + (for some r &gt; 0) as:</p><formula xml:id="formula_0">K(x, y) = E[?(x) ?(y)].</formula><p>( <ref type="formula">3)</ref> We call ?(u) a random feature map for u ? R d . For Q , K ? R L?r with rows given as ?(q i ) and ?(k i ) respectively, Equation 3 leads directly to the efficient attention mechanism of the form:</p><p>Att ? (Q, K, V) = D ?1 (Q ((K ) V)), D = diag(Q ((K ) 1 L )).</p><p>Here Att ? stands for the approximate attention and brackets indicate the order of computations. It is easy to see that such a mechanism is characterized by space complexity O(Lr + Ld + rd) and time complexity O(Lrd) as opposed to O(L 2 + Ld) and O(L 2 d) of the regular attention (see also <ref type="figure">Fig. 1</ref>).</p><p>Figure 1: Approximation of the regular attention mechanism AV (before D ?1 -renormalization) via (random) feature maps. Dashed-blocks indicate order of computation with corresponding time complexities attached.</p><p>The above scheme constitutes the FA-part of the FAVOR+ mechanism. The remaining OR+ part answers the following questions: (1) How expressive is the attention model defined in Equation 3, and in particular, can we use it in principle to approximate regular softmax attention ? (2) How do we implement it robustly in practice, and in particular, can we choose r L for L d to obtain desired space and time complexity gains? We answer these questions in the next sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">HOW TO AND HOW NOT TO APPROXIMATE SOFTMAX-KERNELS FOR ATTENTION</head><p>It turns out that by taking ? of the following form for functions f 1 , ..., f l : R ? R, function g : R d ? R and deterministic vectors ? i or ? 1 , ..., ? m iid ? D for some distribution D ? P(R d ):</p><formula xml:id="formula_2">?(x) = h(x) ? m (f 1 (? 1 x), .</formula><p>.., f 1 (? m x), ..., f l (? 1 x), ..., f l (? m x)),</p><p>we can model most kernels used in practice. Furthermore, in most cases D is isotropic (i.e. with pdf function constant on a sphere), usually Gaussian. For example, by taking h(x) = 1, l = 1 and D = N (0, I d ) we obtain estimators of the so-called PNG-kernels <ref type="bibr" target="#b11">(Choromanski et al., 2017</ref>) (e.g. f 1 = sgn corresponds to the angular kernel). Configurations: h(x) = 1, l = 2, f 1 = sin, f 2 = cos correspond to shift-invariant kernels, in particular D = N (0, I d ) leads to the Gaussian kernel K gauss <ref type="bibr" target="#b38">(Rahimi &amp; Recht, 2007)</ref>. The softmax-kernel which defines regular attention matrix A is given as:</p><formula xml:id="formula_4">SM(x, y) def = exp(x y).<label>(6)</label></formula><p>In the above, without loss of generality, we omit ? d-renormalization since we can equivalently renormalize input keys and queries. Since: SM(x, y) = exp( x 2 2 )K gauss (x, y) exp( y 2 2 ), based on what we have said, we obtain random feature map unbiased approximation of SM(x, y) using trigonometric functions with: h(x) = exp( x 2 2 ), l = 2, f 1 = sin, f 2 = cos. We call it SM trig m (x, y). There is however a caveat there. The attention module from (1) constructs for each token, a convex combination of value-vectors with coefficients given as corresponding renormalized kernel scores. That is why kernels producing non-negative scores are used. Applying random feature maps with potentially negative dimension-values (sin / cos) leads to unstable behaviours, especially when kernel scores close to 0 (which is the case for many entries of A corresponding to low relevance tokens) are approximated by estimators with large variance in such regions. This results in abnormal behaviours, e.g. negative-diagonal-values renormalizers D ?1 , and consequently either completely prevents training or leads to sub-optimal models. We demonstrate empirically that this is what happens for SM trig m and provide detailed theoretical explanations showing that the variance of SM trig m is large as approximated values tend to 0 (see: Section 3). This is one of the main reasons why the robust random feature map mechanism for approximating regular softmax attention was never proposed.</p><p>We propose a robust mechanism in this paper. Furthermore, the variance of our new unbiased positive random feature map estimator tends to 0 as approximated values tend to 0 (see: Section 3). Lemma 1 (Positive Random Features (PRFs) for Softmax). For x, y ? R d , z = x + y we have:</p><formula xml:id="formula_5">SM(x, y) = E ??N (0,I d ) exp ? x? x 2 2 exp ? y? y 2 2 = ?E ??N (0,I d ) cosh(? z),<label>(7)</label></formula><p>where ? = exp(? x 2 + y 2 2 ) and cosh is hyperbolic cosine. Consequently, softmax-kernel admits a positive random feature map unbiased approximation with h(</p><formula xml:id="formula_6">x) = exp(? x 2 2 ), l = 1, f 1 = exp and D = N (0, I d ) or: h(x) = 1 ? 2 exp(? x 2</formula><p>2 ), l = 2, f 1 (u) = exp(u), f 2 (u) = exp(?u) and the same D (the latter for further variance reduction). We call related estimators: SM  Larger values indicate regions of (?, l)-space with better performance of positive random features. We see that for critical regions with ? large enough (small enough softmax-kernel values) our method is arbitrarily more accurate than trigonometric random features. Plot presented for domain [??, ?] ? [?2, 2]. Right: The slice of function r for fixed l = 1 and varying angle ?. Right Upper Corner: Comparison of the MSEs of both the estimators in a low softmax-kernel value region.</p><p>In <ref type="figure" target="#fig_1">Fig. 2</ref> we visualize the advantages of positive versus standard trigonometric random features. In critical regions, where kernel values are small and need careful approximation, our method outperforms its counterpart. In Section 4 we further confirm our method's advantages empirically, using positive features to efficiently train softmax-based linear Transformers. If we replace in (7) ? with ? d ? ? , we obtain the so-called regularized softmax-kernel SMREG which we can approximate in a similar manner, simply changing D = N (0, I d ) to D = Unif( ? dS d?1 ), a distribution corresponding to Haar measure on the sphere of radius ? d in R d , obtaining estimator SMREG + m . As we show in Section 3, such random features can also be used to accurately approximate regular softmax-kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">ORTHOGONAL RANDOM FEATURES (ORFS)</head><p>The above constitutes the R+ part of the FAVOR+ method. It remains to explain the O-part. To further reduce the variance of the estimator (so that we can use an even smaller number of random features r), we entangle different random samples ? 1 , ..., ? m to be exactly orthogonal. This can be done while maintaining unbiasedness whenever isotropic distributions D are used (i.e. in particular in all kernels we considered so far) by the standard Gram-Schmidt orthogonalization procedure (see <ref type="bibr" target="#b11">(Choromanski et al., 2017)</ref> for details). ORFs is a well-known method, yet it turns out that it works particularly well with our introduced PRFs for softmax. This leads to the first theoretical results showing that ORFs can be applied to reduce the variance of softmax/Gaussian kernel estimators for any dimensionality d rather than just asymptotically for large enough d (as is the case for previous methods, see: next section) and leads to the first exponentially small bounds on large deviations probabilities that are strictly smaller than for non-orthogonal methods. Positivity of random features plays a key role in these bounds. The ORF mechanism requires m ? d, but this will be the case in all our experiments. The pseudocode of the entire FAVOR+ algorithm is given in Appendix B.</p><p>Our theoretical results are tightly aligned with experiments. We show in Section 4 that PRFs+ORFs drastically improve accuracy of the approximation of the attention matrix and enable us to reduce r which results in an accurate as well as space and time efficient mechanism which we call FAVOR+.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THEORETICAL RESULTS</head><p>We present here the theory of positive orthogonal random features for softmax-kernel estimation. All these results can be applied also to the Gaussian kernel, since as explained in the previous section, one can be obtained from the other by renormalization (see: Section 2.3). All proofs and additional more general theoretical results with a discussion are given in the Appendix. Lemma 2 (positive (hyperbolic) versus trigonometric random features). The following is true:</p><formula xml:id="formula_7">MSE( SM trig m (x, y)) = 1 2m exp( x + y 2 )SM ?2 (x, y)(1 ? exp(? x ? y 2 )) 2 , MSE( SM + m (x, y)) = 1 m exp( x + y 2 )SM 2 (x, y)(1 ? exp(? x + y 2 )), MSE( SM hyp+ m (x, y)) = 1 2 (1 ? exp(? x + y 2 ))MSE( SM + m (x, y)),<label>(8)</label></formula><p>for independent random samples ? i , and where MSE stands for the mean squared error.</p><p>Thus, for SM(x, y) ? 0 we have: MSE( SM trig m (x, y)) ? ? and MSE( SM + m (x, y)) ? 0. Furthermore, the hyperbolic estimator provides additional accuracy improvements that are strictly better than those from SM + 2m (x, y) with twice as many random features. The next result shows that the regularized softmax-kernel is in practice an accurate proxy of the softmax-kernel in attention. Theorem 1 (regularized versus softmax-kernel). Assume that the L ? -norm of the attention matrix for the softmax-kernel satisfies: A ? ? C for some constant C ? 1. Denote by A reg the corresponding attention matrix for the regularized softmax-kernel. The following holds:</p><formula xml:id="formula_8">inf i,j A reg (i, j) A(i, j) ? 1 ? 2 d 1 3 + o 1 d 1 3 , and sup i,j A reg (i, j) A(i, j) ? 1.<label>(9)</label></formula><p>Furthermore, the latter holds for d ? 2 even if the L ? -norm condition is not satisfied, i.e. the regularized softmax-kernel is a universal lower bound for the softmax-kernel.</p><p>Consequently, positive random features for SMREG can be used to approximate the softmax-kernel. Our next result shows that orthogonality provably reduces mean squared error of the estimation with positive random features for any dimensionality d &gt; 0 and we explicitly provide the gap.</p><p>Theorem 2. If SM ort+ m (x, y) stands for the modification of SM + m (x, y) with orthogonal random features (and thus for m ? d), then the following holds for any d &gt; 0:</p><formula xml:id="formula_9">MSE( SM ort+ m (x, y)) ? MSE( SM + m (x, y)) ? 1 ? 1 m 2 d + 2 SM 2 (x, y).<label>(10)</label></formula><p>Furthermore, completely analogous result holds for the regularized softmax-kernel SMREG.</p><p>For the regularized softmax-kernel, orthogonal features provide additional concentration results -the first exponentially small bounds for probabilities of estimators' tails that are strictly better than for non-orthogonal variants for every d &gt; 0. Our next result enables us to explicitly estimate the gap. Theorem 3. Let x, y ? R d . The following holds for any a &gt; SMREG(x, y) and m ? d:</p><formula xml:id="formula_10">P[ SMREG + m (x, y) &gt; a] ? exp(?mL X (a)), P[ SMREG ort+ m (x, y) &gt; a] ? d d + 2 exp(?mL X (a))</formula><p>where SMREG ort+ m (x, y) stands for the modification of SMREG</p><formula xml:id="formula_11">+ m (x, y) with ORFs, X = ? exp( ? d ? ? 2 (x + y)), ? ? N (0, I d )</formula><p>, ? is as in Lemma 1 and L Z is a Legendre Transform of Z defined as: L Z (a) = sup ?&gt;0 log( e ?a M Z (?) ) for the moment generating function M Z of Z.</p><p>We see that ORFs provide exponentially small and sharper bounds for critical regions where the softmax-kernel is small. Below we show that even for the SM trig mechanism with ORFs, it suffices to take m = ?(d log(d)) random projections to accurately approximate the attention matrix (thus if not attention renormalization, PRFs would not be needed). In general, m depends on the dimensionality d of the embeddings, radius R of the ball where all queries/keys live and precision parameter (see: Appendix F.6 for additional discussion), but does not depend on input sequence length L. Theorem 4 (uniform convergence for attention approximation). Assume that L 2 -norms of queries/keys are upper-bounded by R &gt; 0. Define l = Rd ? 1 4 and take h * = exp( l 2 2 ). Then for any &gt; 0, ? = (h * ) 2 and the number of random projections m = ?( d ? 2 log( 4d 3 4 R ? )) the following holds for the attention approximation mechanism leveraging estimators SM trig with ORFs: A ? A ? ? with any constant probability, where A approximates the attention matrix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We implemented our setup on top of pre-existing Transformer training code in Jax <ref type="bibr" target="#b21">(Frostig et al., 2018)</ref> optimized with just-in-time (jax.jit) compilation, and complement our theory with empirical evidence to demonstrate the practicality of FAVOR+ in multiple settings. Unless explicitly stated, a Performer replaces only the attention component with our method, while all other components are exactly the same as for the regular Transformer. For shorthand notation, we denote unidirectional/causal modelling as (U) and bidirectional/masked language modelling as (B).</p><p>In terms of baselines, we use other Transformer models for comparison, although some of them are restricted to only one case -e.g. Reformer <ref type="bibr" target="#b27">(Kitaev et al., 2020)</ref> is only (U), and Linformer  is only (B). Furthermore, we use PG-19 <ref type="bibr" target="#b37">(Rae et al., 2020)</ref> as an alternative (B) pretraining benchmark, as it is made for long-length sequence training compared to the (now publicly unavailable) BookCorpus <ref type="bibr" target="#b59">(Zhu et al., 2015)</ref> + Wikipedia dataset used in BERT <ref type="bibr" target="#b18">(Devlin et al., 2018)</ref> and Linformer. All model and tokenization hyperparameters are shown in Appendix A. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">COMPUTATIONAL COSTS</head><p>We compared speed-wise the backward pass of the Transformer and the Performer in (B) setting, as it is one of the main computational bottlenecks during training, when using the regular default size (n heads , n layers , d f f , d) = <ref type="bibr">(8,</ref><ref type="bibr">6,</ref><ref type="bibr">2048,</ref><ref type="bibr">512)</ref>, where d f f denotes the width of the MLP layers. We observed ( <ref type="figure" target="#fig_2">Fig. 3</ref>) that in terms of L, the Performer reaches nearly linear time and sub-quadratic memory consumption (since the explicit O(L 2 ) attention matrix is not stored). In fact, the Performer achieves nearly optimal speedup and memory efficiency possible, depicted by the "X"-line when attention is replaced with the "identity function" simply returning the V-matrix. The combination of both memory and backward pass efficiencies for large L allows respectively, large batch training and lower wall clock time per gradient step. Extensive additional results are demonstrated in Appendix E by varying layers, raw attention, and architecture sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">SOFTMAX ATTENTION APPROXIMATION ERROR</head><p>We further examined the approximation error via FAVOR+ in <ref type="figure" target="#fig_3">Fig. 4</ref>. We demonstrate that 1. Orthogonal features produce lower error than unstructured (IID) features, 2. Positive features produce lower error than trigonometric sin/cos features. These two empirically validate the PORF mechanism. To further improve overall approximation of attention blocks across multiple iterations which further improves training, random samples should be periodically redrawn ( <ref type="figure">Fig. 5, right</ref>). This is a cheap procedure, but can be further optimized (Appendix B.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">SOFTMAX APPROXIMATION ON TRANSFORMERS</head><p>Even if the approximation of the attention mechanism is tight, small errors can easily propagate throughout multiple Transformer layers (e.g. MLPs, multiple heads), as we show in <ref type="bibr">Fig. 14 (Appendix)</ref>. In other words, the model's Lipschitz constant can easily scale up small attention approximation error, which means that very tight approximations may sometimes be needed. Thus, when applying FAVOR(+)'s softmax approximations on a Transformer model (i.e. "Performer-X-SOFTMAX"), we demonstrate that:</p><p>1. Backwards compatibility with pretrained models is available as a benefit from softmax approximation, via small finetuning (required due to error propagation) even for trigonometric features <ref type="figure">(Fig. 5</ref>, left) on the LM1B dataset <ref type="bibr" target="#b3">(Chelba et al., 2014)</ref>. However, when on larger dataset PG-19, 2. Positive (POS) softmax features (with redrawing) become crucial for achieving performance matching regular Transformers <ref type="figure">(Fig. 5</ref>, right).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 5:</head><p>We transferred the original pretrained Transformer's weights into the Performer, which produces an initial non-zero 0.07 accuracy (dotted orange line), but quickly recovers accuracy in a small fraction of the original number of gradient steps. However on PG-19, Trigonometric (TRIG) softmax approximation becomes highly unstable (full curve in Appendix D.2), while positive features (POS) (without redrawing) and Linformer (which also approximates softmax) even with redrawn projections, plateau at the same perplexity. Positive softmax with feature redrawing is necessary to match the Transformer, with SMREG (regularization from Sec. 3) allowing faster convergence. Additional ablation studies over many attention kernels, showing also that trigonometric random features lead even to NaN values in training are given in Appendix D.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">MULTIPLE LAYER TRAINING FOR PROTEINS</head><p>We further benchmark the Performer on both (U) and (B) cases by training a 36-layer model using protein sequences from the Jan. 2019 release of TrEMBL <ref type="bibr" target="#b14">(Consortium, 2019)</ref>, similar to . In <ref type="figure" target="#fig_4">Fig. 6</ref>, the Reformer and Linformer significantly drop in accuracy on the protein dataset. Furthermore, the usefulness of generalized attention is evidenced by Performer-RELU (taking f = ReLU in Equation 5) achieving the highest accuracy in both (U) and (B) cases. Our proposed softmax approximation is also shown to be tight, achieving the same accuracy as the exact-softmax Transformer and confirming our theoretical claims from Section 3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">LARGE LENGTH TRAINING -COMMON DATASETS</head><p>On the standard (U) ImageNet64 benchmark from  with L = 12288 which is unfeasible for regular Transformers, we set all models to use the same (n heads , d f f , d) but varying n layers . Performer/6-layers matches the Reformer/12-layers, while the Performer/12-layers matches the Reformer/24-layers ( <ref type="figure" target="#fig_5">Fig. 7</ref>: left). Depending on hardware (TPU or GPU), we also found that the Performer can be 2x faster than the Reformer via Jax optimizations for the (U) setting.</p><p>For a proof of principle study, we also create an initial protein benchmark for predicting interactions among groups of proteins by concatenating protein sequences to length L = 8192 from TrEMBL, long enough to model protein interaction networks without the large sequence alignments required by existing methods <ref type="bibr" target="#b13">(Cong et al., 2019)</ref>. In this setting, a regular Transformer overloads memory even at a batch size of 1 per chip, by a wide margin. Thus as a baseline, we were forced to use a significantly smaller variant, reducing to (n heads , n layers , d f f , d) = (8, {1, 2, 3}, 256, 256). Meanwhile, the Performer trains efficiently at a batch size of 8 per chip using the standard (8, 6, 2048, 512) architecture. We see in <ref type="figure" target="#fig_5">Fig. 7</ref> (right subfigure) that the smaller Transformer (n layer = 3) is quickly bounded at ? 19%, while the Performer is able to train continuously to ? 24%.  <ref type="bibr">(8,</ref><ref type="bibr">2048,</ref><ref type="bibr">512)</ref>. We further show that our positive softmax approximation achieves the same performance as ReLU in Appendix D.2. For concatenated TrEMBL, we varied n layers ? {1, 2, 3} for the smaller Transformer. Hyperparameters can be found in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We presented Performer, a new type of Transformer, relying on our Fast Attention Via positive Orthogonal Random features (FAVOR+) mechanism to significantly improve space and time complexity of regular Transformers. Our mechanism provides to our knowledge the first effective unbiased estimation of the original softmax-based Transformer with linear space and time complexity and opens new avenues in the research on Transformers and the role of non-sparsifying attention mechanisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">BROADER IMPACT</head><p>We believe that the presented algorithm can be impactful in various ways:</p><p>Biology and Medicine: Our method has the potential to directly impact research on biological sequence analysis by enabling the Transformer to be applied to much longer sequences without constraints on the structure of the attention matrix. The initial application that we consider is the prediction of interactions between proteins on the proteome scale. Recently published approaches require large evolutionary sequence alignments, a bottleneck for applications to mammalian genomes <ref type="bibr" target="#b13">(Cong et al., 2019)</ref>. The potentially broad translational impact of applying these approaches to biological sequences was one of the main motivations of this work. We believe that modern bioinformatics can immensely benefit from new machine learning techniques with Transformers being among the most promising. Scaling up these methods to train faster more accurate language models opens the door to the ability to design sets of molecules with pre-specified interaction properties. These approaches could be used to augment existing physics-based design strategies that are of critical importance for example in the development of new nanoparticle vaccines <ref type="bibr" target="#b34">(Marcandalli et al., 2019)</ref>.</p><p>Environment: As we have shown, Performers with FAVOR+ are characterized by much lower compute costs and substantially lower space complexity which can be directly translated to CO 2 emission reduction <ref type="bibr" target="#b43">(Strubell et al., 2019)</ref> and lower energy consumption <ref type="bibr" target="#b56">(You et al., 2020)</ref>, as regular Transformers require very large computational resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Research on Transformers:</head><p>We believe that our results can shape research on efficient Transformers architectures, guiding the field towards methods with strong mathematical foundations. Our research may also hopefully extend Transformers also beyond their standard scope (e.g. by considering the Generalized Attention mechanism and connections with kernels). Exploring scalable Transformer architectures that can handle L of the order of magnitude few thousands and more, preserving accuracy of the baseline at the same time, is a gateway to new breakthroughs in bio-informatics, e.g. language modeling for proteins, as we explained in the paper. Our presented method can be potentially a first step.</p><p>Backward Compatibility: Our Performer can be used on the top of a regular pre-trained Transformer as opposed to other Transformer variants. Even if up-training is not required, FAVOR+ can still be used for fast inference with no loss of accuracy. We think about this backward compatibility as a very important additional feature of the presented techniques that might be particularly attractive for practitioners.</p><p>Attention Beyond Transformers: Finally, FAVOR+ can be applied to approximate exact attention also outside the scope of Transformers. This opens a large volume of new potential applications including: hierarchical attention networks (HANS) <ref type="bibr" target="#b55">(Yang et al., 2016)</ref>, graph attention networks <ref type="bibr" target="#b48">(Velickovic et al., 2018)</ref>, image processing <ref type="bibr" target="#b22">(Fu et al., 2019)</ref>, and reinforcement learning/robotics <ref type="bibr" target="#b44">(Tang et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX: RETHINKING ATTENTION WITH PERFORMERS A HYPERPARAMETERS FOR EXPERIMENTS</head><p>This optimal setting (including comparisons to approximate softmax) we use for the Performer is specified in the Generalized Attention (Subsec. A.4), and unless specifically mentioned (e.g. using name "Performer-SOFTMAX"), "Performer" refers to using this generalized attention setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 METRICS</head><p>We report the following evaluation metrics:</p><p>1. Accuracy: For unidirectional models, we measure the accuracy on next-token prediction, averaged across all sequence positions in the dataset. For bidirectional models, we mask each token with 15% probability (same as <ref type="bibr" target="#b18">(Devlin et al., 2018)</ref>) and measure accuracy across the masked positions.</p><p>2. Perplexity: For unidirectional models, we measure perplexity across all sequence positions in the dataset. For bidirectional models, similar to the accuracy case, we measure perplexity across the masked positions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Bits Per Dimension/Character (BPD/BPC):</head><p>This calculated by loss divided by ln <ref type="formula">(2)</ref>.</p><p>We used the full evaluation dataset for TrEMBL in the plots in the main section, while for other datasets such as ImageNet64 and PG-19 which have very large evaluation dataset sizes, we used random batches (&gt;2048 samples) for plotting curves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.1 PG-19 PREPROCESSING</head><p>The PG-19 dataset <ref type="bibr" target="#b37">(Rae et al., 2020)</ref> is presented as a challenging long range text modeling task. It consists of out-of-copyright Project Gutenberg books published before 1919. It does not have a fixed vocabulary size, instead opting for any tokenization which can model an arbitrary string of text. We use a unigram SentencePiece vocabulary <ref type="bibr" target="#b29">(Kudo &amp; Richardson, 2018)</ref> with 32768 tokens, which maintains whitespace and is completely invertible to the original book text. Perplexities are calculated as the average log-likelihood per token, multiplied by the ratio of the sentencepiece tokenization to number of tokens in the original dataset. The original dataset token count per split is: train=1973136207, validation=3007061, test=6966499. Our sentencepiece tokenization yields the following token counts per split: train=3084760726, valid=4656945, and test=10699704. This gives log likelihood multipliers of train=1.5634, valid=1.5487, test=1.5359 per split before computing perplexity, which is equal to exp(log likelihood multiplier * loss).</p><p>Preprocessing for TrEMBL is extensively explained in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 TRAINING HYPERPARAMETERS</head><p>Unless specifically stated, all Performer + Transformer runs by default used 0.5 grad clip, 0.1 weight decay, 0.1 dropout, 10 ?3 fixed learning rate with Adam hyperparameters (? 1 = 0.9, ? 2 = 0.98, = 10 ?9 ), with batch size maximized (until TPU memory overload) for a specific model.</p><p>All 36-layer protein experiments used the same amount of compute (i.e. 16x16 TPU-v2, 8GB per chip). For concatenated experiments, 16x16 TPU-v2's were also used for the Performer, while 8x8's were used for the 1-3 layer (d = 256) Transformer models (using 16x16 did not make a difference in accuracy).</p><p>Note that Performers are using the same training hyperparameters as Transformers, yet achieving competitive results -this shows that FAVOR can act as a simple drop-in without needing much tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 APPROXIMATE SOFTMAX ATTENTION DEFAULT VALUES</head><p>The optimal values, set to default parameters 1 , are: renormalize_attention = True, numerical stabilizer = 10 ?6 , number of features = 256, ortho_features = True, ortho_scaling = 0.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 GENERALIZED ATTENTION DEFAULT VALUES</head><p>The optimal values, set to default parameters 2 , are: renormalize_attention = True, numerical stabilizer = 0.0, number of features = 256, kernel = ReLU, kernel_epsilon = 10 ?3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 REFORMER DEFAULT VALUES</head><p>For the Reformer, we used the same hyperparameters as mentioned for protein experiments, without gradient clipping, while using the defaults 3 (which instead use learning rate decay) for ImageNet-64. In both cases, the Reformer used the same default LSH attention parameters.</p><p>A.6 LINFORMER DEFAULT VALUES Using our standard pipeline as mentioned above, we replaced the attention function with the Linformer variant via Jax, with ? = 10 ?6 , k = 600 (same notation used in the paper ), where ? is the exponent in a renormalization procedure using e ?? as a multiplier in order to approximate softmax, while k is the dimension of the projections of the Q and K matrices. As a sanity check, we found that our Linformer implementation in Jax correctly approximated exact softmax's output within 0.02 error for all entries.</p><p>Note that for rigorous comparisons, our Linformer hyperparameters are even stronger than the defaults found in , as:</p><p>? We use k = 600, which is more than twice than the default k = 256 from the paper, and also twice than our default m = 256 number of features.</p><p>? We also use redrawing, which avoids "unlucky" projections on Q and K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B MAIN ALGORITHM: FAVOR+</head><p>We outline the main algorithm for FAVOR+ formally:</p><p>Algorithm 1: FAVOR+ (bidirectional or unidirectional).</p><formula xml:id="formula_12">Input : Q, K, V ? R L?d , isBidirectional -binary flag. Result: Att ? (Q, K, V) ? R L?L if isBidirectional, Att ? (Q, K, V) ? R L?L otherwise.</formula><p>Compute Q and K as described in Section 2.2 and Section 2.3 and take C :</p><formula xml:id="formula_13">= [V 1 L ]; if isBidirectional then Buf 1 := (K ) C ? R M ?(d+1) , Buf 2 := Q Buf 1 ? R L?(d+1) ; else</formula><p>Compute G and its prefix-sum tensor G PS according to <ref type="formula">(11)</ref>;</p><formula xml:id="formula_14">Buf 2 := G PS 1,:,: Q 1 . . . G PS L,:,: Q L ? R L?(d+1) ; end [Buf 3 buf 4 ] := Buf 2 , Buf 3 ? R L?d , buf 4 ? R L ; return diag(buf 4 ) ?1 Buf 3 ;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 UNIDIRECTIONAL CASE AND PREFIX SUMS</head><p>We explain how our analysis from Section 2.2 can be extended to the unidirectional mechanism in this section. Notice that this time attention matrix A is masked, i.e. all its entries not in the lower-triangular part (which contains the diagonal) are zeroed (see also <ref type="figure">Fig. 8</ref>).</p><p>Figure 8: Visual representation of the prefix-sum algorithm for unidirectional attention. For clarity, we omit attention normalization in this visualization. The algorithm keeps the prefix-sum which is a matrix obtained by summing the outer products of random features corresponding to keys with value-vectors. At each given iteration of the prefix-sum algorithm, a random feature vector corresponding to a query is multiplied by the most recent prefix-sum (obtained by summing all outer-products corresponding to preceding tokens) to obtain a new row of the matrix AV which is output by the attention mechanism.</p><p>For the unidirectional case, our analysis is similar as for the bidirectional case, but this time our goal is to compute tril(Q (K ) )C without constructing and storing the L ? L-sized matrix tril(Q (K ) ) explicitly, where C = [V 1 L ] ? R L?(d+1) . In order to do so, observe that ?1 ? i ? L:</p><formula xml:id="formula_15">[tril(Q (K ) )C] i = G PS i,:,: ? Q i , G PS i,:,: = i j=1 G j,:,: , G j,:,: = K j C j ? R M ?(d+1) (11)</formula><p>where G, G PS ? R L?M ?(d+1) are 3d-tensors. Each slice G PS :,l,p is therefore a result of a prefix-sum (or cumulative-sum) operation applied to G :,l,p : G PS i,l,p = i j=1 G i,l,p . An efficient algorithm to compute the prefix-sum of L elements takes O(L) total steps and O(log L) time when computed in parallel <ref type="bibr" target="#b30">(Ladner &amp; Fischer, 1980;</ref><ref type="bibr" target="#b15">Cormen et al., 2009)</ref>. See Algorithm 1 for the whole approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 ORTHOGONAL RANDOM FEATURES -EXTENSIONS</head><p>As mentioned in the main text, for isotropic ? (true for most practical applications, including regular attention), instead of sampling ? i independently, we can use orthogonal random features (ORF) <ref type="bibr" target="#b57">(Yu et al., 2016;</ref><ref type="bibr" target="#b11">Choromanski et al., 2017;</ref><ref type="bibr" target="#b8">2018b)</ref>: these maintain the marginal distributions of samples ? i while enforcing that different samples are orthogonal. If we need m &gt; d, ORFs still can be used locally within each d ? d block of W <ref type="bibr" target="#b57">(Yu et al., 2016)</ref>.</p><p>ORFs were introduced to reduce the variance of Monte Carlo estimators <ref type="bibr" target="#b57">(Yu et al., 2016;</ref><ref type="bibr" target="#b11">Choromanski et al., 2017;</ref><ref type="bibr" target="#b8">2018b;</ref><ref type="bibr" target="#b9">2019a;</ref><ref type="bibr" target="#b7">Choromanski et al., 2018a;</ref><ref type="bibr" target="#b10">2019b)</ref> and we showed in the theoretical and experimental sections from the main body that they do indeed lead to more accurate approximations and substantially better downstream results. There exist several variants of the ORF-mechanism and in the main body we discussed only the base one (that we refer to here as regular). Below we briefly review the most efficient ORF mechanisms (based on their strengths and costs) to present the most complete picture.</p><p>(1) Regular ORFs [R-ORFs]: Applies Gaussian orthogonal matrices <ref type="bibr" target="#b57">(Yu et al., 2016)</ref>. Encodes matrix W of ?-samples (with different rows corresponding to different samples) in O(md) space. Provides algorithm for computing Wx in O(md) time for any x ? R d . Gives unbiased estimation. Requires one-time O(md 2 ) preprocessing (Gram-Schmidt orthogonalization).</p><p>(2) Hadamard/Givens ORFs [H/G-ORFs]: Applies random Hadamard <ref type="bibr" target="#b11">(Choromanski et al., 2017)</ref> or Givens matrices <ref type="bibr" target="#b10">(Choromanski et al., 2019b)</ref>. Encodes matrix W in O(m) or O(m log(d)) space. Provides algorithm for computing Wx in O(m log(d)) time for any x ? R d . Gives small bias (tending to 0 with d ? ?).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 TIME AND SPACE COMPLEXITY -DETAILED ANALYSIS</head><p>We see that a variant of bidirectional FAVOR+ using iid samples or R-ORFs has O(md + Ld + mL) space complexity as opposed to ?(L 2 + Ld) space complexity of the baseline. Unidirectional FAVOR+ using fast prefix-sum pre-computation in parallel <ref type="bibr" target="#b30">(Ladner &amp; Fischer, 1980;</ref><ref type="bibr" target="#b15">Cormen et al., 2009)</ref> has O(mLd) space complexity to store G PS which can be reduced to O(md + Ld + mL) by running a simple (though non-parallel in L) aggregation of G PS i,:,: without storing the whole tensor G PS in memory. From Subsec. B.2, we know that if instead we use G-ORFs, then space complexity is reduced to O(m log(d) + Ld + mL) and if the H-ORFs mechanism is used, then space is further reduced to O(m+Ld+mL) = O(Ld+mL). Thus for m, d L all our variants provide substantial space complexity improvements since they do not need to store the attention matrix explicitly.</p><p>The time complexity of Algorithm 1 is O(Lmd) (note that constructing Q and K can be done in time O(Lmd)). Note that the time complexity of our method is much lower than O(L 2 d) of the baseline for L m.</p><p>As explained in Subsec. B.2, the R-ORF mechanism incurs an extra one-time O(md 2 ) cost (negligible compared to the O(Lmd) term for L d). H-ORFs or G-ORFs do not have this cost, and when FAVOR+ uses them, computing Q and K can be conducted in time O(L log(m)d) as opposed to O(Lmd) (see: Subsec. B.2). Thus even though H/G-ORFs do not change the asymptotic time complexity, they improve the constant factor from the leading term. This might play an important role in training very large models.</p><p>The number of random features m allows a trade-off between computational complexity and the level of approximation: bigger m results in higher computation costs, but also in a lower variance of the estimate of A. In the theoretical section from the main body we showed that in practice we can take M = ?(d log(d)).</p><p>Observe that the FAVOR+ algorithm is highly-parallelizable, and benefits from fast matrix multiplication and broadcasted operations on GPUs or TPUs.  <ref type="table">Table 1</ref>: Statistics for the TrEMBL single sequence and the long sequence task. We used the TrEMBL dataset 4 , which contains 139,394,261 sequences of which 106,030,080 are unique. While the training dataset appears smaller than the one used in Madani et al. , we argue that it includes most of the relevant sequences. Specifically, the TrEMBL dataset consists of the subset of UniProtKB sequences that have been computationally analyzed but not manually curated, and accounts for ? 99.5% of the total number of sequences in the UniProtKB dataset 5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C EXPERIMENTAL DETAILS FOR PROTEIN MODELING TASKS</head><p>Following the methodology described in , we used both an OOD-Test set, where a selected subset of Pfam families are held-out for valuation, and an IID split, where the remaining protein sequences are split randomly into train, valid, and test tests. We held-out the following protein families (PF18369, PF04680, PF17988, PF12325, PF03272, PF03938, PF17724, PF10696, PF11968, PF04153, PF06173, PF12378, PF04420, PF10841, PF06917, PF03492, PF06905, PF15340, PF17055, PF05318), which resulted in 29,696 OOD sequences. We note that, due to deduplication and potential TrEMBL version mismatch, our OOD-Test set does not match exactly the one in . We also note that this OOD-Test selection methodology does not guarantee that the evaluation sequences are within a minimum distance from the sequences used during training. In future work, we will include rigorous distance based splits.</p><p>The statistics for the resulting dataset splits are reported in <ref type="table">Table 1</ref>. In the standard sequence modeling task, given the length statistics that are reported in the table, we clip single sequences to maximum length L = 1024, which results in few sequences being truncated significantly. In the long sequence task, the training and validation sets are obtained by concatenating the sequences, separated by an end-of-sequence token, and grouping the resulting chain into non-overlapping sequences of length L = 8192. A random baseline, with uniform probability across all the vocabulary tokens at every position, has accuracy 5% (when including only the 20 standard amino acids) and 4% (when also including the 5 anomalous amino acids <ref type="bibr" target="#b14">(Consortium, 2019)</ref>). However, the empirical frequencies of the various amino acids in our dataset may be far from uniform, so we also consider an empirical baseline where the amino acid probabilities are proportional to their empirical frequencies in the training set. <ref type="figure" target="#fig_6">Figure 9</ref> shows the estimated empirical distribution. We use both the standard and anomalous amino acids, and we crop sequences to length 1024 to match the data processing performed for the Transformer models. The figure shows only the 20 standard amino acids, colored by their class, for comparison with the visualization on the TrEMBL web page 6 .  <ref type="table" target="#tab_1">Table 2</ref>: Results on single protein sequence modeling (L = 1024). We note that the empirical baseline results are applicable to both the unidirectional (UNI) and bidirectional (BID) models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 EMPIRICAL BASELINE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 TABULAR RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 ATTENTION MATRIX ILLUSTRATION</head><p>In this section we illustrate the attention matrices produced by a Performer model. We focus on the bidirectional case and choose one Performer model trained on the standard single-sequence TrEMBL task for over 500K steps. The same analysis can be applied to unidirectional Performers as well.</p><p>We note that while the Transformer model instantiates the attention matrix in order to compute the attention output that incorporates the (queries Q, keys K, values V ) triplet (see Eq. 1 in the main paper), the FAVOR mechanism returns the attention output directly (see Algorithm 1). To account for this discrepancy, we extract the attention matrices by applying each attention mechanism twice: once on each original (Q, K, V ) triple to obtain the attention output, and once on a modified (Q, K, V ? ) triple, where V ? contains one-hot indicators for each position index, to obtain the attention matrix. The choice of V ? ensures that the dimension of the attention output is equal to the sequence length, and that a non-zero output on a dimension i can only arise from a non-zero attention weight to the i th sequence position. Indeed, in the Transformer case, when comparing the output of this procedure with the instantiated attention matrix, the outputs match.</p><p>Attention matrix example. We start by visualizing the attention matrix for an individual protein sequence. We use the BPT1_BOVIN protein sequence 7 , one of the most extensively studied globular proteins, which contains 100 amino acids. In <ref type="figure">Figure 10</ref>, we show the attention matrices for the first 4 layers. Note that many heads show a diagonal pattern, where each node attends to its neighbors, and some heads show a vertical pattern, where each head attends to the same fixed positions. These patterns are consistent with the patterns found in Transformer models trained on natural language <ref type="bibr" target="#b28">(Kovaleva et al., 2019)</ref>. In <ref type="figure" target="#fig_1">Figure 12</ref> we highlight these attention patterns by focusing on the first 25 tokens, and in <ref type="figure">Figure 11</ref>, we illustrate in more detail two attention heads.</p><p>Amino acid similarity. Furthermore, we analyze the amino-acid similarity matrix estimated from the attention matrices produced by the Performer model, as described in <ref type="bibr" target="#b51">Vig et al. (Vig et al., 2020)</ref>. We aggregate the attention matrix across 800 sequences. The resulting similarity matrix is illustrated in <ref type="figure" target="#fig_2">Figure 13</ref>. Note that the Performer recognizes highly similar amino acid pairs such as (D, E) and (F, Y). <ref type="figure">Figure 10</ref>: We show the attention matrices for the first 4 layers and all 8 heads (each row is a layer, each column is head index, each cell contains the attention matrix across the entire BPT1_BOVIN protein sequence). Note that many heads show a diagonal pattern, where each node attends to its neighbors, and some heads show a vertical pattern, where each head attends to the same fixed positions. <ref type="figure">Figure 11</ref>: We illustrate in more detail two attention heads. The sub-figures correspond respectively to: (1)</p><p>Head 1-2 (second layer, third head), (2) Head 4-1 (fifth layer, second head). Note the block attention in Head 1-2 and the vertical attention (to the start token ('M') and the 85th token ('C')) in Head 4-1. <ref type="figure" target="#fig_1">Figure 12</ref>: We highlight the attention patterns by restricting our attention to the first 25 tokens (note that we do not renormalize the attention to these tokens). The illustration is based on Vig et al. <ref type="bibr" target="#b49">(Vig, 2019;</ref><ref type="bibr" target="#b50">Vig &amp; Belinkov, 2019)</ref>. Note that, similar to prior work on protein Transformers , the attention matrices include both local and global patterns.</p><formula xml:id="formula_16">A C D E F G H I K L M N P Q R S T V W Y A C D E F G H I K L M N P Q R S T V W Y 0.0 0.2 0.4 0.6 0.8 1.0 A C D E F G H I K L M N P Q R S T V W Y A C D E F G H I K L M N P Q R S T V W Y 0.0 0.2 0.4 0.6</formula><p>0.8 <ref type="figure" target="#fig_2">Figure 13</ref>: Amino acid similarity matrix estimated from attention matrices aggregated across a small subset of sequences, as described in <ref type="bibr" target="#b51">Vig et al. (Vig et al., 2020)</ref>. The sub-figures correspond respectively to: (1) the normalized BLOSUM matrix, (2) the amino acid similarity estimated via a trained Performer model. Note that the Performer recognizes highly similar amino acid pairs such as (D, E) and (F, Y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D EXTENDED APPROXIMATION AND COMPARISON RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 BACKWARDS COMPATIBILITY -ERROR PROPAGATION</head><p>Although mentioned previously (Sec. 4.2) that the Performer with additional finetuning is backwards compatible with the Transformer, we demonstrate below in <ref type="figure" target="#fig_3">Fig. 14 that error</ref> propagation due to nonattention components of the Transformer is one of the primary reasons that pretrained Transformer weights cannot be immediately used for inference on the corresponding Performer. We show the following properties of our softmax approximation, in <ref type="figure" target="#fig_8">Fig. 15</ref>:</p><p>Redrawing: While the benefits of redrawing features was shown in Subsec. 4.3 of the main body of the paper, we also its benefits when there are multiple layers with large scale (16x16 TPU-v2) training.</p><p>Unidirectional: While we have shown on TrEMBL that Performer with generalized ReLU attention outperforms softmax, we also show that approximate softmax attention can still be a solid choice, for example on ImageNet64 (U). After 100K steps of training, the Performer-ReLU, Performer-Softmax, and Performer-Softmax (SMREG) variants achieve respectively, 3.67, 3.69, 3.67 BPD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instability of Trigonometric Features:</head><p>We see the full view of the unstable training curve when using Trigonometric softmax. We investigated Generalized Attention mechanisms (mentioned in Sec. 2.2) on TrEMBL when L = 512 for various kernel functions. This is similar to <ref type="bibr" target="#b46">(Tsai et al., 2019)</ref> which also experiments with various attention kernels for natural language. Using hyperparameter sweeps across multiple variables in FAVOR, we compared several kernels and also renormalization on/off ( <ref type="figure" target="#fig_4">Fig. 16</ref> and <ref type="figure" target="#fig_5">Fig. 17)</ref>, where Renormalize corresponds to applying D ?1 operator in attention, as for the standard mechanism, though we noticed that disabling it does not necessarily hurt accuracy) to produce the best training configuration for the Performer. We note that the effective batch size slightly affects the rankings (as shown by the difference between 2x2 and 4x4 TPU runs) -we by default use the generalized ReLU kernel with other default hyperparameters shown in Appendix A, as we observed that they are empirically optimal for large batch size runs (i.e. 8x8 or 16x16 TPU's). <ref type="figure" target="#fig_4">Figure 16</ref>: To emphasize the highest accuracy runs but also show the NaN issues with certain kernels which caused runs to stop early, we set both x and y axes to be log-scale. We tested kernels defined by different functions f (see: Sec. 2.2): sigmoid, exponential, ReLU, absolute, gelu, cosine (original softmax approximation), tanh, and identity. All training runs were performed on 2x2 TPU-v2's, 128 batch size per device. <ref type="figure" target="#fig_5">Figure 17</ref>: We also performed a similar setup as <ref type="figure" target="#fig_4">Fig. 16</ref> for 4x4 TPU-v2's. D.4 COMPARISON WITH LINEAR TRANSFORMER We use the attention implementation of the Linear Transformer from <ref type="bibr" target="#b26">(Katharopoulos et al., 2020)</ref>, which mainly involves setting our feature map ?(x) = elu(x) + 1, where elu(x) is the shifted-eLU function from <ref type="bibr" target="#b12">(Clevert et al., 2016)</ref>. <ref type="figure">Figure 18</ref>: Left: In the unidirectional 36-ProGen setting, we ran 3 seeds of the Linear Transformer, and found that all 3 seeds produced exploding gradients very early on, stopping the training run. Right: The Linear Transformer in the bidirectional setting also produced an exploding gradient in the middle of training, near 125K steps. Exploding gradients can be evidenced by the sharp drop in train accuracy right before a NaN error. For the sake of fairness and to prevent confounding results, while <ref type="bibr" target="#b26">(Katharopoulos et al., 2020)</ref> also uses the GeLU nonlinearity for the MLPs in the Linear Transformer, we instead use the original ReLU nonlinearity. We also used the exact same training hyperparameters as Performer-ReLU on our exact ProGen setting from <ref type="figure" target="#fig_4">Fig. 6</ref>. Ultimately, we empirically found that the Linear Transformer possessed numerical instability during training via unstable training curves, ultimately stopping training by producing exploding gradients (NaNs) <ref type="figure">(Fig. 18</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5 LONG RANGE ARENA</head><p>Performers are compared against many additional (scalable and not scalable) methods not included in our paper: Local Attention, Sparse Attention, Longformer, Sinkhorn Transformer, Synthesizer, Big Bird and the aforementioned Linear Transformer on challenging long range context tasks in the Long Range Arena <ref type="bibr" target="#b45">(Tay et al., 2021)</ref>, with <ref type="figure" target="#fig_6">Fig. 19</ref> displaying the original paper's results. Performers obtain the largest LRA (Long Range Arena) score among all tested scalable Transformers methods (which we define by having speed of &gt; 100 examples/sec).</p><p>Tasks used for comparison include: (1) a longer variation of the standard ListOps task proposed in <ref type="bibr" target="#b35">(Nangia &amp; Bowman, 2018)</ref>, (2) byte-level text classification using real-world data, (3) byte-level document retrieval, (4) image classification on sequences of pixels, and (5) Pathfinder task (longrange spatial dependency problem). In the Long Range Arena paper, the authors found that all models do not learn anything on Path-X task (denoted by FAIL), contrary to the Pathfinder task, which shows that increasing the sequence length can cause seriously difficulties for model training. <ref type="table">Table:</ref> Results on Long-Range Arena benchmark. Best model is in boldface and second best is underlined. Lower <ref type="table">Table:</ref> Benchmark results of all X-former models with a consistent batch size of 32 across all models. The authors report relative speed increase/decrease in comparison with the vanilla Transformer in brackets besides the steps per second. Memory usage refers to per device memory usage across each TPU device. Benchmarks are run on 4x4 TPU-v3 chips. Right <ref type="figure">Fig: Performance (y-axis)</ref>, speed (x-axis), and memory footprint (size of the circles) of different models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 19: Upper</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E COMPUTATION COSTS -EXTENDED RESULTS</head><p>In this subsection, we empirically measure computational costs in terms wall clock time on forward and backward passes for three scenarios in <ref type="figure" target="#fig_1">Fig. 20:</ref> 1. Performer, with varying number of layers. We show that our method can scale up to (but not necessarily limited to) even 20 layers.</p><p>2. Attention time complexities when comparing standard attention (from Transformer) and FAVOR (from Performer). Note that the maximum memory size here is not reflective of the maximum memory size in an actual model (shown below), as this benchmark requires computing explicit tensors (causing memory increases) in Jax, while a model does not.</p><p>3. Time complexities when comparing the Transformer and Performer models. "X" (OPT) denotes the maximum possible speedup achievable, when attention simply returns the Vvector, showing that the Performer is nearly optimal. We see that the maximum possible power of 2 length allowed on a V100 GPU (16GB) is 2 15 = 32768 using regular dimensions.</p><p>Since some of the computational bottleneck in the Transformer may originate from the extra feed-forward layers <ref type="bibr" target="#b27">(Kitaev et al., 2020)</ref>, we also benchmark the "Small" version, i.e. (n heads , n layers , d f f , d) = (1, 6, 64, 64) as well, when the attention component is the dominant source of computation and memory. We remind the reader that the "Regular" version consists of (n heads , n layers , d f f , d) = <ref type="bibr">(8,</ref><ref type="bibr">6,</ref><ref type="bibr">2048,</ref><ref type="bibr">512)</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F THEORETICAL RESULTS</head><p>We provide here the proofs of all theoretical results presented in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 PROOF OF LEMMA 1</head><p>Proof. We first deduce that for any a, b ? R d SM(x, y) = exp(x y) = exp(? x 2 /2) ? exp( x + y 2 /2) ? exp(? y 2 /2).</p><p>Next, let w ? R d . We use the fact that</p><formula xml:id="formula_17">(2?) ?d/2 exp(? w ? c 2 2 /2)dw = 1</formula><p>for any c ? R d and derive:</p><formula xml:id="formula_18">exp( x + y 2 /2) = (2?) ?d/2 exp( x + y 2 /2) exp(? w ? (x + y) 2 /2)dw = (2?) ?d/2 exp(? w 2 /2 + w (x + y) ? x + y 2 /2 + x + y 2 /2)dw = (2?) ?d/2 exp(? w 2 /2 + w (x + y))dw = (2?) ?d/2 exp(? w 2 /2) ? exp(w x) ? exp(w y)dw = E ??N (0 d ,I d ) [exp(? x) ? exp(? y)].</formula><p>That completes the proof of the first part of the lemma. An identity involving hyperbolic cosine function is implied by the fact that for every u ? R d and ? ? N (0, I d ) the following is true:</p><formula xml:id="formula_19">E[exp(? u)] = ? i=0 E[(? u) 2i ] (2i)! = 1 2 ? i=0 E[(? u) 2i ] + E[(?? u) 2i ] (2i)! .<label>(12)</label></formula><p>The cancellation of the odd moments E[(? u) 2i+1 ] follows directly from the fact that ? is taken from the isotropic distribution (i.e. distribution with pdf function constant on each sphere). That completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 PROOF OF LEMMA 2</head><p>Proof. Denote: z = x + y and ? = x ? y. Note that by using standard trigonometric identities (and the fact that the variance of the sum of independent random variables is the sum of variances of those random variables), we can get the following for ? ? N (0, I d ):</p><formula xml:id="formula_20">MSE( SM trig m (x, y)) = 1 m exp( x 2 + y 2 )Var(cos(? ?)).<label>(13)</label></formula><p>Using the fact that (see: Lemma 1 in <ref type="bibr" target="#b57">(Yu et al., 2016)</ref>; note that in that lemma they use notation: z for what we denote as: ? ):</p><formula xml:id="formula_21">Var(cos(? ?)) = 1 2 (1 ? exp(? ? 2 )) 2 ,<label>(14)</label></formula><p>we obtain:</p><formula xml:id="formula_22">MSE( SM trig m (x, y)) = 1 2m exp( x 2 + y 2 )(1 ? exp(? ? 2 )) 2 = 1 2m exp( z 2 )SM ?2 (x, y)(1 ? exp(? ? 2 )) 2 ,<label>(15)</label></formula><p>which completes the first part of the proof. To obtain the formula for: MSE( SM + m (x, y)) notice first that:</p><formula xml:id="formula_23">E ??N (0,I d ) [exp(? z)] = exp( z 2 2</formula><p>).</p><p>The above immediately follows from the fact that positive random feature maps provide unbiased estimation of the softmax-kernel, thus the following is true:</p><formula xml:id="formula_25">SM(x, y) = exp(? x 2 + y 2 2 )E ??N (0,I d ) [exp(? z)].<label>(17)</label></formula><p>Therefore we obtain:</p><formula xml:id="formula_26">MSE( SM + m (x, y)) = 1 m exp(?( x 2 + y 2 ))Var(exp(? z)) = 1 m exp(?( x 2 + y 2 )) E[exp(2? z)] ? (E[exp(? z)]) 2 = 1 m exp(?( x 2 + y 2 ))(exp(2 z 2 ) ? exp( z 2 )),<label>(18)</label></formula><p>where the last equality follows from Equation 16. Therefore we have:</p><formula xml:id="formula_27">MSE( SM + m (x, y)) = 1 m exp(?( x 2 + y 2 )) exp( z 2 )(exp( z 2 ) ? 1) = 1 m exp( z 2 )SM 2 (x, y)(1 ? exp(? z 2 )).<label>(19)</label></formula><p>Finally,</p><formula xml:id="formula_28">MSE( SM hyp+ m (x, y)) = 1 4m exp(? x 2 + y 2 2 ) 2 (Var(exp(? z)) + Var(exp(?? z))+ 2Cov(exp(? z)), exp(?? z)))) = 1 4m exp(? x 2 + y 2 2 ) 2 (2Var(exp(? z))+ 2Cov(exp(? z)), exp(?? z))))) = 1 2m exp(?( x 2 + y 2 )) (Var(exp(? z)) + 1 ? (E[exp(? z)]) 2 ) = 1 2m exp(?( x 2 + y 2 )) (exp(2 z 2 ) ? exp( z 2 ) + 1 ? exp( z 2 )) = 1 2m exp(?( x 2 + y 2 ))(exp( z 2 ) ? 1) 2 = 1 2 (1 ? exp(? z 2 ))MSE( SM + m (x, y)).<label>(20)</label></formula><p>In the chain of equalities above we used the fact that random variables exp(? z) and exp(?? z) have the same distribution. This is true since ? and ?? have the same distribution (? is Gaussian). That completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3 PROOF OF THEOREM 1</head><p>Proof. Let x, y ? R d be respectively a query/key. Note that from the definition of SMREG(x, y) we have for z = x + y:</p><formula xml:id="formula_29">SMREG(x, y) = exp(? x 2 + y 2 2 ) ? k=0 1 (2k)! z 2k d k E ??N (0,I d ) [( ? ? 2 e 1 ) 2k ],<label>(21)</label></formula><p>where e 1 def = (1, 0, ..., 0) ? R d . To obtain the above we used the fact that N (0, I d ) is isotropic (that in particular implies zeroing of the even terms in the Taylor expansion).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Let us denote:</head><formula xml:id="formula_30">A(k, d) def = E ??N (0,I d ) [( ? ? 2 e 1 ) 2k</formula><p>]. It turns out that:</p><formula xml:id="formula_31">A(2k, d) = (2k ? 1)!! (d + 2k ? 2)(d + 2k ? 4) ? ... ? d .<label>(22)</label></formula><p>The proof of that fact can be found in the supplement of <ref type="bibr" target="#b8">(Choromanski et al., 2018b</ref>), yet we provide it below for completeness and the convenience of the Reader:</p><p>Lemma 3. Expression A(2k, d) satisfies the following for k ? N :</p><formula xml:id="formula_32">A(2k, d) = (2k ? 1)!! (d + 2k ? 2)(d + 2k ? 4) ? ... ? d .<label>(23)</label></formula><p>Proof. Note first that for d ? 2 the density function p d (?) of the angle between a vector r ? R d chosen uniformly at random from the unit sphere and e 1 is given by the following formula:</p><formula xml:id="formula_33">p d (?) = sin d?2 (?) ? 0 sin d?2(?) d? .<label>(24)</label></formula><p>Let us denote:</p><formula xml:id="formula_34">F (k, d) def = ? 0 cos k (?) sin d (?)d?.</formula><p>Using partial integration, we get:</p><formula xml:id="formula_35">? 0 cos k (?) sin d (?)d? = ? 0 cos k?1 (?) sin d (?)(sin(?)) d? = cos k?1 (?) sin d+1 (?)| ? 0 ? ? 0 sin(?)((k ? 1) cos k?2 (?)(? sin(?)) sin d (?)+ d cos k (?) sin d?1 (?))d?.<label>(25)</label></formula><p>Thus we conclude that: F (k, d) = k?1 d+1 F (k ? 2, d + 2). Therefore we have:</p><formula xml:id="formula_36">F (2k, d) = (2k ? 1)!! (d + 1)(d + 3) ? ... ? (d + 2k ? 1) ? 0 sin d+2k (?)d?.<label>(26)</label></formula><p>We again conduct partial integration and get:</p><formula xml:id="formula_37">? 0 sin d (?)d? = ? 1 d sin d?1 (?) cos(?)| ? 0 + d ? 1 d ? 0 sin d?2 (?)d? = d ? 1 d ? 0 sin d?2 (?)d?.<label>(27)</label></formula><p>Therefore we conclude that:</p><formula xml:id="formula_38">A(2k, d) = 1 d?3 d?2 d?5 d?4 ? ... (2k ? 1)!! (d ? 1)(d + 1) ? ... ? (d + 2k ? 3) d + 2k ? 3 d + 2k ? 2 d + 2k ? 5 d + 2k ? 4 ? .... = (2k ? 1)!! (d + 2k ? 2)(d + 2k ? 4) ? ... ? d ,<label>(28)</label></formula><p>which completes the proof.</p><p>Applying the above lemma, we get:</p><formula xml:id="formula_39">SMREG(x, y) = exp(? x 2 + y 2 2 ) ? k=0 1 (2k)! z 2k d k (2k ? 1)!! (d + 2k ? 2)(d + 2k ? 4) ? ... ? d = exp(? x 2 + y 2 2 ) ? k=0 w k k! f (k, d),<label>(29)</label></formula><p>where w = z 2 2 and f (k, d) = d k (d+2k?2)(d+2k?4)?...?d . Thus we obtain:</p><formula xml:id="formula_40">SMREG(x, y) SM(x, y) = e ?w ? k=0 w k k! f (k, d).<label>(30)</label></formula><p>Note first that for k ? 1 we have: f (k, d) ? 1, thus:</p><p>SMREG(x, y) ? SM(x, y).</p><p>We will consider two estimators of the beautiful functions from Definition 1 that directly lead (through Remark 1) to: PRF-based approximation of the softmax-kernel and its enhanced version with orthogonal features. Standard Monte Carlo estimator samples independently ? iid 1 , ..., ? iid m iid ? ?, where m stands for the number of samples and then computes:</p><formula xml:id="formula_42">F iid m (z) def = 1 m m i=1 g((? iid i ) z).<label>(38)</label></formula><p>Orthogonal Monte Carlo estimator samples ? ort 1 , ..., ? ort m (m ? d) in such a way that marginally we have: ? ort i ? ?, but (? ort i ) ? ort j = 0 for i = j (such an orthogonal ensemble can be always created if ? is isotropic, as we already mentioned in the main body of the paper). We define:</p><formula xml:id="formula_43">F ort m (z) def = 1 m m i=1 g((? ort i ) z).<label>(39)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.4.1 ORTHOGONALITY UNIVERSALLY IMPROVES CONCENTRATION</head><p>Denote by M Z (?) = E[e ?Z ] a moment generating function of the random variable Z. Note first that estimators of beautiful functions based on standard Monte Carlo procedure using independent vectors ? iid i guarantee strong concentration bounds since independent ? i s provide a way to obtain exponentially small upper bounds on failure probabilities through moment generating functions. We summarize this classic observation which is a standard application of Markov's Inequality below.</p><p>Lemma 4. Consider an estimator F iid m (z) of the beautiful function F evaluated at z. Then the following holds for any a &gt; F (z):</p><formula xml:id="formula_44">P[ F iid m (z) &gt; a] ? e ?mL X (a) ,<label>(40)</label></formula><p>where X = g(w z), w ? D and L Z stands for a Legendre Transform of the random variable Z defined as: L Z (a) = sup ?&gt;0 log( e ?a M Z (?) ). Furthermore, L X (a) &gt; 0.</p><p>The above result provides us with exponentially small (in Legendre Transform) upper bounds on tail probabilities for the standard estimator. Below we provide our two main theoretical results. Theorem 5 (orthogonality provides smaller tails). If F ?,g is a beautiful function then the following holds for m ? d, X as in Lemma 4 and any a &gt; F (z):</p><formula xml:id="formula_45">P[ F ort m (z)) &gt; a] ? d d + 2 e ?mL X (a) .<label>(41)</label></formula><p>This result shows that features obtained from the ensembles of pairwise orthogonal random vectors provide exponentially small bounds on tail probabilities and that these bounds are strictly better than for estimators using unstructured features. Furthermore, the result is universal, i.e. holds for any dimensionality d, not just asymptotically for d large enough.</p><p>We also obtain similar result regarding mean squared errors (MSEs) of the considered estimators: Theorem 6. If F ?,g is a beautiful function then the following holds for m ? d:</p><formula xml:id="formula_46">MSE( F ort m (z)) ? MSE( F iid m (z)) ? (1 ? 1 m ) 2 d + 2 F 2 ?,g (z).<label>(42)</label></formula><p>As before, an orthogonal estimator leads to better concentration results and as before, this is the case for any d &gt; 0, not only asymptotically for large enough d.</p><p>Note that from what we have said above, Theorem 2 and Theorem 3 follow immediately from Theorem 6 and Theorem 5 respectively.</p><p>Thus in the remainder of this section we will prove Theorem 6 and Theorem 5.</p><p>Proof. Note that by the analogous application of Markov's Inequality as in Lemma 4, we get:</p><formula xml:id="formula_47">P[ F ort m (z)) &gt; a] ? E[e ?(X ort 1 +...+X ort m ) ] e ?ma ,<label>(43)</label></formula><p>where we have: X ort i = g((? ort i ) z). We see that it suffices to show that for any ? &gt; 0 the following holds: E[e ?(X ort 1 +...+X ort m ) ] &lt; E[e ?(X iid 1 +...+X iid m ) ]. We have:</p><p>E[e ?(X ort 1 +...+X ort</p><formula xml:id="formula_48">m ) ] = E[ ? j=0 (? m i=1 X ort i ) j j! ] = E[ ? j=0 ? j j! ( m i=1 X ort i ) j ] = ? j=0 ? j j! E[( m i=1 X ort i ) j ] = ? j=0 ? j j! E[ (j1,.</formula><p>..,jm)?Sj c(j 1 , ..., j m )(X ort 1 ) j1 ? ... ? (X ort m ) jm ],</p><p>where S j = {(j 1 , ..., j m ) ? N ? ... ? N : j 1 , ..., j m ? 0, j 1 + ... + j m = j} and for some positive constants c(j 1 , ..., j m ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Thus we have:</head><p>E[e ?(X ort 1 +...+X ort m ) ] = ? j=0 ? j j! (j1,...,jm)?Sj c(j 1 , ..., j m )E[(X ort 1 ) j1 ? ... ? (X ort m ) jm ].</p><p>Similarly, we get: </p><formula xml:id="formula_51">E[e ?(X iid 1 +...+X iid m ) ] = ? j=0 ? j j! (j1,...,jm)?Sj c(j 1 , ..., j m )E[(X iid 1 ) j1 ? ... ? (X iid m ) jm ].<label>(46)</label></formula><p>Note first that using the fact that f is entire, we can rewrite each X ort i as:</p><formula xml:id="formula_53">X ort i = ? s=0 a s ((? ort i ) z) s ,<label>(48)</label></formula><p>where f (x) = ? s=0 a s x s and a 0 , a 1 , ... ? 0. Similarly,</p><formula xml:id="formula_54">X iid i = ? s=0 a s ((? iid i ) z) s .<label>(49)</label></formula><p>By plugging in the above formulae for X ort i and X iid i int the formula for ? and expanding powerexpressions, we obtain: ? = ? j=0 ? j j! (j1,...,jm)?Sj c(j 1 , ..., j m ) (d1,...,dm)?D(j1,...,jm) ?(d 1 , ..., d m ),</p><p>for some ordered subsets of indices (with potentially repeating entries) D(j 1 , ..., j m ) (exact formula for those can be given but we do not need it to complete the proof and since it is technical, it would unnecessarily complicate the proof so we skip it) and ?(d 1 , ..., d m ) defined as:</p><p>?(d 1 , ..., d m ) = E[((? iid 1 ) z) d1 ? ... ? ((? iid m ) z) dm ] ? E[((? ort 1 ) z) d1 ? ... ? ((? ort m ) z) dm ]. <ref type="formula" target="#formula_3">(51)</ref> Our next goal is to re-write the formula for ?(d 1 , ..., d m ). Denote: Y = ((? ort 1 ) z) d1 ? ... ? ((? ort m ) z) dm . </p><p>That completes the proof.</p><p>hard attention on sequences of unlimited length with a fixed m. When the sequence length increases, even the standard attention requires longer and longer vectors to make the softmax concentrated enough to pick single elements. Nevertheless, as seen in our experiments, this limitation does not manifest itself in practice at the lengths we experimented with.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Left: Symmetrized (around origin) utility function r (defined as the ratio of the mean squared errors (MSEs) of estimators built on: trigonometric and positive random features) as a function of the angle ? (in radians) between input feature vectors and their lengths l.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Comparison of Transformer and Performer in terms of forward and backward pass speed and maximum L allowed. "X" (OPT) denotes the maximum possible speedup achievable, when attention simply returns the V-matrix. Plots shown up to when a model produces an out of memory error on a V100 GPU with 16GB. Vocabulary size used was 256. Best in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>MSE of the approximation output when comparing Orthogonal vs IID features and trigonometric sin/cos vs positive features. We took L = 4096, d = 16, and varied the number of random samples m. Standard deviations shown across 15 samples of appropriately normalized random matrix input data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Train = Dashed, Validation = Solid. For TrEMBL, we used the exact same model parameters (n heads , n layers , d f f , d) = (8, 36, 1024, 512) from (Madani et al., 2020) for all runs. For fairness, all TrEMBL experiments used 16x16 TPU-v2's. Batch sizes were maximized for each separate run given the compute constraints. Hyperparameters can be found in Appendix A. Extended results including dataset statistics, out of distribution evaluations, and visualizations, can be found in Appendix C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Train = Dashed, Validation = Solid. For ImageNet64, all models used the standard (n heads , d f f , d) =</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Visualization of the estimated empirical distribution for the 20 standard amino acids, colored by their class. Note the consistency with the statistics on the TrEMBL web page.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 14 :</head><label>14</label><figDesc>Output approximation errors between a vanilla Transformer and a Performer (with orthogonal features) for varying numbers of layers. D.2 APPROXIMATE SOFTMAX -EXTENDED PROPERTIES</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 15 :</head><label>15</label><figDesc>Best viewed zoomed in. Left: The importance of redrawing features. If redrawing is not used, an "unlucky" set of random features may cause training degradation, shown by the early-stopped curve with Seed 1, while a 'lucky' set of random features may cause no issue, shown by the curve with Seed 2. Redrawing allows the training to correct itself, as seen at the black vertical line. Middle: Using the same 8x8 TPU-v2 compute and same 6-layer standard model, approximate softmax with positive features achieves the same result as generalized ReLU attention. Right: Zoomed out view of right subfigure of Fig. 5, showing that Trigonometric softmax causes very unstable training behaviors. D.3 GENERALIZED ATTENTION</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 20 :</head><label>20</label><figDesc>Captions (1) and (2) for each 2x2 subfigure mentioned above.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 21 :</head><label>21</label><figDesc>Caption (3) for this 2x2 subfigure mentioned above.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>1 , ..., j m ) E[(X iid 1 ) j1 ? ... ? (X iid m ) jm ] ? E[(X ort 1 ) j1 ? ... ? (X ort m ) jm ]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>contains the results on the single protein sequence modeling task (L = 1024). We report accuracy and perplexity as defined in Appendix A:</figDesc><table><row><cell>Model Type Set Name</cell><cell>Model</cell><cell cols="2">Accuracy Perplexity</cell></row><row><cell></cell><cell>Empirical Baseline</cell><cell>9.92</cell><cell>17.80</cell></row><row><cell>Test</cell><cell>Transformer</cell><cell>30.80</cell><cell>9.37</cell></row><row><cell>UNI</cell><cell>Performer (generalized) Empirical Baseline</cell><cell>31.58 9.07</cell><cell>9.17 17.93</cell></row><row><cell>OOD</cell><cell>Transformer</cell><cell>19.70</cell><cell>13.20</cell></row><row><cell></cell><cell>Performer (generalized)</cell><cell>18.44</cell><cell>13.63</cell></row><row><cell></cell><cell>Transformer</cell><cell>33.32</cell><cell>9.22</cell></row><row><cell>Test</cell><cell>Performer (generalized)</cell><cell>36.09</cell><cell>8.36</cell></row><row><cell>BID</cell><cell>Performer (softmax) Transformer</cell><cell>33.00 25.07</cell><cell>9.24 12.09</cell></row><row><cell>OOD</cell><cell>Performer (generalized)</cell><cell>24.10</cell><cell>12.26</cell></row><row><cell></cell><cell>Performer (softmax)</cell><cell>23.48</cell><cell>12.41</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/google-research/google-research/blob/master/ performer/fast_attention</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/google-research/google-research/blob/master/ performer/fast_attention 3 https://github.com/google/trax/blob/master/trax/supervised/configs/ reformer_imagenet64.gin</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://www.uniprot.org/statistics/TrEMBL 5 https://www.uniprot.org/uniprot/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://www.uniprot.org/statistics/TrEMBL 7 https://www.uniprot.org/uniprot/P00974</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">) , where ? is the so-called Gamma-function.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ACKNOWLEDGEMENTS</head><p>We thank Nikita Kitaev and Wojciech Gajewski for multiple discussions on the Reformer, and also thank Aurko Roy and Ashish Vaswani for multiple discussions on the Routing Transformer. We further thank Joshua Meier, John Platt, and Tom Weingarten for many fruitful discussions on biological data and useful comments on this draft. We lastly thank Yi Tay and Mostafa Dehghani for discussions on comparing baselines.</p><p>Valerii Likhosherstov acknowledges support from the Cambridge Trust and DeepMind. Lucy Colwell acknowledges support from the Simons Foundation. Adrian Weller acknowledges support from a Turing AI Fellowship under grant EP/V025379/1, The Alan Turing Institute under EPSRC grant EP/N510129/1 and U/B/000074, and the Leverhulme Trust via CFI.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>where Po(w) stands for the random variable of Poisson distribution with parameter w. Therefore we get for t = ln( l w ):</p><p>where the last equality is implied by the formula for the Laplace Transform for the Poisson random variable:</p><p>Notice that: w = z 2 2 = ln(SM(x,x))+ln(SM(y,y))+2 ln(SM(x,y)) 2 ? 2 ln(C). We conclude that:</p><p>That completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.4 PROOFS OF THEOREM 2,THEOREM 3 &amp; BEAUTIFUL FUNCTIONS</head><p>We will provide here much more general theoretical results which will imply Theorem 3 and Theorem 2. We need the following definition: Definition 1. We say that function F : R n ? R is beautiful if F can be expressed as:</p><p>for a probabilistic isotropic distribution ?, and where g : R ? R is an entire function with nonnegative power-series coefficients (i.e. g(x) = ? i=0 a i x i for every x ? R and with a i ? 0 for i = 0, 1, ...). In the formula above we assume that the expectation on the RHS exists.</p><p>Interestingly, beautiful functions can be used to define softmax and consequently, Gaussian kernels (both standard and regularized), leading to our PRF mechanism presented in the main body of the paper, as we explain below. Remark 1. If one takes ? = N (0, I d )(note that N (0, I d ) is isotropic) and g : x ? exp(x) (such g is clearly entire with nonnegative power-series coefficient) then the following is true for z = x + y:</p><p>Similarly: SMREG(x, y) = exp(? x 2 + y 2 2 )F ?reg,g (z), where ? reg stands for the distribution corresponding to Haar measure on the sphere of radius ? d (which is clearly isotropic). Therefore general concentration results for Monte Carlo estimators of beautiful functions immediately imply corresponding results for the (standard and regularized) softmax (and thus also Gaussian) kernel.</p><p>where the first equality comes from the fact that different ? iid i s are independent and the second one is implied by the analogous analysis to the one conducted above.</p><p>We will need the following lemma:</p><p>Lemma 5. For every s ? N + such that s ? n and every k 1 , ..., k s ? N + the following holds:</p><p>Proof. Take r = g g 2 g 2 , whereg is an independent copy of g. Note that r ? g. We have:</p><p>where the first equality comes from the independence of different elements of z = (z 1 , ..., z n ) and the second equality is implied by the fact thatg is independent from g.</p><p>Therefore we have:</p><p>.</p><p>That completes the proof since z ? g andg ? g.</p><p>Note that by Lemma 5, we can rewrite the right expression from the formula on ?(d 1 , ..., d m ) as:</p><p>The left expression from the formula on ?(d 1 , ..., d m ) can be rewritten as:</p><p>]</p><p>.</p><p>Since marginal distributions of ? ort i and ? iid i are the same, we can rewrite ?(d 1 , ..., d n ) as:</p><p>where ? (d 1 , ..., d m ) is defined as:</p><p>We need now few observations regarding ?(d 1 , ..., d m ). Note firsr that since odd moments of the Gaussian scalar distribution N (0, 1) are zero, ?(d 1 , ..., d m ) is zero if at least of of d i is odd. Furthermore, ?(d 1 , ..., d m ) is trivially zero if all but at most one d i are zero.</p><p>With our new notation, ? can be rewritten as:</p><p>..,jm)?Sj c(j 1 , ..., j m ) (d1,...,dm)?D(j1,...,jm)</p><p>Note also that we have:</p><p>Therefore (see: our observations on ?(d 1 , ..., d m )) to complete the proof it suffices to show that: ? (d 1 , ..., d m ) ? d d+2 if at least two: d i , d j for i = j are nonzero and all d i are even. Lemma 6. The following holds if for some i = j we have: d i , d j &gt; 0 and all d i are even:</p><p>Proof. Note that ? (d 1 , ..., d m ) can be rewritten as:</p><p>where ? d (j) stands for the j th moment of the ?-distribution with d degrees of freedom. Note that</p><p>Published as a conference paper at ICLR 2021 F.5 PROOF OF THEOREM 4</p><p>We showed in the main body of the paper that in contrast to other methods approximating the attention matrix A, our algorithm provides strong concentration guarantees. This is the case also for trigonometric random features, yet, as discussed in the main body of the paper, due to attention renormalization and higher variance of the estimation of small entries of the attention matrix, trigonometric mechanism is sub-optimal. We show here that m opt , the optimal number of random projections for the trigonometric orthogonal mechanism for accurate estimation of the attention matrix does not depend on L but only on d. In fact, we prove that if we take m opt = ? <ref type="figure">(d log(d)</ref>), then with O <ref type="figure">(Ld 2 log(d)</ref>)-time, we can approximate A up to any precision, regardless of the number of tokens L. In order to provide those guarantees, we leverage recent research on the theory of negative dependence for ORFs .</p><p>We prove the more general version of Theorem 4 from the main body of the paper: Theorem 7 (Uniform convergence for the trigonometric mechanism). Define entries of the attention matrix A as follows:</p><p>k j )h(k j ) for some g, h : R d ? R and where K is a radial basis function (RBF) kernel <ref type="bibr" target="#b8">(Choromanski et al., 2018b)</ref> with corresponding spectral distribution ? (e.g. Gaussian kernel for which ? = N (0, I d )). Assume that the rows of matrices Q and K are taken from a ball B(R) of radius R, centered at 0 (i.e. norms of queries and keys are upperbounded by R). Define l = Rd ? 1 4 and take g * = max x?B(l) |g(x)| and h * = max x?B(l) |h(x)|. Then for any &gt; 0, ? = g * h * and the number of random projections m = ?( d ? 2 log( 4?R The result holds in particular for regular softmax-attention for which K is a Gaussian kernel and</p><p>Proof. Let D Q be a diagonal matrix with entries of the form: g(q i ) and let D K be a diagonal matrix with entries of the form:</p><p>Denote by A and approximation of the attention matrix obtained from trigonometric orthogonal random features and by B an approximation of matrix B that those random features provide. We rely on Theorem 3 from . Note that we can apply it in our case, since for RBF kernels the corresponding functions f i satisfy f 1 (x) = sin(x), f 2 (x) = cos(x) (thus in particular are bounded). Also, it is not hard to observe (see for instance analysis in Claim 1 from <ref type="bibr" target="#b38">(Rahimi &amp; Recht, 2007)</ref>) that we can take: L f = 1 (for L f as in Theorem 3 from ). Using Theorem 3 from , we conclude that: . Since</p><p>and thus one can take diam(M) = 4R d 1 4</p><p>. We have:</p><p>Taking ? = g * h * completes the proof.</p><p>F.6 DISCUSSION OF THEOREM 4</p><p>As a consequence of Theorem 4, the number m of random projections required to approximate the attention matrix within error is a function of data dimensionality d, the parameter and the radius R of the ball within which the queries and keys live: m = ?( , d, R).</p><p>The dependence on d and is fairly easy to understand: with a larger dimensionality d we need more random projeections (on the order of magnitude d log(d)) to get an approximation within error. The dependence on R means that the length of queries and keys cannot grow at a fixed m if we want to retain the quality of the approximation. In particular, this means that FAVOR cannot approximate</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Attention augmented convolutional networks. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1904.09925" />
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2004.05150" />
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Imputer: Sequence modelling via imputation and dynamic programming. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2002.08926" />
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">One billion word benchmark for measuring progress in statistical language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH 2014, 15th Annual Conference of the International Speech Communication Association</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2635" to="2639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Faster transformer decoding: N-gram masked self-attention. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mia</forename><surname>Xu Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2001.04589" />
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The best of both worlds: Combining recent advances in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mia</forename><surname>Xu Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">F</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Macduff</forename><surname>Hughes</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1008</idno>
		<ptr target="https://www.aclweb.org/anthology/P18-1008/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-07-15" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="76" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1904.10509" />
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Initialization matters: Orthogonal predictive state recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlton</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron</forename><surname>Boots</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HJJ23bW0b" />
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The geometry of random features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Rowland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tam?s</forename><surname>Sarl?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Weller</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v84/choromanski18a.html" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics, AISTATS</title>
		<meeting><address><addrLine>Playa Blanca, Lanzarote, Canary Islands, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04" />
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">KAMA-NNs: Low-dimensional rotation based neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aldo</forename><surname>Pacchiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhao</forename><surname>Tang</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v89/choromanski19a.html" />
	</analytic>
	<monogr>
		<title level="m">The 22nd International Conference on Artificial Intelligence and Statistics</title>
		<meeting><address><addrLine>Naha, Okinawa, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-04-18" />
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="236" to="245" />
		</imprint>
	</monogr>
	<note>AISTATS</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unifying orthogonal Monte Carlo methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Rowland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Weller</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v97/choromanski19a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-15" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="1203" to="1212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of structured random orthogonal embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Krzysztof Marcin Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Rowland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09" />
			<biblScope unit="page" from="219" to="228" />
		</imprint>
	</monogr>
	<note>Published as a conference paper at ICLR 2021</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djork-Arn?</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1511.07289" />
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-05-02" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Protein interaction networks revealed by proteome coevolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Anishchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ovchinnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">365</biblScope>
			<biblScope unit="issue">6449</biblScope>
			<biblScope unit="page" from="185" to="189" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Uniprot: a worldwide hub of protein knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uniprot</forename><surname>Consortium</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="506" to="515" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Introduction to Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">E</forename><surname>Cormen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">L</forename><surname>Leiserson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clifford</forename><surname>Rivest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stein</surname></persName>
		</author>
		<idno>978-0-262-03384-8</idno>
		<ptr target="http://mitpress.mit.edu/books/introduction-algorithms" />
		<imprint>
			<date type="published" when="2009" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
	<note>3rd Edition</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Transformer-XL: Language modeling with longer-term dependency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HJePno0cYm" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Universal transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HyzdRiR9Y7" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>abs/1810.04805</idno>
		<ptr target="http://arxiv.org/abs/1810.04805" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Energy-based models for atomic-resolution protein conformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rives</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.13167</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">End-to-end multitask learning, from protein language to protein features without alignments. bioRxiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Elnaggar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Heinzinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Dallago</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burkhard</forename><surname>Rost</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">864405</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Compiling machine learning programs via highlevel tracing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Leary</surname></persName>
		</author>
		<ptr target="http://www.sysml.cc/doc/2018/146.pdf" />
	</analytic>
	<monogr>
		<title level="m">Conference on Machine Learning and Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Conformer: Convolution-augmented transformer for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anmol</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Music transformer: Generating music with long-term structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Zhi Anna</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Curtis</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monica</forename><surname>Dinculescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Eck</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJe4ShAcF7" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generative models for graphbased protein design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Ingraham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15794" to="15805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Transformers are rnns: Fast autoregressive transformers with linear attention. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelos</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Fleuret</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2006.16236" />
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rkgNKkHtvB" />
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Revealing the dark secrets of bert</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Kovaleva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Romanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08593</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. CoRR, abs/1808.06226</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Richardson</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1808.06226" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Parallel prefix computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Ladner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Fischer</surname></persName>
		</author>
		<idno type="DOI">10.1145/322217.322232</idno>
		<ptr target="https://doi.org/10.1145/322217.322232" />
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="831" to="838" />
			<date type="published" when="1980-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Demystifying orthogonal Monte Carlo and beyond. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Laroche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Simplified self-attention for transformer-based end-to-end speech recognition. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoneng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Xie</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2005.10463" />
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Progen: Language modeling for protein generation. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Madani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namrata</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><forename type="middle">R</forename><surname>Eguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Ssu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2004.03497" />
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Induction of potent neutralizing antibody responses by a designed protein nanoparticle vaccine for respiratory syncytial virus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Marcandalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Fiala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ols</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michela</forename><surname>Perotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willem</forename><surname>De Van Der Schueren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Snijder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Hodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Benhaim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lauren</forename><surname>Carter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">176</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1420" to="1431" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Listops: A diagnostic dataset for latent tree learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n18-4013</idno>
		<ptr target="https://doi.org/10.18653/v1/n18-4013" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-02" />
			<biblScope unit="page" from="92" to="99" />
		</imprint>
	</monogr>
	<note>Student Research Workshop</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v80/parmar18a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsm?ssan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07-10" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="4052" to="4061" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Compressive transformers for long-range sequence modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Potapenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SylKikSYDH" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Random features for large-scale kernel machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/3182-random-features-for-large-scale-kernel-machines" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-First Annual Conference on Neural Information Processing Systems</title>
		<meeting>the Twenty-First Annual Conference on Neural Information Processing Systems<address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1177" to="1184" />
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. bioArxiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rives</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Demi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="DOI">10.1101/622803</idno>
		<imprint>
			<biblScope unit="volume">04</biblScope>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Orthogonal estimation of Wasserstein distances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Rowland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Hron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tam?s</forename><surname>Sarl?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Weller</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v89/rowland19a.html" />
	</analytic>
	<monogr>
		<title level="m">The 22nd International Conference on Artificial Intelligence and Statistics</title>
		<meeting><address><addrLine>Naha, Okinawa, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-04-18" />
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="186" to="195" />
		</imprint>
	</monogr>
	<note>AISTATS</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Efficient content-based sparse attention with routing transformers. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2003.05997" />
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Factorized attention: Self-attention with linear complexities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoran</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyu</forename><surname>Zhao</surname></persName>
		</author>
		<idno>abs/1812.01243</idno>
		<ptr target="http://arxiv.org/abs/1812.01243" />
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Energy and policy considerations for deep learning in NLP. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananya</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1906.02243" />
		<imprint>
			<date type="published" when="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Neuroevolution of self-interpretable agents. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duong</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2003.08165" />
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Long range arena: A benchmark for efficient transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Transformer dissection: An unified understanding for transformer&apos;s attention via the lens of kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4335" to="4344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJXMpikCZ" />
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">A multiscale visualization of attention in the transformer model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Vig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05714</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Analyzing the structure of attention in a transformer language model. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1906.04284" />
		<imprint>
			<date type="published" when="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Bertology meets biology: Interpreting attention in protein language models. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Madani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lav</forename><forename type="middle">R</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazneen Fatema</forename><surname>Rajani</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2006.15222" />
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belinda</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2006" />
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Sharing attention weights for fast transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinqiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengtao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongran</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2019/735</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2019/735" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence<address><addrLine>Macao, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="5292" to="5298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n16-1174</idno>
		<ptr target="https://doi.org/10.18653/v1/n16-1174" />
	</analytic>
	<monogr>
		<title level="m">The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>San Diego California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>The Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
	<note>NAACL HLT 2016</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Drawing early-bird tickets: Toward more efficient training of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaojian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonggan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyan</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BJxsrgStvr" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Orthogonal random features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananda</forename><forename type="middle">Theertha</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krzysztof Marcin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">N</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Holtmann-Rice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05" />
			<biblScope unit="page" from="1975" to="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning with relational inductive biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vin?cius Flores Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">P</forename><surname>Tuyls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Reichert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murray</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Shanahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Langston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Battaglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.11</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2015.11" />
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision, ICCV 2015</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
