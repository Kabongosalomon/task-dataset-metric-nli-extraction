<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MuMiN: A Large-Scale Multilingual Multimodal Fact-Checked Misinformation Social Network Dataset</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">S</forename><surname>Nielsen</surname></persName>
							<email>dan.nielsen@bristol.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering Mathematics</orgName>
								<orgName type="institution">University of Bristol</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcconville</surname></persName>
							<email>ryan.mcconville@bristol.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Engineering Mathematics</orgName>
								<orgName type="institution">University of Bristol</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MuMiN: A Large-Scale Multilingual Multimodal Fact-Checked Misinformation Social Network Dataset</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>dataset</term>
					<term>misinformation</term>
					<term>graph</term>
					<term>twitter</term>
					<term>social network</term>
					<term>fake news</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Misinformation is becoming increasingly prevalent on social media and in news articles. It has become so widespread that we require algorithmic assistance utilising machine learning to detect such content. Training these machine learning models require datasets of sufficient scale, diversity and quality. However, datasets in the field of automatic misinformation detection are predominantly monolingual, include a limited amount of modalities and are not of sufficient scale and quality. Addressing this, we develop a data collection and linking system (MuMiN-trawl), to build a public misinformation graph dataset (MuMiN), containing rich social media data (tweets, replies, users, images, articles, hashtags) spanning 21 million tweets belonging to 26 thousand Twitter threads, each of which have been semantically linked to 13 thousand fact-checked claims across dozens of topics, events and domains, in 41 different languages, spanning more than a decade. The dataset is made available as a heterogeneous graph via a Python package (mumin). We provide baseline results for two node classification tasks related to the veracity of a claim involving social media, and demonstrate that these are challenging tasks, with the highest macro-average F1score being 62.55% and 61.45% for the two tasks, respectively. The MuMiN ecosystem is available at https://mumin-dataset.github.io/, including the data, documentation, tutorials and leaderboards.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>While it may be possible to track the history of misinformation, or 'fake news', back to Octavian of the Roman Republic <ref type="bibr" target="#b42">[43]</ref>, or Browne in the 17th century <ref type="bibr" target="#b6">[7]</ref>, it was the World Wide Web and the rise of online social networks that has provided new and powerful ways for the rapid dissemination of information, both true and false, with false information having negative effects across many aspects of society, such as politics and health.</p><p>A universal consensus has yet to be reached on the definition of misinformation. One convincing definition of misinformation is that it is 'false or misleading' information, with a subcategory of misinformation being disinformation, which is misinformation with the intention to deceive <ref type="bibr" target="#b19">[20]</ref>. In this work, we do not specifically distinguish disinformation from misinformation.</p><p>There exist over one hundred fact checking organisations that manually verify the veracity of claims made online, often within news articles or social media posts. This is a time consuming task involving a multitude of different documents and sources in order to classify a claim. To build intelligent tools to help with this process, datasets that accurately represent misinformation are required.</p><p>Online misinformation is multimodal, multilingual and multitopical. The multimodal aspect of misinformation manifests online in the use of image and video in addition to the commonly studied textual communication. Moreover, we also posit that an additional modality is the social behaviour of users online, which exhibits itself in the form of a graph, or network, of interactions and behaviours. These interactions vary by platform, but on Twitter, they can be considered actions such as 'retweeting', 'quote tweeting' or 'replying' to a tweet, or 'following' a user. While research typically studies such modalities in isolation, or occasionally, some subset, the integration of all modalities may be necessary to accurately capture the underlying classification of misinformation.</p><p>The multilingual dimension of misinformation can be challenging due to the focus of existing research on misinformation within the English language, with most existing misinformation datasets only covering the English language. See <ref type="table" target="#tab_1">Table 2</ref> for an overview of existing misinformation datasets. Further, there exists a focus of natural language processing research towards the English language. Indeed, in Sections 3.1.2 and 3.3 we observe that the transformer based models perform better when text is translated to English when compared to a multilingual model.</p><p>Finally, misinformation has the potential to permeate across all aspects of society and life, and is not restricted to a single topic or domain. For example, a significant number of tweeted articles analysed on Twitter, in the months preceding the 2016 United States presidential election, contained fake or extremely biased news <ref type="bibr" target="#b5">[6]</ref>. During the COVID-19 pandemic, the World Health Organization director-general stated that not only are they fighting the COVID-19 pandemic but also an 'infodemic' <ref type="bibr" target="#b37">[38]</ref>. Naming but two examples, this alone provides further motivation that automated misinformation detection systems must not be trained on a single topic (e.g.,  or domain (e.g., politics), and thus justifies the collection of datasets that capture the pervasiveness of misinformation across many aspects of society.</p><p>Given the severity of online misinformation, there have been numerous public datasets made available for researchers to develop and evaluate automated misinformation detection models with. These publicly available datasets cover topics ranging from celebrities <ref type="bibr" target="#b31">[32]</ref>, rumours <ref type="bibr" target="#b45">[46]</ref>, politics <ref type="bibr" target="#b35">[36]</ref> and health <ref type="bibr" target="#b22">[23]</ref>. These datasets typically include data from a social network, usually Twitter, along with labels assigning them to a category, categorizing arXiv:2202.11684v2 <ref type="bibr">[cs.</ref>LG] 8 Mar 2022 them as some equivalent of 'true' or 'false'. These labels often come from 'fact-checking' resources such as PolitiFact 1 and Poynter 2 .</p><p>There are, however, a number of limitations of existing datasets. We believe that in order to make advances on the development of automated misinformation detection systems, datasets that capture the breadth, complexity and scale of the problem are required. Specifically, we believe that an effective dataset should be large scale, as misinformation is an extremely varied and wide ranging phenomenon, with thousands of manually fact checked claims available online from fact-checking organisations across a range of topics. To ensure that misinformation detection models are able to generalise to new events, we need models to be able to learn event-independent predictors of misinformation. We believe that such predictors will not be possible from the claim texts alone, as they are inherently event-dependent. Instead, we argue that models (and thus datasets to train them) should utilise the context of the claim, for example, the social network surrounding the claim, or the article in which the claim was posted.</p><p>Given the short message length of posts on Twitter, we believe that additional context is required in order to properly capture how claims are being discussed on social networks. This can take the form of the media involved in the posts, articles that users are sharing on the social network, or indeed the social network of the user themselves (i.e. who they follow, who follows them), as well as interactions with these posts, such as replies or retweets. Therefore, we semantically link fact-checked claims not only to the social network posts, but also to this additional information. Further, given that misinformation is a global challenge, a useful dataset should not be limited to a single language, and should contain data in as many languages as possible.</p><p>Further, most misinformation datasets consist of a single data dump which, given the dynamic nature of the problem, means that datasets can become outdated. Therefore, we open source our data collection and linking infrastructure which connects claims to social networks, MuMiN-trawl, in order to provide a platform for future research to continue to build and extend our work.</p><p>We see the goal of an automatic misinformation detection system as a tool that can help people identity misinformation so that they can act on it accordingly. Considering that a lot of the misinformation today is spread on social media networks, such a system should be able to retrieve, connect and utilise the information in these networks to identify misinformation as accurately as possible. This is the core rationale behind our proposed two tasks, which we further discuss in Section 6.2:</p><p>(1) Determine the veracity of a claim, given its social network context. (2) Determine the likelihood that a social media post to be factchecked is discussing a misleading claim, given its social network context. To this end, we present MuMiN, which addresses the limitations of existing work. In summary, our main contributions are as follows:</p><p>? We release a graph dataset, MuMiN, containing rich social media data (tweets, replies, users, images, articles, hashtags) spanning 21 million tweets belonging to 26 thousand Twitter threads, each of which have been semantically linked to 13 thousand fact-checked claims across dozens of topics, events and domains, in 41 different languages, spanning more than a decade. ? We release the data collection and linking system, MuMiN-trawl, which was used to build the MuMiN dataset. ? We release a Python package, mumin, which eases the compilation of the dataset as well as enabling easy export to the Deep Graph Learning framework <ref type="bibr" target="#b40">[41]</ref>. ? We propose two representative tasks involving claims and social networks. We provide baseline results considering both text-only models, image-only models as well as using a heterogeneous graph neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>There is a number of publicly available datasets on the topic of misinformation. Some datasets have a narrow focus on topics, for example on politics <ref type="bibr" target="#b41">[42]</ref>, COVID-19 <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b22">23]</ref>, or celebrities <ref type="bibr" target="#b31">[32]</ref>, but others, such as the PHEME5 <ref type="bibr" target="#b45">[46]</ref> and PHEME9 <ref type="bibr" target="#b18">[19]</ref> datasets, have explicitly sought to include data from different events, although typically with much smaller numbers of events, claims and tweets than MuMiN.</p><p>A popular approach is to make extensive use of fact-checking websites in order to provide ground truth labels for misinformation datasets. Popular fact checking data sources include PolitiFact 3 , which has been used by numerous datasets such as Liar <ref type="bibr" target="#b41">[42]</ref>, which has around twelve thousand facts of a political nature and associated short statements. Others have used PolitiFact instead with news articles, such as those from FakeNewsNet <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>. A more recent dataset from FakeNewsNet, <ref type="bibr" target="#b35">[36]</ref>, links articles to claims, along with tweets sharing the referenced articles. In both cases, the number labelled news articles are, again, fewer than MuMiN, with 240 news articles labelled in <ref type="bibr" target="#b36">[37]</ref> and 1,056 news articles labelled in <ref type="bibr" target="#b35">[36]</ref>.</p><p>Other work has sought to extend the number of fact-checking organisations used to construct datasets such as the CoAID <ref type="bibr" target="#b8">[9]</ref> and MM-COVID <ref type="bibr" target="#b22">[23]</ref> datasets, which contain claims from 6 and 97 fact checking organisations, respectively. Of the 97 used by MM-COVID, 96 of them come from the Poynter fact-checking network of factchecking organisations. In total they have over 4,000 claims. MM-COVID is the first to address the monolingual problem with existing datasets by including data in 6 different languages, albeit on a single topic. This dataset is perhaps the most related work to ours in that it addresses several of the problems we outline. However, our dataset, MuMiN, is significantly larger in terms of the number of claims, tweets, languages and indeed topics, as we do not limit our dataset to COVID-19 related misinformation. Further, we use a more sophisticated procedure to link claims to tweets; the MM-COVID dataset links an article to social media posts by searching for the URL, title and first sentence of the article on Twitter, while our dataset performs linking based on cosine similarity between transformer based embeddings of the claims, tweets and articles.</p><p>One important aspect of this line of research to consider is around evidence based fact checking approaches. This line of research seeks to utilise available evidence, such as online news sources, Wikipedia, as well as social networks, in order to classify a claim as true or  <ref type="bibr" target="#b32">[33]</ref> proposes a system to replace manual verification of claims with such a system. Using only the text of the claim, they retrieve online search results using the text, and use linguistic features of the resulting articles, as well as the reliability of the sources in order to classify a claim. To deal with the variability in labels given by fact checkers, such as 'mostly true' or 'partially false', they map 'mostly' to true, and ignore those of a 'partial' veracity. Another weakness of these approaches are that they tend to be limited in the fact checkers that they utilise for their sources. The MultiFC dataset <ref type="bibr" target="#b0">[1]</ref> seeks to address this by including claims from 26 fact checking sources, in English, producing a dataset of 34,918 claims. While they do include extra context and metadata, they do not include additional modalities (such as images), nor do they include social network data. Recent work by Hanselowski et al. <ref type="bibr" target="#b15">[16]</ref> released a dataset, that while containing documents retrieved from different domains, such as blogs, social networks and news websites, as with other work in this area, theirs has a strong focus on text, overlooking the relevant and rich information contained in other modalities such as images, videos and social graphs of interaction. The same sentiment applies also to the very recent X-FACT dataset <ref type="bibr" target="#b13">[14]</ref>, that while multilingual (25 languages), contains only text data.</p><p>While significant attention has been paid to the use of fact checking organisations as a source of ground truth for claim veracity and verification, there has also been work studying artificial claims. One such dataset is FEVER <ref type="bibr" target="#b38">[39]</ref> which consists of 185,445 claims created by manipulating sentences from Wikipedia, and then annotated manually into one of three categories, supported, refuted or not enough information. Also using Wikipedia is the HoVer dataset <ref type="bibr" target="#b16">[17]</ref>, which addresses a weakness of the FEVER dataset, in that to verify claims, often a single Wikipedia article is not enough, and often requires multiple sources, or 'hops'. In HoVer, claims can require evidence from up to four Wikipedia articles. However, HoVer is still a monolingual dataset with an emphasis on text data, differing significantly from MuMiN, which considers multiple modalities across multiple languages as important characteristics to consider for this problem.</p><p>See <ref type="table" target="#tab_1">Table 2</ref> for an overview of these datasets, which demonstrates the key differences between them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DATASET CREATION</head><p>The dataset creation consists of two parts, one concerning the claims and their fact-checked verdicts, and the second part concerning the collection of the surrounding social context. As described in Section 1, this lends itself to two application tasks, the first being, given a claim, predict its veracity given the social context. The second being, given a Twitter post to be fact-checked and its social context, predict the veracity of the claim made in the Twitter post.</p><p>The general strategy is to collect claims as spatiotemporally diverse as possible, and to collect as many high-quality social features surrounding these as possible. The dataset creation was performed using MuMiN-trawl on a single workstation with an Intel Core i9-9900K CPU, 64GB of RAM, with two Nvidia 2080Ti GPUs, with the collection taking several months. Baseline results were produced on the same workstation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Claims</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Data Collection.</head><p>For the collection of fact-checked claims we utilise the Google Fact Check Tools API 4 , which is a resource that collects fact-checked claims from fact-checking organisations around the world. This API was also used in <ref type="bibr" target="#b34">[35]</ref> to create a dataset for automatic misinformation detection, but our aim was to collect a much larger amount of claims that were sufficiently diverse, both in terms of content and language.</p><p>We started by querying the API for the queries coronavirus and covid to ensure that we got results from active fact-checking organisations. To ensure language diversity, we used the Google Translation API 5 to translate the two queries into 70 languages(see the appendix for a list of all the languages). We then queried the Fact Check API for up to 1,000 fact-checked claims for each of the resulting 132 queries.</p><p>From the collected fact-checked claims, we collected all the factchecking organisations involved, resulting in a list of 115 factchecking organisations(see the appendix for a list of all the organisations). From this list, we scraped all the fact-checked claims from each of them, from the fact-checking organisation's inception up until present day. This resulted in 128,070 fact-checked claims.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Data</head><p>Processing. The claim data collected from the procedure in Section 3.1.1 also included various metadata, and we extracted the following: (1) source: The source of the claim, which can be both names of people as well as generic descriptions such as "multiple social media posts"; (2) reviewer: The URL of the fact-checking website that reviewed the claim; (3) language: The language the claim was uttered or written in; (4) verdict: The fact-checking organisation's verdict; (5) date: The date the claim was uttered. If this date was not available then the date of the review was used. If neither of those two were available then we extracted a potential date from the URL of the fact-checking article using the regular expression [0-9]{4}-[0-9]{2}-[0-9]{2} 6 . Note, from this, we release only the date, keywords from the claim, the predicted verdict </p><formula xml:id="formula_0">? ? ? ? ? ? UPFD-POL [11] 314 40,740 ? ? ? ? ? ? UPFD-GOS [11] 5,464 308,798 ? ? ? ? ? ? X-FACT [14] 31,189 ? ? ? MuMiN 12,914 21,565,018 ? ? ? ? ? ? ? ?</formula><p>(using the verdict classification model described below) and the reviewer involved.</p><p>The first challenge is that the verdict is unstructured freetext and can be written in any language at any length. To remedy this, we trained a 'verdict classifier', a machine learning model that classifies the freetext verdicts into three pre-specified categories: misinformation, factual and other. Towards this, we manually labelled 2,500 unique verdicts. Aside from the simple classifications such as labelling "false" and "misleading" as misinformation, and labelling "true" and "correct" as factual, we adhered to the following labelling guidelines. In the cases where the claim was "mostly true", we decided to label these as factual. When the claim was deemed "half true" or "half false" we labelled these as misinformation, with the justification that a statement containing a significant part of false information should be deemed as being misleading. When there was no clear verdict then the verdict was labelled as other. This happens when the answer is not known, or when the verdict is merely discussing the issue rather than assessing the veracity of the claim. The claims with the other label were not included in the final dataset.</p><p>To be able to properly deal with the multilingual verdicts, we attempted two approaches: (1) Translate them into English and use a pre-trained English language model; (2) Embed them using a pre-trained multilingual language model.</p><p>For the first approach, we used the Google Cloud Translation API 7 to translate the verdicts into English and train a model to classify these translated verdicts. The verdict classifier is based on the roberta-base model <ref type="bibr" target="#b23">[24]</ref>, with an attached classification module. This classification module consists of 10% dropout, followed by a 7 See https://cloud.google.com/translate/docs/reference/rest/. linear layer with hidden size 768, a tanh activation, another 10% dropout layer, and finally a projection down to the three classes.</p><p>The model was trained on the dataset further augmented with casing. Specifically, we converted all the verdicts in the training set to lower case, upper case, title case and first letter capitalised. This resulted in a training dataset of 10,000 verdicts. We manually labelled 1,000 further verdicts to use as the test dataset. These verdicts were not deduplicated, to ensure that their distribution matches the true one. The model was trained for 10 epochs using the AdamW optimizer <ref type="bibr" target="#b24">[25]</ref> with a learning rate of 2e-5, and a batch size of 8. <ref type="bibr" target="#b7">8</ref> The model achieved a macro-F1 score of 0.99 among the misinformation and factual classes, and 0.92 if the other class is included.</p><p>For the second multilingual approach, we augmented the original (multilingual) verdicts, both by translating all of the 2,500 unique verdicts into 65 languages, using the Google Cloud Translation API 9 as well as applying the casing augmentation as described above, as finetuning a multilingual model directly on the original verdicts resulted in poor performance for the minority languages. The resulting dataset consisted of roughly 5 million verdicts, and we finetuned the xlm-roberta-base model <ref type="bibr" target="#b7">[8]</ref> for 4 epochs on the dataset with the same hyperparameters as the model trained on the English-only dataset. On the same test dataset of 1,000 multilingual verdicts, this multilingual model achieved a macro-average F1-score of 0.98 among the misinformation and factual classes, and 0.85 if the other class is included.</p><p>As the English-only model was marginally better than the multilingual model, we opted to use that in building the dataset. However, we appreciate the convenience of not having to translate the verdicts, so we release both the English-only and multilingual verdict classifiers on the Hugging Face Hub <ref type="bibr" target="#b9">10</ref> .</p><p>See <ref type="table" target="#tab_2">Table 3</ref> for some examples of the verdicts and resulting predicted verdicts. With the performance satisfactory, we then used the model to assign labels to all of the plaintext verdicts in the dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Twitter</head><p>From the claims and verdicts, we next collected relevant social media data. This data was collected from Twitter 11 using their Academic Search API 12 , where we aimed to collect as many relevant Twitter threads that shared and discussed content related the claims obtained through the method described in Section 3.1.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Data Collection.</head><p>From each claim, we first extracted the top 5 keyphrases 13 , with a keyphrase being either one or two words from the claim, whose associated sentence embedding has a large cosine similarity to the sentence embedding of the entire claim. We then queried the Twitter Academic Search API for the first 100 results for each of the five keyphrases, where we imposed that the tweets should not be replies <ref type="bibr" target="#b13">14</ref> , they had to share either a link or an image <ref type="bibr" target="#b14">15</ref> and they had to have been posted at most three days prior to the claim date and at the latest on the claim date itself. The idea behind this is to obtain as high a recall as possible, i.e. obtaining as many of the potentially relevant tweets as possible, from which we can filter the irrelevant tweets. From the Twitter API we requested the tweets, users as well as media shared in the tweets. This resulted in approximately 30 million tweets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Data</head><p>Processing. As our goal with a automatic misinformation detection system is to be able to act on stories shared on social media before they go viral, we decided to filter the tweets to only keep the ones that have gained a reasonable amount of interest. We measure such interest in terms of 'retweets', and we chose a minimum of 5 retweets to be a conservative threshold, which reduced the number of tweets by 90%, leaving about 2.5 million tweets.</p><p>From these we then extracted all the URLs and hashtags from the tweets, as well as from the descriptions of each user. If the URL pointed to an article, we downloaded the title, body and top image <ref type="bibr" target="#b15">16</ref> . We also extracted the hyperlinks of all images shared by the tweets. All of this information was then populated in a graph database <ref type="bibr" target="#b16">17</ref> with approximately 17 million nodes and 50 million relations. Uniqueness constraints were imposed on all nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Data Linking</head><p>From the database of tweets, the next task was to find all the Twitter threads that were relevant to each claim. As the tweets, claims and articles were multilingual, we again had to make the same choice as in Section 3.1.2: either embed the texts with a multilingual language model, or translate them to English and use an English language model. We did both, with the translation being vastly superior, as the multilingual model seemed to "collapse" on texts in certain languages like Thai, Tamil, Telugu, Bengali and Malayalam. Translating texts always comes with a risk of losing context, but as our goal was to find tweets that were discussing a claim at hand, we argue that a translated text will still be able to carry that information. The translation was done with the Google Translation API, as with the verdicts in Section 3.1.1.</p><p>Prior to embedding the approximately one million articles we first summarised the concatenated title and abstract, using the BART-large-CNN transformer <ref type="bibr" target="#b20">[21]</ref>. This was done to enable embedding of additional tokens from the article content, as the Sentence Transformers have a limit of 512 tokens with the summarisation model being able to process 1,024 tokens. We then embedded these summarised articles along with the claims and tweets, all using the paraphrase-mpnet-base-v2 Sentence Transformer <ref type="bibr" target="#b33">[34]</ref>.</p><p>Computing cosine similarities between every tweet-claim and article-claim pair would be computationally unfeasible. Instead, we grouped the claims in batches of size 100, fetched all the tweets and articles that were posted from three days prior to one of the claims in the batch up until the claim date, and computed cosine similarities between these 18 .</p><p>The resulting cosine similarity distribution can be found in the appendix.We decided to release three datasets, corresponding to the three thresholds 0.7, 0.75 and 0.8. These thresholds were chosen based on a qualitative evaluation of a subset of the linked claims; see examples of such linked claims at various thresholds in <ref type="table" target="#tab_3">Table 4</ref>. The lower threshold dataset is of course larger, but also contains more label noise, whereas the higher threshold dataset is considerably smaller, but with higher quality labels. See various statistics of these datasets in <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Data Enrichment</head><p>From the resulting Twitter posts linked to the claims as described in Section 3.3, we next queried Twitter for the surrounding context of these posts. We retrieved a sample of 100 users that retweeted  the tweets, 100 users who followed the authors of the tweets, 100 users who were followed by the authors of the tweets, 500 users who posted a reply to the tweets and all users who was mentioned in the tweets. For each of these users, we queried Twitter for their recent 100 tweets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DATASET DESCRIPTION</head><p>Given the scale and diversity of the data collected, it is not possible to succinctly provide a thorough analysis, which we leave to future work, and other researchers interested in exploring and using our dataset. Nonetheless, we will provide a preliminary analysis of various aspects of the dataset. As mentioned in Section 3.3, we release three datasets, corresponding to the cosine similarity thresholds 0.7, 0.75 and 0.8. The statistics of the datasets can be found in <ref type="table" target="#tab_0">Table 1</ref>. Note the heavy class imbalance of the datasets, which is likely due to the fact that fact-checking organisations are more interested in novel claims, and these tend to favour misinformation <ref type="bibr" target="#b39">[40]</ref>. A common way to fix this issue <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b22">23]</ref> is to collect news articles from "trusted sources" and use tweets connected to these as a means to increase the factual class. However, as these will likely arise from a different distribution than the rest of the datasets (they might not be novel claims, say), we decided against that and left the dataset as-is. We have instead released the source code we used to collect the dataset, MuMiN-trawl, which can be used to collect extra data, if needed <ref type="bibr" target="#b18">19</ref> .</p><p>To adhere to the terms and conditions of Twitter, the dataset will only contain the tweet IDs and user IDs, from which the tweets and the user data can be collected via the Twitter API using our mumin package (see <ref type="bibr">Section 5)</ref>. Further, to comply with copyright restrictions of the fact-checking websites, we do not release the claims themselves. Instead, we release keyphrases, obtained as described in Section 4.1. The datasets thus contain the tweet IDs, user IDs and claim keywords, as well as the POSTED, MENTIONS, FOLLOWS, DISCUSSES and IS_REPLY_TO relations, shown in <ref type="figure" target="#fig_0">Figure 1</ref>. From these, the remaining part of the dataset can be built by using our mumin package, see Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Claim Topic Clusters</head><p>We performed clustering on embeddings of the claim text in order to extract higher level topics or events from the claims. Using a UMAP <ref type="bibr" target="#b27">[28]</ref> projection of embeddings of the claims and HDBSCAN <ref type="bibr" target="#b25">[26]</ref>, a hierarchical density based clustering algorithm, we were able to discover 26 clusters based on the claim text. We optimized the hyperparameters of the projection as well as clustering algorithms <ref type="bibr" target="#b19">20</ref> , achieving a silhouette coefficient of 0.28. The clusters can be seen in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>To provide context for each cluster, we concatenated the claims in each cluster and extracted keyphrases from each cluster <ref type="bibr" target="#b20">21</ref> . From these, it is apparent that the claims can be clustered into diverse topics, ranging from COVID-19 (a cluster of approximately half of all claims), to topics ranging from natural disasters to national and international political and social events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">THE MUMIN PACKAGE</head><p>As we can only release the tweet IDs and user IDs to adhere to Twitter's terms of use, we have built a Python package, mumin, to enable compilation of the dataset as easily as possible. The package can be installed from PyPI using the command pip install mumin, and the dataset can be compiled as follows:</p><p>&gt;&gt;&gt; from mumin import MuminDataset &gt;&gt;&gt; dataset = MuminDataset(bearer_token, size='small') &gt;&gt;&gt; dataset.compile() Here bearer_token is the Twitter API bearer token, which can be obtained from the Twitter API website. The size argument determines the size of the dataset to load and can be set to 'small', 'medium' or 'large'. Further, there are many arguments included in the MuminDataset constructor which controls what data to include in the dataset. For instance, one can set include_tweet_images to False to not include any images <ref type="bibr" target="#b21">22</ref> . With the dataset compiled, the graph nodes can be accessed through dataset.nodes and the relations can be accessed through dataset.rels. A convenience method dataset.to_dgl returns a heterogeneous graph object to be used with the DGL library <ref type="bibr" target="#b40">[41]</ref>.</p><p>We have built a tutorial on how to use the compiled dataset, including building different classifiers. We also release the source code for the mumin package 23 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">MODEL PERFORMANCE 6.1 Dataset Splits</head><p>To enable consistent benchmarking on the dataset, we provide trainval-test splits of the data. These have been created such that the splits are covering distinct events, identified by the claim clusters in Section 4.1. This is done as follows. We start by sorting all the claim clusters by size, in ascending fashion. We next add clusters into the validation set until at least 10% of the claims have been included. We then add clusters into the test set until at least 10% of the claims have been included, and the remaining clusters constitutes the training set. Statistics for each of the splits can be found in <ref type="table" target="#tab_4">Table 5</ref>, which shows that we still roughly maintain the label balance throughout all the dataset splits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Baseline Models</head><p>The MuMiN dataset lends itself to several different classification tasks, relating the various modalities to the verdicts of the associated claims (misinformation or factual). As mentioned in Section 1, we have chosen to provide baselines related to the following two tasks:</p><p>(1) Given a claim and its surrounding subgraph extracted from social media, predict whether the verdict of the claim is misinformation or factual. We name this task "claim classification". (2) Given a source tweet (i.e., not a reply, quote tweet or retweet) to be fact-checked, predict whether the tweet discusses a claim whose verdict is misinformation or factual. We name this task "tweet classification".</p><p>We implement several baseline models to demonstrate the predictive power of the different modalities for these tasks. Firstly, we implement the LaBSE transformer model from <ref type="bibr" target="#b11">[12]</ref> with a linear classification head, and apply this model directly to the claims and the source tweets, respectively. We also benchmark a version of this model where the transformer layers are frozen, and name this model LaBSE-frozen. Secondly, we implement the vision transformer (ViT) model from <ref type="bibr" target="#b9">[10]</ref>, also with a linear classification head, and apply this to the subset of the tweets that include images (preserving the same train/val/test splits).</p><p>As for a graph baseline, we implement a heterogeneous version of the GraphSAGE model from <ref type="bibr" target="#b14">[15]</ref>, as follows. For each node, we sample 100 edges of each edge type connected to it (in any direction), process each of the sampled neighbouring nodes through a GraphSAGE layer, and sum the resulting node representations. Finally, layer normalisation <ref type="bibr" target="#b1">[2]</ref> is applied to the aggregated node representations. The baseline model contains two of these graph layers. This graph baseline is trained on MuMiN without profile images, article images and timelines (i.e., tweets that users in our graph have posted, which are not directly connected to any claim) <ref type="bibr" target="#b23">24</ref> . We call this baseline model HeteroGraphSAGE.</p><p>See <ref type="table">Table 6</ref> and 7 for an overview of the performance of each of these models. We see that both tasks are really challenging, with the HeteroGraphSAGE model achieving the best performance overall, but with the text-only LaBSE model not far behind. We note that the HeteroGraphSAGE model only makes two "hops" through the graph, meaning that it is not able to capture all the information that is present in the graph. Increasing the number of hops resulted in poorer performance, which is the well-known "oversmoothing" problem <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b44">45]</ref>.</p><p>We have created an online leaderboard containing the results of these baselines and invite researchers to submit their own models. We release all the source code we used to conduct the baseline experiments. <ref type="bibr" target="#b24">25</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION 7.1 Representative Data Splits</head><p>In the field of automatic misinformation detection, the splitting of the dataset into training/validation/test datasets is usually done uniformly at random <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b45">46]</ref>. However, we argue that the main purpose of such a system is to be able to handle new <ref type="table">Table 6</ref>: Baseline test performance on the claim classification task, measured in macro-average F1-score (larger is better). Best result for each dataset marked in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>MuMiN  events in which misinformation occurs, and therefore our dataset splits should reflect this. We conduct an experiment in which we analyse the performance differences of the baseline models if we had split the MuMiN dataset at random. Concretely, we repeat two of our baselines on the random splits: the LaBSE classifier and the HeteroGraphSAGE model. Call the resulting models LaBSE-random and HeteroGraphSAGE-random.</p><p>On the tweet classification task, the LaBSE-random model achieved a macro-average F1-score of 71.10% and 73.6% on MuMiN-small and MuMiN-medium, respectively. The HeteroGraphSAGE-random model achieved 74.90%, 89.50% and 79.80% on MuMiN-small, MuMiNmedium and MuMiN-large, respectively. We see that the scores are drastically higher for these "random models" on this task compared to the results of the baseline models, as can be seen in <ref type="table" target="#tab_6">Table 7</ref>.</p><p>For the claim classification task, the LaBSE-random model achieved a macro-average F1-score of 58.85%, 62.80% and 61.50% on MuMiN-small, MuMiN-medium and MuMiN-large, respectively. On this task, the HeteroGraphSAGE-random model achieved 48.50%, 61.40% and 62.55%, respectively. There is not as big of a difference between these "random models" and our baselines as with the tweet classification task, as can be seen from <ref type="table">Table 6</ref>, but the results are still marginally better than the baseline models.</p><p>This shows that having realistic splits of the dataset is important to guide our algorithm development in the right direction, especially in the field of automated misinformation detection, where we are interested in generalisability to new real-world events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Limitations</head><p>Due to the automated linking procedure between facts and tweets, and facts and articles, erroneous labels exist. Nonetheless, this can be somewhat addressed by selecting higher similarity thresholds, as can be seen in <ref type="table" target="#tab_3">Table 4</ref>. We did not make any judgements as to the impartiality or correctness of any of the verdicts provided by the fact-checking organisations. Therefore, this dataset may contain verdicts (labels) that are contentious or inaccurate. As a potential remedy, we provide the fact-checking organisation responsible for each claim and verdict. As the verdicts of each claim from fact-checking organisations are provided in unstructured freetext, we resorted to a machine learning model to classify each verdict into one of three categories, factual, misinformation or other. While we obtained a high performance on a test set, it is likely some verdicts may have been misclassified. As we do not distribute raw social network data, but instead provide code to retrieve it, this means that the dataset is truly dynamic such that if a user deletes a tweet or their account, their data will not be retrievable from the Twitter API. This makes reproducible research, if it involves the contents of the social network data, challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Ethical Considerations</head><p>It is accepted that there are online harms associated with misinformation. Unfortunately, when the number of posts made on social networks daily is considered, the problem exists at a scale where manual curation is exceptionally difficult, thus motivating the use of automated methods to assist in the detection of misinformation online. These methods tend to utilise machine learning, and therefore typically require the collection of large amounts of data upon which to train the model. While the goal with such data collection is to combat an online harm, there is, understandably, ethical considerations related to the potential harms caused from the collection and use of large online datasets of social data, text data, and media data. A major factor for consideration is with respect to the collection of social network data, and the fact that this data is generated from users of the social network. The data collected in this dataset consists of only public Twitter data, accessed through the official Twitter Academic API. While the users of Twitter, in making their posts public, may expect their posts to be visible, in accordance with the Twitter developer terms, we do not include the raw collected data. Instead, we make available only tweet and user IDs, with associated code to 'hydrate' them (i.e. retrieve the full tweet and user data). Therefore, if a user deletes a tweet, or deletes their account, it will be no longer possible to retrieve the deleted data from what we have released. Thus, while we expect that data may disappear over time as a result, this trade-off is required. The ethics of this work has been approved, both by the University of Bristol Faculty of Engineering Research Ethics Committee (ref: 116665), as well as by the Ethics Board at the National Research Centre on Privacy, Harm Reduction and Adversarial Influence Online (REPHRAIN).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">LICENSES</head><p>We release the three versions of the MuMiN under the Creative Commons Attribution-NonCommercial 4.0 International license (CC BY-NC 4.0). The code, which includes the mumin package, the data collection and linking system MuMiN-trawl, as well as the repository containing the baselines, are all released under the MIT license. <ref type="bibr" target="#b25">26</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">CONCLUSION</head><p>In this paper we presented MuMiN, which consists of a large scale graph misinformation dataset that contains rich social media data (tweets, replies, users, images, articles, hashtags) spanning 21 million tweets belonging to 26 thousand Twitter threads, each of which have been semantically linked to 13 thousand fact-checked claims across dozens of topics, events and domains, in 41 different languages, spanning more than a decade. We also presented a data collection and linking system, MuMiN-trawl. The freetext multilingual verdicts were categorised into the consistent categories of factual or misinformation, using a finetuned transformer model which we also release. We further developed a Python package, mumin, which enables simple compilation of the MuMiN as well as providing easy export to Python graph machine learning libraries. Finally, we proposed and provided baseline results for two node classification tasks; a) predicting the veracity of a claim from its surrounding social context, and b) predicting the likelihood that a tweet to be fact-checked discusses a misleading claim. The baselines include text-only and image-only approaches, as well as a heterogeneous graph neural network. We showed that the tasks are challenging, with the highest macro-average F1-score being 62.55% and 61.45% for the two tasks, respectively. The data, along with tutorials and a leaderboard, can be found at https://mumin-dataset.github.io/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A SUPPLEMENTARY TABLES AND FIGURES</head><p>In <ref type="table" target="#tab_7">Table 8</ref> we show the 70 languages that were queried during our data collection, and the resulting 41 languages, in bold, that made it to the final dataset. In <ref type="table" target="#tab_0">Table 12</ref> we show the 115 fact checking organisations whose fact checked claims have made it into the dataset. In <ref type="table" target="#tab_8">Table 9</ref>, <ref type="table" target="#tab_0">Table 10</ref> and <ref type="table" target="#tab_0">Table 11</ref> we show language statistics across the large, medium and small versions of the dataset, respectively. In <ref type="figure" target="#fig_2">Figure 3</ref> we show the cosine similarity distribution of the tweet-claim pairs.      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The graph schema of the MuMiN dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>UMAP projection of the claim text embeddings. The large cluster on the right corresponds to COVID-19 related claims.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The distribution of cosine similarities among tweet-claim pairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The statistics of the three datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="7">#Claims #Threads #Tweets #Users #Articles #Images #Languages %Misinfo</cell></row><row><cell>MuMiN-large</cell><cell cols="4">12,914 26,048 21,565,018 1,986,354 10,920</cell><cell>6,573</cell><cell>41</cell><cell>94.79%</cell></row><row><cell cols="2">MuMiN-medium 5,565</cell><cell cols="3">10,832 12,659,371 1,150,259 4,212</cell><cell>2,510</cell><cell>37</cell><cell>94.20%</cell></row><row><cell>MuMiN-small</cell><cell>2,183</cell><cell>4,344</cell><cell>7,202,506 639,559</cell><cell>1,497</cell><cell>1,036</cell><cell>35</cell><cell>92.71%</cell></row><row><cell>false. Research by Popat et al.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>An overview of publicly available datasets for automatic misinformation detection, ordered by release date. Here ? indicates that the tweet content is not available but that the related users are, and parentheses indicate that it only holds for a subset of the dataset.</figDesc><table><row><cell>Dataset</cell><cell>#Facts</cell><cell>#Tweets</cell><cell cols="6">Verified Multilingual Multitopical Articles Images User Social Replies</cell></row><row><cell>MediaEval15 [4]</cell><cell>413</cell><cell>15,821</cell><cell>?</cell><cell>?</cell><cell></cell><cell>?</cell><cell>?</cell></row><row><cell>MediaEval16 [5]</cell><cell>542</cell><cell>18,049</cell><cell>?</cell><cell>?</cell><cell></cell><cell>?</cell><cell>?</cell></row><row><cell>Liar [42]</cell><cell>12,836</cell><cell></cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell>?</cell></row><row><cell>Weibo [18]</cell><cell></cell><cell>9,528</cell><cell>?</cell><cell>?</cell><cell></cell><cell>?</cell><cell>?</cell></row><row><cell>PHEME5 [46]</cell><cell></cell><cell>5,802</cell><cell>?</cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell>?</cell></row><row><cell>FNN-BuzzFeed [37]</cell><cell>182</cell><cell></cell><cell>?</cell><cell></cell><cell>?</cell><cell></cell><cell>?</cell><cell>?</cell></row><row><cell>FNN-PolitiFact17 [37]</cell><cell>240</cell><cell></cell><cell>?</cell><cell></cell><cell>?</cell><cell></cell><cell>?</cell><cell>?</cell></row><row><cell>PHEME9 [19]</cell><cell></cell><cell>6,425</cell><cell>?</cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell>?</cell></row><row><cell>Celebrity [32]</cell><cell>200</cell><cell></cell><cell>?</cell><cell></cell><cell>?</cell><cell></cell><cell></cell></row><row><cell>FakeNewsAMT [32]</cell><cell>240</cell><cell></cell><cell></cell><cell>?</cell><cell>?</cell><cell></cell><cell></cell></row><row><cell>FEVER [39]</cell><cell>185,445</cell><cell></cell><cell></cell><cell>?</cell><cell></cell><cell></cell><cell></cell></row><row><cell>AFCSDC [3]</cell><cell>422</cell><cell></cell><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell><cell></cell></row><row><cell>UKP Snopes [16]</cell><cell>6,422</cell><cell></cell><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell><cell></cell></row><row><cell>MultiFC [1]</cell><cell>34,918</cell><cell></cell><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell><cell></cell></row><row><cell>HoVer [17]</cell><cell>26,000</cell><cell></cell><cell></cell><cell>?</cell><cell></cell><cell></cell><cell></cell></row><row><cell>FNN-PolitiFact20 [36]</cell><cell>1,056</cell><cell>564,129</cell><cell>?</cell><cell></cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>FNN-GossipCop [36]</cell><cell>22,140</cell><cell>1,396,548</cell><cell>?</cell><cell></cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>CoAID [9]</cell><cell>4,251</cell><cell>160,667</cell><cell>(?)</cell><cell></cell><cell>?</cell><cell></cell><cell></cell><cell>?</cell></row><row><cell>MM-COVID [23]</cell><cell>11,565</cell><cell>105,300</cell><cell>(?)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Sample predictions of the verdict classifier.</figDesc><table><row><cell>factual</cell><cell>misinformation</cell><cell>other</cell></row><row><cell>True</cell><cell>False</cell><cell>Satire</cell></row><row><cell>Correct Attribution</cell><cell>Misleading</cell><cell>Landmarks</cell></row><row><cell>Broadly correct.</cell><cell>Mostly false</cell><cell>Questionable</cell></row><row><cell>According to the most recent data, this is about right</cell><cell>Pants on fire</cell><cell>More complex than that</cell></row><row><cell>This is correct for relative poverty in the UK, measured after housing costs in 2015/16. It's a smaller other measures of poverty.</cell><cell>Three Pinocchios</cell><cell>This video filmed in Equatorial Guinea shows a student attacking one of his teachers</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Examples of claim-article linking.</figDesc><table><row><cell>Translated Claim</cell><cell>Translated Title</cell><cell>Article URL</cell><cell>Similarity</cell></row><row><cell>Google removed the term "Palestine" from Google Maps</cell><cell>Google and Apple remove Palestine from their maps</cell><cell>https://bit.ly/mumi-3</cell><cell>84.93%</cell></row><row><cell>China loses control of part</cell><cell>Heads Up! A Used Chinese</cell><cell></cell><cell></cell></row><row><cell>of its space rocket, and it</cell><cell>Rocket Is Tumbling Back to</cell><cell>https://bit.ly/mumi-4</cell><cell>80.47%</cell></row><row><cell>will soon fall to Earth.</cell><cell>Earth This Weekend.</cell><cell></cell><cell></cell></row><row><cell>Photo shows Aung San Suu Kyi being detained during a 1, 2021 military coup on February</cell><cell>Myanmar's army detains Aung leaders in a possible coup San Suu Kyi and government</cell><cell>https://bit.ly/mumi-5</cell><cell>75.03%</cell></row><row><cell>One of the nurses who made the</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Pfizer-BioNtech vaccine immediately</cell><cell></cell><cell></cell><cell></cell></row><row><cell>fainted from a side effect of the vaccine. Also, the nurse who</cell><cell>Live Nurse Faints After Being Vaccinated Against Covid-19!</cell><cell>https://bit.ly/mumi-6</cell><cell>70.29%</cell></row><row><cell>fainted after having just been</cell><cell></cell><cell></cell><cell></cell></row><row><cell>vaccinated is dead.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Americans Need WHO COVID-19</cell><cell>'Vaccine passport' will define</cell><cell></cell><cell></cell></row><row><cell>Vaccine Card for International</cell><cell>tourism in the world, but</cell><cell>https://bit.ly/mumi-7</cell><cell>65.30%</cell></row><row><cell>Travel</cell><cell>countries bar some immunizers</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Dataset split statistics</figDesc><table><row><cell>Dataset</cell><cell>%Train %Val</cell><cell cols="7">%Test %MisinfoTrain %MisinfoVal %MisinfoTest #ClustersTrain #ClustersVal #ClustersTest</cell></row><row><cell>MuMiN-large</cell><cell cols="2">78.52% 11.39% 10.09%</cell><cell>94.37%</cell><cell>96.73%</cell><cell>95.92%</cell><cell>8</cell><cell>21</cell><cell>8</cell></row><row><cell cols="3">MuMiN-medium 76.98% 11.61% 11.41%</cell><cell>93.79%</cell><cell>96.73%</cell><cell>94.46%</cell><cell>7</cell><cell>18</cell><cell>7</cell></row><row><cell>MuMiN-small</cell><cell cols="2">77.90% 11.35% 10.75%</cell><cell>91.82%</cell><cell>97.15%</cell><cell>94.42%</cell><cell>7</cell><cell>15</cell><cell>6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Baseline test performance on the tweet classification task, measured in macro-average F1-score (larger is better). Best result for each dataset marked in bold. Note that the ViT model is only trained and evaluated on the subset of the tweets containing images.</figDesc><table><row><cell>Model</cell><cell cols="3">MuMiN-small MuMiN-medium MuMiN-large</cell></row><row><cell>Random</cell><cell>37.18%</cell><cell>37.72%</cell><cell>36.90%</cell></row><row><cell>Majority class</cell><cell>48.77%</cell><cell>48.56%</cell><cell>48.87%</cell></row><row><cell>ViT</cell><cell>53.20%</cell><cell>52.00%</cell><cell>48.70%</cell></row><row><cell>LaBSE</cell><cell>54.50%</cell><cell>57.45%</cell><cell>52.80%</cell></row><row><cell>HeteroGraphSAGE</cell><cell>56.05%</cell><cell>54.10%</cell><cell>61.45%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>The 70 languages queried, with the 41 languages in bold present in the final dataset.</figDesc><table><row><cell cols="2">Amharic Georgian</cell><cell>Lithuanian</cell><cell>Sinhala</cell></row><row><cell>Arabic</cell><cell>German</cell><cell>Macedonian</cell><cell>Slovak</cell></row><row><cell cols="2">Armenian Greek</cell><cell>Malayalam</cell><cell>Slovenian</cell></row><row><cell cols="2">Azerbaijani Gujarati</cell><cell>Malay</cell><cell>Spanish</cell></row><row><cell>Basque</cell><cell cols="2">Haitian Creole Marathi</cell><cell>Swedish</cell></row><row><cell cols="2">Bengali Hebrew</cell><cell>Nepali</cell><cell>Tagalog</cell></row><row><cell>Bosnian</cell><cell>Hindi</cell><cell>Norwegian</cell><cell>Tamil</cell></row><row><cell cols="3">Bulgarian Hungarian Oriya</cell><cell>Telugu</cell></row><row><cell cols="2">Burmese Icelandic</cell><cell>Panjabi</cell><cell>Thai</cell></row><row><cell cols="3">Croatian Indonesian Pashto</cell><cell>Traditional Chinese</cell></row><row><cell>Catalan</cell><cell>Italian</cell><cell>Persian</cell><cell>Turkish</cell></row><row><cell>Czech</cell><cell>Japanese</cell><cell>Polish</cell><cell>Ukranian</cell></row><row><cell>Danish</cell><cell>Kannada</cell><cell>Portuguese</cell><cell>Urdu</cell></row><row><cell>Dutch</cell><cell>Kazakh</cell><cell>Romanian</cell><cell>Uyghur</cell></row><row><cell cols="2">English Khmer</cell><cell>Russian</cell><cell>Vietnamese</cell></row><row><cell cols="2">Estonian Korean</cell><cell>Serbian</cell><cell>Welsh</cell></row><row><cell cols="2">Filipino Lao</cell><cell>Simplified Chinese</cell><cell></cell></row><row><cell>Finnish</cell><cell>Latvian</cell><cell>Sindhi</cell><cell></cell></row><row><cell>French</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>The distribution of the top languages in the MuMiNlarge dataset.</figDesc><table><row><cell cols="4">Language Proportion #Claims %misinfo</cell></row><row><cell>English</cell><cell>42.88%</cell><cell>5,538</cell><cell>92.85%</cell></row><row><cell>Portuguese</cell><cell>10.98%</cell><cell>1,418</cell><cell>95.28%</cell></row><row><cell>Spanish</cell><cell>8.26%</cell><cell>1,067</cell><cell>95.41%</cell></row><row><cell>Hindi</cell><cell>6.16%</cell><cell cols="2">796 100.00%</cell></row><row><cell>Arabic</cell><cell>4.34%</cell><cell>560</cell><cell>95.18%</cell></row><row><cell>French</cell><cell>3.46%</cell><cell>447</cell><cell>97.99%</cell></row><row><cell>German</cell><cell>2.91%</cell><cell>376</cell><cell>97.61%</cell></row><row><cell>Indonesian</cell><cell>2.55%</cell><cell>329</cell><cell>99.70%</cell></row><row><cell>Italian</cell><cell>2.33%</cell><cell>301</cell><cell>89.37%</cell></row><row><cell>Bengali</cell><cell>2.26%</cell><cell cols="2">292 100.00%</cell></row><row><cell>Turkish</cell><cell>2.19%</cell><cell>283</cell><cell>95.41%</cell></row><row><cell>Polish</cell><cell>1.73%</cell><cell>224</cell><cell>83.48%</cell></row><row><cell>Other</cell><cell>9.93%</cell><cell>1,283</cell><cell>95.49%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>The distribution of the top languages in the MuMiNmedium dataset.</figDesc><table><row><cell cols="4">Language Proportion #Claims %misinfo</cell></row><row><cell>English</cell><cell>45.46%</cell><cell>2,530</cell><cell>92.29%</cell></row><row><cell>Portuguese</cell><cell>10.75%</cell><cell>598</cell><cell>96.49%</cell></row><row><cell>Spanish</cell><cell>7.82%</cell><cell>435</cell><cell>94.25%</cell></row><row><cell>Hindi</cell><cell>6.50%</cell><cell cols="2">362 100.00%</cell></row><row><cell>Arabic</cell><cell>4.40%</cell><cell>245</cell><cell>93.88%</cell></row><row><cell>French</cell><cell>3.61%</cell><cell>201</cell><cell>97.51%</cell></row><row><cell>Italian</cell><cell>3.04%</cell><cell>169</cell><cell>86.98%</cell></row><row><cell>German</cell><cell>2.57%</cell><cell>143</cell><cell>97.90%</cell></row><row><cell>Indonesian</cell><cell>2.07%</cell><cell cols="2">115 100.00%</cell></row><row><cell>Bengali</cell><cell>1.99%</cell><cell cols="2">111 100.00%</cell></row><row><cell>Turkish</cell><cell>1.90%</cell><cell>106</cell><cell>94.34%</cell></row><row><cell>Polish</cell><cell>1.40%</cell><cell>106</cell><cell>80.77%</cell></row><row><cell>Other</cell><cell>8.48%</cell><cell>472</cell><cell>97.03%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 :</head><label>11</label><figDesc>The distribution of the top languages in the MuMiNsmall dataset.</figDesc><table><row><cell cols="4">Language Proportion #Claims %misinfo</cell></row><row><cell>English</cell><cell>47.41%</cell><cell>1,035</cell><cell>90.34%</cell></row><row><cell>Portuguese</cell><cell>10.86%</cell><cell>237</cell><cell>97.47%</cell></row><row><cell>Spanish</cell><cell>7.42%</cell><cell>162</cell><cell>92.59%</cell></row><row><cell>Hindi</cell><cell>6.92%</cell><cell cols="2">151 100.00%</cell></row><row><cell>Arabic</cell><cell>4.90%</cell><cell>107</cell><cell>89.72%</cell></row><row><cell>Italian</cell><cell>4.49%</cell><cell>98</cell><cell>86.73%</cell></row><row><cell>French</cell><cell>3.71%</cell><cell>81</cell><cell>97.53%</cell></row><row><cell>Turkish</cell><cell>1.83%</cell><cell>40</cell><cell>87.50%</cell></row><row><cell>German</cell><cell>1.51%</cell><cell cols="2">33 100.00%</cell></row><row><cell>Indonesian</cell><cell>1.51%</cell><cell cols="2">33 100.00%</cell></row><row><cell>Bengali</cell><cell>1.42%</cell><cell cols="2">31 100.00%</cell></row><row><cell>Polish</cell><cell>1.15%</cell><cell>25</cell><cell>80.00%</cell></row><row><cell>Other</cell><cell>6.87%</cell><cell>150</cell><cell>96.00%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 12 :</head><label>12</label><figDesc>The 115 fact-checking organisations present in the dataset. The numbers in parentheses indicate how many claims were processed from the website in total.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.politifact.com/ 2 https://www.poynter.org/ifcn/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://www.politifact.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://developers.google.com/fact-check/tools/api/reference/rest 5 https://cloud.google.com/translate/docs/reference/rest/<ref type="bibr" target="#b5">6</ref> This regular expression matches four, two and two numbers, separated by dashes. Thus, 2020-01-30 would be matched, but 20-01-30 would not.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">This was done using the transformers [44] and PyTorch [30] frameworks 9 See https://cloud.google.com/translate/docs/reference/rest/.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">See https://hf.co/saattrupdan/verdict-classifier-en and https://hf.co/saattrupdan/ verdict-classifier. 11 https://www.twitter.com 12 https://developer.twitter.com/en/docs/twitter-api/tweets/search/api-reference/gettweets-search-all<ref type="bibr" target="#b12">13</ref> This was done using the KeyBERT<ref type="bibr" target="#b12">[13]</ref> package together with the paraphrase-multilingual-MiniLM-L12-v2 model from the Sentence Transformer package<ref type="bibr" target="#b33">[34]</ref>. 14 Imposed with the query -(is:reply OR is:retweet OR is:quote).<ref type="bibr" target="#b14">15</ref> Imposed with the query (has:media OR has:links OR has:images).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16">This was done using the newspaper3k Python library<ref type="bibr" target="#b28">[29]</ref>.<ref type="bibr" target="#b16">17</ref> We used the Neo4j framework, see https://neo4j.com/.<ref type="bibr" target="#b17">18</ref> This was done using the PyTorch framework<ref type="bibr" target="#b29">[30]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="19">This can be found at https://mumin-dataset.github.io/.<ref type="bibr" target="#b19">20</ref> This optimization resulted in the hyperparameters n_neighbors=50, n_components=100, random_state=4242 and metric='cosine' for UMAP, and min_samples=15 and min_cluster_size=40 for HDBSCAN. This was done using the Python packages scikit-learn<ref type="bibr" target="#b30">[31]</ref> and hdbscan<ref type="bibr" target="#b26">[27]</ref>.<ref type="bibr" target="#b20">21</ref> This was done using the KeyBERT library<ref type="bibr" target="#b12">[13]</ref> on embeddings produced by a Sentence Transformer paraphrase-multilingual-MiniLM-L12-v2<ref type="bibr" target="#b33">[34]</ref>.<ref type="bibr" target="#b21">22</ref> See https://mumin-build.readthedocs.io for a full list of arguments.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="23">The tutorial and all the source code can be accessed through https://mumin-dataset. github.io/.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="24">Note that, as the graph baseline has two layers, leaving these out does not change the claim classification score, only potentially the tweet classification score.<ref type="bibr" target="#b24">25</ref> See https://mumin-dataset.github.io/ for both the leaderboard and the baseline repository.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="26">See https://mumin-dataset.github.io/ for both mumin, MuMiN-trawl and the baselines.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">MultiFC: A Real-World Multi-Domain Dataset for Evidence-Based Fact Checking of Claims</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Augenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christina</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><forename type="middle">Chaves</forename><surname>Lima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casper</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><forename type="middle">Grue</forename><surname>Simonsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Integrating Stance Detection and Fact Checking in a Unified Corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramy</forename><surname>Baly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitra</forename><surname>Mohtarami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llu?s</forename><surname>M?rquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2004</idno>
		<ptr target="https://doi.org/10.18653/v1/N18-2004" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="21" to="27" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Verifying Multimedia Use at MediaEval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christina</forename><surname>Boididou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Andreadou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Symeon</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duc-Tien</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulia</forename><surname>Boato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiannis</forename><surname>Kompatsiaris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Workshop</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Verifying Multimedia Use at MediaEval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christina</forename><surname>Boididou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Andreadou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Symeon</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duc-Tien</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulia</forename><surname>Boato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiannis</forename><surname>Kompatsiaris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Workshop</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Influence of fake news in Twitter during the 2016 US presidential election</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Bovet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hern?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Makse</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41467-018-07761-2</idno>
		<ptr target="https://doi.org/10.1038/s41467-018-07761-2" />
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Pseudodoxia Epidemica or Enquiries into very many received tenents and commonly presumed truths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Browne</surname></persName>
		</author>
		<editor>Andrew Crook</editor>
		<imprint>
			<date type="published" when="1646" />
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised Cross-lingual Representation Learning at Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzm?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?douard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8440" to="8451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongwon</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.00885</idno>
		<title level="m">Coaid: Covid-19 healthcare misinformation dataset</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">User Preference-aware Fake News Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingtong</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congying</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.12259</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Language-agnostic bert sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangxiaoyu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Arivazhagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01852</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">KeyBERT: Minimal keyword extraction with BERT</title>
		<idno type="DOI">10.5281/zenodo.4461265</idno>
		<idno>Maarten Grootendorst. 2020</idno>
		<ptr target="https://doi.org/10.5281/zenodo.4461265" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">X-Fact: A New Benchmark Dataset for Multilingual Fact Checking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Srikumar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-short.86</idno>
		<ptr target="https://doi.org/10.18653/v1/2021.acl-short.86" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="675" to="682" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A Richly Annotated Corpus for Different Tasks in Automated Fact-Checking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Hanselowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Stab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zile</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K19-1046</idno>
		<ptr target="https://doi.org/10.18653/v1/K19-1046" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)</title>
		<meeting>the 23rd Conference on Computational Natural Language Learning (CoNLL)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="493" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">HoVer: A Dataset for Many-Hop Fact Extraction And Claim Verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikha</forename><surname>Bordia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Dognin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multimodal fusion with recurrent neural networks for rumor detection on microblogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
		<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="795" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">All-in-one: Multitask Learning for Rumour Verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Kochkina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Liakata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arkaitz</forename><surname>Zubiaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3402" to="3413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The science of fake news</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">A</forename><surname>Lazer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yochai</forename><surname>Baum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">J</forename><surname>Benkler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelly</forename><forename type="middle">M</forename><surname>Berinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filippo</forename><surname>Greenhill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><forename type="middle">J</forename><surname>Menczer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Metzger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Nyhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pennycook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Rothschild</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">A</forename><surname>Schudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cass</forename><forename type="middle">R</forename><surname>Sloman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><forename type="middle">A</forename><surname>Sunstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duncan</forename><forename type="middle">J</forename><surname>Thorson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">L</forename><surname>Watts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zittrain</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.aao2998</idno>
		<ptr target="https://science.sciencemag.org/content/359/6380/1094.full.pdf" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">359</biblScope>
			<biblScope unit="page" from="1094" to="1096" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ves</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">MM-COVID: A Multilingual and Multidimensional Data Repository for CombatingCOVID-19 Fake New. arXiv e-prints (2020)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">2011</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Accelerated Hierarchical Density Based Clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leland</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Healy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="33" to="42" />
		</imprint>
	</monogr>
	<note>2017 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">hdbscan: Hierarchical density based clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leland</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Astels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Open Source Software</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">205</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Umap: Uniform manifold approximation and projection for dimension reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leland</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Melville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03426</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Ou-Yang</surname></persName>
		</author>
		<ptr target="https://github.com/codelucas/newspaper" />
	</analytic>
	<monogr>
		<title level="j">Newspaper3k. GitHub</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>version 0.2.8.</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<ptr target="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python. the</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ga?l</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Automatic Detection of Fake News</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ver?nica</forename><surname>P?rez-Rosas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bennett</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Lefevre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3391" to="3401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Credibility Assessment of Textual Claims on the Web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashyap</forename><surname>Popat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhabrata</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jannik</forename><surname>Str?tgen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
		<idno type="DOI">10.1145/2983323.2983661</idno>
		<ptr target="https://doi.org/10.1145/2983323.2983661" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 25th ACM International on Conference on Information and Knowledge Management<address><addrLine>Indianapolis, Indiana, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2173" to="2178" />
		</imprint>
	</monogr>
	<note>CIKM &apos;16)</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1908.10084" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">KI2TE: Knowledge-Infused InterpreTable Embeddings for COVID-19 Misinformation Detection. 1st International Workshop on Knowledge Graphs for Online Discourse Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Shiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Evangelos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Papalexakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fakenewsnet: A data repository with news content, social context, and spatiotemporal information for studying fake news on social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Mahudeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Big Data</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="171" to="188" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Exploiting tri-relationship for fake news detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.07709</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<idno type="DOI">10.1016/S1473-3099(20)30565-X</idno>
		<ptr target="https://doi.org/10.1016/S1473-3099(20)30565-X" />
		<title level="m">The Lancet Infectious Diseases. 2020. The COVID-19 infodemic. The Lancet Infectious Diseases</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">FEVER: a Large-scale Dataset for Fact Extraction and VERification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Christodoulopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arpit</forename><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="809" to="819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The spread of true and false news online</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soroush</forename><surname>Vosoughi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deb</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinan</forename><surname>Aral</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.aap9559</idno>
		<ptr target="https://science.sciencemag.org/content/359/6380/1146.full.pdf" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">359</biblScope>
			<biblScope unit="page" from="1146" to="1151" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Deep Graph Library: Towards Efficient and Scalable Deep Learning on Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mufei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/1909.01315</idno>
		<ptr target="http://arxiv.org/abs/1909.01315" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Liar, Liar Pants on Fire&quot;: A New Benchmark Dataset for Fake News Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="422" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Information literacy in a fake/false news world: An overview of the characteristics of fake news and its historical development</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Carol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Watson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Legal Information</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="93" to="96" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-Art Natural Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rush</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.emnlp-demos.6" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Graph neural networks: A review of methods and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengding</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Open</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="57" to="81" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Exploiting context for rumour detection in social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arkaitz</forename><surname>Zubiaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Liakata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Procter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Social Informatics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="109" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">113) factcheck.thedispatch.com 64 (177) malayalam.factcrescendo.com 15 (245) dpa-factchecking.com 298 (1,474) lemonde.fr 62 (564) factnameh.com 15 (387) vishvasnews.com 298 (5,974) bol.uol.com.br 62 (407) factrakers.org 13 (147) factcheck.org 268 (2,312) factcheckthailand.afp.com 58 (252) factograph.info 12 (253) factuel.afp.com 243 (2,710) projetocomprova.com.br 57 (406) watson.ch 11 (39) facta.news 230 (1,196) noticias.uol.com.br 56 (693) poynter.org 9 (49) fullfact.org 226 (3,302) sprawdzam</title>
		<idno>afp.com 54 (299) br.de 9 (121) thequint.com 223 (1,084) dogrulukpayi.com 53 (641) mygopen.com 8 (440) observador.pt 207 (1,284) aap.com.au 52 (365) factcheckni.org 8 (141) aajtak.in 189 (1,539) newsweek.com 48 (196) hindi.asianetnews.com 8 (165) piaui.folha.uol.com.br 187</idno>
	</analytic>
	<monogr>
		<title level="m">Website Claims included Website Claims included Website Claims included politifact.com 716</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note>060) tamil.factcrescendo.com 47 (1,523) abc.net.au 7 (112) newtral.es 178 (2,353) periksafakta.afp.com 47 (415) liberation.fr 7 (97) checamos.afp.com 165 (1,073) chequeado.com 46 (1,689) theconversation.com 6 (54) polygraph.info 157 (1,128) nytimes.com 44 (497) telugu.newsmeter.in 6 (280) aosfatos.org 155 (1,795) poligrafo.sapo.pt 42 (3,496) factchecker.in 6 (32) teyit.org 154 (2,421) boombd.com 39 (381) open.online 5 (23) usatoday. com 154 (884) fakty.afp.com 38 (220) bbc.co.uk 5 (43) politica.estadao.com.br 151 (1,632) dailyo.in 36 (729) tenykerdes.afp.com 5 (36) factcrescendo.com 145 (896) presseportal.de 35 (466) namibiafactcheck.org.na 4 (36) thelogicalindian.com 139 (994) youturn.in 35 (1,591) factcheckmyanmar.afp.com 4 (79) washingtonpost.com 138 (1,304) 20minutes.fr 33 (255) observers.france24.com 4 (54) cekfakta.com 135 (4,104) altnews.in 31 (4,996) oglobo.globo.com 4 (50) bangla.boomlive.in 131 (1,640) cbsnews.com 30 (231) buzzfeed.com 2 (25) ellinikahoaxes.gr 131 (1,120) napravoumiru.afp.com 29 (172) bangla.aajtak.in 2 (129) newsmeter.in 127 (1,430) semakanfakta.afp.com 29 (198) istinomer.rs 2 (887) boatos.org 125 (1,893) faktencheck.afp.com 27 (335) verify-sy.com 2 (56) maldita.es 123 (1,063) tjekdet.dk 27 (481) thewhistle.globes.co.il 2 (65) colombiacheck.com 118 (802) cinjenice.afp.com 26 (227) azattyq.org 1 (9) demagog.org.pl 115 (3,181) vistinomer.mk 25 (370) radiofarda.com 1 (33) indiatoday.in 115 (1,433) tfc-taiwan.org.tw 25 (1,077) assamese.factcrescendo.com 1 (40) healthfeedback.org 111 (328) factcheckkorea.afp.com 24 (194) tamil.newschecker.in 1 (26) hindi.boomlive.in 109 (1,372) malumatfurus.org 24 (731) cekfakta.tempo.co 95 (1,142) rappler.com 24 (350</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
