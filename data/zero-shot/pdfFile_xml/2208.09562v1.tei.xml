<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Dual Modality Approach For (Zero-Shot) Multi-Label Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichao</forename><surname>Xu</surname></persName>
							<email>shichaoxu2023@u.northwestern.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Northwestern University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">OPPO US Research Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Li</surname></persName>
							<email>yikang.li1@oppo.com</email>
							<affiliation key="aff1">
								<orgName type="department">OPPO US Research Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenhao</forename><surname>Hsiao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">OPPO US Research Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiuman</forename><surname>Ho</surname></persName>
							<email>chiuman@oppo.com</email>
							<affiliation key="aff1">
								<orgName type="department">OPPO US Research Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Qi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northwestern University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Dual Modality Approach For (Zero-Shot) Multi-Label Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In computer vision, multi-label classification, including zeroshot multi-label classification are important tasks with many real-world applications. In this paper, we propose a novel algorithm, Aligned Dual moDality ClaSsifier (ADDS), which includes a Dual-Modal decoder (DM-decoder) with alignment between visual and textual features, for multi-label classification tasks. Moreover, we design a simple and yet effective method called Pyramid-Forwarding to enhance the performance for inputs with high resolutions. Extensive experiments conducted on standard multi-label benchmark datasets, MS-COCO and NUS-WIDE, demonstrate that our approach significantly outperforms previous methods and provides state-of-the-art performance for conventional multilabel classification, zero-shot multi-label classification, and an extreme case called single-to-multi label classification where models trained on single-label datasets (ImageNet-1k, ImageNet-21k) are tested on multi-label ones (MS-COCO and NUS-WIDE). We also analyze how visual-textual alignment contributes to the proposed approach, validate the significance of the DM-decoder, and demonstrate the effectiveness of Pyramid-Forwarding on vision transformer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Image classification tasks are fundamental and critical in computer vision. In recent years, with the advancement of deep neural networks, image classification has made a great impact in many real-world applications, such as medical imaging <ref type="bibr" target="#b20">(Li et al. 2014)</ref>, autonomous driving <ref type="bibr" target="#b9">(Fujiyoshi, Hirakawa, and Yamashita 2019)</ref>, manufacturing <ref type="bibr" target="#b30">(Rendall et al. 2018</ref>), agriculture <ref type="bibr" target="#b13">(Kamilaris and Prenafeta-Bold? 2018)</ref>, etc. The most prevalent and well-studied topic in image classification is the single-label classification, which assumes that each image contains only one item, scene, or concept of interest to label. And a large amount of data has been collected for this task, such as ImageNet-1k <ref type="bibr" target="#b7">(Deng et al. 2009</ref>) and ImageNet-21k <ref type="bibr">(Ridnik et al. 2021a</ref>).</p><p>However, it is common that an image contains multiple objects, scenes, or concepts that are of interest and should be labeled, which requires solving multi-label classification. And a limited number of multi-label datasets such as MS-COCO <ref type="bibr" target="#b21">(Lin et al. 2014</ref>) and NUS-WIDE <ref type="bibr" target="#b6">(Chua et al. 2009</ref>) have been collected. Moreover, in the conventional setting, the testing labels/classes have been seen during training, and we call this task conventional multi-label classification in the paper. In contrast, the case where the testing labels/classes have not been seen during training is called zero-shot multi-label classification, which is also common in practice and very challenging to address. Finally, given the difficulty and cost to collect and annotate multi-label datasets, we also consider a somewhat extreme zero-shot case where models are trained on single-label datasets (e.g., ImageNet-1k) and tested on multi-label datasets (e.g., NUS-WIDE). We call this single-to-multi label classification.</p><p>In this work, we develop a novel approach for multi-label classification that significantly outperforms previous methods and provides the new state-of-the-art (SOTA) performance. <ref type="figure">Figure 1</ref> gives a preview of our experimental results (more details and results are in the Experiments sec- Conventional multi-label classification on MS-COCO <ref type="figure">Figure 1</ref>: Experimental results from this work on (conventional) multi-label classification, zero-shot multi-label classification, and single-to-multi label classification. Our approach significantly outperforms previous methods and provides the new stateof-the-art performance. Details and more results are presented later in the Experiments Section. tion). We can see that in all three classification tasksconventional multi-label, zero-shot multi-label, and singleto-multi label -our approach provides significantly higher mean Average Precision (mAP) than prevoius methods, including SSGRL <ref type="bibr" target="#b3">(Chen et al. 2019a</ref>), MS-CMA <ref type="bibr" target="#b40">(You et al. 2020)</ref>, ASL (Ben-Baruch et al. 2020), Q2L <ref type="bibr" target="#b22">(Liu et al. 2021)</ref>, LESA <ref type="bibr" target="#b12">(Huynh and Elhamifar 2020)</ref>, BiAM , GMLZSL , SDL <ref type="bibr" target="#b2">(Ben-Cohen et al. 2021)</ref>, and ML-Decoder <ref type="bibr" target="#b32">(Ridnik et al. 2021b</ref>).</p><p>Including the above methods for comparison, there are a number of approaches in the literature addressing multilabel classification. We highlight some of those below, and provide a more detailed discussion in the Appendix. Some approaches <ref type="bibr" target="#b37">(Yang et al. 2016;</ref><ref type="bibr" target="#b36">Wang et al. 2017;</ref><ref type="bibr" target="#b10">Gao and Zhou 2021)</ref> first locate each object in the image or capture the attention map, and then perform single-label classification. These methods can leverage existing object detection techniques and attention mechanisms, but may encounter challenges such as classification results being heavily affected by the quality of the extracted proposals, concepts or scenes being hard to localize, regions with duplicate concepts making single-label classification difficult, etc. There are also methods based on label correlations <ref type="bibr" target="#b40">(You et al. 2020;</ref><ref type="bibr" target="#b3">Chen et al. 2019a)</ref>, which try to identify potential relations among the labels within the image to facilitate classification, usually, through building a label graph. Moreover, some methods <ref type="bibr" target="#b32">(Ridnik et al. 2021b;</ref><ref type="bibr" target="#b22">Liu et al. 2021</ref>) create label embeddings from the label text or assign learnable embeddings for each label instead of indirectly creating and learning from the graph relation, allowing for more sophisticated outcomes. Then by utilizing a transformer decoder, the model can extract local discriminative features for different labels and output the per-class probability. However, in these approaches, visual and textual embeddings are in different data spaces, and learning the mapping between the image samples and the text labels is quite challenging. Furthermore, the learned mapping is very hard to generalize on unseen distribution. For example, the previous SOTA method ML-Decoder <ref type="bibr" target="#b32">(Ridnik et al. 2021b)</ref> found that a random matrix for the label embedding has the same performance as using Word2Vec  or learnable embeddings, which is counter-intuitive if we consider the labels' semantic meaning. This motivates our work to explore aligning visual and textual embeddings.</p><p>More specifically, to overcome the challenges in previous methods, we propose a multi-label classification framework ADDS (Aligned Dual moDality ClaSsifier), based on the alignment between visual and textual embeddings, which converts the learning of multi-label classification to a simpler regression task on a cosine-like function. The framework includes a novel DM-decoder (Dual-Modal decoder) design, which leverages the dual modality to enhance transformer decoder layers by progressively fusing visual embeddings with textual information and developing richer semantic understanding. It also includes a Pyramid-Forwarding method to adapt the model pre-trained on lower image resolutions to higher resolution images without re-training.</p><p>To summarize, our work makes the following technical contributions:</p><p>? We have developed ADDS, a multi-label classification framework that builds a soft constraint for visual and textual feature alignment to achieve better generalization ability. The framework includes DM-Decoder, a novel transformer decoder for facilitating the fusion of the semantics from dual-modal information source, and Pyramid-Forwarding, a new adaptation method that addresses images with higher resolutions than training images and is also able to largely reduce the computational cost of vision transformer. ? We have conductecd extensive experiments across various multi-label classification tasks. As shown in Figure 1 and will be detailed in the Experiments Section, Our ADDS framework significantly outperforms the previous SOTA methods in all scenarios, including conventional multi-label classification (e.g., 2 points improvement on MS-COCO), zero-shot multi-label classification (7.9 points improvement on NUS-WIDE), and single-to-multi label classification (24.71 points and 16.49 points improvements from ImageNet-1k to MS-COCO and NUS-WIDE, respectively).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology Overview</head><p>In this section, we present the details of our ADDS method for multi-label classification. As shown in <ref type="figure">Figure 2</ref>, our method receives both the image x img ? R H?W ?3 and the label classes X lbl ? {natural language words} as the inputs, where X lbl contain words of potential labels for identifying the object (tree, apple, computer, . . . ), scene (sea, sky, underground, . . . ) or concept (small, red, . . . ). Then the label classes X lbl are combined with prompts such as "This photo contains @", "This is a @ photo" where @ ? X lbl , and fed through a text tower to get the textual (label) embedding. The image input is fed through the Pyramid-Forwarding module, whose output images are then fed through an image tower to get the visual (image) embeddings. The visual embeddings are aligned with textual embedding, and then stacked and forwarded to the DM-decoder, whose outputs are mapped to per-class probabilities via a shared fullyconnected layer. That is, given the input {x img , X lbl }, our model outputs p pred = [p 1 , p 2 , . . . , p k ], where p i ? [0, 1], k = |X lbl |.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual-Textual Embedding Alignment</head><p>Compared with previous works <ref type="bibr" target="#b22">(Liu et al. 2021;</ref><ref type="bibr" target="#b32">Ridnik et al. 2021b)</ref> where the label embedding is based on Transformer encoder <ref type="bibr" target="#b22">(Liu et al. 2021</ref>), Word2Vec <ref type="bibr" target="#b32">(Ridnik et al. 2021b)</ref> or even randomly initialized matrix <ref type="bibr" target="#b32">(Ridnik et al. 2021b</ref>), a major difference of our approach is that we explore the alignment between visual and textual embeddings. This is not only helpful for conventional multi-label classification, but also critical for boosting the performance of zero-shot multi-label classification. Specifically, in the zero-shot case, the training data contains the images {x (img,seen) } and the labels X (lbl,seen) . The objective is to learn a classifier g to make predictions on an unseen image x (img,unseen) with un-  <ref type="figure">Figure 2</ref>: Overview of our ADDS framework for multi-label classification. Text labels with prompts are fed into a text tower to get the textual embedding. Images are first processed by a Pyramid-Forwarding module and then fed into an image tower to get the visual embeddings, which are aligned with the textual embedding and stacked on the token size dimension. Then the textual embedding (after a selective language supervision module) and the stacked visual embeddings are fused by six layers of DM-decoders with the initial query from textual embedding and the initial key/value from visual embeddings. After a shared mapping among all labels, the network outputs the probability for each label class.</p><p>seen categories X <ref type="bibr">(lbl,unseen)</ref> : <ref type="bibr">unseen)</ref> x <ref type="bibr">(lbl,unseen)</ref> , else = 0. Here, x <ref type="bibr">(img,unseen)</ref> x <ref type="bibr">(lbl,unseen)</ref> means that x (img,unseen) contains object/scene/concept x <ref type="bibr">(lbl,unseen)</ref> . This learning task is very challenging as x (img,unseen) is of high dimension,</p><formula xml:id="formula_0">g(x (img,unseen) , x (lbl,unseen) ) ? {0, 1},<label>(1)</label></formula><formula xml:id="formula_1">g(, ) = 1 if x (img,</formula><p>x <ref type="bibr">(lbl,unseen)</ref> is in natural language, and we only have limited observations in this hyper-space while the extrapolation of this mapping to the unseen space with unknown distribution is quite difficult.</p><p>Previous methods struggled in the zero-shot case. Take the previous SOTA method <ref type="bibr" target="#b32">(Ridnik et al. 2021b)</ref> as an example, the target mapping g is as below:</p><formula xml:id="formula_2">g(h img (x (img,unseen) ), h w2v (x (lbl,unseen) )) ? {0, 1}, (2)</formula><p>where h img is the image encoder and h w2v is a word2Vec encoder. This benefits learning the classifier g as the visual input dimension of g is decreased and each label text is simplified into a 1 ? 300 vector. However, the learning task is still very challenging since the visual and textual embeddings are in different data space without any explicit relationship between them so that the mapping g on unseen data can be quite different from what is learned on seen samples.</p><p>In our approach, inspired by the Vision-Language Pretraining (VLP), we align the visual and textual embeddings with the help of the pre-trained model from Contrastive Language-Image Pre-training (CLIP) <ref type="bibr" target="#b29">(Radford et al. 2021)</ref>, which is built on the correlation between text and image. Specifically, we employ the ViT <ref type="bibr" target="#b8">(Dosovitskiy et al. 2020)</ref> model as the image encoder f img and the multi-layer transformer as the text encoder f lbl , with the parameters of both encoders from CLIP. They are all frozen during training to maintain the alignment. Considering an ideal CLIP model, we set an additional soft constraint on the visual and textual embedding as:</p><formula xml:id="formula_3">? ? ? g(f img (x (img,unseen) ), f lbl (x (lbl,unseen) )) ? {0, 1}. cos( fimg(x (img,unseen) ) |fimg(x (img,unseen) )| , f lbl (x (lbl,unseen) ) |f lbl (x (lbl,unseen) )| ) ? 1 ? , if g(f img (x (img,unseen) ), f lbl (x (lbl,unseen) )) = 1 , else ,<label>(3)</label></formula><p>where is a small value determined by the CLIP model. This soft constraint holds among the entire data space for an ideal CLIP model. Then the learning objective of zero-shot multilabel classification is converted into a regression task: given the inputs {(a, b)}, and the corresponding label</p><formula xml:id="formula_4">{l} (l = 1 if cos( a |a| , b |b| ) = 1 ? , else 0), learn a model g that satisfies g( a |a| , b |b| ) = 1.<label>(4)</label></formula><p>It is clear that g is similar to a cosine function. In practice, the classification performance relies on these factors: 1) the quality of the CLIP model -constraints in Equation <ref type="formula" target="#formula_3">(3)</ref> could be violated sometimes because of the CLIP's training data imperfection, 2) how well the rest of the network capturing the cosine-like function, and 3) the noise level of the data. Similar analysis holds for conventional multi-label classification (with only x <ref type="bibr">(lbl,unseen)</ref> changed to x (lbl,seen) ). Note that not all VLP models are suitable for maintaining the alignment, which is much more important than using a better image/text encoder, as the ablation studies in the Experiments Section will show. After the visual and textual feature extraction steps, a single layer cross-attention (we omit the description of the fully-connected layer, dropout, and layer normalization for space reason) is a good choice for querying the textual embedding from the visual embedding to determine the perclass probability, as shown in <ref type="bibr" target="#b32">(Ridnik et al. 2021b</ref>). Moreover, the work in <ref type="bibr" target="#b34">(Vaswani et al. 2017)</ref> shows that the visual and multi-modal tasks can benefit from increasing of the network depth and stacking of the transformer decoder. However, we observe some issues when we stack decoder layers similarly as in <ref type="bibr" target="#b32">(Ridnik et al. 2021b</ref>):</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dual-Modal Decoder</head><p>? The model performance will often decrease after stacking more than 3 decoder layers. ? The key and value inputs are always the same from the visual embedding. As the output of the cross-attention layer is a weighted sum of its value input, the outputs of decoder layers in different levels are actually in the same (or close) semantic level and all from the same visual embedding.</p><p>To address these issues, we redesign the decoder module, with two major differences from the previous SOTA method in <ref type="bibr" target="#b32">(Ridnik et al. 2021b)</ref>, as shown in <ref type="figure" target="#fig_0">Figure 3</ref> and Equation <ref type="formula">(5)</ref>. Specifically, we add an additional multi-head cross-attention layer MultiHdAttn 2 , and we use the visual embedding V img to query the output Q 5 mid from the previous cross-attention layer MultiHdAttn 1 instead of using the textual embedding as the query (MultiHdAttn 1 utilizes the textual embedding to query the visual features). The output Q 5 mid contains the weighted sum of image tokens' embedding guided by the textual information, so we can redistribute them back to each image tokens' embedding through MultiHdAttn 2 to further refine the visual embedding according to the correlation of Q 5 mid with the key inputs. Moreover, apart from the original skipping structures, we add an additional skipping connection from the query input to the query output, which is transformed by addition and normalization. Formally, we denote the input query, key, value as Q lbl , K img , V img , and the block's output query, key and value for the next block as Q lbl , K img , V img . We output Q lbl only if it is the last layer. We denote DP as the dropout layer, and FFN 1 , FFN 2 as the fully-connected layer. Then, each new decoder block can be formulated as: A major challenge for using pre-trained models learned from large-scale datasets (e.g., the VLP models) is that they are often trained on low-resolution images (e.g., 224x224, 336x336, etc.) due to the limitation of computational resources. Thus the extracted features are often not compatible with higher-resolution images. We could consider downsampling the higher-resolution input images to lower resolution, but that will lose significant local features. Conducting finetuning on higher-resolution images might slightly reduce the incompatibility with extracted features, but we still face several issues: 1) Does the model support training on arbitrary scale images? 2) How much fine-tuning data are available? 3) How long or how well the model should be trained?</p><formula xml:id="formula_5">Q 1 mid = LayerNorm(Q lbl + DP(Q lbl )) Q 2 mid = MultiHdAttn 1 (Q 1 mid , K img , V img ) Q 3 mid = LayerNorm(Q 2 mid + Q 1 mid ) Q 4 mid = DP(FFN 1 (ReLU(FFN 2 (Q 3 mid )))) Q 5 mid = LayerNorm(Q 4 mid + Q 3 mid ) Q lbl = LayerNorm(Q 5 mid + Q lbl ) V 1 img = MultiHdAttn 2 (V img , Q 5 mid , Q 5 mid ) V img = LayerNorm(V 1 img + V img ) K img = V img (5) Pyramid-Forwarding Down-sample ? Down-sample ?</formula><p>To address these challenges, we propose Pyramid-Forwarding, a resolution-compatible method that enables deploying a pre-trained model trained from low-resolution images on high-resolution images without retraining. This method applies to many pre-trained image decoders, and below we use a vision transformer (ViT) <ref type="bibr" target="#b8">(Dosovitskiy et al. 2020)</ref> pre-trained on S Img ? S Img resolution images as an example. It will be applied on S Img * d ? S Img * d target images (d is a positive integer), and our method helps reduce the computation cost from O(d 4 ? c) to O(d 2 ? c), where c is a constant for all resolutions.</p><p>Specifically, given a pre-trained model with S Img ? S Img resolution and an image of size S Img * d?S Img * d, Pyramid-Forwarding constructs log(d) + 1 levels (first we suppose log(d) is an integer and we will discuss the non-integer case later). In level i ? [0 . . . log(d)], the image is resized to S Img * 2 i ? S Img * 2 i and split into (i + 1) ? (i + 1) patches, as visualized in <ref type="figure" target="#fig_1">Figure 4</ref>. Then these patches are fed into the image encoder (usually wrapped into the batch dimension for parallel processing on GPUs) and get the feature tokens. In ViT, these tokens are stacked on the token dimension and then feed into the decoder, and the computation cost on the S Img * d ? S Img * d image is actually O((d 2 ) 2 ) = O(d 4 ) times more than the S Img ? S Img image because of the self-attention layer. However, with Pyramid-Forwarding, this computation cost is reduced to 1 + 2 2 + 4 2 + ? ? ? + (2 log(d) ) 2 ? O(d 2 ). If log(d) is not an integer, the size of the top-level image will be changed to S o Img , and we allow the divided patches to be overlapped with their neighborhoods, while the total number of patches in this level i is still (i + 1)</p><p>2 . In addition, the computation cost of Pyramid-Forwarding can be further reduced by randomly disposing the image patches from the non-bottom levels i ? [1 . . . log(d)], which provides a trade-off between the accuracy and the computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Selective Language Supervision</head><p>As the label classes can be object/scene/concept names that are selected from the natural language, the total number of the labels can be extremely large (e.g., 1k in ImageNet-1k, 21k in ImageNet-21k, or even more). This presents a challenge in the training stage -if we feed all the labels into the network, it is a heavy burden on the inference speed and the memory usage. Thus we propose a selective language supervision method that utilizes the positive labels and part of the negative labels selected during the network training, and consider the data distribution of the positive and negative samples for reducing data imbalance.</p><p>Specifically, given multi-label L = {l 1 , l 2 , . . . , l k } from the training batch B with k classes in total, S pos = {i|l i = 1, l i ? L, L ? B}, and S neg = {1, 2, . . . , k} ? S pos . Then the selected label set for batch B training is</p><formula xml:id="formula_6">S = S pos ? S slt ,<label>(6)</label></formula><p>where elements in S slt is randomly selected from S neg . |S slt | = min(? * |S pos |, k ? |S pos |), where ? is a hyperparameter balancing the number of positive and negative samples (we choose ? = 3 in our experiments if not mentioned specifically). The selective language supervision is utilized in our experiments for the ImageNet-21k dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Implementation Settings</head><p>Our code is implemented in PyTorch 1.10.2. All experiments are conducted on a server cluster running Ubuntu 18.04.6 and equipped with NVIDIA V100 GPUs. We conduct the experiments on MS-COCO <ref type="bibr" target="#b21">(Lin et al. 2014)</ref>, NUS-WIDE <ref type="bibr" target="#b6">(Chua et al. 2009</ref>), and ImageNet-1k <ref type="bibr" target="#b7">(Deng et al. 2009</ref>) dataset. All models are optimized by the Adam optimizer <ref type="bibr" target="#b16">(Kingma and Ba 2014</ref> , where bs = 56 is the batch size, n t is the number of output tokens from vision transformer, and l is the embedding length determined by the type of vision transformer. All image and text towers in our method are fixed during the training. The model is trained for 40 epochs, and the weight decay is 10 ?4 .Our model applies the loss function ASL from (Ben-Baruch et al. 2020), which is also used in the most recent multi-label classification works <ref type="bibr" target="#b22">(Liu et al. 2021;</ref><ref type="bibr" target="#b32">Ridnik et al. 2021b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conventional Multi-label Classification</head><p>We conduct experiments on the MS-COCO dataset for evaluating conventional multi-label classification task, and the results are shown in <ref type="table">Table 1</ref>. We run a few variations of our approach and compare them with a number of baselines. We first use the ViT-L backbone <ref type="bibr" target="#b8">(Dosovitskiy et al. 2020</ref>) to test on 224 ? 224 resolution images. We observe that our approach achieves an mAP of 89.82%, which is 5.6 points higher than the previous SOTA method ML-Decoder (Ridnik et al. 2021b) on 224 ? 224 images. We then switch to a larger image encoder ViT-L-336 for image resolution of 336 ? 336. Our approach achieves an mAP of 91.76%, which is even better than what ML-Decoder can achieve on higher-resolution images of 640 ? 640. And when we train our model with ViT-L-336 on 640 ? 640 images, the mAP reaches 93.41%, 2.0 points higher than ML-Decoder at the same resolution. With Pyramid-Forwarding, our model can also be deployed efficiently on an even higher resolution of 1344 ? 1344 with the model ViT-L-336 pre-trained on 336 ? 336 resolution, and our approach can achieve an mAP of 93.54% (with much less computational cost than pre-training a model on 1344 ? 1344 images). Overall, it is clear that our approach significantly outperforms previous methods on conventional multi-label classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Zero-shot Multi-label Classification</head><p>For zero-shot multi-label classification, we show our results on the NUS-WIDE dataset in <ref type="table">Table 2</ref>. First, we use imageencoder ViT-B-32 and its corresponding text encoder with CLIP pre-trained weights. Using 224?224 input images, we already achieve a 36.56% mAP, 5.46 points higher than the previous SOTA method ML-Decoder. Then we use imageencoder ViT-L-336 and its corresponding text encoder with CLIP pre-trained weights on 336 ? 336 images. The result further improves to a 39.01% mAP, 7.9 points higher than ML-Decoder (which even uses higher resolution images than both of our settings). This clearly shows that our approach provides significantly better results than previous methods on zero-shot multi-label classification.  <ref type="table">Table 2</ref>: Comparison on zero-shot multi-label classification methods for NUS-WIDE dataset. Our method provides significantly better results than previous methods, including CONSE <ref type="bibr" target="#b28">(Norouzi et al. 2013)</ref>, LabelEM <ref type="bibr" target="#b0">(Akata et al. 2015)</ref>, Fast0Tag <ref type="bibr" target="#b41">(Zhang, Gong, and Shah 2016)</ref>, One Attention per Label <ref type="bibr" target="#b14">(Kim, Jun, and Zhang 2018)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single-to-multi Label Classification</head><p>We compare our approach with the previous SOTA method ML-Decoder in an extreme case of zero-shot multi-label classification, where models are trained on the single-label ImageNet-1k dataset and tested on the multi-label MS-COCO and NUS-WIDE datasets. The results are in <ref type="table" target="#tab_5">Tables 3  and 4</ref>. Our approach greatly outperforms ML-Decoder, despite our model uses lower-resolution images (224 ? 224 for ViT-B-32 backbone and 336 ? 336 for ViT-L-336 backbone) than . This again shows that   our approach greatly outperforms previous methods in single-to-multi label classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Studies</head><p>Effectiveness of DM-Decoder: We validate the effectiveness of our Dual-Modal decoder (DM-Decoder) design. In particular, we conduct the zero-shot multi-label classification experiments on NUS-WIDE. We replace the DM-Decoder in our approach with the previous SOTA ML-Decoder under various number of stacking layers. The image resolution is 336 ? 336, and all image encoder is chosen as ViT-L-336, with their corresponding text encoder. As shown in <ref type="table" target="#tab_8">Table 5</ref>, DM-Decoder significantly outperforms ML-Decoder in almost all cases, showing its effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of textual-visual alignment:</head><p>We also validate the effectiveness of our textual-visual alignment, with results shown in  encoder is ViT-L and the textual embedding is from a random matrix, with 87.57% mAP. This is because ViT-L performs better than TresNet-L. Then in the fourth line, even if we choose a model that performs much better than ViT-L, ConvNeXt-XL (87.3 Top-1 accuracy on ImageNet-1k) <ref type="bibr" target="#b24">(Liu et al. 2022)</ref>, with CLIP's text encoder, the mAP only improves 0.01. However, when we finally use ViT-L together with CLIP's text encoder, as shown in the fifth line, the model performance achieves a much better result at 89.43%. This clearly shows the effectiveness of our textual-visual alignment.  <ref type="table" target="#tab_7">Table 6</ref>: Comparison of different components for validating the effectiveness of textual-visual alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Encoder Text Encoder</head><p>Full Pyramid-Forwarding vs. single-layer Pyramid-Forwarding: We then evaluate the effectiveness of Pyramid-Forwarding by comparing the results of using only a single layer of Pyramid-Forwarding versus using full Pyramid-Forwarding on MS-COCO. In <ref type="table" target="#tab_11">Table 7</ref>, the second column shows the level index in Pyramid-Forwarding. The first line shows the result of using the model on 336 ? 336 images without Pyramid-Forwarding. The second line shows the model with Pyramid-Forwarding on 1344 ? 1344 images, but only with the level of highest resolution (third level) and cutting into 16 patches. The third line shows the model with full Pyramid-Forwarding on 1344?1344 resolution images. We can see that using full Pyramid-Forwarding cannot provides much more performance boost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of the number of training classes:</head><p>We claim that increasing the number of available training classes can benefit the single-to-multi label classification, and we validate this through training our model (on ViT-L-336) with the selective language supervision technique on ImageNet-21k   Experiments on other VLP models: Finally, we are also curious about how other VLP models perform under our method. We consider two examples BLIP <ref type="bibr" target="#b18">(Li et al. 2022)</ref> and SLIP <ref type="bibr" target="#b26">(Mu et al. 2021)</ref>. They both have the contrastive loss similar to CLIP to ensure an alignment between the visual and textual features. In the second line of  <ref type="table" target="#tab_13">Table 9</ref>: Applying other VLP models for alignment to our method on NUS-WIDE zero-shot classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we present a novel multi-label classification framework called Aligned Dual moDality ClaSsifier (ADDS). The framework leverages textual-visual alignment with a novel Dual-Modal Decoder design and Pyramid-Forwarding technique. Our method significantly outperforms all previous results and becomes the state-of-the-art method on conventional multi-label classification, zero-shot multi-label classification, and also single-to-multi label classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-label Classification</head><p>Multi-label classification, which aims to classify multiple objects, scenes or concepts in a given image, is generally a more challenging task than the typical single-label classification. It has been studied in the literature by various approaches, as discussed below.</p><p>Region of interests based methods: In some previous works <ref type="bibr" target="#b37">(Yang et al. 2016;</ref><ref type="bibr" target="#b36">Wang et al. 2017;</ref><ref type="bibr" target="#b10">Gao and Zhou 2021;</ref><ref type="bibr" target="#b40">You et al. 2020;</ref><ref type="bibr" target="#b10">Gao and Zhou 2021)</ref>, multi-label classification is solved by locating each object in the image or capturing the attention map, and then performing singlelabel classification on it. <ref type="bibr" target="#b37">Yang et al. (Yang et al. 2016</ref>) extract object proposals from the image and combine them with the local information from the nearest-neighbor regions to enhance the discriminative ability of the features. <ref type="bibr" target="#b36">Wang et al. (Wang et al. 2017</ref>) propose a model consisting of a CNN feature extractor, a spatial transformer layer, and LSTM sub-networks that iteratively locate the interest regions and make predictions. The work in <ref type="bibr" target="#b40">(You et al. 2020)</ref> captures the cross-modality attention maps with the guidance of semantic label embedding that is learned from the adjacency-based similarity graph embedding method. The approach in <ref type="bibr" target="#b10">(Gao and Zhou 2021)</ref> tries to localize the region of interest with guidance from the global information, and then obtains the class prediction from the local stream model. However, these methods often suffer from issues like coarse discovered regions, heavy computation costs, some concepts or scenes being hard to localize, and some regions containing duplicate concepts.</p><p>Label correlations based methods: Some works based on label correlations try to identify the potential relations among the labels within the image to facilitate classification. For instance, the method in <ref type="bibr" target="#b3">(Chen et al. 2019a</ref>) splits the feature representation into category semantics-specific representations, and applies a graph neural network to explore the interactions among them. The work in <ref type="bibr" target="#b4">(Chen et al. 2020</ref>) explicitly models label correlations, and learns the embedding by measuring the similarity of the labels to help the representation learning for multi-label classification. The method in <ref type="bibr" target="#b35">(Wang et al. 2020</ref>) learns the label embedding by graph convolution and fuses the learned features with the embedding from the image model during forwarding. And there are other works such as <ref type="bibr" target="#b33">(Shi et al. 2020;</ref><ref type="bibr" target="#b39">Ye et al. 2020</ref>) that adopt a similar perspective on addressing multi-label classification.</p><p>Methods learning label semantics: Instead of indirectly learning relations from the label graph, some methods try to learn the label semantics directly through the data and combine with the transformer decoder to further improve the multi-label classification performance. For instance, the work in <ref type="bibr" target="#b22">(Liu et al. 2021</ref>) sets a learnable feature vector for each label as the query and uses the standard transformer decoder <ref type="bibr" target="#b34">(Vaswani et al. 2017</ref>) to extract the correlated features from the image embedding. Another recent work in <ref type="bibr" target="#b32">(Ridnik et al. 2021b</ref>) finds that replacing the learnable feature vector of the labels with a fixed random initialized matrix will not decrease the model performance. However, in these works, the image and text embeddings are in different feature spaces and have weak connections, and learning the relation between the image samples and the text labels is quite challenging and also hard to generalize.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vision-Language Pre-training (VLP)</head><p>The vision language pre-training learns the semantic correspondence between image and language by pretraining on large-scale data with different tasks. In the literature, some works such as VisualBERT <ref type="bibr" target="#b19">(Li et al. 2019</ref>), Unicoder-VL , and ViLT <ref type="bibr" target="#b15">(Kim, Son, and Kim 2021)</ref> extract image tokens from the interest regions, combine them with language tokens together as the inputs, and fuse the information by the transformer in the early stage. Other works such as CLIP <ref type="bibr" target="#b29">(Radford et al. 2021)</ref>, SLIP <ref type="bibr" target="#b26">(Mu et al. 2021)</ref>, BLIP <ref type="bibr" target="#b18">(Li et al. 2022)</ref>, and TCL <ref type="bibr" target="#b38">(Yang et al. 2022</ref>) first extract the deep features of the image and the text individually, and then conduct modality interaction after the feature extraction. In this paper, we mainly maintain the alignment of the visual and textual embeddings through CLIP <ref type="bibr" target="#b29">(Radford et al. 2021)</ref>, which is built on the cosine similarity between the image and text embedding pairs and trained with a large and noisy dataset. Moreover, in the Ablation Studies of the main paper text, we also introduce additional experiments on using other VLP models such as BLIP <ref type="bibr" target="#b18">(Li et al. 2022)</ref> and SLIP <ref type="bibr" target="#b26">(Mu et al. 2021</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Examples of Zero-shot Multi-label Classification by Our Approach</head><p>We demonstrate some of the zero-shot label classification results on the NUS-WIDE dataset by our approach, as shown in <ref type="figure" target="#fig_3">Figure 5</ref> of the Appendix. Left column of each figure shows the top 8 prediction scores among various classes, which are generated from our models with ViT-L-336 backbone on 336 ? 336 images. From them, the ones above the threshold (0.5 in our experiments) are chosen as the predicted labels. We can see that for this challenging task, our approach can generate relevant labels in some cases. As shown quantitatively in <ref type="table">Table 2</ref> of the main paper text, our approach offers significant improvement over previous methods and becomes the new SOTA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Examples of Generating Multiple Labels on</head><p>Single-label Dataset <ref type="figure">Figure 6</ref> of the Appendix shows some examples of our approach in generating multiple labels for a single-label dataset (ImageNet-1k) after being trained on the same single-label dataset (different training and testing partitions).</p><p>Left column of each figure shows the top 7 prediction scores among various classes, which are generated from our models with ViT-L-336 backbone on 336 ? 336 images. We observe that our approach can effectively generate multiple labels that correspond to various classes/objects in the figure. This motivates us to explore the single-to-multi label classification problem in the main paper text (although in that problem, our model is tested on multiple-label datasets that were not seen during the training, and thus much more challenging).   <ref type="figure">Figure 6</ref>: Examples of our approach in generating multiple labels on a single-label dataset (ImageNet-1k) after being trained on the same single-label dataset (different training and testing partitions).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Overview of our Dual-Modal Decoder design.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Overview of the Pyramid-Forwarding method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>, One Attention per Cluster (M=10) (Huynh and Elhamifar 2020), LESA (Huynh and Elhamifar 2020), BiAM (Narayan et al. 2021), Generative ML-ZSL (Gupta et al. 2021), SDL (Ben-Cohen et al. 2021), and ML-Decoder (Ridnik et al. 2021b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Examples of zero-shot multi-label classification by our approach on the NUS-WIDE dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>). The learning rate is set as 3 ? 10 ?4 for all models trained on 224 ? 224 and 336 ? 336</figDesc><table><row><cell>Model</cell><cell>Backbone</cell><cell>Input Resolution</cell><cell>mAP(%)</cell></row><row><cell>ML-GCN</cell><cell>ResNet101</cell><cell>448x448</cell><cell>83.0</cell></row><row><cell>KSSNET</cell><cell>ResNet101</cell><cell>448x448</cell><cell>83.7</cell></row><row><cell>SSGRL</cell><cell>ResNet101</cell><cell>576x576</cell><cell>83.8</cell></row><row><cell>MS-CMA</cell><cell>ResNet101</cell><cell>448x448</cell><cell>83.8</cell></row><row><cell>ASL</cell><cell>TResNet-L</cell><cell>448x448</cell><cell>88.4</cell></row><row><cell>Q2L</cell><cell>TResNet-L</cell><cell>448x448</cell><cell>89.2</cell></row><row><cell cols="2">ML-Decoder TResNet-M</cell><cell>224x224</cell><cell>84.2</cell></row><row><cell cols="2">ML-Decoder TResNet-L</cell><cell>448x448</cell><cell>90.1</cell></row><row><cell cols="2">ML-Decoder TResNet-XL</cell><cell>640x640</cell><cell>91.4</cell></row><row><cell>Ours</cell><cell>ViT-L</cell><cell>224x224</cell><cell>89.82</cell></row><row><cell>Ours</cell><cell>ViT-L-336</cell><cell>336x336</cell><cell>91.76</cell></row><row><cell>Ours</cell><cell>ViT-L-336</cell><cell>640x640</cell><cell>93.41</cell></row><row><cell>Ours</cell><cell>ViT-L-336</cell><cell>1344x1344</cell><cell>93.54</cell></row><row><cell cols="4">Table 1: Comparison on conventional multi-label classifi-</cell></row><row><cell cols="4">cation for MS-COCO dataset. Our ADDS approach shows</cell></row><row><cell cols="4">significant improvement over previous methods, including</cell></row><row><cell cols="4">ML-GCN (Chen et al. 2019b), KSSNET (Liu et al. 2018),</cell></row><row><cell cols="4">SSGRL (Chen et al. 2019a), MS-CMA (You et al. 2020),</cell></row><row><cell cols="4">ASL (Ben-Baruch et al. 2020), Q2L (Liu et al. 2021), and</cell></row><row><cell cols="3">ML-Decoder (Ridnik et al. 2021b).</cell><cell></cell></row></table><note>images, and as 1 ? 10 ?4 for all models trained on images larger than 336 ? 336. We test the input resolution of the images on 224 ? 224, 336 ? 336, 640 ? 640, 1344 ? 1344, and they are specified in each experiment. We adopt Vi- sion Transformer (ViT) (Dosovitskiy et al. 2020) as the im- age encoder and use CLIP (Radford et al. 2021) pre-trained weights. The output of ViT is [bs, n t , l]</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Comparison on single-to-multi label classification.</figDesc><table><row><cell cols="5">Models are trained on ImageNet-1k and tested on MS-</cell></row><row><cell cols="5">COCO. Our method greatly outperforms ML-Decoder.</cell></row><row><cell></cell><cell cols="3">With overlapped classes</cell><cell></cell></row><row><cell>Model</cell><cell cols="4">Backbone mAP(%) F1(k=3) F1(k=5)</cell></row><row><cell cols="3">ML-Decoder TResNet-L 14.15</cell><cell>7.07</cell><cell>7.30</cell></row><row><cell>Ours</cell><cell>ViT-B-32</cell><cell>27.34</cell><cell>20.39</cell><cell>20.39</cell></row><row><cell>Ours</cell><cell>ViT-L-336</cell><cell>31.07</cell><cell>24.69</cell><cell>24.69</cell></row><row><cell></cell><cell cols="3">Without overlapped classes</cell><cell></cell></row><row><cell>Model</cell><cell cols="4">Backbone mAP(%) F1(k=3) F1(k=5)</cell></row><row><cell cols="3">ML-Decoder TResNet-L 13.19</cell><cell>6.26</cell><cell>6.27</cell></row><row><cell>Ours</cell><cell>ViT-B-32</cell><cell>26.96</cell><cell>16.07</cell><cell>16.07</cell></row><row><cell>Ours</cell><cell>ViT-L-336</cell><cell>30.66</cell><cell>19.18</cell><cell>19.18</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Comparisons of single-to-multi label classification task which is trained on ImageNet-1k and tested on NUS- WIDE dataset. Our method shows the SOTA performance.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>For a fair comparison, we use the ML-Decoder instead of the DM-Decoder on all experiments and test on 224 ? 224 images. All label embeddings are fixed and the image encoder of TresNet-L and ConvNeXt-XL are unfreezed for achieving better performance. The first line shows the result of the ML-Decoder method with the TResNet-L image encoder and the Random Matrix text encoder, with 85.78% mAP. And as shown in the second line, when we replace the text encoder with the CLIP's text encoder corresponding to the ViT-L model, the mAP almost does not change, which means that the textual embedding from CLIP performs similarly as random matrix representation when the visual and textual embeddings are not aligned. The third line shows the result of the case where the image</figDesc><table><row><cell>Model</cell><cell cols="3">mAP(%) F1 (k=3) F1 (k=5)</cell></row><row><cell>ML-Decoder?1 (ViT-L-336, 336x336)</cell><cell>36.15</cell><cell>32.45</cell><cell>35.50</cell></row><row><cell>ML-Decoder?3 (ViT-L-336, 336x336)</cell><cell>36.35</cell><cell>31.33</cell><cell>34.89</cell></row><row><cell>ML-Decoder?6 (ViT-L-336, 336x336)</cell><cell>36.34</cell><cell>29.83</cell><cell>33.56</cell></row><row><cell>DM-Decoder?1 (ViT-L-336, 336x336)</cell><cell>36.88</cell><cell>32.95</cell><cell>35.48</cell></row><row><cell>DM-Decoder?3 (ViT-L-336, 336x336)</cell><cell>38.68</cell><cell>34.46</cell><cell>37.50</cell></row><row><cell>DM-Decoder?6 (ViT-L-336, 336x336)</cell><cell>39.01</cell><cell>36.96</cell><cell>39.28</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: Comparison between DM-Decoder and ML-</cell></row><row><cell>Decoder on NUS-WIDE dataset for zero-shot multi-label</cell></row><row><cell>classification.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Full vs. single-layer Pyramid-Forwarding.</figDesc><table><row><cell cols="3">dataset. For a fair comparison, we filter out the overlapped</cell></row><row><cell cols="3">classes with NUS-WIDE in the ImageNet-1k. And we se-</cell></row><row><cell cols="3">lect the first 15k classes in ImageNet-21k without the over-</cell></row><row><cell cols="3">lapped classes with NUS-WIDE, and select 100 images for</cell></row><row><cell cols="3">each class, so that the selected dataset contains 1.3M images</cell></row><row><cell cols="3">which is the same level as ImageNet-1k (1.3M). The result</cell></row><row><cell cols="3">of training on ImageNet-1k is shown on the first line of Ta-</cell></row><row><cell cols="3">ble 8, and the second line shows the result of ImageNet-21k.</cell></row><row><cell cols="3">With the number of available training classes becomes 15</cell></row><row><cell cols="3">times than before, the model mAP increases 6.9 points.</cell></row><row><cell cols="3">Train Dataset Backbone mAP(%) F1(k=3) F1(k=5)</cell></row><row><cell>ImageNet-1k ViT-L-336 31.02</cell><cell>24.98</cell><cell>24.98</cell></row><row><cell>ImageNet-21k ViT-L-336 37.92</cell><cell>39.82</cell><cell>40.39</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell>: Comparisons of our method training on ImageNet-</cell></row><row><cell>1k/ImageNet-21k dataset (filter the overlapped classes with</cell></row><row><cell>NUS-WIDE) and test on NUS-WIDE dataset, with all</cell></row><row><cell>336x336 resolution and ViT-L-336 backbone.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9</head><label>9</label><figDesc></figDesc><table><row><cell>, the</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Label-embedding for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1425" to="1438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Asymmetric loss for multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ben-Baruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.14119</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic diversity learning for zero-shot multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ben-Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ben-Baruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="640" to="650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning semantic-specific graph representation for multilabel image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="522" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Disentangling, embedding and ranking label cues for multilabel image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1827" to="1840" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-label image recognition with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5177" to="5186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Nus-wide: a real-world web image database from national university of singapore</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM international conference on image and video retrieval</title>
		<meeting>the ACM international conference on image and video retrieval</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep learning-based image recognition for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fujiyoshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirakawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamashita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IATSS research</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="244" to="252" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to discover multi-class attentional regions for multi-label image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5920" to="5932" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Generative multi-label zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11606</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A shared multiattention framework for multi-label zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8776" to="8786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Deep learning in agriculture: A survey. Computers and electronics in agriculture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kamilaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Prenafeta-Bold?</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">147</biblScope>
			<biblScope unit="page" from="70" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bilinear attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Vilt: Vision-andlanguage transformer without convolution or region supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5583" to="5594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11336" to="11344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Blip: Bootstrapping language-image pre-training for unified visionlanguage understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.12086</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03557</idno>
		<title level="m">Visualbert: A simple and performant baseline for vision and language</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Medical image classification with convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 13th international conference on control automation robotics &amp; vision (ICARCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="844" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Query2label: A simple transformer way to multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.10834</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-label image classification via knowledge distillation from weakly-supervised detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM international conference on Multimedia</title>
		<meeting>the 26th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="700" to="708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A convnet for the 2020s</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="11976" to="11986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Slip: Self-supervision meets language-image pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.12750</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Discriminative region-based multilabel zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8731" to="8740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Zero-shot learning by convex combination of semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5650</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Image-based manufacturing analytics: Improving the accuracy of an industrial pellet classification system using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Colegrove</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Broadway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">180</biblScope>
			<biblScope unit="page" from="26" to="35" />
		</imprint>
		<respStmt>
			<orgName>Chemometrics and Intelligent Laboratory Systems</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ben-Baruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zelnik-Manor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10972</idno>
		<title level="m">L. 2021a. Imagenet-21k pretraining for the masses</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ben-Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ben-Baruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.12933</idno>
		<title level="m">Ml-decoder: Scalable and versatile classification head</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-label graph convolutional network representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multi-label classification with label graph superimposing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12265" to="12272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multilabel image recognition by recurrently discovering attentional regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="464" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Exploit bounding box annotations for multi-label object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tianyi Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="280" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Vision-Language Pre-Training with Triple Contrastive Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chilimbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="15671" to="15680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attention-driven dynamic graph convolutional network for multi-label image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="649" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Cross-modality attention with semantic graph embedding for multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12709" to="12716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Fast zero-shot image tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5985" to="5994" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
