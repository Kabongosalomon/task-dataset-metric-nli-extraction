<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 1 Tackling the Challenges in Scene Graph Generation with Local-to-Global Interactions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Sangmin</forename><surname>Woo</surname></persName>
							<affiliation key="aff0">
								<address>
									<postCode>61005</postCode>
									<settlement>Gwangju</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Junhyug</forename><surname>Noh</surname></persName>
							<affiliation key="aff0">
								<address>
									<postCode>61005</postCode>
									<settlement>Gwangju</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Kangil</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<address>
									<postCode>61005</postCode>
									<settlement>Gwangju</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 1 Tackling the Challenges in Scene Graph Generation with Local-to-Global Interactions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TNNLS.2022.3159990</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Higher-order contexts: the other components in the scene serve as contexts while predicting the relationship. Data statistics are based on the Visual Genome dataset <ref type="bibr" target="#b0">[1]</ref>.</p><p>Abstract-In this work, we seek new insights into the underlying challenges of the Scene Graph Generation (SGG) task. Quantitative and qualitative analysis of the Visual Genome dataset implies -1) Ambiguity: even if inter-object relationship contains the same object (or predicate), they may not be visually or semantically similar, 2) Asymmetry: despite the nature of the relationship that embodied the direction, it was not well addressed in previous studies, and 3) Higher-order contexts: leveraging the identities of certain graph elements can help to generate accurate scene graphs. Motivated by the analysis, we design a novel SGG framework, Local-to-Global Interaction Networks (LOGIN). Locally, interactions extract the essence between three instances of subject, object, and background, while baking direction awareness into the network by explicitly constraining the input order of subject and object. Globally, interactions encode the contexts between every graph component (i.e., nodes and edges). Finally, Attract &amp; Repel loss is utilized to fine-tune the distribution of predicate embeddings. By design, our</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T O understand a scene, inferring underlying properties such as the relationship between entities (In this work, we use the term "entity" to describe individual detected object instances to distinguish them from "object" in the semantic sense) is just as important as observing explicit information about what and where entities are. However, most state-ofthe-art visual recognition models focus on detecting individual entities in isolation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>, and they are still far from reaching the goal of capturing their relationships. In an effort to incorporate the relational reasoning ability into the model, a scene graph representation -a structured description that captures semantic summaries of entities and their relationshipshas been presented recently <ref type="bibr" target="#b6">[7]</ref>. Since then, a number of works have proposed deep network-based approaches for generating the scene graphs, confirming its importance to the field <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>. While scene graph representation holds tremendous promise, extracting scene graphs from images is known to be challenging.</p><p>In Sec. III, we first explore what the fundamental challenges of the task are: 1) Ambiguity: We postulate the main cause of ambiguity is due to high intra-and low inter-class variability of predicates. Although there is little visual difference between the images, the predicates can be different, and vice versa ( <ref type="figure" target="#fig_0">Fig. 1 (a)</ref>). Therefore, the model should be aware of the inconsistency between the visual and the actual predicate. In other words, we require a model that can recognize a subtle visual difference to differentiate the predicates and learn that the predicates can be the same in a completely different visual context. 2) Asymmetry: By nature, a relationship has a direction.</p><p>Also, we can always define relationships in both directions. Nevertheless, we see that understanding the relational direction has not been well established in previous studies, and there is a lack of consideration on how to effectively address it. In this work, we are particularly interested in bidirectional relationships with asymmetry ( <ref type="figure" target="#fig_0">Fig. 1 (b)</ref>).</p><p>To analyze how much the model understands relational direction, we introduce a new diagnostic task called Bidirectional Relationship Classification (BRC). 3) Higher-order contexts: Often relations need to be considered with the contextual dependency of the whole scene beyond being defined as a pair-wise relation. Suppose there is a horse close to a man ( <ref type="figure" target="#fig_0">Fig. 1 (c)</ref>). Without any other clue, one might say that their relationship is "next to". However, if the presence of other entities and relationships (e.g., hay, eating) is known, the relationship between the horse and the man is more likely to be "feeding". To examine the benefits of higherorder contexts, we quantitatively analyze the amount of information gain given each graph component.</p><p>In Sec. IV, with the aforementioned issues in mind, we present a novel framework, Local-to-Global Interaction Networks (LOGIN). First, LOGIN highlights informative representation between three entity-level instances by weighing how much each pair-wise interaction contributes to relational representation. Second, direction awareness is baked into the model by fusing feature instances in a constrained order (e.g., subject precedes object). Third, LOGIN considers interaction between every scene graph element. The informative contexts essential to accurately predict each graph component are propagated to every graph component. Last but not least, we introduce Attract &amp; Repel Loss, which effectively scales the variability within and between classes making the model robust against ambiguity. We explain this in more detail in Sec. IV-D. By design, LOGIN effectively leverages the complementariness of entity-level interactions and graph-level interactions.</p><p>Finally, in Sec. V, we evaluate our final model on both the Visual Genome benchmark and the BRC task. By ablating each network design, we observe that all design principles cooperate in generating visually grounded scene graphs. Unifying all design principles into a single framework, LOGIN achieved state-of-the-art results on the Visual Genome benchmark while outperforming competing approaches by a comfortable margin on the BRC task.</p><p>Our contributions can be summarized as follows:</p><p>? Through quantitative and qualitative analysis on the Visual Genome dataset, we identify fundamental challenges in the SGG task: 1) Ambiguity, 2) Asymmetry, 3) Higher-order contexts. ? We design a novel framework, Local-to-Global Interaction Networks (LOGIN), to address the aforementioned issues, which achieved competitive results against state-of-the-arts on the Visual Genome benchmark. ? To quantify and concretely see how well the model understands the relational direction, we introduce a new Bidirectional Relationship Classification (BRC) task. Here, LOGIN significantly outperformed state-of-the-art by a 6% of mean performance gain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Numerous works have actively studied the task of recognizing entities and their relationships in various forms. This includes entity localization from natural language expressions <ref type="bibr" target="#b14">[15]</ref>, human-entity interactions <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>, or the more general tasks of visual relationship detection <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>, and scene graph generation <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref>.</p><p>Among them, the scene graph generation has recently drawn much attention.</p><p>The challenging and open-ended nature of the task lends itself to a variety of diverse methods. For example, refining entity and predicate labels using iterative message passing <ref type="bibr" target="#b26">[27]</ref>; staging the generation process in three-step based on the observation that entity labels are highly predictive of predicate labels <ref type="bibr" target="#b27">[28]</ref>; explicitly modeling inter-dependency among entire entities using bi-linear pooling <ref type="bibr" target="#b28">[29]</ref>; leveraging the idea of proposal network <ref type="bibr" target="#b40">[41]</ref> and graph convolution <ref type="bibr" target="#b41">[42]</ref> jointly <ref type="bibr" target="#b29">[30]</ref>; combining both visual and linguistic features to exploit linguistic analogies <ref type="bibr" target="#b31">[32]</ref>; using statistical correlations between entity pairs and their relationships to regularize semantic space <ref type="bibr" target="#b32">[33]</ref>; presenting a multi-agent policy gradient method to replace standard cross-entropy loss and maximize a graph-level metric <ref type="bibr" target="#b34">[35]</ref>; disentangles entity and predicate recognition, enabling sub-quadratic performance <ref type="bibr" target="#b36">[37]</ref>.</p><p>We shed light on three underlying challenges that have not been dealt with in-depth in previous studies: 1) Ambiguity, 2) Asymmetry, and 3) Higher-order contexts (see <ref type="figure" target="#fig_0">Fig. 1</ref>). Similar to ours, the ambiguity issue has been addressed in <ref type="bibr" target="#b35">[36]</ref>, which is a proximal relationship ambiguity arising from multiple subject-object pairs being gathered nearby. On the other hand, we interpret it differently as visual and semantic ambiguity caused by high intra-class variance and low inter-class variance. To the best of our knowledge, the asymmetry issue in SGG is explicitly and importantly addressed for the first time in this work. We believe that some works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b42">43]</ref> can also cope with relational directions, albeit they do not suggest an effective method. Higher-order context problems have been addressed a lot in previous SGG studies <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>, but most focus on context utilization aspects. We rather take a slightly different approach to find the answer to the question, "Are we fully exploiting all the information available?". To this end, we examine the predictability of identity according to given graph components and then apply the most promising way we find to the context propagation step. In summary, we design LOGIN, an integrated framework based on the analysis, to tackle the challenges simultaneously. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. IDENTIFYING CHALLENGES IN SCENE GRAPH GENERATION</head><p>This section seeks quantitative insights on the underlying challenges of the SGG task by analyzing the Visual Genome dataset. In particular, 1) Ambiguity (Sec. III-A): how intra-and inter-class variance hinder clearly differentiating the predicate class boundary, 2) Asymmetry (Sec. III-B): how has relational direction been overlooked, and how can direction awareness be quantified, and 3) Higher-order contexts (Sec. III-C): what higher-order context should be considered to predict the identity of each graph element. Motivated by our findings, we design the LOGIN to better integrate local and global contexts, which will be described in more detail in Sec. IV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Ambiguity</head><p>To gain insight into the Visual Genome scene graphs, we first examine the intra-class variance and inter-class distance within and in between predicate categories. Specifically, we take a close look at label statistics (e.g., subject-object-predicate co-occurrence). Intra-class variance within ith predicate can be calculated as:</p><formula xml:id="formula_0">var intra (i) = 1 N 2 N 2 k=1 (f ik ? ? i ) 2 .<label>(1)</label></formula><p>The inter-class distance between ith and jth predicate is normalized by the co-occurrence frequency:</p><formula xml:id="formula_1">dist inter (i, j) = M i=1 M j=1 N 2 k=1 |f ik ? f jk | M i=1 f i M j=1 f j ,<label>(2)</label></formula><p>where N and M are the number of entities and predicates respectively. f ik denotes the co-occurrence frequency of ith predicate and kth entity pairing, and ? i denotes the mean value. The results are depicted in <ref type="figure" target="#fig_1">Fig. 2</ref>. From the figure, it can be observed that frequently occurring predicates tend to have high intra-class variance, which implies the dominant predicates can be used in various contexts repeatedly (i.e., even the same predicate can pair with various entity pair candidates). In contrast, most of the predicate-predicate pairs have similar subject-object co-occurrence distribution. In this case, even if subject-object identity is known, it becomes difficult to predict the predicate (i.e., predicates in different categories can often pair with the same entity pair).</p><p>In summary, even the same predicates may not be similar visually (see <ref type="figure" target="#fig_0">Fig. 1</ref>) nor semantically. Accordingly, solving these ambiguity issues could play a key role in generating accurate scene graphs. Among the total 108,073 images in the Visual Genome Dataset <ref type="bibr" target="#b0">[1]</ref>, 11,683 images contain 31,660 bidirectional relationships, which can be break down into 29,544 asymmetric relationships and 2,116 symmetric relationships (see <ref type="figure" target="#fig_0">Fig. 1</ref> (b)). Since the majority of relationships are asymmetric (? 93.3%), modeling the relational direction (with regard to the entity orderings) is crucial. b) Modeling Relational Direction: One straightforward approach to obtain a visual representation of a predicate is to use a union appearance feature 1 directly, which is the form used by many previous works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b43">44]</ref>. Using only the union feature is straightforward and reflects the holistic representation, but it entails a fatal problem. For example, even the position of two entities is reversed, the union feature remains the same thus cannot embody directionality without the assistance of external features (e.g., spatial coordinates, contexts). This weakness is more pronounced when predicting relationships in opposite directions at the same time.</p><p>c) Diagnosis of Direction Awareness: It is common sense that all relationships have directions and can always be defined in both directions. However, since most of the relationships that make up the Visual Genome dataset are uni-directional, a rigorous analysis on the direction awareness of model is limited. In other words, good performance can be achieved in the Visual Genome dataset without due consideration of the direction awareness.</p><p>To this end, we introduce a new diagnostic task called Bidirectional Relationship Classification (BRC) to quantify and concretely see how well the model understands the relational direction. The task is solely based on the collected images containing bidirectional relationships. Therefore, in all cases, good performance can only be achieved by understanding the direction.</p><p>What we want to observe in the BRC benchmark is how much the model understands the direction of the relationship. Among the three common criteria for evaluating performance in SGG, SGGen includes not only predicate prediction but also object localization and object class prediction, making it difficult to evaluate direction prediction intensively. Likewise, since SGCls includes class prediction of objects, it may be difficult to strictly verify the directional understanding. Therefore, we adopt PredCls evaluation criterion that measures only predicate predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Higher-Order Contexts</head><p>To investigate the benefits of higher-order context, we measure how much information is gained given the identity of different scene graph elements. Motivated by <ref type="bibr" target="#b27">[28]</ref>, we plot the likelihood of guessing labels of target element given labels of other graph elements in <ref type="figure" target="#fig_5">Fig. 4</ref>. In addition to node-conditioned guessing performed on prior work <ref type="bibr" target="#b27">[28]</ref>, we further analyze the predictability improvement given the edge identity. To disentangle the significance of semantic knowledge from image cues (e.g., appearance, spatial), no image features are used <ref type="bibr" target="#b0">1</ref> A union appearance feature is pooled from the RoI feature that tightly encompasses two (subject and object) entities.   and are guessed using only label statistics (i.e., subject-objectpredicate co-occurrence). A higher curve implies that given graph elements are more decisive in guessing the target element.</p><p>In the case of edge, it is greatly affected by the identity of neighboring nodes, consistent with our intuition. What is more noteworthy here is that even only one edge in the opposite direction is known, nearly 90% accuracy can be achieved within just five guesses. It can also provide complementary information in determining the identity of the target edge when given with the neighboring nodes' identity.</p><p>In the case of node, it has less correlation with adjacent graph elements than edges. However, as shown in the figure, a significant amount of information can be obtained whenever the identity of adjacent graph elements is known one by one. This fact motivates the use of as much information as possible to correctly recognize the identity of each element.</p><p>To sum up, we see that both node and edge can most effectively exploit inductive bias when utilizing all the identities of adjacent graph elements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. LOCAL-TO-GLOBAL INTERACTION NETWORK</head><p>Based on the analysis in Sec. III, we design a novel framework LOGIN that aims to handle said issues in a bottomup manner. Each building block in LOGIN is specialized in + Initial Scene Graph  tackling specific challenges and also works complementary to each other. An overview of LOGIN is shown in <ref type="figure" target="#fig_7">Fig. 5</ref>. a) Problem Setup: Given an image I, the detector predicts a set of entity proposals. For each entity proposal, it outputs a Region of Interest (RoI) Aligned <ref type="bibr" target="#b2">[3]</ref> visual appearance feature a i ? R 256?7?7 , a bounding box prediction b i ? R 4 , and initial classification logit c i ? R 151 . In practice, a standard entity detector Faster R-CNN <ref type="bibr" target="#b40">[41]</ref> is used as a bounding box model.</p><p>Starting from a set of entity proposals (equivalent to a set of nodes in scene graphs), visual features are pooled from the subject and object boxes that form a relationship and from a union box to utilize contextual information (e.g., background) via RoI-Align operation, then predict the node and edge labels through scene graph generation head in turn.</p><p>The initial scene graph comprises a set of node representations N and a set of edge representations E. The ith initial node representation x i ? N is obtained by fusing three important cues in the image:</p><formula xml:id="formula_2">x i = ?([a i || b i || c i ]),</formula><p>where ? is an embedding function and || denotes concatenation operation. The edge representations E are obtained through several stages of process that will be described in the following.</p><p>The final scene graph is composed of a set of node label distributions N ? R N ?151 (including no-object class) and a set of edge label distributions E ? R M ?51 (including no-relation class), where N and M is the number of total nodes and edges respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Local Interaction Head</head><p>We posit the underlying vulnerability to ambiguity stems from the inability to capture subtle yet discriminative repre- sentations. Inspired by the recent successes of attention based fine-grained recognition works <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51]</ref>, where the intra-class variance is usually high and vice versa for inter-class, we adopt the idea of attention mechanism. In particular, as for Local-Interaction Head (LIH), we formulate the instance-level interaction as a non-local operation <ref type="bibr" target="#b47">[48]</ref>. normalized by the sum:</p><formula xml:id="formula_3">X = [ || || ] Z=[ || || ] 3?C?H?W q k v 3HW?C? ? 3?C? ?H?W 3?C? ?H?W ? f + 3?C? ?H?W C? ?3HW 3HW?C? 3HW?3HW softmax 3HW?C? 3?C?H?W 3?C? ?H?W</formula><formula xml:id="formula_4">? ij = e q(xi) T k(xj ) ?j e q(xi) T k(xj ) .<label>(3)</label></formula><p>The interaction intensity ? ij is multiplied with the representation of ith individual v(x i ) followed by a transformation function f(?). The output of LIH opeartion is given by:</p><formula xml:id="formula_5">z ij = f(? T ij v(x i )) + x i .<label>(4)</label></formula><p>For the sake of better gradient flow while learning the LIH, a residual-connection (+x i ) is added. In practice, 1 ? 1 ? 1 convolutional operations is used for all embedding functions (q(?), k(?), v(?), f(?)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Encoding Direction-Awareness</head><p>Before moving on to the next stage, there is an open choice on how to fuse three instance features {z s , z o , z u } obtained earlier to initialize a graph-level predicate (edge) representation. As suggested in <ref type="bibr" target="#b51">[52]</ref>, summing up all possible permutations of instance features could be a generic method for relational inference. The effectiveness of using permutation has been empirically demonstrated in prior works <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55]</ref>, but the directionality cannot be guaranteed since summing is commutative. In other words, the permutation sets are identical even if the ordering of two instances are reversed (i.e., the identity of subject and object are switched). A simple sidestep is to use the embeddings of concatenated instance features, which is the form used in the several previous works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b42">43]</ref>. However, this also has the disadvantage of losing the benefits of permutation.</p><p>We would like the LOGIN to be equivariant under the subject-object ordering (i.e., relational direction) while being invariant to the permutation. Let the interaction between two entities as a set of permutations, S, and directional relationship as any subset except the empty and the universal </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Node-to-Edge</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Edge-to-Node</head><p>Edge-to-Edge <ref type="figure">Fig. 8</ref>: An illustration of layer-wise propagation of Global-Interaction Head. Nodes and edges are colored in blue and green respectively. The layerwise context propagation of GIH in scene graph (a) can be represented as a bipartite graph in (b). As a comparison, conventional GCN <ref type="bibr" target="#b41">[42]</ref> and GAT <ref type="bibr" target="#b55">[56]</ref> only consider node-wise propagation (black and blue edges) and are unable to leverage edge information. An experimental comparison of GIH with GCN and GAT is in <ref type="table" target="#tab_3">Table VI.</ref> set, r i ? S, where r i = ? and r i = S, i ? {f, b}. If the two opposite-sided relationship (forward and backward) subsets are disjoint, r f ? r b = ?, and their union is universal, r f ? r b = S, the relationship encoding of two subsets can always be semantically distinguished. Under these premises, we specifically use a regulated set of permutations in which the subject always precedes the object -{SOU, SU O, U SO} -to represent a relationship in one direction. This simple strategy guarantees that subject only appears in the first two bins, and that object only appears in the last two bins. Thus the model can clearly distinguish between forward and backward relationships while sharing entity instance features. Note that this strategy is just a straightforward method to make half of the entire permutation set represent the forward direction and the other half represent the backward direction, and it does not matter which combination of permutations is used.</p><p>Formally, the three instances from an input set {z s , z o , z u } are concatenated in a constrained order, providing inherent bias of the directionality. They are then transformed via shared MLP (denoted as ? in the below equation) and additively fused to make the predicate representation invariant to the input permutations. i th predicate representation e i ? E can be obtained as:</p><formula xml:id="formula_6">e i = j,k,l?{z s ,z o ,z u } ?([j || k || l]),</formula><p>where z s precedes z o and j = k = l.</p><p>(5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Global Interaction Head</head><p>From a graph perspective, the fully-connected layer can be seen as the most basic form of message passing network with all nodes connected, but it is known to be not effective in learning the graph. For effective context aggregation, well structuring the message paths (i.e., connectivity between nodes) is the key issue. Based on the observation in Sec. III-C, we design a Global Interaction Head (GIH) that enables effective message flow between informative graph components. We formulate the graph-level interaction with global message passing scheme <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b56">57]</ref>.</p><p>To maintain a structured representation of a scene graph, we utilize local connectivity information in the form of a block matrix with four quadrants A ? R (N +M )?(N +M ) . Each    <ref type="bibr" target="#b0">(1)</ref> or not (0) -the number of nodes and edges are denoted as N and M respectively. We consider that all node pairs and node-edge pairs that make up a relationship are interconnected. In the case of edge-edge, they are considered to be connected when the opposite direction edge exists, although there is no explicit connection on the graph (see <ref type="figure">Fig. 8</ref>).</p><formula xml:id="formula_7">A = A n?n A n?e A e?n A e?e .<label>(6)</label></formula><p>To preserve the original message, identity matrix (selfconnection) is added to A, resulting? = A + I. An initial graph-level feature matrix G (0) ? R (N +M )?D is defined as:</p><formula xml:id="formula_8">G (0) = N E .<label>(7)</label></formula><p>The lth layer-wise propagation rule for GIH is defined as:</p><formula xml:id="formula_9">G (l) = ? ? ? max 0,?G (l?1) W (l?1) , l = odd.</formula><p>G (l?2) + max 0,?G (l?1) W (l?1) , l = even.</p><p>We add residual connections between the layers for a better optimization <ref type="bibr" target="#b57">[58]</ref>. Multi-layer GIH can perform long-range multi-hop communication, effectively modeling the desired higher-order relational reasoning. While training, weight matrix W ? R D?D is learned by gradient.</p><p>Finally, the upper N rows (N ? R N ?D ) and the lower M rows (E ? R M ?D ) of the output matrix are softmax-ed and used to predict entity and predicate labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Attract &amp; Repel Loss</head><p>We introduce an Attract &amp; Repel Loss to explicitly handle the intra-and inter-class variance. The conceptual mechanism of Attract &amp; Repel loss is shown in <ref type="figure" target="#fig_0">Fig. 10</ref>. In a nutshell, if the identities of the input and reference embeddings are the same (i.e., category matches), the loss forces them to attract each other; otherwise, the loss compels them to repel each other. The reference embeddings can be divided into two types: we refer to the running mean of the matched reference embeddings as positive (pos), and negative (neg) for that of non-matched. Note that the reference type is only an abstract distinction and can be vary depending on the identity of the input embedding. As the input embeddings are learned to approach the positive and move farther away from the negatives, the distribution within categories becomes dense, and between categories becomes sparse. As a result, the loss gathers the embeddings of each class into compact and well-separated clusters. Since most bidirectional relationships are asymmetric (i.e., identities of predicates in opposite directions are mostly different) as we have seen in Sec. III-B, the loss has the potential benefit in predicting predicates in opposite directions differently. Formally, at tth batch, given a set of predicate embeddings <ref type="bibr" target="#b50">51</ref> } is adjusted with the following update rule:</p><formula xml:id="formula_11">E (t) = {e (t) 1 , ..., e (t) M }, a set of references R (t) = {r (t) 1 , ..., r (t)</formula><formula xml:id="formula_12">r (t) m = r (t?1) m * N(r (t?1) m ) + i?pos e (t) i ? j?neg e (t) j N(r (t?1) m ) + N(pos) + N(neg)</formula><p>, <ref type="bibr" target="#b8">(9)</ref> where N(?) denotes the number of embeddings considered in reference update. Finally, the input predicate embeddings are adjusted with the following Attract &amp; Repel loss:</p><formula xml:id="formula_13">L ar = m i?pos 1 ? r (t) m ? e (t) i |r (t) m | |e (t) i | + m j?neg r (t) m ? e (t) j |r (t) m | |e (t) j | .<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Loss Function</head><p>LOGIN can be trained in an end-to-end manner, allowing the network to predict bounding boxes, entity categories, and relationship categories at once. The total loss function for an image is defined as:</p><formula xml:id="formula_14">L image = L ent + L pred + L ar ,<label>(11)</label></formula><p>where L ent and L pred are both cross-entropy loss for entity and predicate classification, respectively. L ar stands for the Attract &amp; Repel loss. By default, hyperparameters of joint loss function are set as 1:1:1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>In this section, we conduct comprehensive studies to validate the efficacy of LOGIN. We perform extensive ablation experiments to thoroughly demonstrate the effectiveness of each building block of LOGIN. LOGIN is evaluated on Visual Genome <ref type="bibr" target="#b0">[1]</ref> benchmark and achieves state-of-the-art results. Notably, in our proposed Bidirectional Relationship   Classification (BRC) task, LOGIN successfully distinguishes asymmetric relationships and is more accurate than existing methods.</p><p>The model referred to as the BASELINE in this section is a model without any proposed design principles. It directly predicts the entity and predicate categories from the RoI-Aligned visual features of entity instances and that of union of two entity instances, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Settings a) Model Parameter and Training Details:</head><p>For a fair comparison, most of the settings and details follow pioneer work <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>. We adopt the Faster R-CNN <ref type="bibr" target="#b40">[41]</ref> detector with VGG backbone <ref type="bibr" target="#b5">[6]</ref>. Following <ref type="bibr" target="#b27">[28]</ref>, we use per-class NMS to reduce the number of entity proposals. The number of entity proposals is 64 (i.e., N =64). We optimize the model using SGD with the following details: initial learning rate (1e-3), momentum (0.9), and weight decay (5e-4). We first pre-train the detector on Visual Genome Dataset and then train the proposed scene graph generation head while fixing the detector weight. To model geometric relationships, we first concatenate two extra channels with coordinates hard-coded (2?7?7) to the initial visual representation and then pass them through a convolutional layer <ref type="bibr" target="#b58">[59]</ref>. As for the Attract &amp; Repel loss, we sample negatives by the number of positives to avoid being heavily affected by negatives. b) VG Dataset: We train and evaluate LOGIN on Visual Genome (VG) Dataset <ref type="bibr" target="#b0">[1]</ref>. We use the publicly released preprocessed data (train and test split is 75K and 32K) <ref type="bibr" target="#b26">[27]</ref>. The number of entity and predicate categories are 150 and 50, respectively. c) BR Dataset: We build a Bidirectional Relationship (BR) dataset to evaluate the direction awareness of the model. The BR dataset is a subset of VG dataset and is created by filtering out relationships with only one edge between the two nodes. As a result, the BR dataset always includes the relationships that have two bidirectional edges between the nodes (e.g., man riding ? ??? ? horse, horse under ? ??? ? man). As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, about 93% of bidirectional edges form different relationships depending on the direction (i.e., direction-sensitive), and only about 7% of bidirectional edges have the same relationship regardless of direction (i.e., direction-agnostic). The distribution of BR Dataset is shown in <ref type="figure" target="#fig_12">Fig. 9</ref>. Here, the five most frequent entity categories and predicate categories are "man (5466), window (1976), woman (1912), building (1640), shirt (1632)", and "on (8766), has (6669), of (3137), with (2292), wearing (2238)", respectively. Note that the top-5 predicate categories account for about 73% of the total predicates. This shows that the dominant predicate categories are often used in various contexts repeatedly, implying that variance may be high even within the same predicate category. That is to say, this biasness supports our argument that dealing with ambiguity issue is essential. d) Evaluation Setup: Model is evaluated with the following three standard evaluation criteria <ref type="bibr" target="#b26">[27]</ref>:</p><p>1) Predicate Classification (PredCls): Given ground truth boxes and labels, predict edge labels. 2) Scene Graph Classification (SGCls): Given ground truth boxes, predict box and edge labels. 3) Scene Graph Generation (SGGen): Predict boxes, box labels, and edge labels. As for SGG, following the prior works <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30]</ref>, we use Recall@K (R@K) as an evaluation metric since mAP-like metrics are not appropriate due to the sparse annotation in Visual Genome. Specifically, we use image-wise Recall@{20,50,100}, which computes the fraction of ground-truth triplets found in the top-K predicted triplets. We also adopt the mean Recall@K (mR@K) metric <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b37">38]</ref> for evaluation, which retrieves each individual predicate and then averages R@K over all predicate categories.</p><p>As for BRC, conventional triplet recall-based metrics only consider uni-direction, making it difficult to make a rigorous evaluation of direction awareness. To this end, we have come to introduce a new metric called pair-wise Recall (pR@K) that fits the BRC task. The proposed metric is considered to be "matched" only when bidirectional relationships are both correct. Formally, the pR@K calculates the fraction of the total amount of matched bidirectional relationships (BRs):</p><formula xml:id="formula_15">pR@K = |{top-K predicted BRs} ? {total BRs}| |{total BRs}| .<label>(12)</label></formula><p>This constraint severely penalizes if the relationship predictions in the opposite direction are the same. Therefore, models without direction awareness cannot receive a high score on this metric. For example, if only union features are used, there is no chance that asymmetric relationships are correctly predicted since only the same results are output for BRs. To get a high score from this metric, the model needs direction awareness that is essential to correctly predict asymmetric relationships that account for most BRs in the BR datasets. Specifically, we use pR@{2, 4, 8, 16} in the BRC task since only a few bidirectional relationships are annotated per image (?3 BRs / image).  compared in <ref type="table" target="#tab_3">Table I</ref> for each evaluation criterion. We compare LOGIN with the recent approaches <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref>. While LOGIN appears to show competitive results against the state-of-the-arts in all criteria, note that there is no specific method that achieves the best performances in every evaluation criteria, making it difficult to judge the superiority among the SGG methods. We also benchmark LOGIN under the mean Recall (mR@K) criteria. The results are shown in <ref type="table" target="#tab_3">Table II</ref>. The mean Recall is measured by averaging the Recall per each class for the entire classes. Therefore, unlike conventional Recall (R@K), it is irrelevant to the number of samples in each class, and even if high performance is obtained in a class with a large number of samples, it is difficult to achieve good values if low performance is obtained in a class with a small number of samples. That is, every class should obtain a good overall recall to achieve high performance. In short, it is important to accurately predict the class with few data, especially in tail, among long-tailed VG dataset. The long-tailed distribution of VG dataset also implies that the dominant predicates frequently appear in multiple contexts. Thus, it is also related to the ambiguity issue. We see that LOGIN consistently outperforms recent methods in mean Recall criteria (see <ref type="table" target="#tab_3">Table VII</ref> for Recall of individual predicate), implying that our system effectively deals with ambiguity issue. b) Bidirectional Relationship Classification (BRC): To independently evaluate the direction-awareness of the model, we specifically use PredCls criteria, which is orthogonal to the entity detection. We compare LOGIN with recent approaches <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36]</ref>. The results are summarized in <ref type="table" target="#tab_3">Table III</ref>. Here, although <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33]</ref> use the initial predicate representation as a union feature, they enable understanding of relational direction by incorporating contexts with iterative bipartite message passing, attentional graph convolution, and  knowledge embedded routing, respectively. By using direction sensitive embedding and contextual information at the same time, LOGIN can outperform the recent methods by a large margin (6% of mean performance gain compared to the stateof-the-art), implying that directional bias as well as contexts are crucial in recognizing direction. LOGIN is in a competitive position for VG dataset, which mainly contains uni-directional relationships, but significantly improves performance, especially for bidirectional relationships, which are common in the real world.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison with</head><p>C. Quantitative Analysis a) Model Ablations: We consider several ablations to investigate the importance of the major design choices in Table IV (a). For clarity, we show the performance in the SGG task and the BRC task in a single is not significant, using the loss with other components (Exp7) can further push the performance than without it (Exp 6). When all model components are combined (Exp 7), the model achieves the best performance in both SGG and BRC tasks, which implies that each component contains an orthogonal factor that complementarily boosts the performance. b) Optimal Variables:</p><p>We conduct experiments to decide optimal variables of LOGIN in <ref type="table" target="#tab_3">Table IV</ref> The optimal feature extraction method is first investigated. Here, flattening the feature maintains richer information than pooling, thus shows the best results among the three choices: AVGPOOL, MAXPOOL, FLATTEN. Then we examine the optimal number of layers in GIH: 4-layers produce the best results. Stacking multiple layers enables multi-hop communication, though it also increases the chance of introducing noisy information. On the other hand, stacking few layers cannot fully capture the higher-order contexts.</p><p>c) Design Choices of DSE: In this experiment, we further explore the four design choices of Direction-Sensitive Encoding (DSE). Specifically, we investigate two approaches that have been adopted in most existing SGG literature -(a) using only a union feature <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b43">44]</ref> (BASELINE) and (b) fusing subject, object, and union withoutpermutations <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b42">43]</ref> -and two variants of subject, object, and union fusion under the subject-precedesobject constraint -(c) sequential fusion and (d) parallel fusions (see <ref type="figure" target="#fig_0">Fig. 11</ref>). Except for the (a) among the four cases, the ordering of the subject and the object is fixed and therefore meets the directionality condition. Additionally, (c) and (d) consider the sum of all possible permutations. The difference between (c) and (d) is the order of fusion. We conduct experiments in two settings for performance comparison on fusion methods. The results are summarized in <ref type="table" target="#tab_7">Table V</ref>. Here, the performance difference between the four fusion methods in the SGC setting (Table V (a)) is not prominent, while the significance of combining three features is particularly evident in the BRC setting (Table V (b)), suggesting that union feature alone cannot give relational direction. In both settings, the use of permutations at the fusion phase showed better results than otherwise, and especially when fused in parallel, it showed the best results. d) Effectiveness of GIH: We examine the effectiveness of GIH by comparing GIH with two representative message passing graph neural networks in <ref type="table" target="#tab_3">Table VI</ref>: Graph Convolutional Network (GCN) <ref type="bibr" target="#b41">[42]</ref> and Graph Attention Network (GAT) <ref type="bibr" target="#b55">[56]</ref>. GCN aggregates feature information via a non-  euclidean convolution operation from a node's neighborhood. As opposed to GCNs, GAT allows for implicitly assigning different importances to nodes of a same neighborhood, enabling a leap in model capacity. Unlike them, layer-wise propagation rule of GIH considers not only nodes but also edges as a neighborhood, allowing the model to leverage higher-order contexts for node update. From the results, we see that GAT does not improve the performance upon the GCN. The results demonstrate the effectiveness of GIH in predicting both object and relationships categories (Scene Graph Classification). Since LOGIN equipped with GIH exploits richer information (e.g., edge), it is also strong in understanding relational direction (Bidirectional Relationship Classification). e) Per-type Predicate Recall: We expect the model to better understand each predicate by allowing attention mechanism of LIH to capture the predicate label-specific representation well and Attract &amp; Repel Loss to help separate inter-class and aggregate intra-class predicates in the embedding space. In order to ensure that the proposed model solves the ambiguity issue well, we compare our LOGIN with Baseline under the Recall@50 metric for the top-20 frequent predicates in <ref type="table" target="#tab_3">Table VII</ref>. Compared to Baseline, we observe a significant performance improvement in all predicate classes. Specifically, our system better understands the geometric predicate (e.g.., on, in front of, behind, above, under), possessive predicates (e.g., has, of, wearing), and semantic predicates (e.g., holding, walking). This suggests that explicit separation on predicate embedding space properly solves the ambiguity problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Qualitative Analysis</head><p>To better see how LOGIN understands the relational direction, we provide qualitative examples in <ref type="figure" target="#fig_0">Fig. 12</ref>. Here, We compare the result of BASELINE model and LOGIN with the corresponding ground-truth scene graph. As we can see in the results of first two rows, BASELINE model produces the same result for a pair of entities regardless of direction. What is worse is that the whole scene graphs use almost the same predicates for defining relationships. In other words, the BASELINE model neither considers relational-direction nor lexical diversity. On the other hand, LOGIN can successfully identify relational direction, thanks to the embedded direction-awareness, and it is also more diverse in terms of vocabulary. More interestingly, even though predictions of LOGIN are not matched, the results are seemingly plausible. For example, in the third row, detected tail, legs, and face of an elephant are false positives in terms of ground-truth, but they seem to be correct in reality. Also, relationships associated with false positives are somewhat reasonable (e.g., elephant has ? ? ? leg, leg of ? ? elephant).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>This paper discusses three fundamental challenges in SGG task: 1) Ambiguity, 2) Asymmetry, and 3) Higher-order contexts. Motivated by the analysis and to tackle the issues effectively, we present a new unified framework, LOGIN. Our framework enables predicting the scene graph in a localto-global and bottom-to-up manner, leveraging the possible complementariness effectively. We achieved state-of-the-art on Visual Genome benchmark. Last but not least, we present a new diagnostic task called Bidirectional Relationship Classification (BRC) and observe that our method outperforms competing methods significantly. : : <ref type="figure" target="#fig_0">Fig. 12</ref>: Qualitative examples. The first column shows input images with entity proposals. From the second to fourth columns, we show the scene graphs of ground-truth, BASELINE, and LOGIN respectively. The bounding boxes or nodes are colored in either blue (correct) or red (wrong). The predicates are colored in either green (correct) or yellow (wrong). Examples of the first two rows contain bidirectional relationships, but not the rest. We see that LOGIN produces more diverse predicates and can successfully distinguish asymmetric relationships while BASELINE model fails.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Challenges in Scene Graph Generation. (a) Ambiguity: different predicates may be visually similar (first two), and the same predicate may not be visually similar (last two). (b) Asymmetry: relationships have direction, and those relationships in the opposite direction are mostly different (asymmetric). (c)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Intra-and inter-class analysis on predicates. (top) predicate labels are arranged in the order of proportion (green bar). More frequent predicates tend to have higher intra-class variance (orange line). (bottom) Each block represents the degree to which predicate-predicate pair share the same entity pairs in color -the more overlapping entity pairs, the brighter the block color. Except for a few predicate pairs (e.g., on-flying in), most predicate pairs have low inter-class distance -the closer the brighter. Axis labels best viewed zoomed in on screen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Examples of bidirectional relationships in Visual Genome dataset. (left) Input images with ground-truth bounding boxes (right) corresponding bidirectional relationship scene graph. nodes and edges are colored in blue and green respectively. As can be seen in the figure, most bidirectional relationships are asymmetric.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>B. Asymmetry a) Bidirectional Relationships: Given two different entities A and B, if the both A ? ? ? B and B ? ? ? A relationships are defined, we consider those relationships as bidirectional relationships. Among them, if ? = ?, we denote as asymmetric relationships, otherwise as symmetric relationships. Examples of bidirectional relationships are shown in Fig. 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 :</head><label>4</label><figDesc>How much information does each graph component contain? The figures show the likelihood of guessing the label of target element given the identity of neighboring graph components -head, tail, and edge. Guesses were made by looking up the empirical distribution over label statistics in the training set (e.g., top-k frequent classes given graph elements). h2t refers to the edge from the head node to the tail node, and t2h refers to the edge of the opposite direction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 :</head><label>5</label><figDesc>A high-level overview of LOGIN. (a) Local Interaction Head (Sec. IV-A): locally, interactions extract the essence between three instances -subject, object, and background. (b) Direction-Sensitive Encoding (Sec. IV-B): model become aware of relational direction by constraining the input order. (c) Global Interaction Head (Sec. IV-C): globally, interactions encode the contexts between every graph components -nodes and edges, allowing the model to encode richer contextual information. (d) Attract &amp; Repel Loss (Sec. IV-D): embeddings of each predicate categories are gathered into compact and well separated clusters by the loss. Combining all together, we build an end-to-end, unified framework that predicts a visually grounded scene graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>3?C?H?WFig. 6 :</head><label>6</label><figDesc>Local-Interaction Head learns what (3?C) and where (H?W) to attend. It adaptively learns to emphasize informative representation between three entity-level instances by weighing how much each pair-wise interaction contributes to relational representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7 :</head><label>7</label><figDesc>LIH learns to highlight relationship-centric representations and suppress the noise since the non-local operation considers all individuals to compute responses to the target individual.The intensity of pair-wise interaction is calculated over the three entity (node) features {x s , x o , x u } (each refers to subject, object, and union) (seeFig. 6). Given concatenated featuresX = [x s ||x o ||x u ], LIH outputs refined features Z = [z s ||z o ||z u ].The interaction intensity between i and jth individual is computed by the embedded gaussian kernel (e q(?) , e k(?) ) and Direction-sensitive encoding. Between a pair of entities, the subject and the object can be switched, and two opposite-direction relationships are usually asymmetric. For example, the relationships of Man ? Horse and Horse ? Man are generally different. For both-side relationships, we fed all possible permutations of entity-level features (e.g., subject, object and union) that satisfy the conditions of "subject precedes object" into the same MLP. Note that although both relationships follow the same condition, color combinations (e.g., orange, green, blue) vary with direction. The final relationship is predicted after summing all the outputs of the MLP. During training, the MLP learns to generate different outputs for the opposite-direction relationships, thus it becomes aware of the relational direction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 9 :</head><label>9</label><figDesc>The distribution of categories in the BR Dataset. (a) Frequency of entity categories and (b) predicate categories. For both entity and predicate, the top-10 categories and the bottom-10 categories are highlighted based on frequency. Axis labels best viewed zoomed in on screen. quadrant from top left to bottom right indicates whether the node-node (A n?n ? R N ?N ), node-edge (A n?e ? R N ?M ), edge-node (A e?n ? R M ?N ), and edge-edge (A e?e ? R M ?M ) are connected</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 10 :</head><label>10</label><figDesc>Mechanism of Attract &amp; Repel loss. The reference embeddings attract the positives and repel the negatives, making intra-class distribution dense and inter-class distribution sparse. For simplicity, we visualize only one positive and negative instance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>State-of-the-Art a) Scene Graph Generation (SGG): The Recall performance of the proposed method and existing methods are</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 11 :</head><label>11</label><figDesc>Illustration of feature fusion methods to obtain initial predicate representation. (a) BASELINE: use union feature only. (b) w/o Permutation: concatenate all and fuse them without permutation. (c) Sequential: fuse subject and object first, and then with union. (d) Parallel: fuse all the permutations at once where the subject precedes object. Here, S, O, U respectively denotes subject, object, and union.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I :</head><label>I</label><figDesc>Comparison with the state-of-the arts on Visual Genome benchmark. R@k denotes Recall in the top-k predictions.</figDesc><table><row><cell></cell><cell></cell><cell>SGGen</cell><cell></cell><cell></cell><cell>SGCls</cell><cell></cell><cell></cell><cell>PredCls</cell><cell></cell></row><row><cell>Models</cell><cell>R@20</cell><cell cols="8">R@50 R@100 R@20 R@50 R@100 R@20 R@50 R@100</cell></row><row><cell>IMP [27]</cell><cell>-</cell><cell>3.4</cell><cell>4.2</cell><cell>-</cell><cell>21.7</cell><cell>24.4</cell><cell>-</cell><cell>44.8</cell><cell>53.0</cell></row><row><cell>MOTIFNET [28]</cell><cell>21.4</cell><cell>27.2</cell><cell>30.3</cell><cell>32.9</cell><cell>35.8</cell><cell>36.5</cell><cell>58.5</cell><cell>65.2</cell><cell>67.1</cell></row><row><cell>GRAPH R-CNN [30]</cell><cell>-</cell><cell>11.4</cell><cell>13.7</cell><cell>-</cell><cell>29.6</cell><cell>31.6</cell><cell>-</cell><cell>54.2</cell><cell>59.1</cell></row><row><cell>KERN [33]</cell><cell>-</cell><cell>27.1</cell><cell>29.8</cell><cell>-</cell><cell>36.7</cell><cell>37.4</cell><cell>-</cell><cell>65.8</cell><cell>67.6</cell></row><row><cell>CMAT [35]</cell><cell>22.1</cell><cell>27.9</cell><cell>31.2</cell><cell>35.9</cell><cell>39.0</cell><cell>39.8</cell><cell>60.2</cell><cell>66.4</cell><cell>68.1</cell></row><row><cell>VCTREE [34]</cell><cell>22.0</cell><cell>27.9</cell><cell>31.3</cell><cell>35.2</cell><cell>38.1</cell><cell>38.8</cell><cell>60.1</cell><cell>66.4</cell><cell>68.1</cell></row><row><cell>RELDN [36]</cell><cell>21.1</cell><cell>28.3</cell><cell>32.7</cell><cell>36.1</cell><cell>36.8</cell><cell>36.8</cell><cell>66.9</cell><cell>68.4</cell><cell>68.4</cell></row><row><cell>LOGIN (OURS)</cell><cell>22.2</cell><cell>28.2</cell><cell>31.4</cell><cell>35.5</cell><cell>38.8</cell><cell>40.5</cell><cell>61.1</cell><cell>66.6</cell><cell>68.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II :</head><label>II</label><figDesc>The SGG results on mean Recall (mR@K). mR@k denotes average R@K over all predicate categories.</figDesc><table><row><cell></cell><cell></cell><cell>SGGen</cell><cell></cell><cell></cell><cell>SGCls</cell><cell></cell><cell></cell><cell>PredCls</cell><cell></cell></row><row><cell>Models</cell><cell>mR@20</cell><cell>mR@50</cell><cell cols="7">mR@100 mR@20 mR@50 mR@100 mR@20 mR@50 mR@100</cell></row><row><cell>IMP [27]</cell><cell>-</cell><cell>3.8</cell><cell>4.8</cell><cell>-</cell><cell>5.8</cell><cell>6.0</cell><cell>-</cell><cell>9.8</cell><cell>10.5</cell></row><row><cell>MOTIFNET [28]</cell><cell>4.2</cell><cell>5.7</cell><cell>6.6</cell><cell>6.3</cell><cell>7.7</cell><cell>8.2</cell><cell>10.8</cell><cell>14.0</cell><cell>15.3</cell></row><row><cell>KERN [33]</cell><cell>-</cell><cell>6.4</cell><cell>7.3</cell><cell>-</cell><cell>9.4</cell><cell>10.0</cell><cell>-</cell><cell>17.7</cell><cell>19.2</cell></row><row><cell>VCTREE [34]</cell><cell>5.2</cell><cell>6.9</cell><cell>8.0</cell><cell>8.2</cell><cell>10.1</cell><cell>10.8</cell><cell>14.0</cell><cell>17.9</cell><cell>19.4</cell></row><row><cell>LOGIN (OURS)</cell><cell>5.9</cell><cell>7.7</cell><cell>9.1</cell><cell>8.6</cell><cell>11.2</cell><cell>12.4</cell><cell>16.0</cell><cell>19.2</cell><cell>22.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III :</head><label>III</label><figDesc>Comparison with recent approaches in the BRC task.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">PredCls</cell><cell></cell></row><row><cell>Models</cell><cell cols="2">pR@2 pR@4</cell><cell cols="2">pR@8 pR@16</cell></row><row><cell>IMP [27]</cell><cell>6.3</cell><cell>9.1</cell><cell>12.2</cell><cell>15.0</cell></row><row><cell>MOTIFNET [28]</cell><cell>7.7</cell><cell>11.5</cell><cell>15.9</cell><cell>19.5</cell></row><row><cell>GRAPH R-CNN [30]</cell><cell>7.9</cell><cell>11.7</cell><cell>16.3</cell><cell>21.0</cell></row><row><cell>KERN [33]</cell><cell>7.7</cell><cell>12.1</cell><cell>16.7</cell><cell>20.7</cell></row><row><cell>VCTREE [34]</cell><cell>8.0</cell><cell>11.9</cell><cell>16.1</cell><cell>21.0</cell></row><row><cell>RELDN [36]</cell><cell>8.0</cell><cell>12.5</cell><cell>16.4</cell><cell>20.8</cell></row><row><cell>LOGIN (OURS)</cell><cell>8.6</cell><cell>13.1</cell><cell>17.6</cell><cell>21.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV :</head><label>IV</label><figDesc>(a) Ablation studies on network design. (b) Optimal variable search.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">(a) Ablation Studies</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(b) Optimal Variable Search</cell></row><row><cell></cell><cell>Ablations</cell><cell></cell><cell>SGCls</cell><cell></cell><cell></cell><cell cols="2">PredCls</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SGCls</cell></row><row><cell>Exp</cell><cell>LIH DSE GIH Lar</cell><cell cols="7">R@20 R@50 R@100 pR@2 pR@4 pR@8 pR@16</cell><cell></cell><cell>Variables</cell><cell cols="3">R@20 R@50 R@100</cell></row><row><cell>1 2 3</cell><cell></cell><cell>30.8 33.5 31.8</cell><cell>34.7 37.5 36.2</cell><cell>36.2 39.6 37.7</cell><cell>0.2 8.0 8.1</cell><cell>0.4 11.5 11.5</cell><cell>0.7 17.0 16.4</cell><cell>1.3 20.3 20.1</cell><cell>Feature</cell><cell>AVGPOOL MAXPOOL FLATTEN</cell><cell>34.3 34.1 34.5</cell><cell>38.4 38.4 38.8</cell><cell>40.2 40.1 40.5</cell></row><row><cell>4</cell><cell></cell><cell>33.2</cell><cell>38.4</cell><cell>39.9</cell><cell>8.2</cell><cell>12.0</cell><cell>16.9</cell><cell>20.6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>5 6 7</cell><cell></cell><cell>31.2 34.4 34.5</cell><cell>35.6 38.5 38.8</cell><cell>36.9 40.3 40.5</cell><cell>7.4 8.4 8.6</cell><cell>11.1 12.9 13.1</cell><cell>16.0 17.5 17.6</cell><cell>19.7 21.0 21.1</cell><cell>GIH</cell><cell>2-LAYERS 4-LAYERS 6-LAYERS</cell><cell>34.1 34.5 34.1</cell><cell>38.4 38.8 38.5</cell><cell>39.9 40.5 40.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE V :</head><label>V</label><figDesc>Comparison of feature fusion methods.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(a) Scene Graph</cell></row><row><cell cols="5">Classification results on Visual Genome dataset. (b) Bidirectional Relationship</cell></row><row><cell cols="2">Classification results on BR dataset.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">(a) Scene Graph Classification</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">SGCls</cell><cell></cell></row><row><cell>Fusion</cell><cell cols="4">R@20 R@50 R@100</cell></row><row><cell>BASELINE</cell><cell></cell><cell>30.8</cell><cell>34.7</cell><cell>36.2</cell></row><row><cell cols="2">w/o Permutation</cell><cell>33.6</cell><cell>38.0</cell><cell>39.9</cell></row><row><cell>Sequential</cell><cell></cell><cell>34.1</cell><cell>38.4</cell><cell>40.1</cell></row><row><cell cols="2">Parallel (Ours)</cell><cell>34.5</cell><cell>38.8</cell><cell>40.5</cell></row><row><cell cols="5">(b) Bidirectional Relationship Classification</cell></row><row><cell></cell><cell></cell><cell cols="2">PredCls</cell><cell></cell></row><row><cell>Fusion</cell><cell cols="4">pR@2 pR@4 pR@8 pR@16</cell></row><row><cell>BASELINE</cell><cell>0.2</cell><cell>0.4</cell><cell>0.7</cell><cell>1.3</cell></row><row><cell>w/o Permutation</cell><cell>8.2</cell><cell>12.2</cell><cell>17.0</cell><cell>20.6</cell></row><row><cell>Sequential</cell><cell>8.3</cell><cell>12.7</cell><cell>17.3</cell><cell>20.9</cell></row><row><cell>Parallel (Ours)</cell><cell>8.6</cell><cell>13.1</cell><cell>17.6</cell><cell>21.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>table .</head><label>.</label><figDesc>Exp 1 is the result of a vanilla version of LOGIN, i.e., BASELINE, which shows the abysmal result, especially in the BRC setting. This means that the BASELINE has no understanding of relational direction at all; thus, it can only predict symmetric relationships correctly. Exp 2 -Exp 5 examine the individual contributions of each model component. Especially, LIH (Exp2) and GIH (Exp4) have a significant impact on both SGG and BRC settings. It is noteworthy that contextual information (driven from GIH) also plays a key role in recognizing directions. We can see in Exp3 that DSE is relatively unremarkable in SGG settings, while it improves performance in BRC settings by a large margin.</figDesc><table /><note>Although the unary effect of Attract &amp; Repel Loss L ar (Exp 4)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VI :</head><label>VI</label><figDesc>Effectiveness of Graph Interaction Head (GIH) compared to other graph neural networks (e.g., GCN<ref type="bibr" target="#b41">[42]</ref>, GAT<ref type="bibr" target="#b55">[56]</ref>).</figDesc><table><row><cell cols="4">(a) Scene Graph Classification</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">SGCls</cell><cell></cell></row><row><cell>Methods</cell><cell cols="2">R@20</cell><cell>R@50</cell><cell>R@100</cell></row><row><cell>LOGIN /W GCN [42]</cell><cell></cell><cell>33.8</cell><cell>37.5</cell><cell>39.7</cell></row><row><cell>LOGIN /W GAT [56]</cell><cell></cell><cell>33.2</cell><cell>37.1</cell><cell>39.5</cell></row><row><cell cols="2">LOGIN /W GIH (Ours)</cell><cell>34.5</cell><cell>38.8</cell><cell>40.5</cell></row><row><cell cols="5">(b) Bidirectional Relationship Classification</cell></row><row><cell></cell><cell></cell><cell cols="2">PredCls</cell><cell></cell></row><row><cell>Fusion</cell><cell>pR@2</cell><cell cols="2">pR@4</cell><cell>pR@8</cell><cell>pR@16</cell></row><row><cell>LOGIN /W GCN [42]</cell><cell>7.8</cell><cell>12.4</cell><cell></cell><cell>16.5</cell><cell>20.5</cell></row><row><cell>LOGIN /W GAT [56]</cell><cell>7.9</cell><cell>12.3</cell><cell></cell><cell>16.3</cell><cell>20.3</cell></row><row><cell>LOGIN /W GIH (Ours)</cell><cell>8.6</cell><cell>13.1</cell><cell></cell><cell>17.6</cell><cell>21.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE VII :</head><label>VII</label><figDesc>Per-type predicate classification results. Only top-20 frequent predicates are shown. The evaluation metric is R@50.</figDesc><table><row><cell>predicate</cell><cell cols="2">Baseline LOGIN</cell><cell>prediate</cell><cell cols="2">Baseline LOGIN</cell></row><row><cell>on</cell><cell>66.3</cell><cell>88.1</cell><cell>sitting on</cell><cell>32.2</cell><cell>61.0</cell></row><row><cell>has</cell><cell>47.7</cell><cell>87.5</cell><cell>under</cell><cell>35.9</cell><cell>52.5</cell></row><row><cell>wearing</cell><cell>68.9</cell><cell>93.7</cell><cell>riding</cell><cell>26.3</cell><cell>83.0</cell></row><row><cell>of</cell><cell>42.8</cell><cell>82.4</cell><cell>in front of</cell><cell>8.9</cell><cell>29.4</cell></row><row><cell>in</cell><cell>47.7</cell><cell>64.1</cell><cell>standing on</cell><cell>16.7</cell><cell>37.7</cell></row><row><cell>near</cell><cell>19.4</cell><cell>52.3</cell><cell>at</cell><cell>39.5</cell><cell>57.8</cell></row><row><cell>with</cell><cell>18.1</cell><cell>45.9</cell><cell>attached to</cell><cell>12.1</cell><cell>21.0</cell></row><row><cell>behind</cell><cell>20.7</cell><cell>57.0</cell><cell>carrying</cell><cell>23.9</cell><cell>62.1</cell></row><row><cell>holding</cell><cell>31.4</cell><cell>78.7</cell><cell>walking on</cell><cell>10.5</cell><cell>59.3</cell></row><row><cell>above</cell><cell>18.2</cell><cell>51.2</cell><cell>over</cell><cell>9.5</cell><cell>28.4</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (CVPR</title>
		<meeting>the IEEE International Conference on Computer Vision (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fully Convolutional Networks for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">You Only Look Once: Unified, Real-Time Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image Retrieval Using Scene Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3668" to="3678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Graph-Structured Representations for Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Exploring Visual Relationship for Image Captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="684" to="699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image Generation From Scene Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1219" to="1228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attend and Interact: Higher-Order Object Interactions for Video Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Melvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alregib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Peter</forename><surname>Graf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6790" to="6800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Auto-Encoding Scene Graphs for Image Captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<idno>pp. 10 685- 10 694. 1</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Specifying Object Attributes and Relations in Interactive Scene Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ashual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4561" to="4569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Context-Aware Group Captioning via Self-Attention and Contrastive Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.03708</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Modeling Relationships in Referential Expressions With Compositional Modular Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1115" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Detecting and Recognizing Human-Object Interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8359" to="8367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to Detect Human-Object Interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="381" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Transferable Interactiveness Knowledge for Human-Object Interaction Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3585" to="3594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visual Relationship Detection With Language Priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Visual Translation Embedding Network for Visual Relation Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kyaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">VIP-CNN: Visual Phrase Guided Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1347" to="1356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Phrase Localization and Visual Relationship Detection With Comprehensive Image-Language Cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1928" to="1937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">PPR-FCN: Weakly Supervised Visual Relation Detection via Parallel Pairwise R-FCN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kyaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4233" to="4241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Detecting Visual Relationships With Deep Relational Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep Variation-Structured Reinforcement Learning for Visual Relationship and Attribute Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="848" to="857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hierarchical Graph Attention Network for Visual Relationship Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">895</biblScope>
			<biblScope unit="page" from="13" to="886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Scene Graph Generation by Iterative Message Passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note>2, 4, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural Motifs: Scene Graph Parsing With Global Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note>2, 4, 6, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Linknet: Relational Embedding for Scene Graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="560" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Graph R-CNN for Scene Graph Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note>2, 4, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Factorizable Net: An Efficient Subgraph-Based Framework for Scene Graph Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attentive Relational Networks for Mapping Images to Scene Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Knowledge-Embedded Routing Network for Scene Graph Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note>2, 4, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning to Compose Dynamic Tree Structures for Visual Contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note>2, 4, 6, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Counterfactual Critic Multi-Agent Training for Scene Graph Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note>2, 4, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Graphical Contrastive Losses for Scene Graph Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note>543. 2, 6, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Weakly Supervised Visual Semantic Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3736" to="3745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unbiased Scene Graph Generation From Biased Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Bridging Knowledge Graphs to Generate Scene Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Sketching Image Gist: Human-Mimetic Hierarchical Scene Graph Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards Real-Time Object Detection With Region Proposal Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification With Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Shuffle-Then-Assemble: Learning Object-Agnostic Visual Relationship Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Scene Graph Generation From Objects, Phrases and Region Captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Look Closer to See Better: Recurrent Attention Convolutional Neural Network for Fine-Grained Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4438" to="4446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning Multi-Attention Convolutional Neural Network for Fine-Grained Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5209" to="5217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Mask-CNN: Localizing Parts and Selecting Descriptors for Fine-Grained Bird Species Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition (PR)</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="704" to="714" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Non-Local Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">CBAM: Convolutional Block Attention Module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>So Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Looking for the Devil in the Details: Learning Trilinear Attention Sampling Network for Fine-Grained Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5012" to="5021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<idno>2020. 5</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A Simple Neural Network Module for Relational Reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4967" to="4976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep Sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3391" to="3401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deep Set Prediction Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prugel-Bennett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3212" to="3222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Set Transformer: A Framework for Attention-Based Permutation-Invariant Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3744" to="3753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Neural Message Passing for Quantum Chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning (ICML)</title>
		<meeting>the 34th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deepgcns: Can Gcns Go As Deep as CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9267" to="9276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">An Intriguing Failing of Convolutional Neural Networks and the Coordconv Solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">P</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9605" to="9616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">His research interests lie in computer vision and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangmin</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Student Member, IEEE) is currently pursuing the Ph.D. degree in electrical engineering at Korea Advanced Institute of Science and Technology (KAIST)</title>
		<meeting><address><addrLine>Daejeon, Korea; Gwangju, Korea; Daegu, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
		<respStmt>
			<orgName>Electrical Engineering and Computer Science from Gwangju Institute of Science and Technology (GIST) ; Electrical Engineering from Kyungpook National University</orgName>
		</respStmt>
	</monogr>
	<note>He received an M.S. degree in. especially in a high-level visual understanding</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">His research has focused on artificial intelligence, machine learning, and computer vision with a particular interest in object detection and its related high-level vision tasks such as semantic/instance segmentation</title>
	</analytic>
	<monogr>
		<title level="m">in 2013, and the M.S. and Ph.D. in Computer Science Engineering from Seoul National University in 2015 and 2020, respectively</title>
		<imprint/>
		<respStmt>
			<orgName>S. in Computer Science and Engineering &amp; Statistics from Seoul National University</orgName>
		</respStmt>
	</monogr>
	<note>He received the B. scene understanding, and image captioning</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
