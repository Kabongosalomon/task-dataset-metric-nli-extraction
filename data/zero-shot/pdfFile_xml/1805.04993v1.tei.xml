<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Discourse Coherence in the Wild: A Dataset, Evaluation and Methods</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
							<email>aylai2@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign *</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">Tetreault</forename><surname>Grammarly</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign *</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Discourse Coherence in the Wild: A Dataset, Evaluation and Methods</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To date there has been very little work on assessing discourse coherence methods on real-world data. To address this, we present a new corpus of real-world texts (GCDC) as well as the first large-scale evaluation of leading discourse coherence algorithms. We show that neural models, including two that we introduce here (SENTAVG and PARSEQ), tend to perform best. We analyze these performance differences and discuss patterns we observed in low coherence texts in four domains.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Discourse coherence is an important aspect of text quality. It encompasses how sentences are connected as well as how the entire document is organized to convey information to the reader. Developing discourse coherence models to distinguish coherent writing from incoherent writing is useful to a range of applications. An automated coherence scoring model could provide writing feedback, e.g. identifying a missing transition between topics or highlighting a poorly organized paragraph. Such a model could also improve the quality of natural language generation systems.</p><p>One approach to modeling coherence is to model the distribution of entities over sentences. The entity grid <ref type="bibr" target="#b0">(Barzilay and Lapata, 2005)</ref>, based on Centering Theory <ref type="bibr" target="#b10">(Grosz et al., 1995)</ref>, was the first of these models. Extensions to the entity grid include additional features <ref type="bibr">Charniak, 2008, 2011;</ref><ref type="bibr" target="#b9">Feng et al., 2014)</ref>, a graph representation <ref type="bibr" target="#b11">(Guinaudeau and Strube, 2013;</ref><ref type="bibr">Mesgar and Strube, 2015)</ref>, and neural convolutions <ref type="bibr">(Tien Nguyen and Joty, 2017)</ref>. Other approaches have used lexical cohesion (Morris and Hirst, * Research performed while at <ref type="bibr">Grammarly. 1991;</ref><ref type="bibr">Somasundaran et al., 2014)</ref>, discourse relations <ref type="bibr" target="#b19">(Lin et al., 2011;</ref><ref type="bibr" target="#b9">Feng et al., 2014)</ref>, and syntactic features <ref type="bibr">(Louis and Nenkova, 2012)</ref>. Neural networks have also been successfully applied to coherence <ref type="bibr" target="#b17">(Li and Hovy, 2014;</ref><ref type="bibr">Tien Nguyen and Joty, 2017;</ref><ref type="bibr" target="#b18">Li and Jurafsky, 2017)</ref>. However, until now, these approaches have not been benchmarked on a common dataset.</p><p>Past work has focused on the discourse coherence of well-formed texts in domains like newswire <ref type="bibr" target="#b0">(Barzilay and Lapata, 2005;</ref><ref type="bibr" target="#b6">Elsner and Charniak, 2008)</ref> via tasks like sentence ordering that use artificially constructed data. It was unknown how well the best methods would fare on real-world data that most people generate.</p><p>In this work, we seek to address the above deficiencies via four main contributions. First, we present a new corpus, the Grammarly Corpus of Discourse Coherence (GCDC), for real-world discourse coherence. The corpus contains texts the average person might write, e.g. emails and online reviews, each with a coherence rating from expert annotators (see examples in <ref type="table" target="#tab_17">Table 11</ref> and supplementary material). Second, we introduce two simple yet effective neural network models to score coherence. Third, we perform the first large-scale benchmarking of 7 leading coherence algorithms. We show that prior models, which performed at a very high level on well-formed and artificially generated data, have markedly lower performance in these new domains. Finally, the data, annotation guidelines, and code have all been made public. 1 2 A Corpus for Discourse Coherence</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Related Work</head><p>Most previous work in discourse coherence has been evaluated on a sentence ordering task that assumes each text is well-formed and perfectly co-Score Text Low Should I be flattered? Even a little bit? And, as for my alibi, well, let's just say it depends on the snow and the secret service. So, subject to cross for sure. Do you think there could be copycats? Do you think the guy chose that mask or just picked up the nearest one? Please keep me informed as the case unfolds-On another matter, can you believe Dan Burton will be the chair of one of the House subcommittees we'll have to deal w? Irony and satire are the only sane responses.</p><p>Happy New Year-and here's hoping for many more stories that make us laugh! High Cheryl, I just spoke with Vidal Jorgensen. They expect to be on the ground in about 8 months. They have not yet raised enough money to get the project started -the total needed is $6M and they need $2M to get started. Vidal said they process has been delayed because their work in Colombia and China is consuming all their resources at the moment. Once on the ground, they will target the poorest of the poor and go to the toughest areas of Haiti. They anticipate an average loan size of $200 and they expect to reach about 10,000 borrowers in five years. They expect to be profitable in 4-5 years. Meghann <ref type="table">Table 1</ref>: Examples of texts and coherence scores from the Clinton domain.</p><p>herent, and any reordering of the same sentences is less coherent. Presented with a pair of texts -the original and a random permutation of the same sentences -a coherence model should be able to identify the original text. More challenging versions of this task (sentence insertion <ref type="bibr" target="#b8">(Elsner and Charniak, 2011)</ref> and paragraph reconstruction <ref type="bibr" target="#b16">(Lapata, 2003;</ref><ref type="bibr" target="#b18">Li and Jurafsky, 2017)</ref>) all assume that the original text is perfectly coherent. Datasets for the sentence ordering task tend to use texts that have been professionally written and extensively edited. These have included the Accidents and Earthquakes datasets <ref type="bibr" target="#b0">(Barzilay and Lapata, 2005)</ref>, the Wall Street Journal <ref type="bibr">Charniak, 2008, 2011;</ref><ref type="bibr" target="#b19">Lin et al., 2011;</ref><ref type="bibr" target="#b9">Feng et al., 2014;</ref><ref type="bibr">Tien Nguyen and Joty, 2017)</ref>, and Wikipedia <ref type="bibr" target="#b18">(Li and Jurafsky, 2017)</ref>.</p><p>Another task, summary evaluation <ref type="bibr" target="#b0">(Barzilay and Lapata, 2005)</ref>, uses human coherence judgments, but include machine-generated texts. Coherence models are only required to identify which of a pair of texts is more coherent (presumably identifying human-written texts).</p><p>The line of work most closely related to our approach is the application of coherence modeling to automated essay scoring. Essays are written by test-takers, not professional writers, so they are not assumed to be coherent. Manual annotation is required to assign the essay an overall quality score <ref type="bibr" target="#b9">(Feng et al., 2014)</ref> or to rate the coherence of the essay <ref type="bibr">(Somasundaran et al., 2014;</ref><ref type="bibr" target="#b3">Burstein et al., 2010</ref><ref type="bibr" target="#b4">Burstein et al., , 2013</ref>. While this line of work goes beyond sentence ordering to examine the qualities of a low-coherence text, it has only been applied to test-taker essays.</p><p>In contrast to previous datasets, we collect writ-ing from non-professional writers in everyday contexts. Rather than using permuted or machinegenerated texts as examples of low coherence, we want to investigate the ways in which people try but fail to write coherently. We present a corpus that contains texts from four domains, covering a range of coherence, each annotated with a document-level coherence score. In Sections 2.2-2.6, we describe our data collection process and the characteristics of the resulting corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Domains</head><p>For a robust evaluation, we selected domains that reflect what an average person writes on a regular basis: forum posts, emails, and product reviews. For online forum posts, we sampled responses from the Yahoo Answers L6 corpus 2 for the Yahoo domain. For emails, we used the State Department's release of emails from Hillary Clinton's office 3 and emails from the Enron Corpus 4 to make up our Clinton and Enron domains. Finally, we sampled reviews of businesses from the Yelp Open Dataset 5 for our Yelp domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Text Selection</head><p>We randomly selected texts from each domain given a few filters. We want each text to be long enough to exhibit a range of characteristics of local and global coherence, but not so long that the labeling process is tedious for annotators. Therefore, we considered texts between 100 and 300 words in length. We ignored texts containing URLs (as they often quote writing from other sources) and texts with too many line breaks (usually lists).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Annotation</head><p>We collected coherence judgments both from expert raters with prior linguistic annotation experience, as in <ref type="bibr" target="#b3">Burstein et al. (2010)</ref> and from untrained raters via Amazon Mechanical Turk. This allows us to assess the efficacy of using untrained raters for this task. We asked the raters to rate the coherence of each text on a 3-point scale from 1 (low coherence) to 3 (high coherence) given the following instructions, which are based on prior coherence annotation efforts <ref type="bibr" target="#b1">(Barzilay and Lapata, 2008;</ref><ref type="bibr" target="#b4">Burstein et al., 2013)</ref>: A text with high coherence is easy to understand, well-organized, and contains only details that support the main point of the text. A text with low coherence is difficult to understand, not well organized, or contains unnecessary details. Try to ignore the effects of grammar or spelling errors when assigning a coherence rating.</p><p>Expert Rater Annotation We solicited judgments from 13 expert raters with previous annotation experience. We provided a high-level description of coherence but no detailed rubric, as we wanted them to use their own judgment. We also provided examples of low, medium, and high coherence along with a brief justification for each label. The raters went through a calibration phase during which we provided feedback about their judgments. In the annotation phase, we collected 3 expert rater judgments for each text.</p><p>Mechanical Turk Annotation We collected 5 MTurk judgments for each text from a group of 62 Mechanical Turk annotators who passed our qualification test. We again provided a high-level description of coherence. However, we only provided a few examples for each category so as not to overwhelm the annotators. We were mindful of how the characteristics of each domain might affect the resulting coherence scores. For example, after rating a batch of generally low coherence forum data, business emails may appear to be more coherent. However, our goal is to discover the characteristics of a low coherence business email or a low coherence forum post, not to compare the two domains. Therefore, we recruited new MTurk raters for each domain so as not to bias their scores. The same 13 expert raters worked on all four domains, but we specifically instructed them to consider whether each text was a coherent document for its domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Grammarly Corpus of Discourse Coherence</head><p>The resulting four domains each contain 1200 texts (1000 for training, 200 for testing). Each text has been scored as {low, medium, high} coherence by 5 MTurk raters and 3 expert raters. There is one consensus label for the expert ratings and another consensus label for the MTurk ratings. We computed the consensus label by averaging the integer values of the coherence ratings (low = 1, medium = 2, high = 3) over the MTurk or expert ratings and thresholding the mean coherence score (low ? 1.8 &lt; medium ? 2.2 &lt; high) to produce a 3-way classification label <ref type="table" target="#tab_1">(Table 2</ref>). We observed that the MTurk raters tended to label more texts as "medium" coherence than the expert raters. Since the MTurk raters did not go through an extensive training session, they may be less confident in their ratings, defaulting to medium as the safe option. <ref type="table" target="#tab_2">Table 3</ref> contains type and token counts for the full dataset, and <ref type="figure" target="#fig_0">Figure 1</ref> shows the number of paragraphs, sentences, and words per document.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Annotation Agreement</head><p>To quantify agreement among annotators, we follow Pavlick and Tetreault (2016)'s approach to   simulate two annotators from crowdsourced labels. We repeat the simulation 1000 times and report the mean agreement values in <ref type="table" target="#tab_4">Table 4</ref> for both intraclass correlation (ICC) and quadratic weighted Cohen's ? for an ordinal scale. The expert raters have fair agreement (Landis and <ref type="bibr" target="#b15">Koch, 1977)</ref> for three of the domains, but agreement among MTurk raters is quite low. These agreement numbers are the result of an extensive annotation development process and emphasize the difficulty of the task. We recommend that future work in this area leverages raters with a strong annotation background and the time for indepth instructions. For evaluation, we use the consensus label from the expert judgments. For comparison, we include an experiment using MTurk consensus labels in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Models</head><p>We evaluate a range of existing discourse coherence models on GCDC: entity-based models, a word embedding graph model, and neural network models. These models from previous work have been very effective on the sentence ordering task, but have not been used to produce coherence scores. We also introduce two new neural sequence models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Baseline</head><p>We compute the Flesch-Kincaid grade level <ref type="bibr" target="#b14">(Kincaid et al., 1975)</ref> of each text and treat it as a coherence score. While Flesch-Kincaid is a readability measure, previous work has treated readability and text coherence as overlapping tasks <ref type="bibr" target="#b1">(Barzilay and Lapata, 2008;</ref><ref type="bibr">Mesgar and Strube, 2015)</ref>. For coherence classification, we search over the grade level scores on the training data and select thresholds that result in the highest accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Entity-based Models</head><p>Entity-based models track entity mentions throughout the text. In the majority of our experiments, we applied <ref type="bibr" target="#b1">Barzilay and Lapata (2008)</ref>'s coreference heuristic and consider two nouns to be coreferent only if they are identical. As Elsner and Charniak (2011) noted, automatic coreference resolution often fails to improve coherence modeling results. However, we also evaluate the effect of adding an automatic coreference system in Section 4.1.</p><p>Entity grid (EGRID) The entity grid <ref type="bibr" target="#b0">(Barzilay and Lapata, 2005</ref>) is a matrix that tracks entity mentions over sentences. We reimplemented the model from <ref type="bibr" target="#b1">Barzilay and Lapata (2008)</ref>, converting the entity grid into a feature vector that expresses the probabilities of local entity transitions. We use scikit-learn (Pedregosa et al., 2011) to train a random forest classifier over the feature vectors.</p><p>Entity graph (EGRAPH) The entity graph <ref type="bibr" target="#b11">(Guinaudeau and Strube, 2013)</ref> interprets the entity grid as a graph whose nodes are sentences. Two nodes are connected if they share at least one entity. Graph edges can be weighted according to the number of entities shared, the syntactic roles of the entities, or the distance between sentences. The coherence score of a text is the average out-degree of its graph, so for classification we identify the thresholds that maximize accuracy on the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Entity grid with convolutions (EGRIDCONV)</head><p>Tien Nguyen and Joty (2017) applied a convolutional neural network to the entity grid to capture long-range transitions. We use the authors' implementation. 6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Lexical Coherence Graph (LEXGRAPH)</head><p>The lexical coherence graph (Mesgar and Strube, 2016) represents sentences as nodes of a graph, connecting nodes with an edge if the two sentences contain a pair of similar words (i.e. the cosine similarity of their pre-trained word vectors is greater than a threshold). From the graph, we can extract a feature vector that expresses the frequency of all k-node subgraphs. We use the authors' implementation 7 and train a random forest classifier over the feature vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Neural Network Models</head><p>We reimplemented a neural network model of coherence, the sentence clique model, to evaluate its effectiveness on GCDC. We also introduce two new neural network models that are more straightforward to implement than the clique model.</p><p>Sentence clique (CLIQUE) Li and Jurafsky (2017)'s model operates over cliques of adjacent sentences. For the sentence ordering task, a positive clique is a sequence of k sentences from the original document. A negative clique is created by replacing the middle sentence of a positive clique with a random sentence from elsewhere in the text. The model contains a single LSTM <ref type="bibr" target="#b12">(Hochreiter and Schmidhuber, 1997</ref>) that takes a sequence of GloVe word embeddings and produces a sentence vector at the final output step. All k sentence vectors are concatenated and passed through a final layer to produce a probability that the clique is coherent. The final coherence score is the average of the scores of all cliques in the document.</p><p>We extend CLIQUE to 3-class classification by labeling each clique with the document class label (low, medium, high). To predict the text label, the model averages the predicted coherence class distributions over all cliques.</p><p>Sentence averaging (SENTAVG) To investigate the extent to which sentence order is important in our data, we introduce a neural network model that ignores sentence order. The model contains a single LSTM that produces a sentence vector (the final output vector) from a sequence of GloVe embeddings for the words in that sentence. The document vector is the average over all sentence vectors in that document, and is passed through a hidden layer and a softmax to produce a distribution over coherence labels.</p><p>Paragraph sequence (PARSEQ) The role of paragraph breaks has not been explicitly discussed in previous work. Models like EGRID assume that entity transitions have the same weight whether adjacent sentences A and B occur in the same paragraph or different paragraphs. We expect paragraph breaks to be important for assessing coherence in longer documents.</p><p>Therefore, we introduce a paragraph sequence model, PARSEQ, that can distinguish between paragraphs.</p><p>PARSEQ contains three stacked LSTMs: the first takes a sequence of GloVe embeddings to produce a sentence vector, the second takes a sequence of sentence vectors to produce a paragraph vector, and the third takes a sequence of paragraph vectors to produce a document vector. The document vector is passed through a hidden layer and a softmax to produce a distribution over coherence labels. A diagram of this model is available in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>We evaluate the models on multiple coherence prediction tasks. The best model parameters, reported in the supplementary material, are the result of 10-fold cross-validation over the training data.</p><p>For all neural models (EGRIDCONV, EGRID-CONV +coref, CLIQUE, SENTAVG, and PARSEQ), the reported results are the mean of 10 runs with different random seeds, as suggested by <ref type="bibr">Reimers and Gurevych (2017)</ref>.</p><p>We indicate ( ?) when the best neural model result is significantly better (p &lt; 0.05) than the best non-neural result. We use the one-sample Wilcoxon signed rank test and adjusted the pvalues to account for the false discovery rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Classification</head><p>For this task, each text has a consensus label expressing how coherent it is: {low, medium, high}.  We report overall accuracy for all systems on predicting the expert rater consensus label <ref type="table" target="#tab_6">(Table 5)</ref>. We repeated this evaluation using the MTurk rater labels and included those results in the supplementary material.</p><p>The neural models outperformed the entitybased and lexical graph models. Non-neural models showed mixed results, performing on par with or worse than our baseline. Most models perform poorly on Yelp, worse than the baseline, perhaps because Yelp has the lowest annotator agreement among expert raters.</p><p>We also tried adding coreference information for the entity-based methods, as it has been shown to be useful in some prior work <ref type="bibr" target="#b1">(Barzilay and Lapata, 2008;</ref><ref type="bibr" target="#b6">Elsner and Charniak, 2008)</ref>. For the base entity model experiments, we used Barzilay and <ref type="bibr" target="#b1">Lapata (2008)</ref>'s heuristic to determine whether two nouns are coreferent. For the +coref setting, we used the Stanford coreference annotator <ref type="bibr" target="#b5">(Clark and Manning, 2015)</ref> as a preprocessing step before computing the entity grid. The coreference system yielded consistent performance improvements of 1-5% accuracy over the corresponding heuristic results, indicating that automatic coreference resolution can help entity-based models in these domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Score Prediction</head><p>A 3-point coherence score might not reflect the range of coherence that actually exists in the data. We can instead present a more fine-grained score prediction task where the gold score is the mean of the three expert rater judgments (low coherence = 1, medium = 2, high = 3). In <ref type="table" target="#tab_8">Table 6</ref>, we report Spearman's rank correlation coefficient  between the gold scores and the predicted coherence scores. As in the classification task, the neural methods convincingly outperformed all other methods, with PARSEQ the top performer in three out of four domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Sentence Ordering</head><p>The sentence ordering ranking task is a somewhat artificial evaluation, as a document whose sentences have been randomly shuffled does not resemble a human-written text that is not very coherent. However, we still want to assess whether good performance on previous sentence ordering datasets translates to GCDC. Since the sentence ordering task assumes well-formed texts, we use only the high coherence texts. As a result, there are fewer texts than for the classification task, as we show below.  <ref type="table" target="#tab_11">Table 7</ref> shows the accuracy of each system on identifying the original text in each (original, permuted) text pair. We leave out the baseline and SENTAVG because they ignore sentence order. We also simplify PARSEQ to a sentence sequence model (SENTSEQ) containing only two LSTMs because the sentence ordering task ignores paragraph information. As in the prior two evaluations, the neural models perform best in most domains, although EGRAPH is best on Yahoo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Minority Class Classification</head><p>One application of a coherence classification system would be to provide feedback to writers by flagging text that is not very coherent. Such a sys-   We relabel a text as low coherence if at least two expert annotators judged the text to be low coherence, and relabel as not low coherence otherwise.</p><p>We report the F 0.5 score of the low coherence class in <ref type="table" target="#tab_12">Table 8</ref>, where precision is emphasized twice as much as recall. 8 This is in line with evaluation standards in other writing feedback applications <ref type="bibr">(Ng et al., 2014)</ref>. Again, the neural models perform best in most domains. However, the results of this experiment in particular show that there is still a large gap between the performance of these models and what might be required for high-precision real-world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Cross-Domain Classification</head><p>Up to this point, we assumed that the four domains are different enough from one another that we should train separate models for each. To test 8 Precision and recall are in the supplementary material.   this assumption, we train PARSEQ, one of the top performing neural models, in one domain (e.g. Yahoo) and evaluate it in a different domain (Clinton, Enron, and Yelp). <ref type="table" target="#tab_14">Table 9</ref> compares the in-domain results (the diagonal) to the cross-domain results. While the model's accuracy generally decreases when transferred to a different domain, sometimes this decrease is not too severe: for example, training on Yahoo/Enron data and testing on Clinton data, or training on Yahoo data and testing on Yelp data. It is reasonable that training on one set of business emails (Clinton or Enron) produces a model that can accurately score the coherence of other sets of business emails. Similarly, both Yahoo and Yelp contain online text written for public consumption which may share coherence characteristics, so it is not surprising that a model trained on Yahoo data works on Yelp (even outperforming the Yelp-trained model).</p><p>These results indicate that we might be able to train a better coherence model by combining all our data across multiple domains. We evaluate this theory in <ref type="table" target="#tab_15">Table 10</ref>, comparing the results of the PARSEQ model evaluated in-domain (e.g. trained and tested on Yahoo data) to a model trained on the combined training data from all four domains. With four times as much training data, the performance of PARSEQ improves in all domains, indicating that better coherence models may be trained from data outside of a specific, narrow domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Discussion</head><p>We observe some trends across our experiments. The basic entity models (EGRID and EGRAPH) tend to perform poorly, often barely outperform-ing the baseline. The entity grids computed from GCDC texts are often extremely sparse, so meaningful entity transitions between sentences are infrequent. In addition, scoring the coherence of a text (either classification or score prediction) is more difficult than the sentence ordering task, where basic entity models do outperform the random baseline by a reasonable margin. Both the data and the difficulty of the tasks contribute to poor performance from the basic entity models.</p><p>The neural network models almost always outperform other models. This supports Li and Jurafsky (2017)'s claim that neural models are better able to extend to other domains compared to previous coherence models. Our PARSEQ and SENTAVG models are easier to implement than CLIQUE and outperform CLIQUE on a majority of experiments. EGRIDCONV usually does not perform as well as the other neural models, but it usually improves over EGRID. Finally, the relative success of SENTAVG, which ignores sentence order, is evidence that identifying a document's original sentence order is not the same as distinguishing low and high coherence documents. The large number of parameters in PARSEQ may explain why it is sometimes outperformed by SENTAVG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>To better understand what distinguishes a low coherence text from a high coherence text, we manually analyzed Yahoo and Clinton texts whose labels were unanimously agreed on by all three raters. Regardless of the domain, many low coherence texts are not well-organized and appear to be written almost as stream of consciousness. They often lack connectives, resembling a list of points rather than a coherent document.</p><p>Incoherent Yahoo texts often contain extremely long sentences, lack paragraph breaks, and veer off-topic without a transition or any connection back to the main point. This is an especially frequent occurrence with personal anecdotes.</p><p>Low coherence Clinton emails make better use of paragraphs, but they too often lack transitions between topics. In addition, missing information was a primary reason for low coherence scores. We provided the raters with individual emails, not the entire email thread, so raters had less information than the original recipient of the email. This amplifies the detrimental effects on coherence of jargon, abbreviation, and missing context. However, overuse of these compression strategies can result in low coherence even for the intended recipient, so it is worth modeling their effects.</p><p>Across domains, coherent texts have a clear topic that is maintained throughout the text, and they are well-organized, with sentences, paragraphs and sub-topics following a logical ordering. Connectives, such as however, for example, in turn, also, in addition are used more frequently to assist the structure and flow.</p><p>Although sentence order is clearly important, rewriting a disorganized text is not as simple as reordering sentences. Even if changing the location of one sentence increases coherence, a true fix would still require rewriting that sentence or the surrounding sentences. Our analysis indicates that the sentence reordering task is not a good evaluation of whether models can truly be useful to the task of identifying low coherence texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we examine the evaluation of discourse coherence by presenting a new corpus (GCDC) to benchmark leading methods on realworld data in four domains. While neural models outperform others across multiple evaluations, much work remains before any of these methods can be used for real-world applications. That said, our SENTAVG and PARSEQ models serve as simple and effective methods to use in future work.</p><p>We recommend that future evaluations move away from the sentence ordering task. While it is an easy evaluation to carry out, the performance numbers overpredict the success of those systems in real-world conditions. For example, prior evaluations (Tien Nguyen and Joty, 2017; Li and Jurafsky, 2017) report performance numbers around or above 90% accuracy, which contrasts with the much lower figures shown in this paper. In addition, we recommend that future annotation efforts leverage expert raters, preferably with a background in annotation, as this task is difficult for untrained workers on crowdsourcing platforms.</p><p>By releasing GCDC, the annotation guidelines, and our code, we hope to encourage future work on more realistic coherence tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Supplementary Material</head><p>A.1 Corpus Examples <ref type="table" target="#tab_17">Table 11</ref> contains additional examples of texts from our corpus, specifically from the Yahoo Answers domain, with their coherence labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Annotator Instructions</head><p>The annotation instructions in Section 2.4 are the simplified instructions that we provided to Mechanical Turk workers. The expert annotators received a longer version of those instructions, which are available in <ref type="table" target="#tab_1">Table 12</ref>.  <ref type="figure" target="#fig_1">Figure 2</ref> shows the structure of PARSEQ. The sentence vectors pictured are the output at the final timestep from the first LSTM (not pictured), which takes GloVe word embeddings as input. A second LSTM takes these sentence vectors as input and produces paragraph vectors, and a third LSTM takes a sequence of paragraph vectors and produces a single document vector. <ref type="table" target="#tab_2">Table 13</ref> contains the classification test results of all systems when the consensus labels come from the Mechanical Turk judgments rather than the expert judgments. <ref type="table" target="#tab_4">Table 14</ref> contains the precision and recall results for the minority class classification test. For neural models, we report precision and recall for one run on test (F0.5 scores in Section 4.4 were averaged over 10 runs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Model Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Additional Results</head><p>To compare all models on an established dataset, we report results on the sentence ordering task using the Wall Street Journal (WSJ) portion of the Penn Treebank. Following previous work, we use 20 random permutations of each article and the train/test split defined by Tien Nguyen and Joty (2017) (train = Section 00-13, test = 14-24). Table 15 contains the results of all models on WSJ. These results verify our re-implementation of the EGRID model, as well as establishing the reasonable performance of our neural sequence model on news text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Model Parameters</head><p>We specify the parameters for all models and experiments in <ref type="table" target="#tab_8">Tables 16 and 17</ref>. Additionally, for the combined training data experiment ( <ref type="table" target="#tab_15">Table 10</ref> in the paper), we train parseq with LSTM dimensionality = 100, hidden layer = 200, dropout = 0.5.</p><p>EGRID Sequence length is the length of the transition sequences used to compute the feature vector from the entity grid. For salience, we follow <ref type="bibr" target="#b1">Barzilay and Lapata (2008)</ref> and split entities into two salience classes (doubling the number of features) based on whether their frequency is greater than the salience threshold. (Salience = off means that there is only one salience class containing all entities.) Syntax indicates whether we consider grammatical roles <ref type="bibr">(subject, object, other)</ref> in building the entity grid.</p><p>EGRAPH The graph type specifies whether we use an unweighted graph (u), a graph weighted by the number of entities shared between sentences (w), or a graph weighted by syntactic role information (syn). Distance indicates whether edge weights are decreased according to the distance between sentences.</p><p>EGRIDCONV We specify dropout rate, batch size, and entity role embedding size. For the convolution layer, we specify filter number, window size, and pooling length.</p><p>LEXGRAPH We define the similarity threshold used to filter out edge weights between sentences, and k as the size of the subgraphs we consider when extracting features from the document graph.</p><p>CLIQUE We define the dropout rate, the LSTM dimensionality, and the hidden layer dimensional-Domain Score Text</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Yahoo</head><p>Low I see it, but then again almost every war entered by the U.S. is connected to gaining something. The U.S. is just using politically correct was of taking over a country without anybody noticing it. They enter a war and some how we come out better than the country we went in to help. We say we are helping but if the country has nothing for us then we don't bother with it. For example: Korea stated and I quote "we have nuclear weapons and we plan to use them" so how come we are in Iraq who have no weapons? Well maybe the U.S. sees no threat but then again somebody did sneak into the country and take over planes. Also not to long ago it was common for somebody to hijack a plane. Well that is all I have to say on the matter. Yahoo</p><p>High Don't be intimidated by Impressionism. It is simply a style worked in loose strokes. The idea is to give an "impression" of the subject. Choose a simple subject, like a still life or bowl of fruit. Then layout your palette using the colors you see (make sure to look for subtle colors only an artist might see...such as the "blue" in an apple), and with a larger than usual brush, stroke the basic shapes in a medium value, then add shadows, then a highlight layer. That should do for a class project in Impressionism. The danger would come from over-working the painting. You don't want fine strokes or details, remember just the "impression" of your subject. The whole idea is to stay loose and free. A lot of people struggle with it. The trick is to just paint without worrying too much. Good luck. You will be given a short text (100-300 words) to read. We will specify which one of several domains the text comes from, and in some domains we will provide additional context for the text.</p><p>Your task is to rate the coherence of the text from 1 to 3 (1 means low coherence, 3 means high coherence).</p><p>Coherence in writing refers to how well ideas flow from one sentence to the next, and from one paragraph to the next. A text that is highly coherent is easy to understand and easy to read. This usually means the text is well-organized, logically structured, and presents only information that supports the main idea. On the other hand, a text with low coherence is difficult to understand. This may be because the text is not well organized, contains unrelated information that distracts from the main idea, or lacks transitions to connect the ideas in the text.</p><p>Try to ignore the effects of grammar or spelling errors when assigning a coherence rating, as long as the errors do not significantly interfere with your ability to read and understand the text. In the email data, assume that jargon and acronyms are used correctly, and do your best to judge coherence despite that.</p><p>You should assign a coherence rating to the text based on whether it is a coherent example of text in that domain. A reader has different expectations about how a business email should be written compared to a post on an online forum, and the coherence rating should reflect this difference. A business email with a score of 1 is not necessarily incoherent in the same way that a very incoherent Yahoo Answers post is, but it is not very coherent for a business email.      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Number of paragraphs, sentences, and words per document.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Structure of PARSEQ model. The sentence vectors are the output from the first LSTM (not pictured), which takes GloVe word embeddings as input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="5">: Distribution of coherence classes as a per-</cell></row><row><cell cols="3">centage of the training data.</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Yahoo</cell><cell>Clinton</cell><cell>Enron</cell><cell>Yelp</cell></row><row><cell># types</cell><cell>13,235</cell><cell>15,564</cell><cell>13,694</cell><cell>12,201</cell></row><row><cell cols="5"># tokens 189,444 220,115 223,347 213,852</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Type and token counts in each domain.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Interannotator agreement (mean and stan- dard deviation) on all domains.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Three-way classification results on test.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Score prediction results on test.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>The number of training examples is 20 times the number of texts, as we generate 20 random permutations for each text.</figDesc><table><row><cell></cell><cell cols="4">Yahoo Clinton Enron Yelp</cell></row><row><cell>Train texts</cell><cell>369</cell><cell>511</cell><cell>507</cell><cell>511</cell></row><row><cell>Test texts</cell><cell>76</cell><cell>111</cell><cell>88</cell><cell>108</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Sentence ordering results on test data.</figDesc><table><row><cell>System</cell><cell cols="3">Yahoo Clinton Enron</cell><cell>Yelp</cell></row><row><cell>Baseline</cell><cell>0.283</cell><cell>0.255</cell><cell>0.341</cell><cell>0.197</cell></row><row><cell>EGRID</cell><cell>0.258</cell><cell>0.260</cell><cell>0.294</cell><cell>0.161</cell></row><row><cell>EGRAPH</cell><cell>0.308</cell><cell>0.382</cell><cell>0.278</cell><cell>0.117</cell></row><row><cell cols="2">EGRIDCONV 0.360</cell><cell>0.238</cell><cell>0.279</cell><cell>0.169</cell></row><row><cell>LEXGRAPH</cell><cell>0.342</cell><cell>0.094</cell><cell>0.357</cell><cell>0.000</cell></row><row><cell>CLIQUE</cell><cell>0.055</cell><cell>0.000</cell><cell>0.077</cell><cell>0.146</cell></row><row><cell>SENTAVG</cell><cell cols="2">0.481  ? 0.332</cell><cell cols="2">0.393  ? 0.199</cell></row><row><cell>PARSEQ</cell><cell>0.447</cell><cell>0.296</cell><cell>0.373</cell><cell>0.112</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Minority class predictions, F 0.5 score on test data.</figDesc><table><row><cell cols="4">tem should identify only the most incoherent areas</cell></row><row><cell cols="4">of the text, to ensure that the feedback is not a false</cell></row><row><cell cols="4">positive. To evaluate this scenario, we present a</cell></row><row><cell cols="4">minority class classification problem where only</cell></row><row><cell cols="3">15-20% of the data is low coherence:</cell><cell></cell></row><row><cell></cell><cell cols="3">Yahoo Clinton Enron Yelp</cell></row><row><cell>Low coherence %</cell><cell>30.0</cell><cell>16.6</cell><cell>18.4 14.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Cross-domain accuracy of PARSEQ on three-way classification test data.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Test accuracy</cell></row><row><cell></cell><cell cols="3">Yahoo Clinton Enron Yelp</cell></row><row><cell>Train in-domain</cell><cell>54.9</cell><cell>60.2</cell><cell>53.2 54.4</cell></row><row><cell>Train all data</cell><cell>58.5</cell><cell>61.0</cell><cell>53.9 56.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 10 :</head><label>10</label><figDesc>Classification accuracy of PARSEQ when trained on data from all four domains.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>Annie Louis and Ani Nenkova. 2012. A coherence model based on syntactic patterns. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Association for Computational Linguistics, Jeju Island, Korea, pages 1157-1168. http://www.aclweb.org/anthology/D12-1106.</figDesc><table><row><cell></cell><cell></cell><cell>Swapna Somasundaran, Jill Burstein, and Martin</cell></row><row><cell></cell><cell></cell><cell>Chodorow. 2014.</cell><cell>Lexical chaining for mea-</cell></row><row><cell></cell><cell></cell><cell>suring discourse coherence quality in test-taker</cell></row><row><cell></cell><cell></cell><cell>essays.</cell><cell>In Proceedings of COLING 2014,</cell></row><row><cell></cell><cell></cell><cell>the 25th International Conference on Compu-</cell></row><row><cell></cell><cell></cell><cell>tational Linguistics: Technical Papers. Dublin</cell></row><row><cell></cell><cell></cell><cell>City University and Association for Computa-</cell></row><row><cell></cell><cell></cell><cell>tional Linguistics, Dublin, Ireland, pages 950-961.</cell></row><row><cell></cell><cell></cell><cell>http://www.aclweb.org/anthology/C14-1090.</cell></row><row><cell cols="3">Mohsen Mesgar and Michael Strube. 2015. Graph-based coherence modeling for assessing read-ability. In Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics. Association for Computational Lin-guistics, Denver, Colorado, pages 309-318. http://www.aclweb.org/anthology/S15-1036.</cell><cell>Dat Tien Nguyen and Shafiq Joty. 2017. A neural local coherence model. In Proceedings of the 55th An-nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Vancouver, Canada, pages 1320-1330. http://aclweb.org/anthology/P17-1121.</cell></row><row><cell cols="3">Mohsen Mesgar and Michael Strube. 2016. Lexi-</cell></row><row><cell cols="3">cal coherence graph modeling using word embed-</cell></row><row><cell cols="3">dings. In Proceedings of the 2016 Conference</cell></row><row><cell cols="3">of the North American Chapter of the Association</cell></row><row><cell cols="3">for Computational Linguistics: Human Language</cell></row><row><cell cols="3">Technologies. Association for Computational Lin-</cell></row><row><cell cols="3">guistics, San Diego, California, pages 1414-1423.</cell></row><row><cell cols="3">http://www.aclweb.org/anthology/N16-1167.</cell></row><row><cell cols="3">Jane Morris and Graeme Hirst. 1991.</cell><cell>Lex-</cell></row><row><cell cols="3">ical cohesion computed by thesaural re-</cell></row><row><cell cols="3">lations as an indicator of the structure of</cell></row><row><cell>text.</cell><cell cols="2">Computational Linguistics 17(1):21-48.</cell></row><row><cell cols="3">http://dl.acm.org/citation.cfm?id=971738.971740.</cell></row><row><cell cols="3">Hwee Tou Ng, Siew Mei Wu, Ted Briscoe,</cell></row><row><cell cols="3">Christian Hadiwinoto, Raymond Hendy Susanto,</cell></row><row><cell cols="2">and Christopher Bryant. 2014.</cell><cell>The CoNLL-</cell></row><row><cell cols="3">2014 shared task on grammatical error correc-</cell></row><row><cell cols="3">tion. In Proceedings of the Eighteenth Confer-</cell></row><row><cell cols="3">ence on Computational Natural Language Learn-</cell></row><row><cell cols="3">ing: Shared Task. Association for Computational</cell></row><row><cell cols="3">Linguistics, Baltimore, Maryland, pages 1-14.</cell></row><row><cell cols="3">http://www.aclweb.org/anthology/W14-1701.</cell></row><row><cell cols="3">Ellie Pavlick and Joel Tetreault. 2016. An empiri-</cell></row><row><cell cols="3">cal analysis of formality in online communication.</cell></row><row><cell cols="3">Transactions of the Association for Computational</cell></row><row><cell cols="2">Linguistics 4:61-74.</cell></row><row><cell cols="3">F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,</cell></row><row><cell cols="3">B. Thirion, O. Grisel, M. Blondel, P. Pretten-</cell></row><row><cell cols="3">hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-</cell></row><row><cell cols="3">sos, D. Cournapeau, M. Brucher, M. Perrot, and</cell></row><row><cell cols="3">E. Duchesnay. 2011. Scikit-learn: Machine learning</cell></row><row><cell cols="3">in Python. Journal of Machine Learning Research</cell></row><row><cell cols="2">12:2825-2830.</cell></row><row><cell cols="3">Nils Reimers and Iryna Gurevych. 2017. Report-</cell></row><row><cell cols="3">ing score distributions makes a difference: Per-</cell></row><row><cell cols="3">formance study of LSTM-networks for sequence</cell></row><row><cell>tagging.</cell><cell cols="2">In Proceedings of the 2017 Confer-</cell></row><row><cell cols="3">ence on Empirical Methods in Natural Language</cell></row><row><cell cols="3">Processing. Association for Computational Lin-</cell></row><row><cell cols="3">guistics, Copenhagen, Denmark, pages 338-348.</cell></row><row><cell cols="3">https://www.aclweb.org/anthology/D17-1035.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 11 :</head><label>11</label><figDesc>Examples of texts with coherence scores.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 12 :</head><label>12</label><figDesc>The annotation instructions we provided to expert annotators.</figDesc><table><row><cell></cell><cell></cell><cell>Accuracy</cell><cell></cell></row><row><cell>System</cell><cell cols="3">Yahoo Clinton Enron Yelp</cell></row><row><cell>Majority class</cell><cell>39.5</cell><cell>40.5</cell><cell>44.0 40.5</cell></row><row><cell>Baseline</cell><cell>35.0</cell><cell>43.5</cell><cell>45.0 41.5</cell></row><row><cell>EGRID</cell><cell>43.0</cell><cell>41.0</cell><cell>45.5 43.0</cell></row><row><cell>EGRAPH</cell><cell>39.5</cell><cell>41.5</cell><cell>44.5 40.5</cell></row><row><cell>EGRIDCONV</cell><cell>41.0</cell><cell>43.5</cell><cell>44.5 54.0</cell></row><row><cell>LEXGRAPH</cell><cell>38.0</cell><cell>36.0</cell><cell>48.0 45.5</cell></row><row><cell>CLIQUE</cell><cell>48.0</cell><cell>45.0</cell><cell>52.5 51.0</cell></row><row><cell>SENTAVG</cell><cell>52.0</cell><cell>48.5</cell><cell>55.5 49.0</cell></row><row><cell>PARSEQ</cell><cell>47.5</cell><cell>51.0</cell><cell>56.5 57.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 13 :</head><label>13</label><figDesc>Three-way classification results on test data. Untrained rater judgments.</figDesc><table><row><cell>System</cell><cell cols="2">Yahoo</cell><cell cols="2">Clinton</cell><cell cols="2">Enron</cell><cell>Yelp</cell></row><row><cell></cell><cell>p</cell><cell>r</cell><cell>p</cell><cell>r</cell><cell>p</cell><cell>r</cell><cell>p</cell><cell>r</cell></row><row><cell>Baseline</cell><cell cols="6">25 61 26 24 33 38</cell><cell cols="2">17 42</cell></row><row><cell>EGRID</cell><cell cols="6">31 16 36 12 57 10</cell><cell>33</cell><cell>5</cell></row><row><cell>EGRAPH</cell><cell cols="6">26 94 35 58 25 45</cell><cell cols="2">10 68</cell></row><row><cell cols="7">EGRIDCONV 31 41 16 24 22 40</cell><cell cols="2">50 05</cell></row><row><cell>LEXGRAPH</cell><cell cols="3">26 29 20</cell><cell cols="3">3 55 15</cell><cell>0</cell><cell>0</cell></row><row><cell>CLIQUE</cell><cell>7</cell><cell>3</cell><cell>0</cell><cell cols="2">0 17</cell><cell cols="3">3 100 05</cell></row><row><cell>SENTAVG</cell><cell cols="6">38 73 39 36 42 33</cell><cell cols="2">36 21</cell></row><row><cell>PARSEQ</cell><cell cols="6">43 51 21 39 57 20</cell><cell cols="2">13 11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 14 :</head><label>14</label><figDesc>Minority class predictions, precision/recall results on test data.</figDesc><table><row><cell>System</cell><cell>Accuracy</cell></row><row><cell>Random baseline</cell><cell>50.0</cell></row><row><cell>EGRID</cell><cell>83.0</cell></row><row><cell>EGRAPH</cell><cell>65.7</cell></row><row><cell>EGRIDCONV</cell><cell>82.2</cell></row><row><cell>LEXGRAPH</cell><cell>72.7</cell></row><row><cell>CLIQUE</cell><cell>60.9</cell></row><row><cell>SENTSEQ</cell><cell>74.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 15 :</head><label>15</label><figDesc>Sentence ordering results on WSJ test data.ity. Window size is the number of sentences in a clique.SENTAVG, PARSEQ For both models, we specify the dropout rate, the LSTM dimensionality, and the hidden layer dimensionality. For PARSEQ, the LSTM dimensionality applies to all 3 LSTMs.</figDesc><table><row><cell>Classification</cell><cell>Score Prediction</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 16 :</head><label>16</label><figDesc>Best parameter values for classification and score prediction experiments.</figDesc><table><row><cell>Sentence Ordering</cell><cell>Minority Class</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 17 :</head><label>17</label><figDesc>Best parameter values for sentence ordering and minority class classification experiments.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://webscope.sandbox.yahoo.com/ catalog.php?datatype=l 3 https://foia.state.gov/Search/ Results.aspx?collection=Clinton_Email 4 https://www.cs.cmu.edu/?./enron/ 5 https://www.yelp.com/dataset</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/datienguyen/cnn_ coherence 7 https://github.com/MMesgar/lcg</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank Yahoo Research and Yelp for making their data available, and Ji-wei Li and Mohsen Mesgar for sharing their code. Thanks also to Michael Strube, Annie Louis, Rebecca Hwa, Dimitrios Alikaniotis, Claudia Leacock, Courtney Napoles, Jill Burstein, Mirella Lapata, Martin Chodorow, Micha Elsner, and the anonymous reviewers for their helpful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Modeling local coherence: An entity-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.3115/1219840.1219858</idno>
		<ptr target="https://doi.org/10.3115/1219840.1219858" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05). Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05). Association for Computational Linguistics<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="141" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Modeling local coherence: An entity-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.1162/coli.2008.34.1.1</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<idno type="DOI">10.1162/coli.2008.34.1.1</idno>
		<ptr target="https://doi.org/10.1162/coli.2008.34.1.1" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Using entity-based features to model coherence in student essays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jill</forename><surname>Burstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slava</forename><surname>Andreyev</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N10-1099" />
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="681" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Holistic discourse coherence annotation for noisy essay writing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jill</forename><surname>Burstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">R</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
		<ptr target="http://dad.uni-bielefeld.de/index.php/dad/article/view/2825" />
	</analytic>
	<monogr>
		<title level="j">D&amp;D</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="34" to="52" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Entity-centric coreference resolution with model stacking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Coreference-inspired coherence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha</forename><surname>Elsner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<title level="m">Proceedings of ACL-08: HLT, Short Papers. Association for Computational Linguistics</title>
		<meeting>ACL-08: HLT, Short Papers. Association for Computational Linguistics<address><addrLine>Columbus, Ohio</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="41" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Extending the entity grid with entity-specific features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha</forename><surname>Elsner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P11-2022" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="125" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The impact of deep hierarchical discourse structures in the evaluation of text coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vanessa</forename><surname>Wei Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/C14-1089" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers. Dublin City University and Association for Computational Linguistics</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers. Dublin City University and Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="940" to="949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Centering: A framework for modeling the local coherence of discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><forename type="middle">J</forename><surname>Grosz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Weinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="225" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Graph-based local coherence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Guinaudeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P13-1010" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="93" to="103" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
		<ptr target="https://doi.org/10.1162/neco.1997.9.8.1735" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Derivation of new readability formulas (automated readability index, fog count and Flesch reading ease formula) for navy enlisted personnel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kincaid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">P</forename><surname>Fishburne</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brad</forename><forename type="middle">S</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chissom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>DTIC Document</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The measurement of observer agreement for categorical data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Landis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><forename type="middle">G</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="159" to="174" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Probabilistic text structuring: Experiments with sentence ordering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.3115/1075096.1075165</idno>
		<ptr target="https://doi.org/10.3115/1075096.1075165" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sapporo, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A model of coherence based on distributed sentence representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D14-1218" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2039" to="2048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural net models of open-domain discourse coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="198" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatically evaluating text coherence using discourse relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Hwee Tou Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kan</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P11-1100" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="997" to="1006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
