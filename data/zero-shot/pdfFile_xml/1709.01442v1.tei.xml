<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dense Face Alignment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaojie</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Michigan State University</orgName>
								<address>
									<region>MI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Jourabloo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Michigan State University</orgName>
								<address>
									<region>MI</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Ren</surname></persName>
							<email>2williamyren@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Monta Vista High School</orgName>
								<address>
									<settlement>Cupertino</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Michigan State University</orgName>
								<address>
									<region>MI</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dense Face Alignment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Face alignment is a classic problem in the computer vision field. Previous works mostly focus on sparse alignment with a limited number of facial landmark points, i.e., facial landmark detection. In this paper, for the first time, we aim at providing a very dense 3D alignment for largepose face images. To achieve this, we train a CNN to estimate the 3D face shape, which not only aligns limited facial landmarks but also fits face contours and SIFT feature points. Moreover, we also address the bottleneck of training CNN with multiple datasets, due to different landmark markups on different datasets, such as 5, 34, 68. Experimental results show our method not only provides highquality, dense 3D face fitting but also outperforms the stateof-the-art facial landmark detection methods on the challenging datasets. Our model can run at real time during testing and it's available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Face alignment is a long-standing problem in the computer vision field, which is the process of aligning facial components, e.g., eye, nose, mouth, and contour. An accurate face alignment is an essential prerequisite for many face related tasks, such as face recognition <ref type="bibr" target="#b7">[8]</ref>, 3D face reconstruction <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b20">21]</ref> and face animation <ref type="bibr" target="#b36">[37]</ref>. There are fruitful previous works on face alignment, which can be categorized as generative methods such as the early Active Shape Model <ref type="bibr" target="#b16">[17]</ref> and Active Appearance Model (AAM) based approaches <ref type="bibr" target="#b12">[13]</ref>, and discriminative methods such as regression-based approaches <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>Most previous methods estimate a sparse set of landmarks, e.g., 68 landmarks. As this field is being developed, we believe that Dense Face Alignment (DeFA) becomes highly desired. Here, DeFA denotes that it's doable to map any face-region pixel to the pixel in other face images, which has the same anatomical position in human faces. For example, given two face images from the same <ref type="figure">Figure 1</ref>. A pair of images with their dense 3D shapes obtained by imposing landmark fitting constraint, contour fitting constraint and sift pair constraint.</p><formula xml:id="formula_0">? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</formula><p>individual but with different poses, lightings or expressions, a perfect DeFA can even predict the mole (i.e. darker pigment) on two faces as the same position. Moreover, DeFA should offer dense correspondence not only between two face images, but also between the face image and the canonical 3D face model. This level of detailed geometry interpretation of a face image is invaluable to many conventional facial analysis problems mentioned above.</p><p>Since this interpretation has gone beyond the sparse set of landmarks, fitting a dense 3D face model to the face image is a reasonable way to achieve DeFA. In this work, we choose to develop the idea of fitting a dense 3D face model to an image, where the model with thousands of vertexes makes it possible for face alignment to go very "dense". 3D face model fitting is well studied in the seminal work of 3D Morphorbal Model (3DMM) <ref type="bibr" target="#b3">[4]</ref>. We see a recent surge when it is applied to problems such as large-pose face alignment <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b40">41]</ref>, 3D reconstruction <ref type="bibr" target="#b4">[5]</ref>, and face recognition <ref type="bibr" target="#b0">[1]</ref>, especially using the convolutional neural network (CNN) architecture.</p><p>However, most prior works on 3D-model-fitting-based face alignment only utilize the sparse landmarks as supervision. There are two main challenges to be addressed in 3D face model fitting, in order to enable high-quality DeFA. First of all, to the best of our knowledge, no public face dataset has dense face shape labeling. All of the in-the-wild face alignment datasets have no more than 68 landmarks in the labeling. Apparently, to provide a high-quality alignment for face-region pixels, we need information more than just the landmark labeling. Hence, the first challenge is to seek valuable information for additional supervision and in-tegrate them in the learning framework.</p><p>Secondly, similar to many other data-driven problems and solutions, it is preferred that multiple datasets can be involved for solving face alignment task since a single dataset has limited types of variations. However, many face alignment methods can not leverage multiple datasets, because each dataset either is labeled differently. For instance, AFLW dataset <ref type="bibr" target="#b22">[23]</ref> contains a significant variation of poses, but has a few number of visible landmarks. In contrast, 300W dataset <ref type="bibr" target="#b22">[23]</ref> contains a large number of faces with 68 visible landmarks, but all faces are in a near-frontal view. Therefore, the second challenge is to allow the proposed method to leverage multiple face datasets.</p><p>With the objective of addressing both challenges, we learn a CNN to fit a 3D face model to the face image. While the proposed method works for any face image, we mainly pay attention to faces with large poses. Large-pose face alignment is a relatively new topic, and the performances in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b40">41]</ref> still have room to improve. To tackle first challenge of limited landmark labeling, we propose to employ additional constraints. We include contour constraint where the contour of the predicted shape should match the detected 2D face boundary, and SIFT constraint where the SIFT key points detected on two face images of the same individual should map to the same vertexes on the 3D face model. Both constraints are integrated into the CNN training as additional loss function terms, where the end-to-end training results in an enhanced CNN for 3D face model fitting. For the second challenge of leveraging multiple datasets, the 3D face model fitting approach has the inherent advantage in handling multiple training databases. Regardless of the landmark labeling number in a particular dataset, we can always define the corresponding 3D vertexes to guide the training.</p><p>Generally, our main contributions can be summarized as: 1. We identify and define a new problem of dense face alignment, which seeks alignment of face-region pixels beyond the sparse set of landmarks.</p><p>2. To achieve dense face alignment, we develop a novel 3D face model fitting algorithm that adopts multiple constraints and leverages multiple datasets.</p><p>3. Our dense face alignment algorithm outperforms the SOTA on challenging large-pose face alignment, and achieves competitive results on near-frontal face alignment. The model runs at real time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We review papers in three relevant areas: 3D face alignment from a single image, using multiple constraints in face alignment, and using multiple datasets for face alignment. 3D model fitting in face alignment Recently, there are increasingly attentions in conducting face alignment by fitting the 3D face model to the single 2D image <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b10">11]</ref>. In <ref type="bibr" target="#b3">[4]</ref>, Blanz and Vetter proposed the 3DMM to represent the shape and texture of a range of individuals. The analysis-by-synthesis based methods are utilized to fit the 3DMM to the face image. In <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b9">10]</ref> a set of cascade CNN regressors with the extracted 3D features is utilized to estimate the parameters of 3DMM and the projection matrix directly. Liu et al. <ref type="bibr" target="#b14">[15]</ref> proposed to utilize two sets of regressors, for estimating update of 2D landmarks and the other set estimate update of dense 3D shape by using the 2D landmarks update. They apply these two sets of regressors alternatively. Compared to prior work, our method imposes additional constraints, which is the key to dense face alignment.</p><p>Multiple constraints in face alignment Other than landmarks, there are other features that are useful to describe the shape of a face, such as contours, pose and face attributes. Unlike landmarks, those features are often not labeled in the datasets. Hence, the most crucial step of leveraging those features is to find the correspondence between the features and the 3D shape. In <ref type="bibr" target="#b19">[20]</ref>, multiple features constraints in the cost function is utilized to estimate the 3D shape and texture of a 3D face. 2D edge is detected by Canny detector, and the corresponding 3D edges' vertices are matched by Iterative Closest Point (ICP) to use this information. Furthermore, <ref type="bibr" target="#b23">[24]</ref> provides statistical analysis about the 2D face contours and the 3D face shape under different poses.</p><p>There is a few work using constraints as separate side tasks to facilitate face alignment. In <ref type="bibr" target="#b30">[31]</ref>, they set a pose classification task, predicting faces as left, right profile or frontal, in order to assist face alignment. Even with such a rough pose estimation, this information boosts the alignment accuracy. Zhang et al. <ref type="bibr" target="#b33">[34]</ref> jointly estimates 2D landmarks update with the auxiliary attributes (e.g., gender, expression) in order to improve alignment accuracy. The "mirrorability" constraint is used in <ref type="bibr" target="#b31">[32]</ref> to force the estimated 2D landmarks update be consistent between the image and its mirror image. In contrast, we integrate a set of constraints in an end-to-end trainable CNN to perform 3D face alignment. Multiple datasets in face alignment Despite the huge advantages (e.g., avoiding dataset bias), there are only a few face alignment works utilizing multiple datasets, owing to the difficulty of leveraging different types of face landmark labeling. Zhu et al. <ref type="bibr" target="#b38">[39]</ref> propose a transductive supervised descent method to transfer face annotation from a source dataset to a target dataset, and use both datasets for training.</p><p>[25] ensembles a non-parametric appearance model, shape model and graph matching to estimate the superset of the landmarks. Even though achieving good results, it suffers from high computation cost. Zhang et al. <ref type="bibr" target="#b32">[33]</ref> propose a deep regression network for predicting the superset of landmarks. For each training sample, the sparse shape regression is adopted to generate the different types of landmark annotations. In general, most of the mentioned prior work learn to map landmarks between two datasets, while our method can readily handle an arbitrary number of datasets since the dense 3D face model can bridge the discrepancy of landmark definitions in various datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dense Face Alignment</head><p>In this section, we explain the details of the proposed dense face alignment method. We train a CNN for fitting the dense 3D face shape to a single input face image. We utilize the dense 3D shape representation to impose multiple constraints, e.g., landmark fitting constraint, contour fitting constraint and SIFT pairing constraint, to train such CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">3D Face Representation</head><p>We represent the dense 3D shape of the face as, S, which contains the 3D locations of Q vertices,</p><formula xml:id="formula_1">S = ? ? x 1 x 2 ? ? ? x Q y 1 y 2 ? ? ? y Q z 1 z 2 ? ? ? z Q ? ? .<label>(1)</label></formula><p>To compute S for a face, we follow the 3DMM to represent it by a set of 3D shape bases,</p><formula xml:id="formula_2">S =S + N id i=1 p i id S i id + Nexp i=1 p i exp S i exp ,<label>(2)</label></formula><p>where the face shape S is the summation of the mean shapeS and the weighted PCA shape bases S id and S exp with corresponding weights of p id , p exp . In our work, we use 199 shape bases S i id , i = {1, ..., 199} for representing identification variances such as tall/short, light/heavy, and male/female, and 29 shape bases S i exp , i = {1, ..., 29} for representing expression variances such as mouth-opening, smile, kiss and etc. Each basis has Q = 53, 215 vertices, which are corresponding to vertices over all the other bases.</p><p>The mean shapeS and the identification bases S id are from Basel Face Model <ref type="bibr" target="#b17">[18]</ref>, and the expression bases S exp are from FaceWarehouse <ref type="bibr" target="#b6">[7]</ref>. A subset of N vertices of the dense 3D face U corresponds to the location of 2D landmarks on the image,</p><formula xml:id="formula_3">U = u 1 u 2 ? ? ? u N v 1 v 2 ? ? ? v N .<label>(3)</label></formula><p>By considering weak perspective projection, we can estimate the dense shape of a 2D face based on the 3D face shape. The projection matrix has 6 degrees of freedom and can model changes w.r.t. scale, rotation angles (pitch ?, yaw ?, roll ?), and translations (t x , t y ). The transformed dense face shape A ? R 3?Q can be represented as,</p><formula xml:id="formula_4">A = ? ? m 1 m 2 m 3 m 4 m 5 m 6 m 7 m 8 m 9 m 10 m 11 m 12 ? ? S 1 (4) U = Pr ? A,<label>(5)</label></formula><p>where A can be orthographically projected onto 2D plane to achieve U. Hence, z-coordinate translation (m 12 ) is out of our interest and assigned to be 0. The orthographic projection can be denoted as matrix Pr = 1 0 0 0 1 0 .</p><p>Given the properties of projection matrix, the normalized third row of the projection matrix can be represented as the outer product of normalized first two rows,</p><formula xml:id="formula_5">[m 9 ,m 10 ,m 11 ] = [m 1 ,m 2 ,m 3 ] ? [m 4 ,m 5 ,m 6 ]. (6)</formula><p>Therefore, the dense shape of an arbitrary 2D face can be determined by the first two rows of the projection parameters m = [m 1 , ? ? ? , m 8 ] ? R 8 and the shape basis coefficients p = [p 1 id , ..., p 199 id , p 1 exp , ...p 29 exp ] ? R 228 . The learning of the dense 3D shape is turned into the learning of m and p, which is much more manageable in term of the dimensionality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">CNN Architecture</head><p>Due to the success of deep learning in computer vision, we employ a convolutional neural network (CNN) to learn the nonlinear mapping function f (?) from the input image I to the corresponding projection parameters m and shape parameters p. The estimated parameters can then be utilized to construct the dense 3D face shape.</p><p>Our CNN network has two branches, one for predicting m and another for p, shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. Two branches share the first three convolutional blocks. After the third block, we use two separate convolutional blocks to extract taskspecific features, and two fully connected layers to transfer the features to the final output. Each convolutional block is a stack of two convolutional layers and one max pooling layer, and each conv/fc layer is followed by one batch normalization layer and one leaky ReLU layer.</p><p>In order to improve the CNN learning, we employ a loss function including multiple constraints: Parameter Constraint (PC) J pr minimizes the difference between the estimated parameters and the ground truth parameters; Landmark Fitting Constraint (LFC) J lm reduces the alignment error of 2D landmarks; Contour Fitting Constraint (CFC) J c enforces the match between the contour of the estimated 3D shape and the contour pixels of the input image; and SIFT Pairing Constraint (SPC) J s encourages that the SIFT feature point pairs of two face images to correspond to the same 3D vertices.</p><p>We define the overall loss function as, arg min</p><formula xml:id="formula_6">m,p J = J pr + ? lm J lm + ? c J c + ? s J s ,<label>(7)</label></formula><p>where the parameter constraint (PC) loss is defined as,</p><formula xml:id="formula_7">J pr = m p ? m p 2 .<label>(8)</label></formula><p>Landmark Fitting Constraint (LFC) aims to minimize the difference between the estimated 2D landmarks and the ground truth 2D landmark labeling U lm ? R 2?N . Given 2D face images with a particular landmark labeling, we first manually mark the indexes of the 3D face vertices that are anatomically corresponding to these landmarks. The collection of these indexes is denoted as i lm . After the shape A is computed from Eqn. 4 with the estimatedm andp, the 3D landmarks can be extracted from A by A(:, i lm ). With projection of A(:, i lm ) to 2D plain, the LFC loss is defined as,</p><formula xml:id="formula_8">J lm = 1 L ? PrA(:, i lm ) ? U lm 2 F ,<label>(9)</label></formula><p>(a) (b) (c) <ref type="figure">Figure 3</ref>. The CFC fitting process. Ac is computed from estimated 3D face shape and Uc is computed from the off-the-shelf edge detector. Contour correspondence is obtained via Closest Pair Algorithm, and loss Jc is calculated based on Eqn. <ref type="bibr" target="#b9">10</ref> where the subscript F represents the Frobenius Norm, and L is the number of pre-defined landmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Contour Fitting Constraint (CFC)</head><p>Contour Fitting Constraint (CFC) aims to minimize the error between the projected outer contour (i.e., silhouette) of the dense 3D shape and the corresponding contour pixels in the input face image. The outer contour can be viewed as the boundary between the background and the 3D face while rendering 3D space onto a 2D plane. On databases such as AFLW where there is a lack of labeled landmarks on the silhouette due to self-occlusion, this constraint can be extremely helpful.</p><p>To utilize this contour fitting constraint, we need to follow these three steps: 1) Detect the true contour in the 2D face image; 2) Describe the contour vertices on the estimated 3D shape A; and 3) Determine the correspondence between true contour and the estimated one, and backpropagate the fitting error.</p><p>First of all, we adopt an off-the-shelf edge detector, HED <ref type="bibr" target="#b28">[29]</ref>, to detect the contour on the face image, U c ? R 2?L . The HED has a high accuracy at detecting significant edges such as face contour in our case. Additionally, in certain datasets, such as 300W <ref type="bibr" target="#b22">[23]</ref> and AFLW-LPFA <ref type="bibr" target="#b9">[10]</ref>, additional landmark labelings on the contours are available. Thus we can further refine the detected edges by only retaining edges that are within a narrow band determined by those contour landmarks, shown in <ref type="figure">Fig 3.</ref>a. This preprocessing step is done offline before the training starts.</p><p>In the second step, the contour on the estimated 3D shape A can be described as the set of boundary vertices A(:, i c ) ? R 3?L . A is computed from the estimatedm and p parameters. By utilizing the Delaunay triangulation to represent shape A, one edge of a triangle is defined as the boundary if the adjacent faces have a sign change in the zvalues of the surface normals. This sign change indicates a change of visibility so that the edge can be considered as a boundary. The vertices associated with this edge are defined as boundary vertices, and their collection is denoted as i c . This process is shown in <ref type="figure">Fig 3.</ref>b.</p><p>In the third step, the point-to-point correspondences between U c and A(:, i c ) are needed in order to evaluate the constraint. Given that we normally detect partial contour pixels on 2D images while the contour of 3D shape is typically complete, we match the contour pixel on the 2D images with closest point on 3D shape contour, and then calculate the minimun distance. The sum of all minimum distances is the error of CFC, as shown in the Eqn. 10. To make CFC loss differentiable, we rewrite Eqn. 10 to compute the vertex index of the closest contour projection point, i.e., k 0 = arg min k?ic PrA(:, k) ? U c (:, j) 2 . Once k 0 is determined, the CFC loss will be differentiable, similar to Eqn. 9. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">SIFT Pairing Constraint (SPC)</head><p>SIFT Pairing Constraint (SPC) regularizes the predictions of dense shape to be consistent on the significant facial points other than pre-defined landmarks, such as edges, wrinkles, and moles. The Scale-invariant feature transform (SIFT) descriptor is a classic local representation that is invariant to image scaling, noise, and illumination. It is widely used in many regression-based face alignment methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b25">26]</ref> to extract the local information.</p><p>In our work, the SIFT descriptors are used to detect and represent the significant points within the face pair. The face pair can either come from the same people with different poses and expressions, or the same image with different augmentation, e.g., cropping, rotation and 3D augmentation, shown in <ref type="figure" target="#fig_1">Fig. 4</ref>. The more face pairs we have, the stronger this constraint is. Given a pair of faces i and j, we first detect and match SIFT points on two face images. The matched SIFT points are denoted as U i s and U j s ? R 2?Lij . With a perfect dense face alignment, the matched SIFT points would overlay with exactly the same vertex in the estimated 3D face shapes, denoted as A i and A j . In practices, to verify how likely this ideal world is true and leverage it as a constraint, we first find the 3D vertices i i s whose projections overlay with the 2D SIFT points, U i s .</p><formula xml:id="formula_9">i i s = arg min i?{1,...,Lij } A i {i i s } ? U i s 2 F ,<label>(11)</label></formula><p>Similarly, we find j j s based on U j s . Now we define the SPC loss function as</p><formula xml:id="formula_10">J s (m j ,p j ,m i ,p i ) = 1 L ij A i {i j s } ? U i s 2 F + A j {i i s } ? U j s 2 F<label>(12)</label></formula><p>where A i is computed using {m i , p i }. As shown in <ref type="figure" target="#fig_1">Fig. 4</ref>, we map SIFT points from one face to the other and compute their distances w.r.t. the matched SIFT points on the other face. With the mapping from both images, we have two terms in the loss function of Eqn. 12.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We evaluate our proposed method on four benchmark datasets: AFLW-LFPA <ref type="bibr" target="#b8">[9]</ref>, AFLW2000-3D <ref type="bibr" target="#b40">[41]</ref>, 300W <ref type="bibr" target="#b22">[23]</ref> and IJBA <ref type="bibr" target="#b11">[12]</ref>. All datasets used in our training and testing phases are listed in Tab. 1. AFLW-LFPA: AFLW contains around 25, 000 face images with yaw angles between ?90 ? , and each image is labeled with up to 21 visible landmarks. In <ref type="bibr" target="#b8">[9]</ref>, a subset of AFLW with a balanced distribution of the yaw angle is introduced as AFLW-LFPA. It consists of 3, 901 training images and 1, 299 testing images. Each image is labeled with 13 additional landmarks. AFLW2000-3D: Prepared by <ref type="bibr" target="#b40">[41]</ref>, this dataset contains 2, 000 images with yaw angles between ?90 ? of the AFLW dataset. Each image is labeled with 68 landmarks. Both this dataset and AFLW-LFPA are widely used for evaluating large-pose face alignment. IJBA: IARPA Janus Benchmark A (IJB-A) <ref type="bibr" target="#b11">[12]</ref> is an inthe-wild dataset containing 500 subjects and 25, 795 images with three landmark, two landmarks at eye centers and one on the nose. While this dataset is mainly used for face recognition, the large dataset size and the challenging variations (e.g., ?90 ? yaw and images resolution) make it suitable for evaluating face alignment as well. 300W: 300W <ref type="bibr" target="#b22">[23]</ref> integrates multiple databases with standard 68 landmark labels, including AFW <ref type="bibr" target="#b42">[43]</ref>, LFPW <ref type="bibr" target="#b2">[3]</ref>, HELEN <ref type="bibr" target="#b35">[36]</ref>, and IBUG <ref type="bibr" target="#b22">[23]</ref>. This is the widely used database for evaluating near-frontal face alignment. COFW <ref type="bibr" target="#b5">[6]</ref>: This dataset includes near-frontal face images with occlusion. We use this dataset in training to make the model more robust to occlusion. Caltech10k <ref type="bibr" target="#b1">[2]</ref>: It contains four labeled landmarks: two on eye centers, one on the top of the nose and one mouth center. We do not use the mouth center landmark since there is no corresponding vertex on the 3D shape existing for it. LFW <ref type="bibr" target="#b13">[14]</ref>: Despite having no landmark labels, LFW can be used to evaluate how dense face alignment method performs via the corresponding SIFT points between two images of the same individual.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental setup</head><p>Training sets and procedures : While utilizing multiple datasets is beneficial for learning an effective model, it also poses challenges to the training procedure. To make the training more manageable, we train our DeFA model in three stages, with the intention to gradually increase the datasets and employed constraints. At stage 1, we use 300W-LP to train our DeFA network with parameter constraint (PL). At stage 2, we additionally include samples from the Caltech10K <ref type="bibr" target="#b1">[2]</ref>, and COFW <ref type="bibr" target="#b5">[6]</ref> to continue the training of our network with the additional landmark fitting constraint (LFC). At stage 3, we fine-tune the model with SPC and CFC constraints. For large-pose face alignment, we fine-tune the model with AFLW-LFPA training set. For near-frontal face alignment, we fine-tune the model with 300W training set. All samples at the third stage are augmented 20 times with up to ?20 ? random in-plain rotation and 15% random noise on the center, width, and length of the initial bounding box. Tab. 2 shows the datasets and  <ref type="bibr" target="#b26">[27]</ref>. To train the network, we use 20, 10, and 10 epochs for stage 1 to 3. We set the initial global learning rate as 1e?3, and reduce the learning rate by a factor of 10 when the training error approaches a plateau. The minibatch size is 32, weight decay is 0.005, and the leak factor for Leaky ReLU is 0.01. In stage 2, the regularization weights ? pr for PC is 1 and ? lm for LFC is 5; In stage 3, the regularization weights ? lm , ? s , ? c for LFC, SPC and CFC are set as 5, 1 and 1, respectively. Evaluation metrics: For performance evaluation and comparison, we use two metrics for normalizing the MSE. We follow the normalization method in <ref type="bibr" target="#b9">[10]</ref> for large-pose faces, which normalizes the MSE by using the bounding box size. We term this metric as "NME-lp". For the nearfrontal view datasets such as 300W, we use the inter-ocular distance for normalizing the MSE, termed as "NME-nf".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experiments on Large-pose Datasets</head><p>To evaluate the algorithm on large-pose datasets, we use the AFLW-LFPA, AFLW2000-3D, and IJB-A datasets. The results are presented in Tab. 3, where the performance of the baseline methods is either reported from the published papers or by running the publicly available source code. For AFLW-LFPA, our method outperforms the best methods with a large margin of 17.8% improvement. For AFLW2000-3D, our method also shows a large improvement. Specifically, for images with yaw angle in [60 ? , 90 ? ], our method improves the performance by 28% (from 7.93 to 5.68). For the IJB-A dataset, even though we are able to only compare the accuracy for the three labeled landmarks, our method still reaches a higher accuracy. Note that the best performing baselines, 3DDFA and PAWF, share the similar overall approach in estimating m and p, and also aim for large-pose face alignment. The consistently superior performance of our DeFA indicates that we have advanced the state of the art in large-pose face alignment. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Experiments on Near-frontal Datasets</head><p>Even though the proposed method can handle largepose alignment, to show its performance on the near-frontal datasets, we evaluate our method on the 300W dataset. The result of the state-of-the-art method on the both common and challenging sets are shown in Tab. 4. To find the corresponding landmarks on the cheek, we apply the landmark marching <ref type="bibr" target="#b41">[42]</ref> algorithm to move contour landmarks from self-occluded location to the silhouette. Our method is the second best method on the challenging set. In general, the performance of our method is comparable to other methods that are designed for near-frontal datasets, especially under the following consideration. That is, most prior face alignment methods do not employ shape constraints such as 3DMM, which could be an advantage for near-frontal face alignment, but might be a disadvantage for large-pose face alignment. The only exception in Tab. 4 in 3DDFA <ref type="bibr" target="#b40">[41]</ref>, which attempted to overcome the shape constraint by using the additional SDM-based finetuning. It is a strong testimony of our model in that DeFA, without further finetuning, outperforms both 3DDFA and its fine tuned version with SDM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Study</head><p>To analyze the effectiveness of the DeFA method, we design two studies to compare the influence of each part in the DeFA and the improvement by adding each dataset.</p><p>Tab. 5 shows the consistent improvement achieved by utilizing more datasets in different stages and constraints according to Tab. 2 on both testing datasets. It shows the advantage and the ability of our method in leveraging more datasets. The accuracy of our method on the AFLW2000-3D consistently improves by adding more datasets. For the AFLW-PIFA dataset, our method achieves 9.5% and 20% relative improvement by utilizing the datasets in the stage  2 and stage 3 over the first stage, respectively. If including the datasets from both the second and third stages, we can have 26% relative improvement and achieve NME of 3.86%. Comparing the second and third rows in Tab. 5 shows that the effectiveness of CFC and SPC is more than LFC. This is due to the utilization of more facial matching in the CFC and SPC. The second study shows the performance improvement achieved by using the proposed constraints. We train models with different types of active constraints and test them on the AFLW-PIFA test set. Due to the time constraint, for this experiment, we did not apply 20 times augmentation of the third stage's dataset. We show the results in the left of <ref type="figure" target="#fig_3">Fig. 5</ref>. Comparing LFC+SPC and LFC+CFC performances shows that the CFC is more helpful than the SPC. The reason is that CFC is more helpful in correcting the pose of the face and leads to more landmark error reduction. Using all constraints achieves the best performance.</p><p>Finally, to evaluate the influence of using the SIFT pairing constraint (SPC), we use all of the three stages datasets to train our method. We select 5, 000 pairs of images from the IJB-A dataset and compute the NME-lp of all matched SIFT points according to Eqn. 12. The right plot in <ref type="figure" target="#fig_3">Fig. 5</ref> illustrates the CED diagrams of NME-lp, for the trained models with and without the SIFT pairing constraint. This result shows that for the images with NME-lp between 5% and 15% the SPC is helpful.</p><p>Part of the reason DeFA works well is that it receives <ref type="figure">Figure 6</ref>. The estimated dense 3D shape and their landmarks with visibility labels for different datasets. From top to bottom, the results on AFLW-LPFA, IJB-A and 300W datasets are shown in two rows each. The green landmark are visible and the red landmarks show the estimated locations for invisible landmarks. Our model can fit to diverse poses, resolutions, and expressions.</p><p>"dense" supervision. To show that, we take all matched SIFT points in the 300W-LP dataset, find their corresponding vertices, and plot the log of the number of SIFT points on each of the 3D face vertex. As shown in <ref type="figure">Fig. 7</ref>, SPC utilizes SIFT points to cover the whole 3D shape and the points in the highly textured areas are substantially used. We can expect that these SIFT constraints will act like anchors to guild the learning of the model fitting process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose a large-pose face alignment method which estimates accurate 3D face shapes by utilizing a deep neural network. In addition to facial landmark fitting, we propose to align contours and the SIFT feature point pairs to extend the fitting beyond facial landmarks. Our method is able to leverage from utilizing multiple datasets with different land- <ref type="figure">Figure 7</ref>. The log plot of the number of matched SIFT points in the 300W-LP training set. It shows that the SIFT constraints cover the whole face, especially the highly textured area. mark markups and numbers of landmarks. We achieve the state-of-the-art performance on three challenging large pose datasets and competitive performance on hard medium pose datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Architecture of CNN in the proposed DeFA method. The structure of each ConvBlock is shown in yellow area in the left bottom corner. Each convolution layer and fully connected layer is followed with one batch normalization layer (BN) and one leaky ReLU layer. The output dimension of each convolution layer is shown in the bottom of each unit, such as conv1: 32, which means the output has 32 channels. pool: 2 denotes the pooling layer adopts a stride of 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>The illustration of the SIFT Matching process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2 = 1 j</head><label>21</label><figDesc>k) ? U c (:, j) L PrA(:, arg min k?ic PrA(:, k) ? U c (:, j) 2 ) ? U c (:, j) 2 . (10) Note that while i c depends on the current estimation of {m, p}, for simplicity i c is treated as constant when performing back-propagation w.r.t. {m, p}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Left: The effect of constraints in enhancing the accuracy on the AFLW-LPFA testing set. The NME-lp of each setting is shown in legend. Right: The influence of the SIFT pairing constraint (SPC) in improving the performance for selected 5, 000 pairs from IJB-A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The list of face datasets used for training and testing.</figDesc><table><row><cell>Database</cell><cell>Landmark</cell><cell>Pose</cell><cell>Images</cell></row><row><cell></cell><cell>T raining</cell><cell></cell><cell></cell></row><row><cell>300W [23]</cell><cell>68</cell><cell cols="2">Near-frontal 3, 148</cell></row><row><cell>300W-LP [41]</cell><cell>68</cell><cell cols="2">[?90 ? , 90 ? ] 96, 268</cell></row><row><cell>Caltech10k [2]</cell><cell>4</cell><cell cols="2">Near-frontal 10, 524</cell></row><row><cell>AFLW-LFPA [9]</cell><cell>21</cell><cell>[?90 ? , 90 ? ]</cell><cell>3, 901</cell></row><row><cell>COFW [6]</cell><cell>29</cell><cell cols="2">Near-frontal 1, 007</cell></row><row><cell></cell><cell>T esting</cell><cell></cell><cell></cell></row><row><cell>AFLW-LFPA [9]</cell><cell>34</cell><cell>[?90 ? , 90 ? ]</cell><cell>1, 299</cell></row><row><cell>AFLW2000-3D [41]</cell><cell>68</cell><cell>[?90 ? , 90 ? ]</cell><cell>2, 000</cell></row><row><cell>300W [23]</cell><cell>68</cell><cell>Near-frontal</cell><cell>689</cell></row><row><cell>IJB-A [12]</cell><cell>3</cell><cell cols="2">[?90 ? , 90 ? ] 25, 795</cell></row><row><cell>LFW [14]</cell><cell>0</cell><cell cols="2">Near-frontal 34, 356</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The list of datasets used in each training stage, and the employed constraints for each dataset: Parameter Constraint (PC); Landmark Fitting Constraint (LFC); SIFT Pairing Constraint (SPC); Contour Fitting Constraint (CFC).</figDesc><table><row><cell>Dataset</cell><cell cols="3">Stage 1 Stage 2 Stage 3</cell></row><row><cell>300W-LP [41]</cell><cell>PC</cell><cell>PC LFC</cell><cell>-</cell></row><row><cell>Caltech10k [2]</cell><cell>-</cell><cell>LFC</cell><cell>-</cell></row><row><cell>COFW [6]</cell><cell>-</cell><cell>LFC</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>LFC</cell></row><row><cell>AFLW-LFPA [9]</cell><cell>-</cell><cell>-</cell><cell>SPC</cell></row><row><cell></cell><cell></cell><cell></cell><cell>CFC</cell></row><row><cell></cell><cell></cell><cell></cell><cell>LFC</cell></row><row><cell>300W [23]</cell><cell>-</cell><cell>-</cell><cell>SPC</cell></row><row><cell></cell><cell></cell><cell></cell><cell>CFC</cell></row><row><cell cols="3">constraints that are used at each stage.</cell><cell></cell></row><row><cell cols="4">Implementation details: Our DeFA model is implemented</cell></row><row><cell>with MatConvNet</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>The benchmark comparison (NME-lp) on three large-pose face alignment datasets.</figDesc><table><row><cell></cell><cell>Baseline</cell><cell cols="6">CFSS [38] PIFA [9] CCL [40] 3DDFA [?] PAWF [10] Ours</cell></row><row><cell></cell><cell>AFLW-LFPA</cell><cell>6.75</cell><cell>6.52</cell><cell>5.81</cell><cell>-</cell><cell>4.72</cell><cell>3.86</cell></row><row><cell></cell><cell>AFLW2000-3D</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>5.42</cell><cell>-</cell><cell>4.50</cell></row><row><cell></cell><cell>IJB-A</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>6.76</cell><cell>6.03</cell></row><row><cell cols="4">Table 4. The benchmark comparison (NME-nf) on 300W dataset.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">The top two performances are in bold.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="3">Common set Challenging set Full set</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RCPR [6]</cell><cell>6.18</cell><cell>17.26</cell><cell>7.58</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SDM [30]</cell><cell>5.57</cell><cell>15.40</cell><cell>7.50</cell><cell></cell><cell></cell><cell></cell></row><row><cell>LBF [19]</cell><cell>4.95</cell><cell>11.98</cell><cell>6.32</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CFSS [38]</cell><cell>4.73</cell><cell>9.98</cell><cell>5.76</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RAR [28]</cell><cell>4.12</cell><cell>8.35</cell><cell>4.94</cell><cell></cell><cell></cell><cell></cell></row><row><cell>3DDFA [41]</cell><cell>6.15</cell><cell>10.59</cell><cell>7.01</cell><cell></cell><cell></cell><cell></cell></row><row><cell>3DDFA+SDM</cell><cell>5.53</cell><cell>9.56</cell><cell>6.31</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DeFA</cell><cell>5.37</cell><cell>9.38</cell><cell>6.10</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>The NME-lp when utilizing more datasets.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Training Stages</cell><cell></cell><cell cols="7">AFLW2000-3D AFLW-LFPA</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>stage1</cell><cell></cell><cell></cell><cell></cell><cell cols="2">6.23</cell><cell></cell><cell></cell><cell cols="2">5.24</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">stage1 + stage2</cell><cell></cell><cell></cell><cell cols="2">5.68</cell><cell></cell><cell></cell><cell cols="2">4.74</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">stage1 + stage3</cell><cell></cell><cell></cell><cell cols="2">4.85</cell><cell></cell><cell></cell><cell cols="2">4.15</cell></row><row><cell cols="7">stage1 + stage2 + stage3</cell><cell></cell><cell cols="2">4.50</cell><cell></cell><cell></cell><cell cols="2">3.86</cell></row><row><cell cols="2">100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Images (%)</cell><cell>50 60 70 80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Images (%)</cell><cell>50 60 70 80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Number of</cell><cell>10 20 30 40</cell><cell></cell><cell></cell><cell cols="3">LFC+CFC+SPC: 4.15 LFC+CFC: 4.37 LFC+SPC: 4.66 LFC: 4.76</cell><cell>of Number</cell><cell>40 10 20 30</cell><cell></cell><cell></cell><cell></cell><cell cols="2">W/ SPC: 6.16 W/O SPC: 6.52</cell></row><row><cell></cell><cell>0</cell><cell>0</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell><cell>0</cell><cell>0</cell><cell>4</cell><cell>8</cell><cell>12</cell><cell>16</cell><cell>20</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">NME-lp (%)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">NME-lp (%)</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Tr?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.04904</idno>
		<title level="m">Regressing robust and discriminative 3D morphable models with a very deep neural network</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pruning training sets for learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Abu-Mostafam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="494" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Localizing parts of faces using a consensus of exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2930" to="2940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A morphable model for the synthesis of 3d faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 26th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<publisher>ACM Press/Addison-Wesley Publishing Co</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="187" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Booth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ploumpis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Panagakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno>2017. 1</idno>
		<imprint/>
	</monogr>
	<note>3d face morphable models&quot; in-the-wild</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust face landmark estimation under occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">P</forename><surname>Burgos-Artizzu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Facewarehouse: A 3d facial expression database for visual computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="413" to="425" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multidirectional multi-level dual-cross patterns for robust face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="518" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pose-invariant 3d face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Large-pose face alignment via cnn-based dense 3d model fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="4188" to="4196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Pose-invariant face alignment via cnn-based dense 3d model fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pushing the frontiers of unconstrained face detection and recognition: Iarpa janus benchmark a</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Taborsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast and exact newton and bidirectional fitting of active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Labeled faces in the wild: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Face Detection and Facial Image Analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="189" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Joint face alignment and 3d face reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="545" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment with a deformable hough transform model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcdonagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="569" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Locating facial features with an extended active shape model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Milborrow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nicolls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="504" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A 3d face model for pose and illumination invariant face recognition. In Advanced video and signal based surveillance, 2009. AVSS&apos;09</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Paysan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Knothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Amberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="296" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Face alignment at 3000 fps via regressing local binary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1685" to="1692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Estimating 3d shape and texture using pixel intensity, edges, specular highlights, texture constraints and a prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="986" to="993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unconstrained 3d face reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Computer Vision and Pattern Recognition<address><addrLine>Bostan, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adaptive 3d face reconstruction from unconstrained photo collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>PP</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">300 faces in-the-wild challenge: The first facial landmark localization challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Statistical 3d face shape estimation from occluding contours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>S?nchez-Escobedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Castel?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">142</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="111" to="124" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Collaborative facial landmark localization for transferring annotations across datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="78" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gauss-newton deformable part models for face alignment in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1851" to="1858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Matconvnet -convolutional neural networks for matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the ACM Int. Conf. on Multimedia</title>
		<meeting>eeding of the ACM Int. Conf. on Multimedia</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Robust facial landmark detection via recurrent attentive-refinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kassim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1395" to="1403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Face alignment assisted by head pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gunes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference<address><addrLine>Swansea, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-07" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="130" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mirror, mirror on the wall, tell me, is the error small?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4685" to="4693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Leveraging datasets with varying annotations for face alignment via deep regression network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3801" to="3809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning deep representation for face alignment with auxiliary attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="918" to="930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fast and precise face alignment and 3d shape reconstruction from a single 2d image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Benitez-Quiroz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="590" to="603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Extensive facial landmark localization with coarse-to-fine convolutional network cascade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="386" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Method for real-time face animation based on single video camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">US Patent</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2016-06-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Face alignment by coarse-to-fine shape searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Transferring landmark annotations for cross-dataset face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0602</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unconstrained face alignment via cascaded compositional learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3409" to="3417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Face alignment across large poses: A 3d solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Computer Vision and Pattern Recognition<address><addrLine>Las Vegas, NV</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Highfidelity pose and expression normalization for face recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="787" to="796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Face detection, pose estimation, and landmark localization in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<title level="m">IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2879" to="2886" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
