<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep convolutional forest: a dynamic deep ensemble approach for spam detection in text</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mai</forename><forename type="middle">A</forename><surname>Shaaban</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Mathematics and Computer Science</orgName>
								<orgName type="department" key="dep2">Faculty of Science</orgName>
								<orgName type="institution">Alexandria University</orgName>
								<address>
									<settlement>Alexandria</settlement>
									<country key="EG">Egypt</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasser</forename><forename type="middle">F</forename><surname>Hassan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Computers and Data Science</orgName>
								<orgName type="institution">Alexandria University</orgName>
								<address>
									<settlement>Alexandria</settlement>
									<country key="EG">Egypt</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawkat</forename><forename type="middle">K</forename><surname>Guirguis</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute of Graduate Studies and Research</orgName>
								<orgName type="institution">Alexandria University</orgName>
								<address>
									<settlement>Alexandria</settlement>
									<country key="EG">Egypt</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep convolutional forest: a dynamic deep ensemble approach for spam detection in text</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Ensemble methods</term>
					<term>Deep learning</term>
					<term>Machine learning</term>
					<term>Spam classification</term>
					<term>Text messages</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The increase in people's use of mobile messaging services has led to the spread of social engineering attacks like phishing, considering that spam text is one of the main factors in the dissemination of phishing attacks to steal sensitive data such as credit cards and passwords. In addition, rumors and incorrect medical information regarding the COVID-19 pandemic are widely shared on social media leading to people's fear and confusion. Thus, filtering spam content is vital to reduce risks and threats. Previous studies relied on machine learning and deep learning approaches for spam classification, but these approaches have two limitations. Machine learning models require manual feature engineering, whereas deep neural networks require a high computational cost. This paper introduces a dynamic deep ensemble model for spam detection that adjusts its complexity and extracts features automatically. The proposed model utilizes convolutional and pooling layers for feature extraction along with base classifiers such as random forests and extremely randomized trees for classifying texts into spam or legitimate ones. Moreover, the model employs ensemble learning procedures like boosting and bagging. As a result, the model achieved high precision, recall, f1-score and accuracy of 98.38%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Mobile messaging service has become one of the most common means of communication among people since they allow individuals to communicate with one another at any time and from any location. Besides, there is a vast number of messaging apps that provide their service for free. According to statistics, 95% of mobile messages in the USA are read and responded to within three minutes of receiving <ref type="bibr" target="#b0">[1]</ref>. In addition, Short Message Service (SMS) offers businesses an enormous chance to communicate and interact with customers as 48% of consumers prefer direct communication from businesses via SMS <ref type="bibr" target="#b0">[1]</ref>. As a result, users are prone to SMS attacks such as spam and phishing, especially users who lack awareness about cyber threats.</p><p>Spam text is any undesired text transmitted to people without their permission and may include a link to a phone number to call, a link to open a website, or a link to download a file. Thus, an attacker can masquerade as a trusted entity and exploit spam texts by attaching malicious links so that victims may be duped into clicking a harmful link, resulting in installing malware or revealing sensitive information, including login credentials and credit card numbers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. For example, phishing attacks can occur by sending fake messages for users telling them to reset their passwords to be able to login to Facebook, Twitter, or any other platform <ref type="bibr" target="#b3">[4]</ref>. Besides, spammers can share misleading information about the COVID-19 pandemic causing a negative impact on society <ref type="bibr" target="#b4">[5]</ref>. Therefore, filtering spam texts is crucial to protect users against social engineering attacks, mobile malware and threats.</p><p>Previous studies in the area of spam classification focused on using machine learning algorithms <ref type="bibr" target="#b5">[6]</ref>, but these algorithms require prior knowledge and domain expertise for identifying useful features in order to achieve accurate classification <ref type="bibr" target="#b6">[7]</ref>. Furthermore, researchers proposed deep learning approaches to detect spam <ref type="bibr" target="#b4">[5]</ref>. However, deep neural networks require much effort in tuning hyper-parameters <ref type="bibr" target="#b7">[8]</ref>. Besides, they require massive data for training to predict new data accurately. Consequently, they require a high computational cost <ref type="bibr" target="#b7">[8]</ref>.</p><p>To overcome the high complexity of deep learning models and to reduce the effort spent in tuning hyper-parameters, Zhou and Feng <ref type="bibr" target="#b7">[8]</ref> developed multi-grained cascade forest (gcForest), a decision tree ensemble approach that can be applied to different classification tasks and has much fewer hyper-parameters than deep learning neural networks. Ensemble methods <ref type="bibr" target="#b8">[9]</ref> train multiple base models to produce a single best-fit predictive model. Kontschieder et al. <ref type="bibr" target="#b9">[10]</ref> demonstrated that employing ensemble approaches like random forests <ref type="bibr" target="#b10">[11]</ref> aided by deep learning model features can be more effective than solely using a deep neural network. gcForest applies multi-grained scanning for extracting features and employs a cascade structure (i.e., layer-by-layer processing of raw features) for learning. Inspired by gcForest, this paper enhances the procedure of feature engineering by replacing the multi-grained scanning with convolutional layers and pooling layers to capture high-level features from textual data. The motivation for using gcForest as a baseline for this paper is that gcForest is the first deep learning model that trains data without relying on neural networks and backpropagation, as the authors claimed <ref type="bibr" target="#b7">[8]</ref>.</p><p>This paper introduces a dynamic (self-adaptive) deep ensemble mechanism to overcome the stated limitations of machine learning and deep learning approaches for detecting spam texts. The main contributions of this paper are as follows:</p><p>-Implementing a dynamic deep ensemble model called Deep Convolutional Forest (DCF).</p><p>-Extracting features automatically by utilizing convolutional layers and pooling layers.</p><p>-Determining the model complexity automatically so that the model can perform accurately on both smallscale data and large-scale data.</p><p>-Classifying text into Spam or Ham (Not-Spam), achieving a remarkable accuracy.</p><p>The rest of this paper is arranged as follows: "Related work" provides the literature review, "Methodology" explains the word embedding technique, followed by the detailed explanation of deep convolutional forest (DCF), "Experimental results" shows the results of the proposed method along with results of traditional machine learning classifiers and existing deep learning methods, "Discussion" discusses the findings and explains why the proposed solution outperforms the existing solutions. Finally, "Conclusion" concludes the proposed work and contains recommendations for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related work</head><p>Over recent years, computer scientists have published a considerable volume of literature on spam detection <ref type="bibr" target="#b13">[12,</ref><ref type="bibr" target="#b15">13,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b4">5]</ref>; these works were limited to using machine learning and deep learning based models.</p><p>Bassiouni et al. <ref type="bibr" target="#b17">[14]</ref> experimented multiple classifiers to filter emails gathered from the Spambase UCI dataset, which contained 4601 instances. They performed data preprocessing, then they selected features using Infinite Latent Feature Selection (ILFS). Finally, they classified emails with an accuracy of 95.45% using Random Forest (RF), while the remaining classifiers: Artificial Neural Network (ANN), Logistic Regression, Support Vector Machine (SVM), Random Tree, K-Nearest Neighbors (KNN), Decision <ref type="table">Table (DT)</ref>, Bayes Net, Naive Bayes (NB) and Radial Basis Function (RBF) scored 92.4%, 92.4%, 91.8%, 91.5%, 90.7%, 90.3%, 89.8%, 89.8% * and 82.6%, respectively.</p><p>Merugu et al. <ref type="bibr" target="#b18">[15]</ref> classified text messages into Spam and Ham category with an accuracy rate of 97.6% using Naive Bayes, which proved to outperform other machine learning algorithms such as Random Forest, Support Vector Machine and K-Nearest Neighbors according to the experimental results. The messages were collected from the UCI repository, which contained 5574 variable-length messages. To feed data into a classification model, the authors converted messages into fixed-length numerical vectors by creating term frequency-inverse document frequency (TF-IDF) <ref type="bibr" target="#b19">[16]</ref> vectors using the bag of words (BoW) model.</p><p>In 2020, Gaurav et al. <ref type="bibr" target="#b20">[17]</ref> proposed spam mail detection (SPMD) method based on the document labeling concept, which sorts the new messages into two categories: Ham and Spam. Experimental results illustrated that Random Forest produced the highest accuracy of 92.97% among the three classification models: Naive Bayes, Decision Tree and Random Forest.</p><p>Lately, researchers have proposed deep learning methods such as Convolutional Neural Network (CNN) <ref type="bibr" target="#b21">[18]</ref> and Short-Term Memory (LSTM) <ref type="bibr" target="#b22">[19,</ref><ref type="bibr" target="#b23">20]</ref> for categorizing Spam and Ham messages. Popovac et al. <ref type="bibr" target="#b21">[18]</ref> applied a CNNbased architecture after performing data preprocessing steps including tokenization, stemming, preservation of sentiment of text and removal of stop words. The feature extraction process involved transforming a text into a matrix of TF-IDF <ref type="bibr" target="#b19">[16]</ref> features. According to their experiment, CNN proved to be effective more than machine learning algorithms with an accuracy score of 98.4%. Another spam filtering model was proposed by <ref type="bibr" target="#b22">[19]</ref>; they combined an N-gram TF-IDF feature selection, modified distribution-based balancing algorithm and a regularized deep multi-layer perceptron neural network model with rectified linear units (DBB-RDNN-ReL). Although their model was computationally intensive, the model provided an accuracy of 98.51%.</p><p>Jain et al. <ref type="bibr" target="#b23">[20]</ref> used sequential stacked CNN-LSTM model (SSCL) to classify SMS spam with an accuracy of 99.01%. They converted each text into semantic word vectors with the help of Word2vec, WordNet and ConceptNet. However, searching for the word vectors in these embeddings caused system overload.</p><p>Ghourabi et al. <ref type="bibr" target="#b24">[21]</ref> presented a hybrid model for classifying text messages written in Arabic or English that is based on the combination of two deep neural network models: CNN and LSTM. The results indicated that the CNN-LSTM model scored an accuracy of 98.37%, which is higher than other techniques like Support Vector Machine, K-Nearest Neighbors, Multinomial Naive Bayes, Decision Tree, Logistic Regression, Random Forest, AdaBoost, Bagging classifier and Extra Trees.</p><p>In 2020, Roy et al. <ref type="bibr" target="#b6">[7]</ref> focused on how to effectively filter Spam and Not-Spam text message downloaded from the UCI Repository <ref type="bibr" target="#b25">[22]</ref>, which contains 5574 instances. They tested deep learning algorithms such as Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) as well as machine learning classifiers such as Naive Bayes (NB), Random Forest (RF), Gradient Boosting (GB), Logistic Regression (LR) and Stochastic Gradient Descent (SGD). The experimental results confirmed that applying CNN with three convolutional layers and a regularization parameter (dropout) on randomly sampled 10-fold cross validation data resulted in an accuracy of 99.44%. However, the authors spent much effort in tuning hyper-parameters.</p><p>As mentioned in "Introduction", this study aims to handle feature relationships in textual data by using convolutional layers together with pooling layers as alternatives to the multi-grained scanning procedure proposed by <ref type="bibr" target="#b7">[8]</ref>. Zhou and Feng <ref type="bibr" target="#b7">[8]</ref> proposed an ensemble of ensembles mechanism, meaning that each level learns from its previous level and each level has an ensemble of decision-tree forests. Descriptions of existing text-based spam detection techniques with respect to datasets, feature extraction and selection methods, types of algorithms, and performance measure are covered in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology</head><p>Data pass through two main phases as shown in <ref type="figure">Figure 1</ref>: the first phase is applying the word embedding technique after preprocessing to convert textual data into a numerical form and the second phase is using deep convolutional forest (DCF) to extract features and classify text as illustrated in <ref type="figure">Figure 2</ref>. The proposed method analyses the SMS spam dataset, which is publicly available <ref type="bibr" target="#b25">[22]</ref>. The dataset has a collection of messages where each message is either 1 (Spam) or 0 (Not-Spam). First of all, text messages were prepared by splitting each message into a list of words and then performing text preprocessing techniques like stemming and stop words removal. Afterward, each word was transformed into a sequence of numbers called a word vector; the word vector is generated using a word embedding technique as explained in "Word embedding". Finally, the generated word vectors are sent as a word matrix to DCF for classification and determining whether a message is Spam or Not-Spam as explained in "Deep convolutional forest".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word embedding</head><p>Word embedding is a technique where each word is represented by a vector holding numbers indicating the semantic similarity to other words in text corpus (i.e., similar words have similar representations). The difference between the word embedding and the one-hot encoding method is that the one-hot encoding method splits each text into a group of words and turns each word into a sequence of numbers disregarding the word meaning within context <ref type="bibr" target="#b26">[23]</ref>, unlike the word embedding technique that transforms each word into a dense vector called a word vector that captures its relative meaning within the document <ref type="bibr" target="#b27">[24]</ref> using GloVe algorithm <ref type="bibr" target="#b28">[25]</ref> implemented by the embedding layer <ref type="bibr" target="#b29">[26]</ref>.</p><p>In word embedding technique, every message is a sequence of words: 1 , 2 , 3 , . . . , ; each word is presented as a word vector of length . After that, all word vectors of a given message (i.e., word vectors) are concatenated to form a word matrix M ? R ? . Finally, DCF receives the word matrix through the input layer and performs the convolution operation to produce feature maps through the convolutional layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep convolutional forest</head><p>The deep convolutional forest (DCF) model employs a cascade approach inspired by the structure of deep neural networks and deep forest <ref type="bibr" target="#b7">[8]</ref> as shown in <ref type="figure">Figure 2</ref>. Each level receives processed feature information from its prior level and outputs its processing outcome to the posterior level. The output of each level is the probabilities of both classes: Spam and Not-Spam; these probabilities are then concatenated with the feature maps to form the input of the next level. The model predicts the class of a given message by taking the average of the probabilities of Spam and the probabilities of Not-Spam separately from the last level output, and then it takes the maximum average as a final prediction result.</p><p>The model accuracy is the main factor that determines the number of levels. Whenever the accuracy of validation data increases, DCF continues to generate new levels and stops when there is no significant improvement in the accuracy score, unlike deep neural networks where the number of hidden layers is a pre-defined parameter. As a result, DCF is applicable to different scales of datasets, not limited to large-scale ones, as it automatically adjusts its complexity by terminating the training process when adequate.</p><p>The core units of each level in DCF are the convolutional layer, the pooling layer and the classification layer, which contains four base classifiers: two random forests <ref type="bibr" target="#b10">[11]</ref> and two extremely randomized trees <ref type="bibr" target="#b31">[27]</ref>. The convolutional layer is responsible for the feature extraction task, whereas the pooling layer helps reduce overfitting in the proposed model. Moreover, the classification layer predicts the probabilities of Spam and Not-Spam for a given message.</p><p>DCF combines the advantages of two techniques: bagging and boosting, they work interchangeably for decreasing variance and bias <ref type="bibr" target="#b33">[28,</ref><ref type="bibr" target="#b34">29]</ref>. Bagging refers to a group of weak learners that learns from each other independently in parallel and combines the outcomes to determine the model average <ref type="bibr" target="#b33">[28]</ref>, whereas boosting is a group of weak learners that learns from each other in series where the next learner tries to improve the results of the previous learner <ref type="bibr" target="#b33">[28]</ref>. DCF represents bagging through combining outputs from each forest in the classification layer as well as using Random Forest as a base classifier, which combines predictions from each decision tree and outputs the model average. Moreover, DCF supports boosting since it keeps adding levels where the next level tries to correct errors present in the previous level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convolution operation</head><p>The convolutional layer extracts hidden features from the textual data by performing the convolution operation on a word matrix and applying the Rectified Linear Unit (ReLU) activation function <ref type="bibr" target="#b35">[30]</ref> on the output. Let M ? R ? be the input word matrix having words and -dimensional word vector, a filter F ? R ? slides over the input, resulting in a feature vector O of dimension ? + 1, also known as a feature map, where is the region size of the kernel. The process of finding the feature vector assuming that = 2 is shown in <ref type="bibr" target="#b0">(1)</ref>. <ref type="bibr">11 12</ref> . . . . . .  . . . . . .</p><formula xml:id="formula_0">Let = ? ? ? ? ? ? ? ? ?</formula><formula xml:id="formula_1">2 . . . . . . . . . . . . 1 2 . . . ? ? ? ? ? ? ? ? ? ,</formula><formula xml:id="formula_2">1 2 ? ? ? ? ? ? ? ? ? . Then = ? ? ? ? ? ? ? ? ? ? 1 2 . . . ?2+1 ? ? ? ? ? ? ? ? ?<label>(1)</label></formula><p>Where is the convolution operator, which results in a feature vector O of length ( ? 2 + 1) in which each feature is calculated as follows: <ref type="bibr" target="#b0">1</ref>  </p><formula xml:id="formula_3">= ( ?1)1 11 + ( ?1)2 12 +. . .+ ( ?1) 1 + 1 21 + 2 22 + . . . + 2</formula><p>The feature vector O passes through the Rectified Linear Unit (ReLU) activation function. As shown in (2), ReLU takes each value and returns if the value is positive; otherwise, it returns zero, meaning that ReLU finds the maximum value between and zero. This value is noted by?.</p><p>The output of the convolutional layer after applying several filters becomes a set of feature maps as each filter produces one feature map? of length ( ?2+1) having positive values only.?=</p><formula xml:id="formula_4">(0, )<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pooling operation</head><p>The pooling layer in DCF applies the pooling operation for downsampling feature maps to avoid overfitting <ref type="bibr" target="#b36">[31]</ref>. Pool-ing is a process that aggregates the output of each filter by pulling a small set of features out of large sets to knock down the amount of computation required for processing the next level. Hence, pooling should reduce overfitting, which arises from high model complexity and causes misclassification of unseen data as the model learns the noise in textual data <ref type="bibr" target="#b23">[20,</ref><ref type="bibr" target="#b24">21,</ref><ref type="bibr" target="#b26">23]</ref>. In addition, DCF supports the early stopping procedure, meaning that the model stops training when the performance starts to degrade. Pooling has three common variations <ref type="bibr" target="#b6">[7]</ref>; max-pooling is the one that yielded better results than min-pooling and average pooling. The max-pooling operation highlights the most present feature in each feature map by calculating the maximum value. The pooling size was configured to be equal to the input size, so the output vector O = [ 1 , 2 , . . . , ] has features, where each element is the maximum value of each feature map and can be calculated by (3).</p><formula xml:id="formula_5">= (?)<label>(3)</label></formula><p>In the end, the features extracted using the convolution and the pooling layer (i.e., O) pass through the classification layer. Algorithm 1 provides the detailed steps of the feature extraction process, considering that the number of features is equal to the number of filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Feature extraction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input:</head><p>? R ? Output:</p><p>of length (number of features)  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random forests and extremely randomized trees</head><p>The base classifiers in the proposed model are random forests <ref type="bibr" target="#b10">[11]</ref> and extremely randomized trees <ref type="bibr" target="#b31">[27]</ref>, which rely on the decision tree ensemble approach. Ensemble methods improve prediction results by combining multiple classifiers for the same task <ref type="bibr" target="#b8">[9]</ref>. Each level in DCF involves different forest types to support diversity; diversity is crucial in constructing ensemble methods <ref type="bibr" target="#b8">[9]</ref>. Both forest types: random forests <ref type="bibr" target="#b10">[11]</ref> and extremely randomized trees <ref type="bibr" target="#b31">[27]</ref>, consist of a vast number of decision trees, where the prediction of every tree participates in the final decision of a forest by taking the majority vote. Furthermore, the growing tree procedure is the same in both techniques as they select a subset of features randomly, but they have two exceptions, as explained below.</p><p>Random forests build a decision tree by subsampling the input and selecting a subset of features randomly. Then, choosing the optimum feature for the split at each tree node according to the one with the best Gini value <ref type="bibr" target="#b34">[29]</ref>, whereas extremely randomized trees manipulate the whole input and choose a random feature for the split at each node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Deep Convolutional Forest</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input:</head><p>? R ? Output: (the message label: 0 or 1) 1:</p><formula xml:id="formula_6">? E F ( ) 2: = ? 3: repeat 4: ? = ?, = ? 5: for = 1 to 4 do 6: ? , ? F ( , ) 7: ? ? C ( ? , ? ) 8: ? C ( ,<label>) 9:</label></formula><p>end for <ref type="bibr" target="#b9">10</ref>:</p><formula xml:id="formula_7">? C ( ? ,<label>) 11:</label></formula><p>? U F ( ) <ref type="bibr">12:</ref> until no significant improvement in performance <ref type="bibr" target="#b15">13</ref> ? M (?) <ref type="bibr">26:</ref> return <ref type="bibr" target="#b31">27</ref>: end function</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Final prediction</head><p>As shown in <ref type="figure">Figure 2</ref>, the DCF first level extracts features from a text through the convolutional layer and the maxpooling layer. Next, each forest in the first level classification layer takes the extracted features and outputs two probabilities: the probability of a given message being Spam and being Not-Spam. After that, DCF computes the accuracy of the first level to be compared with the new accuracy of the second level. The second level in DCF produces new features, which are then concatenated with the probabilities generated by the first level to form the input to the second level classification layer, which outputs new probabilities (predictions). A new accuracy is calculated and compared with the previous accuracy so that DCF will continue to generate levels until it finds no significant improvement in accuracy or reaches the maximum number of levels. Each level has a convolutional layer, a max-pooling layer and a classification layer consisting of four forests: two random forests <ref type="bibr" target="#b10">[11]</ref> and two extremely randomized trees <ref type="bibr" target="#b31">[27]</ref>. Hence, each level in DCF outputs eight probabilities (i.e., four probabilities for each class). The last level in DCF takes the average of probabilities for Spam and the average of probabilities for Not-Spam; the higher average value will be the final prediction. Algorithm 2 provides the full implementation of DCF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental results</head><p>This section analyzes the model performance in detecting Spam messages gathered from the UCI repository <ref type="bibr" target="#b25">[22]</ref> and compares the results with multi-grained cascade forest (gc-Forest) and the traditional machine learning classifiers as well as the existing deep learning techniques. As shown in <ref type="table" target="#tab_4">Table 1</ref>, the number of spam instances is extremely lower than the number of legitimate ones. Therefore, balancing the class distribution is necessary to obtain accurate results. Initially, the dataset was split into two subsets: 80% of the messages are for training and the remaining 20% are for testing and validation. Then, the SMOTE <ref type="bibr" target="#b37">[32]</ref> over-sampling technique was applied for balancing data before feeding it into the classifier. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Configuration</head><p>The proposed approach was implemented with Python 3.7 along with TensorFlow [33], Keras <ref type="bibr" target="#b39">[34]</ref> and Scikit-learn <ref type="bibr" target="#b40">[35]</ref>. The embedding layer in Keras converted textual data into word vectors using GloVe word embeddings, which contains pre-trained 100-dimensional word vectors. The convolutional layer yielded the most promising performance by using 64 filters for applying the convolution operation on each input, where each filter (kernel) is a two-dimensional array of weights that moves one unit at a time (i.e., stride is set to 1). Although many studies used a different number of filters in each convolutional layer in convolutional neural networks <ref type="bibr" target="#b4">[5]</ref>, DCF uses the same number of filters as there is no significant change in performance and to facilitate the process of tuning hyper-parameters. The experiment showed that max-pooling with a pool size equals the size of the input was better than min-pooling and average-pooling. Moreover, each forest in the classification layer contained 100 trees. However, using more trees failed to increase the accuracy. All the remaining parameters of the base classifiers were set to default. Consequently, the proposed model predicted Spam messages with an accuracy equals 98.38% after generating two levels only. <ref type="table" target="#tab_5">Table 2</ref> summarizes the configuration setup of DCF. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation metrics</head><p>As discussed in "Deep convolutional forest", the accuracy score is the main factor in determining the number of DCF levels. After each level, DCF estimates the performance on the validation set until it finds no significant gain in performance. The experiment showed that two levels were enough to classify Spam messages. As a result, the training procedure was terminated and the model was evaluated on the test set based on the following well-known classification metrics: -F1-score: is a weighted average of precision and recall.</p><formula xml:id="formula_8">-</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1-</head><formula xml:id="formula_9">= 2 ? ? +<label>(6)</label></formula><p>-Accuracy: compares the set of predicted labels to the corresponding set of actual labels.</p><formula xml:id="formula_10">= + + + +<label>(7)</label></formula><p>-Receiver Operating Characteristic (ROC) curve: plots True Positive Rate (TPR) on y-axis as defined in <ref type="bibr" target="#b7">(8)</ref> and False Positive Rate (FPR) on x-axis as defined in <ref type="bibr" target="#b8">(9)</ref>. The area under the ROC curve (AUC) measures the model performance; the higher the AUC value, the better model. = + (8)</p><formula xml:id="formula_11">= +<label>(9)</label></formula><p>According to the confusion matrix described in <ref type="table" target="#tab_6">Table 3</ref>, the proposed algorithm identified Spam messages with low false-negative and false-positive rates. Moreover, from the data in <ref type="table" target="#tab_7">Table 4</ref>, it can be seen that DCF performed well on the test set, resulting in high precision, recall, f1-score, accuracy, and AUC score. The goal of constructing ensemble models is to minimize the generalization error. As long as the individual learners are diverse and independent, the prediction error of the ensemble model decreases <ref type="bibr" target="#b42">[36]</ref>. DCF encourages diversity by employing different structures of forests as base classifiers. <ref type="table" target="#tab_8">Table 5</ref> shows that the results of DCF having four forests of the same type (i.e., four random forests) are indeed worse than having four forests with diverse building strategies as shown in <ref type="table" target="#tab_7">Table 4</ref>. Hence, the diversity affects the performance of detecting Spam messages.  The cross-entropy loss detects if the model suffers from overfitting as computed in <ref type="bibr" target="#b9">(10)</ref>, where ? {0,1} is the true label of a single sample and is the predicted probability. In a good fit model, the loss should keep decreasing until reaching a point of stability whenever the number of levels is increasing. When using pooling layers during the experiment, the cross-entropy loss decreased from 0.101 to 0.084, while removing pooling layers caused an increase in the loss from 0.131 to 0.182. Upon further analysis, adding pooling layers after convolutional layers enhances the learning performance.</p><formula xml:id="formula_12">log ( , ) = ?( log( ) + (1 ? ) log(1 ? ))<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Machine learning classifiers</head><p>To apply machine learning classifiers: Support Vector Machine (SVM), Naive Bayes (NB), K-Nearest Neighbors (KNN) and Random Forest (RF), text preprocessing techniques such as tokenization, removal of stop words and stemming were applied for extracting features manually from the SMS spam dataset. <ref type="table" target="#tab_10">Table 6</ref> presents 10 features that were extracted after data preprocessing. As indicated in <ref type="table" target="#tab_11">Table  7</ref>, DCF outperformed other classifiers in categorizing Spam and Not-Spam messages in terms of precision, recall, f1score and accuracy. According to the ROC curve in <ref type="figure" target="#fig_4">Figure   3</ref>, the AUC score of the proposed model is significantly higher than the other classifiers, considering that the hyperparameters of the mentioned machine learning algorithms were set to default during the experiment.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep learning techniques</head><p>Convolutional neural networks (CNN) and long short-term memory (LSTM) were implemented to compare their results with DCF. The number of convolutional layers affects the performance of CNN <ref type="bibr" target="#b6">[7]</ref>. Hence, three models of CNN were applied: the first model has one convolutional layer (1-CNN), the second model has two convolutional layers (2-CNN) and the third model has three convolutional layers (3-CNN). All of the mentioned deep learning models start with an embedding layer to generate 100-dimensional word vectors using GloVe; these word vectors are then used as inputs to the convolutional layer or the LSTM layer to produce feature maps. The convolutional layer in CNN has 64 filters of size 2 to match the configuration of DCF, and the number of units in LSTM was also set to 64. The models used the ReLU activation function as well as applying the Adam optimizer to reduce the error rate, in addition to adding a max-pooling layer in CNN models to avoid overfitting. Finally, the output layer used the Softmax activation function as defined in <ref type="bibr" target="#b10">(11)</ref> to find the final decision, where is the input and = 2 is the number of classes (i.e., Spam and Not-Spam). <ref type="table" target="#tab_12">Table 8</ref> indicates that the proposed model realized the best performance with respect to precision, recall, f1-score and accuracy. In addition, DCF and 1-CNN achieved an equal AUC score, as indicated in <ref type="figure" target="#fig_2">Figure 4</ref>.</p><formula xml:id="formula_13">( ) = =1 = 1, 2, . . . ,<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-grained cascade forest</head><p>The gcForest model proposed in <ref type="bibr" target="#b7">[8]</ref> depends on the multigrained scanning procedure for feature extraction. However, this procedure is not capable of manipulating textual  <ref type="bibr" target="#b43">[37]</ref> number misspelled count number of misspelled words 0 or more emails count number of emails 0 or more phones count number of phone numbers 0 or more is currency found indicates the existence of currency symbol 0 or 1 IP address count number of IP addresses 0 or more urls count number of URLs 0 or more has blacklist url indicates the existence of blacklisted URL 0 or 1   data. Therefore, in order to evaluate the gcForest model on the SMS spam dataset, the messages were represented by GloVe word embeddings before feeding to the model. Considering that the word embedding method conveys semantic relationships, unlike the TF-IDF method <ref type="bibr" target="#b44">[38]</ref>. <ref type="table" target="#tab_13">Table 9</ref> signifies that DCF achieved better performance than gcForest. Nevertheless, TF-IDF features led to poor performance on DCF as the accuracy decreased to 75% compared with word embeddings. The introduced model extracted hidden features from data with the help of convolutional layers and pooling layers, unlike machine learning classifiers that require manual feature extraction from textual data, which requires domain knowledge. Furthermore, deep learning methods have fixed complexity, which means that they perform inefficiently on small-scale data. On the other hand, the proposed model can set the complexity automatically as the number of levels is determined according to the rate of accuracy increase, which means that it can perform efficiently on both small-scale data and large-scale data.</p><p>The main gaps in literature, which are addressed by the proposed algorithm, are stated as follows:</p><p>-No domain expertise is required to carry out the classification process.</p><p>-Dynamic increase in the model complexity in proportion to the increase in performance.</p><p>To sum up, the model developed in this paper can separate legitimate text messages from fraudulent ones with high accuracy and low complexity. This filtering process will reduce the possibility of stealing people's sensitive data and will ensure that the users will be able to focus on messages from multiple industry sectors, which will help companies grow their businesses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>This paper presents a dynamic deep ensemble model for categorizing text messages into Spam and Not-Spam. The model starts from passing the word embeddings through convolutional and pooling layers to dispense with manual feature extraction. Then, the model sends the feature maps to the classification layers where the base classifiers: two random forests and two extremely randomized trees carry out the predictions. Adopting ensemble techniques like boosting and bagging in constructing the model accomplished more accurate outcomes than single classifiers. Ensemble procedure is implemented by processing the input in a level-by-level manner until reaching the last level in which the average of class probabilities is calculated to take the highest average value representing the predicted label. This procedure facilitates the adjusting of the model complexity, unlike deep learning where the model complexity is determined in advance. As confirmed by the experimental findings, the proposed model surpassed the traditional machine learning classifiers as well as the existing deep neural networks in terms of precision, recall and f1-score in addition to achieving the highest accuracy rate of 98.38%. Overall, the suggested solution in this paper can significantly minimize the risks related to security attacks such as SMS phishing by filtering spam messages. Future work may include the detection of spam content written in different languages other than English. Furthermore, a slight change in the model architecture may be considered for classifying messages that involve images. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Workflow of data: text preprocessing, word embedding, feature extraction and classification. Structure of deep convolutional forest (DCF).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>? 4 :</head><label>4</label><figDesc>?? R LU( )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>True Positives ( ): number of positive (Spam) messages that are correctly predicted as positive (Spam). -True Negatives ( ): number of negative (Not-Spam) messages that are correctly predicted as negative (Not-Spam). -False Positives ( ): number of negative (Not-Spam) messages that are predicted as positive (Spam). -False Negatives ( ): number of positive (Spam) messages that are predicted as negative (Not-Spam).-Precision: determines the ability not to label a negative (Not-Spam) message as positive (Spam).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>ROC curve analysis of machine learning classifiers and the proposed model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>ROC curve analysis of deep learning techniques and the proposed model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the SMS spam dataset.</figDesc><table><row><cell cols="3">Label Number of instances Class distribution (%)</cell></row><row><cell>Ham</cell><cell>4825</cell><cell>86.59</cell></row><row><cell cols="2">Spam 747</cell><cell>13.41</cell></row><row><cell>Total</cell><cell>5572</cell><cell>100</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Configuration setup of DCF.</figDesc><table><row><cell>Parameter</cell><cell cols="2">Best value Location</cell></row><row><cell>Number of filters</cell><cell>64</cell><cell>Convolutional layers</cell></row><row><cell>Kernel size</cell><cell>2</cell><cell>Convolutional layers</cell></row><row><cell>Stride</cell><cell>1</cell><cell>Convolutional layers</cell></row><row><cell>Number of trees</cell><cell>100</cell><cell>Base classifiers</cell></row><row><cell cols="2">Number of generated levels 2</cell><cell>DCF</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>The confusion matrix of DCF.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Actual%</cell></row><row><cell></cell><cell></cell><cell cols="2">Spam Not-Spam</cell></row><row><cell>Predicted%</cell><cell>Spam Not-Spam</cell><cell>83.64 0.18</cell><cell>1.44 14.74</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Classification results of DCF with two levels and diverse forest types.</figDesc><table><row><cell cols="4">Label Precision Recall F1-score Accuracy AUC</cell></row><row><cell>Spam 0.9880 Ham 0.9831</cell><cell>0.9111 0.9480 0.9979 0.9904</cell><cell>98.38%</cell><cell>0.989</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Classification results of DCF with two levels and the same type of forests.</figDesc><table><row><cell cols="4">Label Precision Recall F1-score Accuracy AUC</cell></row><row><cell>Spam 0.9939 Ham 0.9811</cell><cell>0.9000 0.9446 0.9989 0.9899</cell><cell>98.29%</cell><cell>0.955</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>The extracted features that are used in traditional machine learning classifiers Kincaid readability test score, which indicates how difficult a text to understand</figDesc><table><row><cell>Feature</cell><cell>Description</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Comparison of the results obtained by traditional machine learning classifiers and the proposed model.</figDesc><table><row><cell cols="2">Model Label</cell><cell cols="3">Precision Recall F1-score Accuracy</cell></row><row><cell>SVM</cell><cell cols="2">Spam Not-Spam 0.9786 0.5142</cell><cell>0.9056 0.6559 0.8349 0.9011</cell><cell>84.64%</cell></row><row><cell>NB</cell><cell cols="2">Spam Not-Spam 0.9612 0.8938</cell><cell>0.7944 0.8412 0.9818 0.9714</cell><cell>95.15%</cell></row><row><cell>KNN</cell><cell cols="2">Spam Not-Spam 0.9620 0.6000</cell><cell>0.8167 0.6918 0.8950 0.9273</cell><cell>88.23%</cell></row><row><cell>RF</cell><cell cols="2">Spam Not-Spam 0.9644 0.9299</cell><cell>0.8111 0.8665 0.9882 0.9762</cell><cell>95.96%</cell></row><row><cell>DCF</cell><cell cols="2">Spam Not-Spam 0.9831 0.9880</cell><cell>0.9111 0.9480 0.9979 0.9904</cell><cell>98.38%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Comparison of the results obtained by deep learning techniques and the proposed model.</figDesc><table><row><cell cols="4">Model Label</cell><cell></cell><cell cols="5">Precision Recall F1-score Accuracy</cell></row><row><cell cols="2">1-CNN</cell><cell cols="5">Spam Not-Spam 0.9820 0.9588</cell><cell cols="3">0.9056 0.9314 0.9925 0.9872</cell><cell>97.84%</cell></row><row><cell cols="2">2-CNN</cell><cell cols="5">Spam Not-Spam 0.9788 0.9412</cell><cell cols="3">0.8889 0.9143 0.9893 0.9840</cell><cell>97.30%</cell></row><row><cell cols="2">3-CNN</cell><cell cols="5">Spam Not-Spam 0.9817 0.8907</cell><cell cols="3">0.9056 0.8981 0.9786 0.9801</cell><cell>96.68%</cell></row><row><cell cols="2">LSTM</cell><cell cols="5">Spam Not-Spam 0.9819 0.9477</cell><cell cols="3">0.9056 0.9261 0.9904 0.9861</cell><cell>97.66%</cell></row><row><cell cols="2">DCF</cell><cell cols="5">Spam Not-Spam 0.9831 0.9880</cell><cell cols="3">0.9111 0.9480 0.9979 0.9904</cell><cell>98.38%</cell></row><row><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>True Positive Rate</cell><cell>0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">1-CNN, AUC=0.989 2-CNN, AUC=0.987 3-CNN, AUC=0.985 LSTM, AUC=0.982 DCF, AUC=0.989</cell></row><row><cell></cell><cell></cell><cell>0.0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell cols="2">0.4 False Positive Rate 0.5 0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>Comparison of the results obtained by multigrained cascade forest (gcForest) and the proposed model.</figDesc><table><row><cell>Model</cell><cell>Label</cell><cell cols="4">Precision Recall F1-score Accuracy AUC</cell></row><row><cell>gcForest</cell><cell cols="2">Spam Not-Spam 0.9589 1.0000</cell><cell>0.7778 0.8750 1.0000 0.9790</cell><cell>96.40%</cell><cell>0.983</cell></row><row><cell>DCF</cell><cell cols="2">Spam Not-Spam 0.9831 0.9880</cell><cell>0.9111 0.9480 0.9979 0.9904</cell><cell>98.38%</cell><cell>0.989</cell></row><row><cell cols="2">Discussion</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">This paper introduced a dynamic (self-adaptive) deep en-</cell></row><row><cell cols="6">semble technique to classify Spam and Not-Spam messages</cell></row><row><cell cols="6">with remarkable classification results compared to the meth-</cell></row><row><cell cols="6">ods described in literature. The model suggested in this</cell></row><row><cell cols="6">paper outperformed machine learning algorithms as well as</cell></row><row><cell cols="6">deep learning models since ensemble learning connects the</cell></row><row><cell cols="6">decisions from individual learners to improve the final de-</cell></row><row><cell cols="6">cision. Moreover, DCF exceeded the outcomes of gcForest</cell></row><row><cell cols="6">[8], since DCF carries the high-level features of textual data</cell></row><row><cell cols="4">and maintains the semantic relationships.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Appendix A Existing text-based spam detection techniques</head><label></label><figDesc></figDesc><table><row><cell>Model</cell><cell>Dataset(s) used</cell><cell>Feature extrac-</cell><cell>Algorithm(s)</cell><cell>Performance</cell></row><row><cell></cell><cell></cell><cell>tion/selection</cell><cell>used</cell><cell>measure</cell></row><row><cell>[14]</cell><cell>Spambase UCI</cell><cell>ILFS</cell><cell>RF with 100</cell><cell>The best accuracy</cell></row><row><cell></cell><cell></cell><cell></cell><cell>trees, ANN,</cell><cell>= 95.45% for RF</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Logistic, SVM,</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Random Tree,</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>KNN, DT, Bayes</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Net, NB and RBF</cell><cell></cell></row><row><cell>[15]</cell><cell>SMS spam</cell><cell>TF-IDF</cell><cell>NB, KNN, RF,</cell><cell>The best accuracy</cell></row><row><cell></cell><cell></cell><cell></cell><cell>SVM</cell><cell>is close to 98%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>for NB, RF, SVM</cell></row><row><cell>[17]</cell><cell>Enron,</cell><cell>TF-IDF</cell><cell>SPMD with NB,</cell><cell>The highest</cell></row><row><cell></cell><cell>Ling-Spam, and</cell><cell></cell><cell>DT and RF</cell><cell>accuracy =</cell></row><row><cell></cell><cell>PU</cell><cell></cell><cell></cell><cell>92.56% for RF in</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Ling-Spam</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>dataset</cell></row><row><cell>[18]</cell><cell>SMS spam</cell><cell>TF-IDF</cell><cell>CNN</cell><cell>98.4%</cell></row><row><cell>[19]</cell><cell>Enron,</cell><cell>N-gram TF-IDF</cell><cell cols="2">DBB-RDNN-ReL The highest</cell></row><row><cell></cell><cell>SpamAssassin,</cell><cell></cell><cell></cell><cell>accuracy =</cell></row><row><cell></cell><cell>SMS, and Social</cell><cell></cell><cell></cell><cell>99.79% in</cell></row><row><cell></cell><cell>networking</cell><cell></cell><cell></cell><cell>SpamAssassin</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>dataset</cell></row><row><cell>[20]</cell><cell>SMS spam and</cell><cell>Word2Vec,</cell><cell>SSCL</cell><cell>The highest</cell></row><row><cell></cell><cell>Twitter</cell><cell>WordNet and</cell><cell></cell><cell>accuracy =</cell></row><row><cell></cell><cell></cell><cell>ConceptNet</cell><cell></cell><cell>99.01% in SMS</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>spam dataset</cell></row><row><cell>[21]</cell><cell>SMS spam and</cell><cell>Word2Vec and</cell><cell>CNN-LSTM</cell><cell>The highest</cell></row><row><cell></cell><cell>Arabic messages</cell><cell>GloVe</cell><cell></cell><cell>accuracy =</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>98.37% in SMS</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>spam</cell></row><row><cell>[7]</cell><cell>SMS spam</cell><cell cols="2">Word embeddings CNN and LSTM</cell><cell>99.44%</cell></row><row><cell>[39]</cell><cell>SMS spam</cell><cell>Word weighting</cell><cell>Hidden Markov</cell><cell>96.9%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Model (HMM)</cell><cell></cell></row><row><cell>[40]</cell><cell>SMS spam and</cell><cell>TF-IDF and</cell><cell>Spam</cell><cell>The highest</cell></row><row><cell></cell><cell>Twitter</cell><cell>GloVe</cell><cell>Transformer</cell><cell>accuracy =</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>98.92% in SMS</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>spam</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Grossbard</surname></persName>
		</author>
		<ptr target="https://www.smscomparison.com/mass-text-messaging/2021-statistics/" />
		<title level="m">SMS Marketing Statistics 2021 For USA Businesses</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mobile phishing attacks and defence mechanisms: State of art and open research challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cose.2017.12.006</idno>
	</analytic>
	<monogr>
		<title level="j">Computers and Security</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="519" to="544" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A novel approach to detect spam and smishing SMS using machine learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Choudhary</surname></persName>
		</author>
		<idno type="DOI">10.4018/?ESMA.2020010102</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of E-Services and Mobile Applications</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="38" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Online social networks security and privacy: comprehensive review and analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Sahoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaubiyal</surname></persName>
		</author>
		<idno type="DOI">10.1007/s40747-021-00409-7</idno>
		<ptr target="https://doi.org/10.1007/s40747-021-00409-7" />
	</analytic>
	<monogr>
		<title level="j">Complex &amp; Intelligent Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A review on social spam detection: Challenges, open issues, and future directions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bhatia</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2021.115742</idno>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">186</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Advances in spam detection for email spam, web spam, social network spam, and review spam: ML-based and nature-inspired-based techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Akinyelu</surname></persName>
		</author>
		<idno type="DOI">10.3233/JCS-210022</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Security</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="473" to="529" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep learning to filter SMS Spam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.future.2019.09.001https:/linkinghub.elsevier.com/retrieve/pii/S0167739X19306879</idno>
		<ptr target="https://doi.org/10.1016/j.future.2019.09.001https://linkinghub.elsevier.com/retrieve/pii/S0167739X19306879" />
	</analytic>
	<monogr>
		<title level="j">Future Generation Computer Systems</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="524" to="533" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<idno type="DOI">10.1093/nsr/nwy108</idno>
		<ptr target="https://academic.oup.com/nsr/article/6/1/74/5123737" />
	</analytic>
	<monogr>
		<title level="j">National Science Review</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="74" to="86" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Ensemble methods: Foundations and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1201/b12207</idno>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep neural decision forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fiterau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulo</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.172</idno>
		<ptr target="http://ieeexplore.ieee.org/document/7410529/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1467" to="1475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<idno type="DOI">10.1023/A:1010933404324</idno>
		<ptr target="https://doi.org/10" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<idno type="DOI">10.1023/A:1010933404324</idno>
		<imprint>
			<biblScope unit="page">1010933404324</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spam filtering for short messages in adversarial environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="page" from="167" to="176" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<idno type="DOI">DOI10.1016/j.neucom.2014.12.034</idno>
		<ptr target="https://linkinghub.elsevier.com/retrieve/pii/S0925231214016981" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Document representation and feature combination for deceptive spam review detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">254</biblScope>
			<biblScope unit="page" from="33" to="41" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<idno type="DOI">DOI10.1016/j.neucom.2016.10.080</idno>
		<ptr target="https://linkinghub.elsevier.com/retrieve/pii/S0925231217303983" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ham and Spam E-Mails Classification Using Machine Learning Techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bassiouni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>El-Dahshan</surname></persName>
		</author>
		<idno type="DOI">https:/www.tandfonline.com/doi/full/10.1080/19361610.2018.1463136</idno>
		<ptr target="https://www.tandfonline.com/doi/full/10.1080/19361610.2018.1463136" />
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Security Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="315" to="331" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Text message classification using supervised machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Merugu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C S</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Piplani</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-981-13-0212-1\_15</idno>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Electrical Engineering</title>
		<imprint>
			<biblScope unit="volume">500</biblScope>
			<biblScope unit="page" from="141" to="150" />
			<date type="published" when="2019" />
			<publisher>SPRINGER</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multico-training for document classification using various document representations: TF-IDF, LDA, and Doc2Vec</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ins.2018.10.006.URLhttps:/www.sciencedirect.com/science/article/pii/S0020025518308028</idno>
		<ptr target="https://doi.org/10.1016/j.ins.2018.10.006.URLhttps://www.sciencedirect.com/science/article/pii/S0020025518308028" />
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">477</biblScope>
			<biblScope unit="page" from="15" to="29" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Machine intelligence-based algorithms for spam filtering on document labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gaurav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abraham</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00500-019-04473-7http:/link.springer.com/10.1007/s00500-019-04473-7</idno>
		<idno>10.1007/ s00500-019-04473-7</idno>
		<ptr target="https://doi.org/10.1007/s00500-019-04473-7http://link.springer.com/10.1007/s00500-019-04473-7" />
	</analytic>
	<monogr>
		<title level="j">Soft Computing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="9625" to="9638" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Convolutional Neural Network Based SMS Spam Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Popovac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sladojevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arsenovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anderla</surname></persName>
		</author>
		<idno type="DOI">10.1109/TELFOR.2018.8611916</idno>
		<ptr target="https://ieeexplore.ieee.org/document/8611916/" />
	</analytic>
	<monogr>
		<title level="m">26th Telecommunications Forum</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
	<note>TELFOR 2018 -Proceedings</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Spam filtering using integrated distribution-based balancing approach and regularized deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barushka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hajek</surname></persName>
		</author>
		<idno type="DOI">http:/link.springer.com/10.1007/s10489-018-1161-y</idno>
		<idno>10. 1007/s10489-018-1161-y</idno>
		<ptr target="http://link.springer.com/10.1007/s10489-018-1161-y" />
	</analytic>
	<monogr>
		<title level="j">Applied Intelligence</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3538" to="3556" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Spam detection in social media using convolutional and long short term memory neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Agarwal</surname></persName>
		</author>
		<idno type="DOI">http:/link.springer.com/10.1007/s10472-018-9612-z</idno>
		<ptr target="http://link.springer.com/10.1007/s10472-018-9612-z" />
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematics and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="44" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A hybrid CNN-LSTM model for SMS spam detection in arabic and english messages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghourabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">M</forename><surname>Alzubi</surname></persName>
		</author>
		<idno type="DOI">10.3390/FI12090156</idno>
		<ptr target="https://www.mdpi.com/1999-5903/12/9/156" />
	</analytic>
	<monogr>
		<title level="j">Future Internet</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">156</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><forename type="middle">A</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Jos? Mar?a</surname></persName>
		</author>
		<title level="m">SMS Spam Collection</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">TextSpamDetector: textual content based deep learning framework for social spam detection using conjoint attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elakkiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Selvakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Leela Velusamy</surname></persName>
		</author>
		<idno type="DOI">http:/link.springer.com/10.1007/s12652-020-02640-5</idno>
		<ptr target="http://link.springer.com/10.1007/s12652-020-02640-5" />
	</analytic>
	<monogr>
		<title level="j">Journal of Ambient Intelligence and Humanized Computing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Malicious text identification: Deep learning from public comments and emails</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baccouche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sierra-Sosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elmaghraby</surname></persName>
		</author>
		<idno type="DOI">10.3390/info11060312</idno>
		<ptr target="https://www.mdpi.com/2078-2489/11/6/312" />
	</analytic>
	<monogr>
		<title level="j">Information (Switzerland)</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">312</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/d14-1162</idno>
		<ptr target="http://www.aclweb.org/anthology/D14-1162" />
	</analytic>
	<monogr>
		<title level="m">EMNLP 2014 -2014 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">F</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Abdou</surname></persName>
		</author>
		<title level="m">Neural machine translation: past, present, and future. Neural Computing and Applications</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<idno type="DOI">10.1007/s00521-021-06268-0</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Extremely randomized trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Geurts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wehenkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="42" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<idno type="DOI">http:/link.springer.com/10.1007/s10994-006-6226-1</idno>
		<ptr target="http://link.springer.com/10.1007/s10994-006-6226-1" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Spam detection on social networks using cost-sensitive feature selection and ensemble-based regularized deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barushka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hajek</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00521-019-04331-5</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi-split optimized bagging ensemble model selection for multi-class educational data mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Injadat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moubayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Nassif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shami</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10489-020-01776-3</idno>
		<ptr target="https://doi.org/10.1007/s10489-020-01776-3" />
	</analytic>
	<monogr>
		<title level="j">Applied Intelligence</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4506" to="4528" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Deep Learning using Rectified Linear Units (ReLU)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Agarap</surname></persName>
		</author>
		<idno>abs/1803.0</idno>
		<ptr target="http://arxiv.org/abs/1803.08375" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>CoRR</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Interpretation of intelligence in CNN-pooling processes: a methodological survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Ragavendran</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00521-019-04296-5</idno>
		<idno>10.1007/ s00521-019-04296-5</idno>
		<ptr target="https://doi.org/10.1007/s00521-019-04296-5" />
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="879" to="898" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">SMOTE: Synthetic minority over-sampling technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">O</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">P</forename><surname>Kegelmeyer</surname></persName>
		</author>
		<idno type="DOI">10.1613/jair.953</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="321" to="357" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<ptr target="http://download.tensorflow.org/paper/whitepaper2015.pdf" />
		<title level="m">GoogleResearch: TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Others: Keras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://keras.io" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Scikit-Learn: Machine Learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Chapter 2 -Data Science Process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kotu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Deshpande</surname></persName>
		</author>
		<idno type="DOI">DOIhttps:/doi.org/10.1016/B978-0-12-814761-0.00002-2</idno>
		<ptr target="https://www.sciencedirect.com/science/article/pii/B9780128147610000022" />
	</analytic>
	<monogr>
		<title level="m">Data Science</title>
		<editor>V. Kotu, B. Deshpande</editor>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="19" to="37" />
		</imprint>
	</monogr>
	<note>Second Edition. second edi edn.</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Enhancing software comments readability using flesch reading ease score</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eleyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Othman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eleyan</surname></persName>
		</author>
		<idno type="DOI">10.3390/INFO11090430</idno>
	</analytic>
	<monogr>
		<title level="j">Information (Switzerland)</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Effective and scalable legal judgment recommendation using pre-learned word embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dhanani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rana</surname></persName>
		</author>
		<idno type="DOI">10.1007/s40747-022-00673-1</idno>
		<idno>10.1007/ s40747-022-00673-1</idno>
		<ptr target="https://doi.org/10.1007/s40747-022-00673-1" />
	</analytic>
	<monogr>
		<title level="m">Complex &amp; Intelligent Systems</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A weighted feature enhanced Hidden Markov Model for spam SMS filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2021.02.075</idno>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">444</biblScope>
			<biblScope unit="page" from="48" to="58" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A Spam Transformer Model for SMS Spam Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nayak</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2021.3081479</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="80253" to="80263" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
