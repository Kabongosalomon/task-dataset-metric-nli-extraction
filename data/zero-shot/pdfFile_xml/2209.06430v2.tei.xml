<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CLIP-ViP: Adapting Pre-trained Image-Text Model to Video-Language Representation Alignment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Xue</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchong</forename><surname>Sun</surname></persName>
							<email>ycsun@ruc.edu.cnbei.liu</email>
							<affiliation key="aff1">
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Liu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihua</forename><surname>Song</surname></persName>
							<email>rsong@ruc.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
							<email>jluo@cs.rochester.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Rochester</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CLIP-ViP: Adapting Pre-trained Image-Text Model to Video-Language Representation Alignment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The pre-trained image-text models, like CLIP, have demonstrated the strong power of vision-language representation learned from a large scale of web-collected image-text data. In light of the well-learned visual features, some existing works transfer image representation to video domain and achieve good results. However, how to utilize image-language pre-trained model (e.g., CLIP) for videolanguage pre-training (post-pretraining) is still under explored. In this paper, we investigate two questions: 1) what are the factors hindering post-pretraining CLIP to further improve the performance on video-language tasks? and 2) how to mitigate the impact of these factors? Through a series of comparative experiments and analyses, we find that the data scale and domain gap between language sources have great impacts. Motivated by these, we propose a Omnisource Cross-modal Learning method equipped with a Video Proxy mechanism on the basis of CLIP, namely CLIP-ViP. Extensive results show that our approach improves the performance of CLIP on video-text retrieval by a large margin. Our model also achieves SOTA results on a variety of datasets, including MSR-VTT, DiDeMo, LSMDC, and Ac-tivityNet. We will release our code and pre-trained CLIP-ViP models at https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the last few years, vision-language pre-training has achieved great success on cross-modal representation learning from a large scale of web-crawled data <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b54">54]</ref>. Among them, image-text pre-trained models <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b34">35]</ref> have shown powerful representation on vari-* Equal contribution. This work was performed when Hongwei Xue and Yuchong Sun were visiting Microsoft Research Asia as research interns. ? Corresponding authors. ous downstream tasks, including vision understanding tasks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b41">41]</ref> , image-language generation tasks <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b32">33]</ref> and so on <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b56">56]</ref>. In light of well-learned and enriched visual representation, some works directly adapt image-text pre-trained models to video-text downstream tasks without further pre-training on video data <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b57">57]</ref>, while still outperforming models pre-trained on video data <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b48">48]</ref>.</p><p>Utilizing an existing powerful image-language pretrained model for further video-language pre-training is able to reduce the required training cost by making good use of the knowledge learned from images. However, adapting image-text pre-training models to video-language data for post pre-training has not demonstrated significant advantage yet, thus is still under explored. A preliminary study is conducted by CLIP4Clip <ref type="bibr" target="#b28">[29]</ref> which adopts MeanPooling based on the CLIP model on a subset of Howto100M <ref type="bibr" target="#b29">[30]</ref>. While the improvement on video-text retrieval performance is very limited for both zero-shot and fine-tuning settings. In this paper, we aim to explore how to adapt imagelanguage pre-training model (e.g., CLIP) to video representation learning to improve performance on video-language tasks (e.g., text-to-video retrieval) on various datasets.</p><p>To unleash the power of video data for adapting imagetext pre-training to post pre-training, we conduct several preliminary experiments to figure out the challenges that hinder post-pretraining. First, we explore post pre-training an image-text pre-trained model (i.e., CLIP) with Mean-Pooling on video-language datasets with different scales, including WebVid-2.5M <ref type="bibr" target="#b1">[2]</ref> and HD-VILA-100M <ref type="bibr" target="#b48">[48]</ref>. The result shows that the scale of data is critical for video representation learning. Data in small scale makes the model tend to over-fit the new data while the knowledge learned from image-text is suppressed and the performance is reduced. Second, we study the language domain gap between pre-training data and downstream data. By calculating the Normalized Mutual Information (NMI) on clusters of text features, we find that there is a large domain gap between subtitles that are used in large-scale video-language pretraining data and downstream language.</p><p>To mitigate the impact of these factors, we propose a Omnisource Cross-modal learning method equipped with a Video Proxy mechanism for video-language pre-training. More specifically, we introduce auxiliary captions that have small domain gap with downstream data into existing largescale video-text data. To avoid data leakage from downstream tasks and to pursue a high-performance caption generation model, we adopt an image captioning model to generate an auxiliary caption of one frame in each video. During post-pretraining, Omnisource Cross-modal learning fuses caption-frame and video-subtitle pairs in the same batch for joint pre-training. We study a series of variants to find the best fusion strategy. We will release these auxiliary captions to facilitate future research.</p><p>In order to keep generality and extendability, we aim to transfer an image model into video domain with minimal modifications. We propose Video Proxy tokens and a proxy-guided video attention mask mechanism, which only increases negligible parameters and calculations to the model. We introduce a set of tokens named video proxies into the CLIP model. During calculating attention in each block, video proxies can interact with all features, while patch features only interact with video proxies and features within the same frame. By experimental results, we find that this mechanism can better model videos containing temporal information, while reducing conflicts with the pre-trained CLIP model.</p><p>The experimental results show that our approach is able to improve the performance of video-text retrieval by a large margin compared to image-text pre-trained model. We also conduct ablation studies to verify the effectiveness of each part in our approach, including data used for postpretraining, model structure and objective functions. Considering that the generation of auxiliary captions in our approach requires a powerful pre-trained model, we also validate direct introducing existing image-text data into posttraining. The results are in good consistent with our preliminary analysis.</p><p>Our contributions are summarized as follows:</p><p>1. We conduct a preliminary analysis to figure out two factors that hinder video post-pretraining on pretrained image-text models: data scale and language domain gap; 2. We then propose a Omnisource Cross-modal learning method equipped with a Video Proxy mechanism to learn from both image-text and video-text pairs; 3. Extensive experiments verify the effectiveness of our method. Our model outperforms the SOTA results by a large margin on a variety of datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Vision-Language Pre-Training End-to-end models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b54">54]</ref> for vision-language pre-training are replacing the traditional approach using pre-extracted visual features by off-the-shelf models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b58">58]</ref>.</p><p>Training end-to-end models on large-scale web-collected data also gradually demonstrates the big advantages <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b54">54]</ref>. Unlike images that have alt-texts, large-scale video datasets suitable for pre-training usually use subtitles as text sources <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b48">48]</ref>. Subtitles are much more noisy than alt-texts, According to <ref type="bibr" target="#b29">[30]</ref>, Typical examples of incoherence include the content producer asking viewers to subscribe to their channel, talking about something unrelated to the video, or describing something before or after it happens. Bain et al. collect a video dataset WebVid <ref type="bibr" target="#b1">[2]</ref> with manually generated captions. Their texts are well aligned with the video and avoid suffering from ASR errors. However, the vast majority of WebVid videos are sourced from stock footage website, so scaling up is under limitation. The video-subtitle data is more easily accessible on the web thus suitable for scaling up. In this paper, we investigate the unfavorable factors of video-subtitle data and explore how to mitigate the impact of these factors.</p><p>CLIP for Video-Text Retrieval The great success of the CLIP has demonstrated its unprecedented power on varies downstream tasks, including vision understanding tasks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b41">41]</ref> , image-language generation tasks <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b32">33]</ref> and so on <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b56">56]</ref>. By contrastive learning on large-scale imagetext pairs, CLIP learns enriched visual concepts for images. Recently, some works directly transfer CLIP to videotext retrieval without futher pretraining on video data (postpretraining) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b57">57]</ref>. Their work takes the performance of video-text retrieval to a new level, outperforming exsiting models pre-trained on data containing video data <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b48">48]</ref>. They transfer CLIP from views of feature aggregation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b57">57]</ref> or representation alignment <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b43">43]</ref>. In parallel with these works, we study post pre-training with video data on top of CLIP in a effective way. Their approaches may also applicable on top of ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminary Analysis</head><p>In this section, we first study the impact of data scale for adapting image-text pre-training to post video-text pretraining. Then we explore the unfavorable factors of largescale data from the language domain's perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Post-pretraining on Different Data Scales</head><p>To study the effectiveness of different data scales, we use the CLIP-ViT-B/32 model <ref type="bibr" target="#b34">[35]</ref> as the base image-text pre-training model and adopt MeanPooling setting for video adaption like CLIP4Clip <ref type="bibr" target="#b28">[29]</ref>. Two video-language datasets are used: WebVid-2.5M [2] with 2.5 million pairs and HD-VILA-100M <ref type="bibr" target="#b48">[48]</ref> with 100M pairs. We also adopt a subset of HD-VILA-100M containing random 10% data (namely HD-VILA-10M) as a middle setting. We run the same number of steps on all settings, equivalent to 1 epoch on HD-VILA-100M. We uniformly sample 12 frames from each video and apply the same hyper-parameters as described in Section 5 for all settings.</p><p>During post-pretraining, we evaluate the pre-trained models by fine-tuning on MSR-VTT text-to-video retrieval task. <ref type="figure" target="#fig_0">Figure 1</ref> shows the performance trend. We observe an overfitting phenomenon that continuous post-pretraining leads to performance drop. And this phenomenon is more significant on smaller data (e.g., WebVid-2.5M and HD-VILA-10M). As CLIP is pre-trained on 400 million imagetext pairs, further training on small data makes the model tend to overfit the new data while the implicit knowledge learned from the image-text pairs is degrading. As a consequence, the performance will drop, even worse than using CLIP directly. This motivate us to adopt HD-VILA-100M due to its large scale and scalability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Language Domain Gap with Downstream data</head><p>It is intuitive that pre-training data with the same domain as downstream data can benefit downstream tasks. For most video-language tasks like video-text retrieval, the language are descriptive sentences of videos. While for HD-VILA-100M as we decided to use above, the language are autogenerated subtitles, which could have very different correspondence to visual information compared to descriptive texts. Meanwhile, auto-generated subtitles also suffer from irrelevance, misalignment and ASR error. To better explore the domain gap between pre-training data and downstream data, we measure the inconsistency by calculating the dissimilarity between vision-aware language features. For downstream language data, we choose two typical videotext retrieval datasets: MSR-VTT <ref type="bibr" target="#b47">[47]</ref> and DiDemo <ref type="bibr" target="#b0">[1]</ref>. For pre-training language, we select four datasets: video subtitles of HD-VILA-100M (HD-VILA sub ), videos captions of WebVid-2.5M, image captions of MS-COCO <ref type="bibr" target="#b24">[25]</ref>, and web-collected alt-texts of Conceptual Caption 12M <ref type="bibr" target="#b3">[4]</ref>. In addition, we analyze auto-generated captions of HD-VILA-100M (HD-VILA cap ), which will be described in Section 4.</p><p>We use a Transformer Encoder initialized from CLIP <ref type="bibr" target="#b34">[35]</ref> to extract vision-aware language features. To quantify domain gap, we first mix language features from one pretraining data and one downstream data, and use K-means to get two clusters. Then we calculate the Normalized Mutual Information (NMI) between cluster labels and groundtruths. A larger NMI value means that the two types of features are easily to be distinguished, thus there is a larger domain gap. For each comparison, we randomly sample 1000 0.420 0.488 0.454 COCO <ref type="bibr" target="#b24">[25]</ref> 0.373 0.758 0.566 CC12M <ref type="bibr" target="#b38">[38]</ref> 0.445 0.673 0.559 texts from each type of data as one setting and the result is the average of 10 settings. We report the results in <ref type="table" target="#tab_0">Table 1</ref>. From the last column, we find that the NMI score between HD-VILA sub and downstream data is much larger than others, especially for MSR-VTT dataset. This indicates that direct training with subtitles may introduce inconsistency with downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Approach</head><p>In this section, we first propose a Video Proxy mechanism to trasfer CLIP model to video domain. Then we introduce an in-domain auxiliary data generation method and an Omnisource Cross-modal learning method to reduce the domain gap between pre-training data and downstream data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Video Proxy Mechanism</head><p>As video is an ordered sequence of frames, it's critical to learn the frame aggregation and temporality when transfering to video domain. Meanwhile, to keep the high generality and extendability of ViT backbone, we aim to find a simple but effective way to transfer CLIP model to video domain under minimal modifications. Given a video containing T frames:</p><formula xml:id="formula_0">{f 1 , f 2 , f 3 , ..., f T }, we follow CLIP to divide each frame into N patches: {f 1 t , f 2 t , ..., f N t | t ? [1, T ]}.</formula><p>Then we add spatio-temporal positional embedding to each flattened 2D patches:  where Linear( * ) is a linear layer, P os s (n) and P os t (t) is the learnable spatial and temporal positional embedding, respectively. The whole video can be divided and projected into T * N patch tokens.</p><formula xml:id="formula_1">g(f n t ) = Linear(f n t ) + P os s (n) + P os t (t),<label>(1)</label></formula><formula xml:id="formula_2">? ? + ? ? ? ? + ? ? ? ? + ? ? + ? ? ? ? , + ? ? Pos. embed: 0 th frame's 0 th patch ... ...</formula><p>To model spatial information from multi-frame and temporality, one simple way is directly feeding all tokens into CLIP's vision encoder and let them attend to each other. However, this method introduce significant conflicts with CLIP. As CLIP is pre-trained on pairs of text and single image, it never handle interactions of tokens between images/frames during training. This is also verified by our experiments in Section 5. Instead, we introduce a Video Proxy mechanism to act as a proxy that helps each local patch perceive video-level information.</p><p>Before feeding into CLIP, we concatenate patch tokens with a set of learnable parameters called video proxies: {k 1 , k 2 , k 3 , ..., k M }, where M is the number of video proxies. Then all T * N + M tokens will be fed into the ViT of CLIP. The output on first video proxy will be regarded as the video's representation. All the calculation in ViT are the same as original version except for attention mechanism. In the attention score calculation of each block, video proxies attend to all tokens, while patch tokens only at-tend to tokens in the same patch plus video proxies. By this mechanism, patch tokens can obtain global information from video proxies, while reducing inconsistencies with the original CLIP's calculation. Our experiment in Section 5 demonstrates the superiority of this mechanism.</p><p>For the input type of the image/frame, we just use linear interpolation to get a middle temporal positional embedding, then treat the image/frame as a special single-frame video. This method enables joint training on both videos and images in the same batch, as our attention mechanism reduces the difference of calculations between video and image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">In-domain Auxiliary Data Generation</head><p>Motivated by the analysis in Section 3, we introduce auxiliary captions into large-scale video-subtitle data to reduce the domain gap between pre-training and downstream data. We adopt image captioning model for two reasons: 1) The training datasets of current video captioning models have overlap with downstream tasks, e.g. MSR-VTT. We avoid data leakage to perform pre-training agnostic to downstream data. 2) The performance of existing captioning models on videos lags far behind that on images.</p><p>For above reasons, we choose a powerful image captioning model OFA-Caption <ref type="bibr" target="#b42">[42]</ref> to generate one caption for the middle frame of each video in HD-VILA-100M. We use the default setting of OFA-Caption model. As a result, we generate 100M sentences with a max length of 16. This approach can be applied to any video data and we will also release the generated captions on HD-VILA-100M to facilitate future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Omnisource Cross-Modal Learning</head><p>To learn rich video-language alignment from videosubtitle pairs and reduce domain gap with downstream data by corresponding auxiliary frame-caption pairs, we study joint Cross-Modal learning on the omnisource input. Following most works of learning multimodal alignment on dual encoders <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b48">48]</ref>, we use info-NCE loss to perform contrastive learning. There are two formats of visual source : video sequences and single frames, and two types of text source : subtitle and caption in our work. We denote them by V , F , S, and C respectively. We definite a source-wise info-NCE loss by:</p><formula xml:id="formula_3">L v2t = ? 1 B B i=1 log exp v i t i /? B j=1 exp v i t j /? L t2v = ? 1 B B i=1 log exp t i v i /? B j=1 exp t i v j /? L X?Y = L v2t + L t2v ,<label>(2)</label></formula><p>where v i and t j are the normalized embeddings of i-th visual feature in X ? {V, F } and j-th text feature in Y ? {S, C} in a batch of size B. ? is a learnable temperature. For example, L V ?S represents info-NCE loss within video-subtitle pairs in a batch, which pulls aligned pairs together in embedding space while pushing apart misaligned pairs.</p><p>We study the reasonable variants of Omnisource Cross-Modal Learning: (a) L V ?S + L F ?C : Simple combination of two source-wise losses on video-subtitle and framecaption pairs; (b) L V ?S + L V ?C : As there is also content correlation between videos and its middle-frame captions, we explore to add a loss on video-caption pairs to baseline loss L V ?S ; (c) L V ?S + L V ?C + L F ?C : Combination of (a) and (c); (d) L V ?S,C + L F ?C : A video corresponds to both a subtitle and auxiliary caption. Compare to (c), the numbers of negative pairs in L v2t can be expanded. The L v2t in L V ?S,C is rewritten as:</p><formula xml:id="formula_4">L v2t = ? 1 2B B i=1 (log exp v i s i /? B j=1 exp v i s j /? + exp v i c j =i /? + log exp v i c i /? B j=1 exp v i c j /? + exp v i s j =i /? ),<label>(3)</label></formula><p>where s i ? S and c i ? C. The L t2v in L V ?S,C is equal to (c). We compare all variants with the baseline L V ?S and report results in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiment</head><p>We first describe the implementation In this section, we first describe the implementation details during video-language post-pretraining and fine-tuning downstream tasks. Then thorough ablation studies are conducted to demonstrate the effectiveness of our designed CLIP-ViP model which learn video-language representation by video and image joint pre-training. Finally, we make comparisons of our model to existing state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Details</head><p>Video-language post-pretraining. For video input, we uniformly sample 12 frames for each video and resize all frames to 224*224. For language, we adopt the CLIP's tokenizer to split a sentence into word tokens with a max length of 70. We use AdamW optimizer <ref type="bibr" target="#b26">[27]</ref> with an initial learning rate of 5e-6 and a fixed weight decay of 5e-2. For learning rate schedule, we adopt a cosine decay with a warm-up strategy. We train our model with 32 NVIDIA Tesla V100 GPUs in a batch size of 1024. The contrastive similarity is calculated on gathered features from all GPUs. We set training steps equal to one epoch on HD-VILA-100M for all ablation studies and 3 epochs for the full setting.</p><p>Fine-tuned training. We reuse most hyper-parameters in post-pretraining with some exceptions. 1) Batch size: we fine-tune our model with a batch size of 128 for all downstream tasks. 2) Learning rate and weight decay are set to 1e-6 and 0.2, respectively. 3) Number of epochs: due to the various scales of downstream datasets, we set epoch number to 5, 20, 10, 20 for MSR-VTT, Didemo, LSMDC, Activi-tyNet, respectively. 4) Frame number, we set frame number to 32 for ActivityNet Captions as its videos are much longer (180 seconds on average). Note that the hyper-parameters of downstream training are the same in all settings in ablation study.</p><p>Downstream Datasets. We conduct video-text retrieval experiments on four typical datasets. (a) MSR-VTT <ref type="bibr" target="#b47">[47]</ref> contains 10K YouTube videos with 200K descriptions. We follow previous works <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b51">51]</ref> to train models on 9K videos, and report results on the 1K-A test set. (b) DiDeMo [1] consists of 10K Flickr videos annotated with 40K sentences. We follow <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b55">55]</ref>  movies disjoint from the train and validation sets. (d) Ac-tivityNet Captions <ref type="bibr" target="#b18">[19]</ref> contains 20K YouTube videos annotated with 100K sentences. We follow the paragraph-tovideo retrieval setting <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b55">55]</ref> to train models on 10K videos and report results on the val1 set with 4.9K videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation Study</head><p>Video proxy mechanism. For the vision encoder, we evaluate our proposed Video Proxy (ViP) mechanism with different numbers of proxies and compare it with different model structures (i.e. MeanPool, SeqTransformer, Full Attention) by fine-tuning the same pre-training model on MSR-VTT retrieval task. MeanPool simply takes the average of frame features as the representation of the whole video. For SeqTransformer, we follow the seqTransf type in CLIP4Clip <ref type="bibr" target="#b28">[29]</ref> and the residual connection in their implementation. Full attention setting takes all patch tokens as the input of the vision encoder and Attention calculation is conducted on all tokens. We also study the impact of different numbers of video proxies. All models are initialized from a CLIP-ViT-B/32. The results are shown in <ref type="table">Table 3</ref>. The simplest model is MeanPool type, which completely disregards temporality. Compared to the MeanPool baseline, SeqTransformer improves the average Recall@1,5,10 by 0.8%. Full Attention type leads to a significant performance drop, although this type allows individual features to interact globally. Besides, we observe the type has a worse initial status and converges slower than other types. One reason is that Full Attention has a significant calculation conflict with CLIP's in-frame Attention. From <ref type="table">Table 3</ref>, our Video-Proxy mechanism has the most improvement. Different numbers of video proxies all result in significant performance gain on R@1 (e.g., 3.1% by 4 proxies), while only increasing negligible parameters: 3K compared to 86M of ViT backbone.</p><p>Omnisource Cross-modal Learning. To verify the effectiveness of our proposed Omnisource Cross-modal Learning and compare its variants, we set a post-pretraining and fine-tuning pipeline and adopt same hyper-parameters for all experiments. L V ?S is the baseline contrastive loss on video-subtitle pairs. After introducing auxiliary captions, we study four variants of Omnisource Cross-modal Learning Loss: (a)</p><formula xml:id="formula_5">L V ?S + L F ?C ; (b) L V ?S + L V ?C ; (c) L V ?S + L V ?C + L F ?C ; (d) L V ?S,C + L F ?C .</formula><p>The specific approach is described in Section 4. We post-pretrain by each loss function for only one epoch due to the costly training, then finetune on two video-text retrieval datasets: MSR-VTT and DiDemo. We compare with the results of CLIP-MeanPool and CLIP using proposed Video-Proxy mechanism without post-pretraining. The results is listed in  <ref type="table">Table 6</ref>. Comparison of text-to-video retrieval in didemo <ref type="bibr" target="#b0">[1]</ref>. * denotes that the method uses post-processing operations DSL <ref type="bibr" target="#b5">[6]</ref>.</p><p>we use video-subtitle pairs and video-caption data to postpretrain, respectively, by vanilla contrastive loss. For data combination, we apply Omnisource Cross-modal Learning under L V ?S,C +L F ?C setting to post-pretrain on the combined data. From <ref type="table">Table 4</ref>, Omnisource post-pretraining results are much better than two uni-source results. Especially on MSR-VTT, neither uni-source post-pretraining results in a significant improvement: 67.4% and 66.9% compared with 67.0%. while the Omnisource post-pretraining brings a qualitative improvement: 69.6%. On DiDemo data, data combination nearly double the improvements brought by uni-source. These experimental results verify that it is the data combination itself that is effective, rather than the better data are used. As the generation of auxiliary captions is based on OFA, a powerful image-text pretrained model, we also explore only including existing data in our post-pretraining.  <ref type="table">Table 8</ref>. Comparison of text-to-video retrieval in LSMDC <ref type="bibr" target="#b36">[37]</ref>. * denotes that the method uses post-processing operations DSL <ref type="bibr" target="#b5">[6]</ref>.</p><p>We choose image-text pairs from several widely adopted datasets: MS-COCO, Visual Genome (VG) <ref type="bibr" target="#b19">[20]</ref>, Flickr-30K <ref type="bibr" target="#b50">[50]</ref>, SBU <ref type="bibr" target="#b31">[32]</ref>, CC3M <ref type="bibr" target="#b38">[38]</ref>, CC12M as our auxiliary data (ImageCaption). To ablate the contribution of these data, we add experiments of post-pretraining on ImageCap-tion alone and HD-VILA-100M combined with ImageCaption. From <ref type="table">Table 4</ref>, post-pretraining on ImageCaption alone results in performance degradation on MSR-VTT and no improvement on DiDemo. In contrast, ImageCaption yields significant performance gains on both datasets when used as auxiliary data for HD-VILA-100M. This further illustrates the importance of the combination of large-scale noisy data and auxiliary data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison to State-of-the-art</head><p>We compare our model under full setting (3 epochs) with the state-of-the-art (SOTA) works on text-to-video retrieval task. The results of fine-tuning on four datatasets (i.e., MSR-VTT, DiDemo, ActivityNet Captions, LSMDC) are shown in <ref type="table">Table 5</ref>, 6, 7 and 8, respectively. We clarify the backbone for CLIP-based works. Our model achieve SOTA results on all datasets. Note that some existing methods are also applicable on top of our models as our modification to CLIP model is minimal. However, how to extend our model is out of this paper's scope. We only add results with DSL <ref type="bibr" target="#b5">[6]</ref> to make fair comparison with some methods using post-processing operations e.g., DSL or QB-Norm <ref type="bibr" target="#b2">[3]</ref> like CAMoE <ref type="bibr" target="#b5">[6]</ref>.</p><p>For the MSR-VTT, DiDemo and ActivityNet Captions dataset, our model outperforms existing methods by a large margin on both CLIP-ViT-B/32 and CLIP-ViT-B/16. For LSMDC, our model achieves SOTA results on CLIP-ViT-B/32 and by a large margin on CLIP-ViT-B/16. Note that even under no DSL setting, our results still surpass methods using post-processing operations on most datasets. Note that adding DSL will greatly improve performance of our model Since our model has good bidirectional visionlanguage correspondence. The good results on ActivityNet Captions dataset also indicates that our model can generalize well to long videos. Overall, the improvements on different datasets demonstrate the good video-language representation of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we study further pre-training (postpretraining) image-text models like CLIP on large scale video data. We first conduct a preliminary analysis to figure out what are the factors hindering video post-pretraining. Motivated by our results, we propose a Omnisource Crossmodal Learning method equipped with a Video-Proxy mechanism. The Video-Proxy mechanism can well model videos containing temporal information, while reducing conflicts with the pre-trained CLIP model. The Omnisource Cross-modal Learning alleviates the problem brought by domain gap between video-subtitle and downstream data. Extensive results show that our approach improves the performance of CLIP on video-text retrieval by a large margin and also achieves SOTA results on a variety of datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The curve of finetuning results during post-pretraining. The x-axis indicates the percentage of training steps. The y-axis indicates average value of Recall@1, 5 and 10. [Best viewed in color]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The framework of CLIP-ViP, consisting of a text encoder and a vision encoder. Taken features V , F , S, C of videos, frames, subtitles, captions as input, a series of Omnisource cross-modal learning varients are studied: (a) LV ?S + LF ?C ; (b) LV ?S + LV ?C ; (c) LV ?S + LV ?C + LF ?C ; (d) LV ?S,C + LF ?C . In vision encoder, Video Proxies and the corresponding attention mechanism are proposed to transfer CLIP into video domain. [Best viewed in color]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Normalized Mutual Information (NMI) score of language features extracted on series of data and downstream tasks.</figDesc><table /><note>We choose MSR-VTT and DiDemo as downstream tasks. Larger value indicates larger domain gap.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .Table 3</head><label>23</label><figDesc>to evaluate paragraph-to-video retrieval, where all descriptions for a video are concatenated to form a single query. (c) LSMDC<ref type="bibr" target="#b36">[37]</ref> consists of 118,081 video clips sourced from 202 movies. Each video has a caption. Evaluation is conducted on a test set of 1,000 videos from R@5 ? R@10 ? Mean ? R@1 ? R@5 ? R@10 ? Mean ? +LV ?S + LV ?C + LF ?C Ablation study of different losses. We report text-to-video results of models finetuned on MSR-VTT and Didemo. Mean? indicates average value of Recall@1, 5 and 10. For all results, the model is based on CLIP-ViT-B/32. All post-pretraining steps are equivalent to one epoch on HD-VILA-100M.</figDesc><table><row><cell>Model</cell><cell></cell><cell></cell><cell></cell><cell cols="2">MSR-VTT Retrieval</cell><cell></cell><cell></cell><cell cols="2">DiDemo Retrieval</cell></row><row><cell cols="4">R@1 ? CLIP-MeanPool 43.4</cell><cell>70.9</cell><cell>81.1</cell><cell>65.1</cell><cell>40.6</cell><cell>67.5</cell><cell>77.2</cell><cell>61.8</cell></row><row><cell>CLIP-ViP</cell><cell></cell><cell></cell><cell>46.5</cell><cell>72.1</cell><cell>82.5</cell><cell>67.0</cell><cell>40.6</cell><cell>70.4</cell><cell>79.3</cell><cell>63.4</cell></row><row><cell>+LV ?S</cell><cell></cell><cell></cell><cell>47.7</cell><cell>72.1</cell><cell>82.4</cell><cell>67.4</cell><cell>44.6</cell><cell>73.9</cell><cell>81.9</cell><cell>66.8</cell></row><row><cell cols="2">+LV ?S + LF ?C</cell><cell></cell><cell>49.3</cell><cell>74.8</cell><cell>83.8</cell><cell>69.3</cell><cell>48.4</cell><cell>74.5</cell><cell>84.4</cell><cell>69.1</cell></row><row><cell cols="2">+LV ?S + LV ?C</cell><cell></cell><cell>49.6</cell><cell>74.2</cell><cell>84.0</cell><cell>69.3</cell><cell>48.5</cell><cell>76.6</cell><cell>83.6</cell><cell>69.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell>49.6</cell><cell>74.5</cell><cell>83.8</cell><cell>69.3</cell><cell>48.5</cell><cell>76.8</cell><cell>84.1</cell><cell>69.8</cell></row><row><cell cols="2">+LV ?S,C + LF ?C</cell><cell></cell><cell>49.6</cell><cell>74.5</cell><cell>84.8</cell><cell>69.6</cell><cell>48.2</cell><cell>76.7</cell><cell>84.4</cell><cell>69.8</cell></row><row><cell>Model</cell><cell cols="4">R@1 ? R@5 ? R@10 ? Mean ?</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MeanPool</cell><cell>43.4</cell><cell>70.9</cell><cell>81.1</cell><cell>65.1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SeqTransformer</cell><cell>44.6</cell><cell>71.2</cell><cell>81.8</cell><cell>65.9</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Full Attention</cell><cell>42.8</cell><cell>70.1</cell><cell>80.3</cell><cell>64.4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2 Video Proxies</cell><cell>45.8</cell><cell>71.3</cell><cell>81.7</cell><cell>66.3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4 Video Proxies</cell><cell>46.5</cell><cell>72.1</cell><cell>82.5</cell><cell>67.0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>8 Video Proxies</cell><cell>45.7</cell><cell>72.7</cell><cell>81.7</cell><cell>66.7</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>. MSR-VTT text-to-video retrieval results of finetuning CLIP model by different settings. Mean ? indicates average value of Recall@1, 5 and 10. For all results, the model is based on CLIP-ViT-B/32.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>From the results on MSR-VTT we can see that L V ?S brings very little improvement: 0.4% on average of Recall@1, 5, 10. According to our preliminary analysis, this is due to the large domain gap between MSR-VTT and post-pretraining data. Combined with auxiliary captions, four variants of Omnisource Cross-modal Learning loss all bring significant improvements: over 3% on Recall@1 and over 2.3% on average of Recall@1, 5, 10. On Didemo data, Based on the improvement brought by L V ?S , Omnisource Cross-modal Learning further improve the results by a large margin: 8% R@5 ? R@10 ? Mean ? R@1 ? R@5 ? R@10 ? Mean ? L V ?S,C + L F ?C performs best thus we apply this variant as our final setting.</figDesc><table><row><cell cols="2">Post-pretrain Data</cell><cell></cell><cell></cell><cell cols="2">MSR-VTT Retrieval</cell><cell></cell><cell></cell><cell cols="2">DiDemo Retrieval</cell></row><row><cell cols="4">R@1 ? w/o Post-pretrain 46.5</cell><cell>72.1</cell><cell>82.5</cell><cell>67.0</cell><cell>40.6</cell><cell>70.4</cell><cell>79.3</cell><cell>63.4</cell></row><row><cell>HD-VILA sub</cell><cell></cell><cell></cell><cell>47.7</cell><cell>72.1</cell><cell>82.4</cell><cell>67.4</cell><cell>44.6</cell><cell>73.9</cell><cell>81.9</cell><cell>66.8</cell></row><row><cell>HD-VILAcap</cell><cell></cell><cell></cell><cell>45.9</cell><cell>73.0</cell><cell>81.8</cell><cell>66.9</cell><cell>44.9</cell><cell>74.4</cell><cell>82.3</cell><cell>67.2</cell></row><row><cell cols="2">HD-VILA sub+cap</cell><cell></cell><cell>49.6</cell><cell>74.5</cell><cell>84.8</cell><cell>69.6</cell><cell>48.2</cell><cell>76.7</cell><cell>84.4</cell><cell>69.8</cell></row><row><cell cols="2">ImageCaption</cell><cell></cell><cell>45.6</cell><cell>70.7</cell><cell>81.1</cell><cell>65.8</cell><cell>43.7</cell><cell>69.5</cell><cell>77.9</cell><cell>63.7</cell></row><row><cell cols="3">HD-VILA sub + ImageCaption</cell><cell>49.1</cell><cell>73.1</cell><cell>83.5</cell><cell>68.6</cell><cell>47.0</cell><cell>75.3</cell><cell>84.1</cell><cell>68.8</cell></row><row><cell cols="11">Table 4. Ablation study of different data. We report text-to-video results of models finetuned on MSR-VTT and Didemo. Mean ? indicates</cell></row><row><cell cols="11">average value of Recall@1, 5 and 10. None indicates no post-pretraining stage. For all results, the model is 4 video proxies added on</cell></row><row><cell cols="8">CLIP-ViT-B/32. All post-pretraining steps are equivalent to one epoch on HD-VILA-100M.</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="4">R@1 ? R@5 ? R@10 ? MdR ?</cell><cell cols="2">Method</cell><cell></cell><cell cols="3">R@1 ? R@5 ? R@10 ? MdR ?</cell></row><row><cell>ClipBERT [21]</cell><cell>22.0</cell><cell>46.8</cell><cell>59.9</cell><cell>6.0</cell><cell cols="3">ClipBERT [21]</cell><cell>20.4</cell><cell>48.0</cell><cell>60.8</cell><cell>6.0</cell></row><row><cell>VLM [45]</cell><cell>28.1</cell><cell>55.5</cell><cell>67.4</cell><cell>4.0</cell><cell cols="2">Frozen [2]</cell><cell></cell><cell>31.0</cell><cell>59.8</cell><cell>72.4</cell><cell>3.0</cell></row><row><cell>MMT [9]</cell><cell>26.6</cell><cell>57.1</cell><cell>69.6</cell><cell>4.0</cell><cell cols="3">HD-VILA [48]</cell><cell>28.8</cell><cell>57.4</cell><cell>69.1</cell><cell>4.0</cell></row><row><cell>Support Set [34]</cell><cell>30.1</cell><cell>58.5</cell><cell>69.3</cell><cell>3.0</cell><cell cols="3">All-in-One [40]</cell><cell>32.7</cell><cell>61.4</cell><cell>73.5</cell><cell>3.0</cell></row><row><cell>Frozen [2]</cell><cell>31.0</cell><cell>59.5</cell><cell>70.5</cell><cell>3.0</cell><cell cols="3">BridgeFormer [11]</cell><cell>37.0</cell><cell>62.2</cell><cell>73.9</cell><cell>3.0</cell></row><row><cell>VideoCLIP [46] HD-VILA [48] Florence [52] All-in-One [40] BridgeFormer [11]</cell><cell>30.9 35.6 37.6 37.9 37.6</cell><cell>55.4 65.3 63.8 68.1 64.8</cell><cell>66.8 78.0 72.6 77.1 75.1</cell><cell>-3.0 --3.0</cell><cell cols="3">CLIP-ViT-B/32 CLIP4Clip [29] CLIP2TV [10] DRL [43] CAMoE* [6]</cell><cell>43.4 45.5 47.9 43.8</cell><cell>70.2 69.7 73.8 71.4</cell><cell>80.6 80.6 82.7 -</cell><cell>2.0 2.0 2.0 -</cell></row><row><cell>CLIP-ViT-B/32</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Ours</cell><cell></cell><cell>48.6</cell><cell>77.1</cell><cell>84.4</cell><cell>2.0</cell></row><row><cell>CLIP4Clip [29]</cell><cell>44.5</cell><cell>71.4</cell><cell>81.6</cell><cell>2.0</cell><cell cols="2">Ours*</cell><cell></cell><cell>53.8</cell><cell>79.6</cell><cell>86.5</cell><cell>1.0</cell></row><row><cell>CenterCLIP [57] XPool [12] CLIP2Video [7] CLIP2Video ? [3] CLIP2TV [10]</cell><cell>44.2 46.9 45.6 47.2 46.1</cell><cell>71.6 72.8 72.6 73.0 72.5</cell><cell>82.1 82.2 81.7 83.0 82.9</cell><cell>2.0 2.0 2.0 2.0 2.0</cell><cell cols="3">CLIP-ViT-B/16 DRL [43] Ours Ours*</cell><cell>49.0 50.5 55.3</cell><cell>76.5 78.4 82.0</cell><cell>84.5 87.1 89.3</cell><cell>2.0 1.0 1.0</cell></row><row><cell>DRL [43]</cell><cell>47.4</cell><cell>74.6</cell><cell>83.8</cell><cell>2.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CAMoE* [6]</cell><cell>47.3</cell><cell>74.2</cell><cell>84.5</cell><cell>2.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell>50.1</cell><cell>74.8</cell><cell>84.6</cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours*</cell><cell>55.9</cell><cell>77.0</cell><cell>86.8</cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CLIP-ViT-B/16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CenterCLIP [57]</cell><cell>48.4</cell><cell>73.8</cell><cell>82.0</cell><cell>2.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CLIP2TV [10]</cell><cell>49.3</cell><cell>74.7</cell><cell>83.6</cell><cell>2.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DRL [43]</cell><cell>50.2</cell><cell>76.5</cell><cell>84.7</cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DRL ? [43]</cell><cell>53.3</cell><cell>80.3</cell><cell>87.6</cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell>54.2</cell><cell>77.2</cell><cell>84.8</cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours*</cell><cell>57.7</cell><cell>80.5</cell><cell>88.2</cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Table 5. Comparison of text-to-video retrieval in MSR-VTT [47].</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">* and  ? respectively denotes that the method uses DSL [6] and</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">QB-Norm [3] as post-processing operations.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">on average of Recall@1, 5, 10. Finally, Auxiliary Data. In this part, we ablate the contribution of</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">large-scale noisy data and auxiliary data. For uni-source,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Method R@1 ?</head><label>R@1</label><figDesc>R@5 ? R@10 ? MdR ?</figDesc><table><row><cell>ClipBERT [21]</cell><cell>21.3</cell><cell>49.0</cell><cell>63.5</cell><cell>6.0</cell></row><row><cell>MMT [9]</cell><cell>28.7</cell><cell>61.4</cell><cell>-</cell><cell>3.3</cell></row><row><cell>Support Set [34]</cell><cell>29.2</cell><cell>61.6</cell><cell>-</cell><cell>3.0</cell></row><row><cell>Frozen [2]</cell><cell>28.8</cell><cell>60.9</cell><cell>-</cell><cell>3.0</cell></row><row><cell>HD-VILA [48]</cell><cell>28.5</cell><cell>57.4</cell><cell>-</cell><cell>4.0</cell></row><row><cell>All-in-One [40]</cell><cell>22.4</cell><cell>53.7</cell><cell>67.7</cell><cell>5.0</cell></row><row><cell>CLIP-ViT-B/32</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CLIP4Clip [29]</cell><cell>40.5</cell><cell>72.4</cell><cell>-</cell><cell>2.0</cell></row><row><cell>CenterCLIP [57]</cell><cell>43.9</cell><cell>74.6</cell><cell>85.8</cell><cell>2.0</cell></row><row><cell>CLIP2TV [10]</cell><cell>44.1</cell><cell>75.2</cell><cell>-</cell><cell>2.0</cell></row><row><cell>DRL [43]</cell><cell>44.2</cell><cell>74.5</cell><cell>86.1</cell><cell>2.0</cell></row><row><cell>CAMoE* [6]</cell><cell>51.0</cell><cell>77.7</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours</cell><cell>51.1</cell><cell>78.4</cell><cell>88.3</cell><cell>1.0</cell></row><row><cell>Ours*</cell><cell>59.1</cell><cell>83.9</cell><cell>91.3</cell><cell>1.0</cell></row><row><cell>CLIP-ViT-B/16</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CenterCLIP [57]</cell><cell>46.2</cell><cell>77.0</cell><cell>87.6</cell><cell>2.0</cell></row><row><cell>DRL [43]</cell><cell>46.2</cell><cell>77.3</cell><cell>88.2</cell><cell>2.0</cell></row><row><cell>Ours</cell><cell>53.4</cell><cell>81.4</cell><cell>90.0</cell><cell>1.0</cell></row><row><cell>Ours*</cell><cell>61.4</cell><cell>85.7</cell><cell>92.6</cell><cell>1.0</cell></row><row><cell cols="5">Table 7. Comparison of text-to-video retrieval in ActivityNet [19].</cell></row><row><cell cols="5">* denotes that the method uses post-processing operations DSL</cell></row><row><cell>[6].</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="4">R@1 ? R@5 ? R@10 ? MdR ?</cell></row><row><cell>MMT [9]</cell><cell>12.9</cell><cell>29.9</cell><cell>40.1</cell><cell>19.3</cell></row><row><cell>Frozen [2]</cell><cell>15.0</cell><cell>30.8</cell><cell>40.3</cell><cell>20.0</cell></row><row><cell>HD-VILA [48]</cell><cell>17.4</cell><cell>34.1</cell><cell>44.1</cell><cell>15.0</cell></row><row><cell>BridgeFormer [11]</cell><cell>17.9</cell><cell>35.4</cell><cell>44.5</cell><cell>15.0</cell></row><row><cell>CLIP-ViT-B/32</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CLIP4Clip [29]</cell><cell>21.6</cell><cell>41.8</cell><cell>49.8</cell><cell>11.0</cell></row><row><cell>CenterCLIP [57]</cell><cell>21.7</cell><cell>39.8</cell><cell>49.8</cell><cell>11.0</cell></row><row><cell>XPool [12]</cell><cell>22.7</cell><cell>42.6</cell><cell>51.2</cell><cell>10.0</cell></row><row><cell>DRL [43]</cell><cell>24.9</cell><cell>45.7</cell><cell>55.3</cell><cell>7.0</cell></row><row><cell>CAMoE* [6]</cell><cell>25.9</cell><cell>46.1</cell><cell>53.7</cell><cell>-</cell></row><row><cell>Ours</cell><cell>25.6</cell><cell>45.3</cell><cell>54.4</cell><cell>8.0</cell></row><row><cell>Ours*</cell><cell>26.0</cell><cell>46.4</cell><cell>54.9</cell><cell>8.0</cell></row><row><cell>CLIP-ViT-B/16</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CLIP4Clip by [57]</cell><cell>24.1</cell><cell>45.0</cell><cell>55.1</cell><cell>8.0</cell></row><row><cell>CenterCLIP [57]</cell><cell>24.2</cell><cell>46.2</cell><cell>55.9</cell><cell>8.0</cell></row><row><cell>DRL [43]</cell><cell>26.5</cell><cell>47.6</cell><cell>56.8</cell><cell>7.0</cell></row><row><cell>Ours</cell><cell>29.4</cell><cell>50.6</cell><cell>59.0</cell><cell>5.0</cell></row><row><cell>Ours*</cell><cell>30.7</cell><cell>51.4</cell><cell>60.6</cell><cell>5.0</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Localizing moments in video with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">G?l Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Bain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cross modal retrieval with querybank normalisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioana</forename><surname>Simion-Vlad Bogolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Croitoru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Albanie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="5194" to="5205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Conceptual 12m: Pushing web-scale image-text pretraining to recognize long-tail visual concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soravit</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3558" to="3568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Uniter: Universal image-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="104" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Improving video-text retrieval by multi-stream corpus alignment and dual softmax loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hezheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.04290</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luhui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.11097</idno>
		<title level="m">Clip2video: Mastering video-text retrieval via image clip</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Violet: End-to-end video-language transformers with masked visual-token modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsu-Jui</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.12681</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-modal transformer for video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Gabeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="214" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Clip2tv: An empirical study on transformer-based methods for video-text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dedan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwei</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.05610</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bridging video-text retrieval with multiple choice questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuying</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xihui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohu</forename><surname>Qie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="16167" to="16176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">X-pool: Cross-modal language-video attention for textvideo retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">No?l</forename><surname>Satya Krishna Gorti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Vouitsis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyvan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksims</forename><surname>Golestan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Animesh</forename><surname>Volkovs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangwei</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5006" to="5015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Open-vocabulary object detection via vision and language knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuye</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weicheng</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Audioclip: Extending clip to image, text and audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Guzhov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Raue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rn</forename><surname>Hees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Dengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="976" to="980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Seeing out of the box: End-toend pre-training for vision-language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yupan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12976" to="12985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Pixel-bert: Aligning image pixels with text by deep multi-modal transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.00849</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Hsuan</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4904" to="4916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Vilt: Visionand-language transformer without convolution or region supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bokyung</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ildoo</forename><surname>Kim</surname></persName>
		</author>
		<idno>PMLR, 2021. 2</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="5583" to="5594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dense-captioning events in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="706" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Less is more: Clipbert for video-and-language learning via sparse sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="7331" to="7341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuejian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11336" to="11344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Align before fuse: Vision and language representation learning with momentum distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramprasaath</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akhilesh</forename><surname>Deepak Gotmare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shafiq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">C H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2021. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Object-semantics aligned pre-training for vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="121" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Use what you have: Video retrieval using representations from collaborative experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>Arsha Nagrani, and Andrew Zisserman</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Univl: A unified video and language pre-training model for multimodal understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaishao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Botian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taroon</forename><surname>Bharti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06353</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Clip4clip: An empirical study of clip for end to end video clip retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaishao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrui</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08860</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Howto100m: Learning a text-video embedding by watching hundred million narrated video clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2630" to="2640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Mokady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Hertz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit H</forename><surname>Bermano</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.09734</idno>
		<title level="m">Clipcap: Clip prefix for image captioning</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Im2text: Describing images using 1 million captioned photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Styleclip: Text-driven manipulation of stylegan imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Patashnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongze</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2085" to="2094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Support-set bottlenecks for video-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandela</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Yao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2021</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note>PMLR, 2021. 1, 2, 3</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Denseclip: Language-guided dense prediction with contextaware prompting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="18082" to="18091" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atousa</forename><surname>Torabi</surname></persName>
		</author>
		<imprint>
			<pubPlace>Marcus Rohrbach, Niket Tandon, Christopher Joseph Pal, H. Larochelle, Aaron C</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Movie description. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="94" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7464" to="7473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">All in one: Exploring unified video-language pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex Jinpeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuying</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanyu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohu</forename><surname>Qie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><forename type="middle">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.07303</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Actionclip: A new paradigm for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiazheng</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.08472</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.03052</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.07111</idno>
		<title level="m">Disentangled representation learning for textvideo retrieval</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Simvlm: Simple visual language model pretraining with weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Vlm: Task-agnostic videolanguage model pre-training for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gargi</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Yao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prahal</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masoumeh</forename><surname>Aminzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.09996</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Videoclip: Contrastive pre-training for zero-shot video-text understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gargi</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Yao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Okhonko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="6787" to="6800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Msr-vtt: A large video description dataset for bridging video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Advancing high-resolution video-language representation with large-scale video transcriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiankai</forename><surname>Hang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5036" to="5045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Probing intermodality: Visual parsing with self-attention for vision-andlanguage pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yupan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="67" to="78" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A joint sequence fusion model for video question answering and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="471" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ling</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuedong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.11432</idno>
		<title level="m">A new foundation model for computer vision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Merlot reserve: Neural script knowledge through vision and language and sound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ximing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanpeng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Salehi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="16375" to="16387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Merlot: Multimodal neural script knowledge models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ximing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Sung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jize</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Cross-modal and hierarchical modeling of video and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Pointclip: Point cloud understanding by clip</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunchang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xupeng</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="8552" to="8562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.00823</idno>
		<title level="m">Centerclip: Token clustering for efficient text-video retrieval</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Actbert: Learning global-local video-text representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8746" to="8755" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
