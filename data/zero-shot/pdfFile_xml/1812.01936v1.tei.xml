<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Stacked Dense U-Nets with Dual Transformers for Robust Face Alignment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">InsightFace Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IBUG Imperial College London London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niannan</forename><surname>Xue</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IBUG Imperial College London London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IBUG Imperial College London London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Stacked Dense U-Nets with Dual Transformers for Robust Face Alignment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>JIA GUO, JIANKANG DENG: STACKED DENSE U-NETS WITH DUAL TRANSFORMERS 1 https://ibug.doc.ic.ac.uk/people/nxue https://wp.doc.ic.ac.uk/szafeiri/</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Facial landmark localisation in images captured in-the-wild is an important and challenging problem. The current state-of-the-art revolves around certain kinds of Deep Convolutional Neural Networks (DCNNs) such as stacked U-Nets and Hourglass networks. In this work, we innovatively propose stacked dense U-Nets for this task. We design a novel scale aggregation network topology structure and a channel aggregation building block to improve the model's capacity without sacrificing the computational complexity and model size. With the assistance of deformable convolutions inside the stacked dense U-Nets and coherent loss for outside data transformation, our model obtains the ability to be spatially invariant to arbitrary input face images. Extensive experiments on many in-the-wild datasets, validate the robustness of the proposed method under extreme poses, exaggerated expressions and heavy occlusions. Finally, we show that accurate 3D face alignment can assist pose-invariant face recognition where we achieve a new stateof-the-art accuracy on CFP-FP (98.514%).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Facial landmark localisation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref> in unconstrained recording conditions has recently received considerable attention due to wide applications such as humancomputer interaction, video surveillance and entertainment. 2D and 3D 1 in-the-wild face alignments are very challenging as facial appearance can change dramatically due to extreme poses, exaggerated expressions and heavy occlusions.</p><p>The current state-of-the-art 2D face alignment benchmarks <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b37">38]</ref> revolve around applying fully-convolutional neural networks to predict a set of landmark heatmaps, where for a given heatmap, the network predicts the probability of a landmark's presence at each c 2018. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms. * denotes equal contribution to this work. <ref type="bibr" target="#b0">1</ref> In this paper, the 3D facial landmarks refer to the 2D projections of the real-world 3D landmarks, which can preserve face structure and semantic consistency across extreme pose variations. and every pixel. Since the heatmap prediction for face alignment is essentially a dense regression problem, (1) rich features representations that span resolutions from low to high, and <ref type="bibr" target="#b1">(2)</ref> skip connections that preserve spatial information at each resolution, are extensively investigated to combine multi-scale representations to improve inference of where and what <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31]</ref>. In fact, the most recent state-of-the-art performance in 2D face alignment has been held for a while <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b37">38]</ref> and is also believed to be saturated <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> by the stacked Hourglass models <ref type="bibr" target="#b25">[26]</ref>, which repeat resolution-preserved bottom-up and top-down processing in conjunction with intermediate supervision.</p><p>Although lateral connections can consolidate multi-scale feature representations in Hourglass, these connections are shallow themselves due to simple one-step operations. Deep layer aggregation (DLA) <ref type="bibr" target="#b35">[36]</ref> augments shallow lateral connections with deeper aggregations to better fuse information across layers. We further add the down-sampling paths for the aggregation nodes in DLA and create a new Scale Aggregation Topology (SAT) for network design. Following the same insight in the network topology structure, we propose a Channel Aggregation Block (CAB). The decreasing channel in CAB helps to increase contextual modelling, which incorporates global landmark relationships and increases robustness when local observation is blurred. By combining SAT and CAB, we create the network structure designated dense U-Net. Nevertheless, the computation complexity and model size of the proposed dense U-Net dramatically increases and there is optimisation difficulty during model training especially when the training data is limited. Therefore, we further simplify the dense U-net by removing one down-sampling step as well as substituting some normal convolutions with deep-wise separable convolutions and direct lateral connections. Finally, the simplified dense U-net maintains similar computational complexity and model size as Hourglass, but significantly improves the model's capacity.</p><p>Even though stacked dense U-Nets have a high capacity to predict the facial landmark heatmaps, they are still limited by the lack of ability to be spatially invariant to the input face images. Generally, the capability of modelling geometric transformations comes from deeper network design for transformation-invariant feature learning and extensive data augmentation. For transformation-invariant feature learning, Spatial Transform Networks (STN) <ref type="bibr" target="#b20">[21]</ref> is the first work to learn spatial transformation from data by warping the feature map via a global parametric transformation. However, such warping is expensive due to additional calculation on explicit parameter estimation. By contrast, deformable convolution <ref type="bibr" target="#b5">[6]</ref> replaces the global parametric transformation and feature warping with a local and dense spatial sampling by additional offsets learning, thus introduces an extremely light-weight spatial transformer. For data augmentation, Honari et al. <ref type="bibr" target="#b16">[17]</ref> have explored a semi-supervised learning technique for face alignment based on having a model predict equivariant landmarks with respect to transformations applied to the image. Similar idea can be found in <ref type="bibr" target="#b32">[33]</ref>, where mirror-ability, the ability of a model to produce symmetric results in mirrored images, is explored to improve face alignment. Inspired by these works, we innovatively introduce dual transformers into the stacked dense U-Nets. As illustrated in <ref type="figure">Fig. 1</ref>, inside the network, we employ deformable convolution to enhance transformation-invariant feature learning. Outside the network, we design a coherent loss for arbitrary transformed inputs, enforcing the model's prediction to be consistent with different transformations that are applied to the image. With the joint assistance of deformable convolution and coherent loss, our model obtains the ability to be spatially invariant to the arbitrary input face images.</p><p>In conclusion, our major contributions can be summarised as follows:</p><p>? We propose a novel scale aggregation network topological structure and a channel aggregation building block to improve the model's capacity without obviously increasing <ref type="figure">Figure 1</ref>: Stacked dense U-Nets with dual transformers for robust facial landmark localisation. We stack two dense U-Nets, each followed by a deformable convolution layer, as the network backbone. The input of the network is one face image together with its affine or flip transformed counterpart. The loss includes heatmap discrepancy between the prediction and ground truth as well as two predictions before and after transformation. computational complexity and model size. ? With the joint assistance of a deformable convolution inside the stacked dense U-Nets and coherent loss for outside data transformation, our model obtains the ability to be spatially invariant to the arbitrary input face images. ? The proposed method creates new state-of-the-art results on five in-the-wild face alignment benchmarks, IBUG <ref type="bibr" target="#b27">[28]</ref>, COFW <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15]</ref>, 300W-test <ref type="bibr" target="#b27">[28]</ref>, Menpo2D-test <ref type="bibr" target="#b37">[38]</ref> and AFLW2000-3D <ref type="bibr" target="#b39">[40]</ref>. ? Assisted by the proposed 3D face alignment model, we make a breakthrough in the pose-invariant face recognition with the verification accuracy at 98.514% on CFP-FP <ref type="bibr" target="#b29">[30]</ref>.</p><p>2 Dense U-Net</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Scale Aggregation Topology</head><p>The essence of topology design for heatmap regression is to capture local and global features at different scales, while preserving the resolution information simultaneously. As illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>(a) and 2(b), the topology of the U-Net <ref type="bibr" target="#b26">[27]</ref> and Hourglass <ref type="bibr" target="#b25">[26]</ref> are both symmetric with four steps of pooling. At each down-sampling step, the network branches off the high resolution features, which are later combined into the corresponding up-sampling features. By using skip layers, U-Net and Hourglass can easily preserve spatial information at each resolution. Hourglass is similar to U-Net except for the extra convolutional layers within the lateral connections. To improve the model's capacity, DLA ( <ref type="figure" target="#fig_0">Fig. 2(c)</ref>) iteratively and hierarchically merges the feature hierarchy with additional aggregation nodes within the lateral connections. Inspired by DLA, we further propose a Scale Aggregation Topology (SAT) ( <ref type="figure" target="#fig_0">Fig. 2(d)</ref>) by adding down-sampling inputs for aggregation nodes. The proposed SAT sets up a directed acyclic convolutional graph to aggregate multi-scale features for the pixel-wise heatmap prediction. However, the computation complexity and model size of SAT significantly builds up and the aggregation of three scale signals poses optimisation difficulty during model training especially when the training data is limited. To this end, we remove one step of pooling ( <ref type="figure" target="#fig_0">Fig. 2(e)</ref>), thus the lowest resolution is 8 ? 8 pixels. In addition, we further remove some inner down-sampling aggregation paths and change some normal convolutions into depth-wise separable convolutions <ref type="bibr" target="#b17">[18]</ref> and lateral connections <ref type="bibr" target="#b26">[27]</ref> as shown in <ref type="figure" target="#fig_0">Fig. 2(f)</ref>. Finally, the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Channel Aggregation Block</head><p>The original Hourglass <ref type="bibr" target="#b25">[26]</ref> employs the bottleneck residual block ( <ref type="figure" target="#fig_1">Fig. 3(a)</ref>). To improve the block's capacity, a parallel and multi-scale inception residual block is explored in <ref type="bibr" target="#b11">[12]</ref> ( <ref type="figure" target="#fig_1">Fig. 3(b)</ref>). Meanwhile, a novel hierarchical, parallel and multi-scale (HPM) residual block is extensively investigated in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>  <ref type="figure" target="#fig_1">(Fig. 3(c)</ref>). For the building block design, we follow the same insight in the network topology and innovatively propose a Channel Aggregation Block (CAB). As shown in <ref type="figure" target="#fig_1">Fig.3(d)</ref>, CAB is symmetric in channel while SAT is symmetric in scale. The input signals branch off before each channel decrease and converge back before each channel increase to maintain the channel information. Channel compression in the backbone can help contextual modelling <ref type="bibr" target="#b18">[19]</ref>, which incorporates channel-wise heatmap relationships and increases robustness when local observation is blurred. To control the computational complexity and compress the model size, depth-wise separable convolutions <ref type="bibr" target="#b17">[18]</ref> and replication-based channel extensions are employed within CAB.</p><p>3 Dual Transformer</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Inside Transformer</head><p>We further improve the model's capacity by stacking two U-Nets end-to-end <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">26]</ref>, feeding the output of the first U-Net as input into the second U-Net. Stacked U-Nets with intermediate supervision <ref type="bibr" target="#b25">[26]</ref> provide a mechanism for repeated bottom-up, top-down inference allowing for re-evaluation and re-assessment of local heatmap predictions and global spatial configurations. However, stacked U-Nets still lack the transformation modelling capac- ity due to the fixed geometric structures. Here, we consider two different kinds of spatial transformers: parameter explicit transformation by STN <ref type="bibr" target="#b20">[21]</ref> and parameter implicit transformation by deformable convolution <ref type="bibr" target="#b5">[6]</ref>. In <ref type="figure" target="#fig_2">Fig. 4(a)</ref>, we employ the STN to remove the discrepancy of rigid transformation (e.g. translation, scale and rotation) on the input face image, thus the following stacked U-Nets only need to focus on the non-rigid face transformation. Since the variance of the regression target is obviously decreased, the accuracy of face alignment can be easily improved. In <ref type="figure" target="#fig_2">Fig. 4(b)</ref>, the application of deformable convolution behaves the similar way. Nonetheless, the deformable convolution augments the spatial sampling locations by learning additional offsets in a local and dense manner instead of adopting a parameter explicit transformation or warping. In this paper, we employ the deformable convolution as the inside transformer which is not only more flexible to model geometric face transformations but also has higher computation efficiency. During training, data augmentation by random affine transformation on the input images is widely used to enhance the transformation modelling capacity. Nevertheless, the output heatmaps are not always coherent when there is affine or flip transformation on the input images <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b32">33]</ref>. As illustrated in <ref type="figure" target="#fig_3">Fig. 5</ref>(a) and 5(b), there are some obvious local differences between the heatmaps predicted from the original and transformed face images. We explore an outside transformer with an additional loss constraint, which encourages the regression network to output coherent landmarks when there are rotation, scale, translation and flip transformations applied to the images. More specifically, we transform an image during training and enforce the model to produce landmarks that are similarly transformed. Our model is trained end-to-end to minimise the following loss function </p><p>where N is the landmark number, I is the input image, G(I) is the ground truth, H(I) is the predicted heatmaps, T is the affine or flip transformation, and ? is the weight to balance two losses (in <ref type="figure">Fig. 1):</ref> (1) the difference between the prediction and ground truth; and (2) the difference between two predictions before and after transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data</head><p>For the training of 2D face alignment, we collate the training sets of the 300W challenge <ref type="bibr" target="#b27">[28]</ref> and the Menpo2D challenge <ref type="bibr" target="#b37">[38]</ref>. The 300W-train dataset consists of the LFPW, Helen and AFW datasets. The Menpo2D-train dataset consists of 5,658 semi-frontal face images, which are selected from FDDB and ALFW. Hence, a total of 9,360 face images are used to train the 2D68 face alignment model. We extensively test the proposed 2D face alignment method on four image datasets: the IBUG dataset (135 images) <ref type="bibr" target="#b27">[28]</ref>, the COFW dataset (507) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15]</ref>, the 300W-test dataset (600) <ref type="bibr" target="#b27">[28]</ref>, and the Menpo2D-test dataset (5,535) <ref type="bibr" target="#b37">[38]</ref>. For the training of 3D face alignment, we utilise the 300W-LP dataset <ref type="bibr" target="#b39">[40]</ref>, which contains 61,225 synthetic face images. The 300W-LP is generated by profiling and rendering the faces of 300-W <ref type="bibr" target="#b27">[28]</ref> into larger poses (ranging from 90 ? to 90 ? ). We test the proposed 3D face alignment method on the AFLW2000-3D dataset (2,000) <ref type="bibr" target="#b39">[40]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training Details</head><p>Each face region is cropped and scaled to 128 ? 128 pixels based on the face boxes <ref type="bibr" target="#b38">[39]</ref>. We augment the ground truth image with a random combination of horizontal flip, rotation (+/-40 degrees), and scaling (0.8 -1.2). The network starts with a 3 ? 3 convolutional layer, followed by a residual module and a round of max pooling to bring the resolution down from 128 to 64, as it could reduce GPU memory usage while preserving alignment accuracy. The network is trained using MXNet with Nadam optimiser, an initial learning rate of 2.5 ?4 , a batch size of 40, and 30k learning steps. We drop the learning rate by a ratio of 0.2 after 16k and 24k iterations. Each training step paralleled on two NVIDIA GTX Titan X (Pascal) takes 1.233s. Although the Mean Squared Error (MSE) pixel-wise loss is given in Eq. 1, in practice we find the Sigmoid Cross-Entropy (CE) pixel-wise loss <ref type="bibr" target="#b0">[1]</ref> outperforms the MSE loss for L p?g . Therefore, we employ the CE loss for L p?g and the MSE loss for L p?p , respectively. ? is empirically set as 0.001 to guarantee convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Experiments</head><p>In Tab. 1, we compare the alignment accuracy on the most challenging datasets (IBUG and COFW) under different training settings. We denote each training strategy by topology stackblock (? ? down-sampling steps, 4 by default). From Tab. 1, we can draw the following conclusions: (1) Compared to a single Hourglass network, a stack of two Hourglass networks can significantly improve the alignment accuracy even though the model size and inference time have been doubled; (2) Building blocks, such as Inception-Resnet, HPM <ref type="bibr" target="#b1">[2]</ref> or CAB, can progressively improve the performance with similar model size and computation cost; (3) The performance gap is not apparent between three and four down-sampling steps, but three down-sampling steps can remarkably decrease the model size; (4) As the complexity of the network topology increases from U-Net, to Hourglass and to DLA, the localisation accuracy gradually raises; (5) Due to limited training data (? 10k) and the optimisation difficulty with SAT (I) <ref type="figure" target="#fig_0">(Fig. 2(d)</ref>), e.g. aggregation of three scale signals, the performance obviously drops. By removing some downsampling paths, introducing depth-wise separable convolutions and applying direct lateral convolutions <ref type="figure" target="#fig_0">(Fig. 2(f)</ref>), the performance bounces back and eventually surpasses Hourglass and DLA; (6) Deformable convolution outperforms STN even with much fewer parameters; <ref type="bibr" target="#b6">(7)</ref> Outside transformer with coherent loss can also evidently reduce the alignment error; <ref type="bibr" target="#b7">(8)</ref> The proposed stacked dense U-Nets with dual transformers transcend recent state-of-the-art methods on the IBUG (7.02% CVPR18 <ref type="bibr" target="#b13">[14]</ref>) and COFW (5.77% CVPR18 <ref type="bibr" target="#b21">[22]</ref>) datasets without bells and whistles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">2D and 3D Face Alignment Results</head><p>For 2D face alignment, we further report the Cumulative Error Distribution (CED) curves on three standard benchmarks, that is COFW, 300W-test, Menpo2D-test. On COFW <ref type="bibr" target="#b2">[3]</ref> ( <ref type="figure" target="#fig_7">Fig. 6(a)</ref>), our method outperforms the baseline methods in the evaluation toolkit by a prominent margin. The normalised mean error of 5.55% as well as the final success rate at 98.22% are so impressive that the challenge of face alignment under occlusion is even no longer remarkable for our method. In <ref type="figure" target="#fig_8">Fig. 7(b)</ref>, we give some fitting examples on COFW under heavy occlusions. Even by zooming in on these visualised results, we can hardly find any alignment flaw, which indicates that the proposed method can easily capture and consolidate local evidence and global context, and thus improve the model's robustness under occlusions.    On 300W-test <ref type="bibr" target="#b27">[28]</ref>, we compare our method with leading results, such as Deng et al. <ref type="bibr" target="#b6">[7]</ref> and Fan et al. <ref type="bibr" target="#b12">[13]</ref>. Besides, we also compare with the state-of-the-art face alignment method "DenseReg + MDM" <ref type="bibr" target="#b15">[16]</ref>. Once again, our model surpasses those methods with ease. On Menpo2D-test <ref type="bibr" target="#b36">[37]</ref>, we send our alignment results to the organiser and get the CED curves with other best four entries of the competition <ref type="bibr" target="#b37">[38]</ref>. As shown in <ref type="figure" target="#fig_7">Fig. 6(c)</ref>, we find our performance is inferior to the best entry <ref type="bibr" target="#b34">[35]</ref> within the high accurate interval (NME &lt; 1.2%) because our alignment model is initialised from MTCNN <ref type="bibr" target="#b38">[39]</ref> which is less accurate and stable than the detectors applied in <ref type="bibr" target="#b34">[35]</ref>. Nevertheless, our model gradually outperforms the best entry, which indicates that our model is more robust under hard cases, such as large pose variations, exaggerated expressions and heavy occlusions.  <ref type="table">Table 2</ref>: 3D alignment results on the AFLW2000-3D dataset. Performance is reported as the bounding box size normalised mean error <ref type="bibr" target="#b39">[40]</ref>.</p><p>For 3D face alignment, we compare our model on AFLW2000-3D <ref type="bibr" target="#b39">[40]</ref> with the most recent state-of-the-art method proposed by Bulat et al. <ref type="bibr" target="#b0">[1]</ref>, which claimed that the problem of face alignment is almost solved with saturated performance. Nonetheless, our method further decreases the NME by 5.8%. In <ref type="figure" target="#fig_8">Fig. 7(e)</ref>, we give some exemplary alignment results, which demonstrate successful 3D face alignment under extreme poses (ranging from ?90 ? to 90 ? ), accompanied by exaggerated expressions and heavy occlusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">3D Face Alignment Improves Face Recognition</head><p>Even though face alignment is not claimed to be essential for deep face recognition <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b28">29]</ref>, the face normalisation step is still widely applied in recent state-of-the-art recognition methods <ref type="bibr" target="#b10">[11]</ref>. Following the setting of ArcFace <ref type="bibr" target="#b10">[11]</ref>, we train recognition models under different face alignment methods on VGG2 <ref type="bibr" target="#b3">[4]</ref>. As we can see from Tab. 3, the verification performance on LFW <ref type="bibr" target="#b19">[20]</ref> is comparatively close. However, on CFP-FP <ref type="bibr" target="#b29">[30]</ref>, the proposed 3D alignment method obviously decreases the versification error by 48.24% compared to the alignment method proposed in <ref type="bibr" target="#b38">[39]</ref>. The significant improvement implies that accurate full-pose face alignment can hugely assist deep face recognition.  <ref type="table">Table 3</ref>: Face verification accuracy (%) on the LFW and CFP-FP dataset (ArcFace, LResNet50E-IR@VGG2-LFW-CFP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose stacked dense U-Nets with dual transformers for robust 2D and 3D facial landmark localisation. We introduce a novel network structure (Scale Aggregation Topology) and a new building block (Chanel Aggregation Block) to improve the model's capacity without sacrificing computational complexity and model size. With the assistance of deformable convolution and coherent loss, our model obtains the ability to be spatially invariant to the input face images. Extensive experiments on five challenging face alignment datasets demonstrate the robustness of the proposed alignment method. The additional face recognition experiment suggests that the proposed 3D face alignment can obviously improve pose-invariant face recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgement</head><p>J. Deng is supported by the President's Scholarship of Imperial College London. This work is also funded by the EPSRC project EP/N007743/1 (FACER2VM) and the European Community Horizon 2020 [H2020/2014-2020] under grant agreement no. 688520(TeSLA). We thank the NVIDIA Corporation for the GPU donations. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>(a) U-Net (b) Hourglass (c) DLA (d) SAT (I) (e) SAT (II) (f) SAT (III) Different network topologies. SAT can capture local and global features and preserve spatial information by multi-scale information aggregation. simplified SAT maintains similar computational complexity and model size as Hourglass, but significantly improves the model's capacity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Different building blocks. CAB can enhance contextual modelling by channel compression and aggregation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Inside transformer comparison: STN v.s. Deformable Convolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Heatmap incoherence under affine and flip transformation applied on the input images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>2 L 2 L p? g1 + 2 L</head><label>22g12</label><figDesc>n (T I) ? T H n (I) 2 p?p + H n (I) ? G n (I) 2 H n (T I) ? T G n (I) 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>et al Z. He et al W. Wu et al J. Deng et al Our Method (c) Menpo2D-test</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Landmark localisation results on the COFW, 300W-test, Menpo2D-test datasets. Performance is reported as mean error normalised by the eye centre distance (COFW), the out eye corner distance (300W-test), and the diagonal of the ground truth bounding box (Menpo2D-test), respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Example results of 2D and 3D face alignment. The proposed method is robust under pose, expression, occlusion and illumination variations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>Method</cell><cell cols="4">IBUG(%) COFW(%) Size (mb) Time (ms)</cell></row><row><cell>Hourglass 1 -Resnet [26]</cell><cell>7.32</cell><cell>6.26</cell><cell>13</cell><cell>26</cell></row><row><cell>Hourglass 2 -Resnet [26]</cell><cell>7.22</cell><cell>6.18</cell><cell>26</cell><cell>49</cell></row><row><cell>Hourglass 2 -Inception-Resnet</cell><cell>7.07</cell><cell>6.08</cell><cell>38</cell><cell>57</cell></row><row><cell>Hourglass 2 -HPM [2]</cell><cell>6.98</cell><cell>5.81</cell><cell>48</cell><cell>47</cell></row><row><cell>Hourglass 2 -CAB</cell><cell>6.93</cell><cell>5.77</cell><cell>46</cell><cell>52</cell></row><row><cell>Hourglass 2 -HPM (? ?3) [2]</cell><cell>6.95</cell><cell>5.82</cell><cell>38</cell><cell>39</cell></row><row><cell>Hourglass 2 -CAB (? ?3)</cell><cell>6.91</cell><cell>5.78</cell><cell>37</cell><cell>41</cell></row><row><cell>U-Net 2 -CAB</cell><cell>7.17</cell><cell>6.12</cell><cell>36</cell><cell>37</cell></row><row><cell>Hourglass 2 -CAB</cell><cell>6.93</cell><cell>5.77</cell><cell>46</cell><cell>52</cell></row><row><cell>DLA 2 -CAB</cell><cell>6.92</cell><cell>5.75</cell><cell>103</cell><cell>61</cell></row><row><cell>SAT (I) 2 -CAB</cell><cell>7.05</cell><cell>5.91</cell><cell>131</cell><cell>63</cell></row><row><cell>SAT (II) 2 -CAB (? ?3)</cell><cell>7.02</cell><cell>5.89</cell><cell>83</cell><cell>47</cell></row><row><cell>SAT (III) 2 -CAB (? ?3)</cell><cell>6.88</cell><cell>5.74</cell><cell>38</cell><cell>41</cell></row><row><cell>DenseU-Net + STN</cell><cell>6.81</cell><cell>5.70</cell><cell>116</cell><cell>49</cell></row><row><cell>DenseU-Net + Inside Transformer</cell><cell>6.77</cell><cell>5.63</cell><cell>49</cell><cell>45</cell></row><row><cell>DenseU-Net + Outside Transformer</cell><cell>6.80</cell><cell>5.62</cell><cell>38</cell><cell>41</cell></row><row><cell>DenseU-Net + Dual Transformer</cell><cell>6.73</cell><cell>5.55</cell><cell>49</cell><cell>45</cell></row></table><note>Ablation study for different settings on the IBUG and COFW datasets. Performance is reported as the eye centre distance normalised mean error.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">JIA GUO, JIANKANG DENG: STACKED DENSE U-NETS WITH DUAL TRANSFORMERS3.2 Outside Transformer</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">JIA GUO, JIANKANG DENG: STACKED DENSE U-NETS WITH DUAL TRANSFORMERS</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Binarized convolutional landmark localizers for human pose estimation and face alignment with limited resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">How far are we from solving the 2d &amp; 3d face alignment problem?(and a dataset of 230,000 3d facial landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Robust face landmark estimation under occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xavier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Burgos-Artizzu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1513" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Vggface2: A dataset for recognising faces across pose and age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Face alignment by explicit shape regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2887" to="2894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">M 3 csr: multi-view, multi-scale and multi-component cascade shape regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingshan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IVC</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="19" to="26" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Joint multiview face alignment in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.06023</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Marginal loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Uv-gan: Adversarial facial uv map completion for pose-invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niannan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07698</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cascade multiview hourglass model for robust 3d face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In FG</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Approaching human level facial landmark localization by deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqiang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erjin</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IVC</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="27" to="35" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Wing loss for robust facial landmark localisation with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Zhen-Hua Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrik</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Jun</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fowlkes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.08347</idno>
		<title level="m">Occlusion coherence: Detecting and localizing occluded faces</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Densereg: Fully convolutional dense shape regression in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>R?za Alp G?ler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Epameinondas</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snape</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Stefanos Zafeiriou, and Iasonas Kokkinos</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Improving landmark localization with semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Disentangling 3d pose in a dendritic cnn for unconstrained 2d face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dual sparse constrained cascade regression for robust face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingshan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="700" to="712" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adaptive cascade regression model for robust face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingshan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangcan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="797" to="807" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Georgios Tzimiropoulos, Stefanos Zafeiriou, and Maja Pantic. 300 faces in-the-wild challenge: Database and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Epameinondas</forename><surname>Antonakos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IVC</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="3" to="18" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Frontal to profile face verification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumyadip</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Cheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Beyond skip connections: Top-down modulation for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.06851</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>De La Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="532" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mirror, mirror on the wall, tell me, is the error small</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Patras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Facial shape tracking via spatio-temporal cascade shape regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingshan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="41" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Stacked hourglass network for robust facial landmark localisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingshan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<title level="m">Deep layer aggregation. CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The 3d menpo facial landmark tracking challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grigorios</forename><surname>Chrysos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasios</forename><surname>Roussos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Ververas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The menpo facial landmark localisation challenge: A step towards the solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grigorios</forename><surname>Chrysos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Face alignment across large poses: A 3d solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="146" to="155" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
