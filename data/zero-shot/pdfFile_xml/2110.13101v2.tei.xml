<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Latent-Insensitive Autoencoders for Anomaly Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">S</forename><surname>Battikh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">System and Computer Engineering Department</orgName>
								<orgName type="institution">Al-Azhar University</orgName>
								<address>
									<settlement>Cairo</settlement>
									<country key="EG">Egypt</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><forename type="middle">A</forename><surname>Lenskiy</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">The Australian National University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Latent-Insensitive Autoencoders for Anomaly Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Additional Key Words and Phrases: Anomaly Detection</term>
					<term>autoencoders</term>
					<term>One- Class Classification</term>
					<term>Principal Components Analysis</term>
					<term>Self-Taught Learning</term>
					<term>Negative Examples</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Reconstruction-based approaches to anomaly detection tend to fall short when applied to complex datasets with target classes that possess high inter-class variance <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22]</ref>. Similar to the idea of self-taught learning <ref type="bibr" target="#b22">[23]</ref> used in transfer learning, many domains are rich with similar unlabeled datasets that could be leveraged as a proxy for out-of-distribution samples. In this paper we introduce Latent-Insensitive autoencoder (LIS-AE) where unlabeled data from a similar domain is utilized as negative examples to shape the latent layer (bottleneck) of a regular autoencoder such that it is only capable of reconstructing one task. We provide theoretical justification for the proposed training process and loss functions along with an extensive ablation study highlighting important aspects of our model. We test our model in multiple anomaly detection settings presenting quantitative and qualitative analysis showcasing the significant performance improvement of our model for anomaly detection tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Anomaly detection is a classical machine learning field which is concerned with the identification of in-distribution and out-ofdistribution samples that finds applications in numerous fields <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b33">34]</ref>. Unlike traditional multi-label classification where the goal is to find decision boundaries between classes present in a given dataset, the goal of anomaly detection is to find one-versus-all boundaries for classes that are not in the dataset which is significantly more challenging compared to standard classification. Autoencoders <ref type="bibr" target="#b2">[3]</ref> have been used extensively for anomaly detection <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref> under the assumption that reconstruction error incurred by anomalies is higher than that of normal samples <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b35">36]</ref>. However, it has been observed that this assumption might not hold as standard autoencoders might generalize so well even for anomalies <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b35">36]</ref>. In practice, this issue becomes more relevant in two important settings, namely, when the normal data is relatively complex it requires high latent dimension for good reconstitution, and when anomalies share similar compositional features and are from a close domain to the normal data <ref type="bibr" target="#b6">[7]</ref>. To mitigate these issues we present Latent-Insensitive autoencoder (LIS-AE), a new class of autoencoders where the training process is carried out in two phases. In the first phase, the model simply reconstructs the input as a standard autoencoder, in the second phase the entire model except the latent layer is "frozen". We then train the model in such a way that forces the latent layer to only keep reconstructing the target task. We use the concept of a negative dataset from one-class classification <ref type="bibr" target="#b30">[31]</ref> whereby an auxiliary dataset of non-examples from similar domains is used as a proxy for out-of-distribution samples. We change the training objective such that the autoencoder keeps its low reconstruction error for the target dataset while pushing the error of the negative dataset to exceed certain value. In some cases, minimizing and maximizing the reconstruction loss at the same time becomes contradictory, especially for negative  classes that are very similar to the target class. To resolves this issue we introduce another variant with modified first phase loss that ensures that the input of the latent layer is linearly-separable for positive and negative examples during the second phase. This linearly-separable variant (LinSep-LIS-AE) almost always performs better than directly using LIS-AE. Details of architecture, training process, theoretical analysis, and experiments are discussed in detail in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Many reconstruction-based anomaly detection approaches have been proposed starting with classical methods such as PCA <ref type="bibr" target="#b20">[21]</ref>. Robust-PCA mitigates the issue of outlier sensitivity in PCA by decomposing the data matrix into a sum of two low-rank and sparse matrices using nuclear norm and 1 norms as convex relaxation for the objective loss <ref type="bibr" target="#b4">[5]</ref>. autoencoders address the issue of PCA only considering linear relations in feature-space by introducing nonlinearities benefiting from multiple layers of representations <ref type="bibr" target="#b3">[4]</ref>. We elaborate further on the shortcomings of PCA and autoencoders in the theoretical section and use that to motivate our approach.</p><p>Other methods try to improve on base autoencoders by endowing the latent code with particular properties. In the case of VAE <ref type="bibr" target="#b1">[2]</ref>, it does so by having the latent code to follow a prior distribution (usually normal) which also allows sampling from the decoder. However, in the context of anomaly detection, it introduces scaling issues since minimizing KL-Divergence for high latent dimensions required for complex tasks is quite challenging. Another approach is Replicator Neural Networks (RepNN) <ref type="bibr" target="#b11">[12]</ref> which is an autoencoder with a staircase activation function positioned on the output arXiv:2110.13101v2 <ref type="bibr">[cs.</ref>LG] 14 Nov 2021 of the bottleneck layer (Latent Layer). This is mainly used in order to quantize the latent code into a number of discrete values which also aids in forming clusters <ref type="bibr" target="#b31">[32]</ref>. Unfortunately, a discrete staircase function is non-differentiable which prevents learning via backpropagation. Instead, a differentiable approximation involving the sum of hyperbolic tangent functions tanh was introduced in place of the otherwise, non-differentiable discrete staircase function. However, as discussed in <ref type="bibr" target="#b29">[30]</ref>, despite the theoretical appeal for having a quantized latent code via smooth approximation, in practice, having such activation function makes it significantly difficult for the gradient signal to flow. We also note that increasing the number of levels using the aforementioned , tanh sum approximation presents a significant overhead during training and testing since activation functions have to be computed for each batch, moreover, it suffered from scaling issues similar to that of VAE. Another approach is memorizing normality of a given dataset using Memory-augmented autoencoder <ref type="bibr" target="#b8">[9]</ref>. This approach limits the effective space of possible latent codes by constructing a memory module that takes in the output of the encoder as an address and passes to the decoder the most relevant memory items from a stored reservoir of prototypical patterns that have been learned during training. Other non-reconstruction-based approaches include One-Class classification which is tightly connected to anomaly detection in the sense that both problems are concerned with finding one-versus-all boundaries. One-Class SVM is a variation of the classical SVM algorithm <ref type="bibr" target="#b7">[8]</ref> where the objective is to find a hyper-plane that best separates samples from outliers <ref type="bibr" target="#b26">[27]</ref>. Support Vector Data Description (SVDD) <ref type="bibr" target="#b28">[29]</ref> tries to find a circumscribing hyper-sphere that contains all samples while having optimal margin for outliers. It is worth noting that for kernels where ( , ) = 1 such as RBF and Laplacian, OC-SVM and OC-SVDD learn identical decision functions <ref type="bibr" target="#b15">[16]</ref>. To address the lack of representation learning and bad computational scalability of OC-SVM and OC-SVDD, Deep SVDD (OC-DSVDD) employs a deep neural network that learns useful representation while mapping outputs to a hypersphere of minimum volume <ref type="bibr" target="#b23">[24]</ref>. However, due to its sole reliance on optimizing for minimum volume, this approach is prone to hyper-sphere collapse which leads to finding uninformative features <ref type="bibr" target="#b21">[22]</ref>.</p><p>Other approaches have been proposed where an auxiliary datset of non-examples (negative dataset) is drawn from similar domains as a proxy for the otherwise intractable complement for the target class. In <ref type="bibr" target="#b30">[31]</ref>, a collection called the "Universum", allows learning useful representation to the domain of the problem via maximizing the number of contradictions on an equivalence class. Similar to OC-DSVDD, <ref type="bibr" target="#b21">[22]</ref> leverages a labeled dataset from a close domain to fine-tune pre-trained two CNNs in order to learn new good features. The goodness of these features is quantified by the compactness (inter-class variance) for the target class and descriptiveness (crossentropy) for the labeled dataset. Despite avoiding hyper-sphere collapse and outperforming OC-SVDD, this approach requires two pre-trained neural networks and a large labeled dataset along with the target dataset. Another approach that also makes use of a large auxiliary dataset is Outlier Exposure (OE) <ref type="bibr" target="#b12">[13]</ref>, which is a supervised approach that trains a standard neural network classifier while exposing it to a diverse set of non-examples on which the output of the classifier is optimized to follow a uniform distribution using another cross-entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED METHOD 3.1 Architecture</head><p>An undercomplete deep autoencoder is a type of unsupervised feedforward neural network for learning a lower-dimensional feature representation of a particular dataset via reconstructing the input back at the output layer. To prevent autoencoders from converging to trivial solutions such as the identity mapping; a bottleneck layer with output z such that its dimension is less than the dimension of the input x. The forward pass is computed as such:</p><formula xml:id="formula_0">s = ?(x),<label>(1)</label></formula><formula xml:id="formula_1">z = (s),<label>(2)</label></formula><formula xml:id="formula_2">x = (z),<label>(3)</label></formula><p>where x is the input, is the bottleneck layer, ? and are convolutional neural networks representing the encoder and the decoder modules respectively. Typically, such models are trained to minimize the L 2 -norm of the difference between the input and the reconstructed output ?x ? x? 2 . As previously discussed, the choice of the activation function of z plays an important rule in anomaly detection. Activation functions that quantize the latent code or encourage forming clusters are preferable. In our experiments, we find that confining the latent code to have values between [?1, 1] with a tanh activatin function as we maximize the loss over the negative dataset during the latent-shaping phase has a regularizing effect. We also note that unbounded activation functions such as ReLU tend to have poor performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Terminology</head><p>Positive Dataset (D + ): This is the dataset that contains the normal class(es), for example, the plane class from CIFAR-10. Negative Dataset (D ? ): This is a secondary unlabeled dataset containing negative examples from a similar domain as D + . The choice of D ? depends on D + . For example if D + is the digit 0 from MNIST, D ? might be random strokes or another dataset with similar features such as Omniglot <ref type="bibr" target="#b27">[28]</ref>. It is important to note that the model should not be tested on D ? since this violates the assumption of not knowing anomalies.</p><p>Anomaly Dataset (D ): This is a test dataset that contains classes that are neither in D + nor in D ? . Feature Extraction Phase: This is the first phase of training. The model is simply trained to reconstruct its input. Latent-Shaping Phase: This is the second phase of training. The encoder and decoder networks are frozen and only the latent layer is active.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training for Anomaly Detection</head><p>Given a dataset D + and a negative dataset D ? from a similar domain to D + , we divide the training process into two phases; the first phase is reconstructing samples from D + by minimizing the loss function L = ?x + ? x + ? 2 until convergence, where x + is the input drawn from D + andx + is the output of the autoencoder. In the second phase, we freeze the model except for the latent layer and minimize the following loss function:</p><formula xml:id="formula_3">L = ?x + ? x + ? 2 + ? ? ?x ? ? x ? ? 2 ? 2 (4)</formula><p>where x ? is a sampled batch from D ? ,x ? is its reconstruction, is a hyperparameter that controls the effect of the two parts of the loss function and is another hyperparameter indicating that we are satisfied if the reconstruction error ?x ? ? x ? ? 2 of the negative dataset exceeds a certain value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Anomaly Detection Training</head><p>Input: Positive (D + ) and Negative (D ? ) datasets // ?: Encoder, : Latent Layer, : Decoder Output: Trained model // Feature extraction phase // Sample mini batches from D + for x + ? D + until convergence d?</p><formula xml:id="formula_4">x + = D ( (?(x + )) L = ?x + ? x + ? 2 // backpropagation step Minimize L end () () // Latent-shaping phase // Sample mini batches from (D + , D ? ) for (x + , x ? ) ? (D + , D ? ) until convergence do [z + , z ? ] = (?([x + , x ? ])) [x + ,x ? ] = D ( [z + , z ? ]) L = ?x + ? x + ? 2 + ? ? ?x ? ? x ? ? 2 ? 2 // backpropagation step Minimize L end</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Predicting Anomalies</head><p>We use reconstruction error L ( ) = ?x?x? 2 to distinguish between anomalies and normal data where x is the test sample andx is the reconstructed output. More specifically, we set a threshold such that if L (x) &gt; the output is considered to be anomalous.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">THEORETICAL JUSTIFICATION 4.1 Formulation</head><p>In this section, we present theoretical justification for the reasoning behind selective freezing and the second phase loss function. We would like to show that the process described in algorithm 1 implies that the reconstruction loss for a latent-insensitive autoencoder (L ) remains equivalent to the reconstruction loss of a standard autoencoder (L ) for normal (positive) samples but larger for anomalies. More formally, under certain assumptions for negative dataset (D ? ), L (x + ) = L (x + ) and L (x ) ? L (x ) where x is an anomalous sample. From optimiality of autoencoders <ref type="bibr" target="#b3">[4]</ref>, we know that absent any non-linear activation functions, a linear autoencoder corresponds to singular value decomposition (SVD); henceforth, we use SVD interchangeably with linear autoencoders. Given an ? data matrix X + , we decompose R into X | | ? X ? , where X | | := ( + ) and X ? is its orthogonal complement (X + ).</p><p>We further decompose X + using SVD:</p><formula xml:id="formula_5">X + = U?V ,</formula><p>where U and V are orthonormal matrices and ? is a diagonal matrix such that ? = [ 1 ... | 0]. However, in practice it is rarely separated this neatly, specially when dealing with large number of samples of a high-dimensional dataset; therefore, we resort to reduced-SVD where we take the first columns of U with the caveat that the choice of is a hyper-parameter.</p><formula xml:id="formula_6">U = ? ? ? ? ? ? u 1 . . . u u +1 . . . u ? ? ? ? ? ? The U matrix can be divided thus: U = [U |U ]</formula><p>, and from Eckart-Young low-rank approximation theorem, columns of U ? (X | | ) and columns of U ? (X ? ). A linear autoencoder with -dimensional latent layer is equivalent to the following transform:x</p><formula xml:id="formula_7">= U U x</formula><p>Where U and U represent the decoder and the encoder respectively. Furthermore, any data point x ? R can be represented as x = U z + U c, where c and z are ( ? ) and -dimensional real vectors. By orthonormality, we have the following identities: U U = I and U U = 0, where I is an -identity matrix. As a shorthand, we write ?.? instead of ?.? 2 2 . Using these two identities, we rewrite the reconstruction loss ?x ? x? as following:</p><formula xml:id="formula_8">U U x = U U [U z + U c] = U [U U z + U U c] = U z L (x) = ?U U x ? x? = ?U z ? U z ? U c? = ?U c? = ?c?</formula><p>We note that the loss function is agnostic to the nature of c and is only concerned with its magnitude. The assumption for anomaly detection under this setting is that ?c ? &gt; ?c + ?, where c and c + correspond to orthogonal components for anomalies and positive data respectively. We posit that while this agnosticism is desirable for potential generality, it is not optimal for anomaly detection; hence, we modify the loss score to depend on the nature of c:</p><formula xml:id="formula_9">L (x) = ?B c? + ?c?</formula><p>where B is an ? ( ? ) matrix such that the loss is small for normal data but large for anomalies. In other words, we want ?B c + ? = 0 and ?B c ? to be large. We define C | | := orthonormal basis for (C + ) and C ? := orthonormal basis for (C + ), where C + is the matrix of all positive orthogonal components c + . We decompose C ? further into C ?? and C ? where columns of C ?? are the basis of (C ? ) that are not in C | | and columns of C ? are the remaining columns of C ? . Despite the fact that we do not have access to c , we can utilize other negative examples from similar domain and use c ? as a proxy for c . Since c = C | | p + C ?? q + C ? s, maximizing ?B C ?? ? implies maximizing ?B c ? assuming that ?C ?? q? ? 0. The later assumption hinges on the fact that X ? is from a similar domain. Therefore, we end up with the goal of finding B such that ?B c + ? = 0 and ?B c ? ? is large.</p><formula xml:id="formula_10">Since R ? = (C + ) ? (C ?? ) ? (C ? ), any c ? R ? can be written as c = C | | p + C ?? q + C ? s,</formula><formula xml:id="formula_11">B = argmin(?B c + ? ? ?B c ? ?)</formula><p>where controls the importance of the second term.</p><formula xml:id="formula_12">= argmin(?U B c + ? ? ?U B c ? ?)</formula><p>since orthonormal transformations preserve the dot product.</p><formula xml:id="formula_13">= argmin(?U B c + ? + ? ? U c + ? ? ?U B c ? ? ? ? ? U c ? ?)</formula><p>since adding constant to argmin does not affect the objective.</p><formula xml:id="formula_14">= argmin(?U B c + ? U c + ? ? ?U B c ? ? U c ? ?) = argmin(?U z + + U B c + ? U z + ? U c + ? ? ?U z + + U B c ? ? U z ? ? U c ? ?) = argmin(?U z + + U B c + ? x + ? ? ?U z ? + U B c ? ? x ? ?) = argmin(?U U x + + U B U x + ? x + ? ? ?U U x ? + U B U x ? ? x ? ?) = argmin(?U (U + B U )x + ? x + ? ? ?U (U + B U )x ? ? x ? ?) E := U + U B E = argmin(?U E x + ? x + ? ? ?U E x ? ? x ? ?)</formula><p>In practice, we cannot maximize ?U E x ? ? x ? ? indefinitely and we are satisfied if it reaches a certain large :</p><formula xml:id="formula_15">E = argmin(?U E x + ? x + ? + ? ? ?U E x ? ? x ? ??)</formula><p>We notice that in order for this to work, the decoder U has to be known and remain fixed (frozen). This suggests a two-phase training where we first compute the decoder and encoder networks, and in the second phase the decoder is fixed while the encoder E is modified using the new loss. In <ref type="figure" target="#fig_6">fig. 4</ref>, a linear version of LIS-AE is trained on digit-8 from MNIST with Omniglot as a negative dataset. We perform orthogonal decomposition on each input by projecting it onto digit-8 subspace to get its projection and orthogonal vectors. We then feed each vector separately to a regular linear AE and linear LIS-AE. We observe that the regular autoencoder outputs zero images for the orthogonal part of each sample regardless of the class it belongs to. However, in the case of LIS-AE, it behaves differently for normal class than for anomalous classes.</p><p>We also notice that orthogonal projections do not form a semantically meaningful representation in pixel space. In order to gain a better representation we use a deep AE. For this non-linear case, we treat the middle part of the network as an inner linear autoencoder which is operating on a more semantically meaningful transformed version of the data. This suggests a stacked autoencoder archiecture where another loss term for the inner autoencoder is added in the first phase to make sure that the output of the layer after latent is similar to the latent input. In the second phase we freeze the entire network except for the encoder of the inner autoencoder (latent layer of entire model) and minimize the reconstruction error of positive examples while maximizing the loss for negative examples. However, in our experiments we observed that adding these loss terms was not necessary and a similar loss to the linear case produced similar results since we are only considering reconstruction scores of the outer model. Therefore, we keep the entire network frozen except for the latent layer while directly minimizing the following loss as before. (eq. 4)</p><formula xml:id="formula_16">(a) (b) (c) (d) (e) (f)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Intuition</head><p>For concreteness, we consider the following simple, supervised case. Given a dataset X + such that for each x + ? X + :</p><formula xml:id="formula_17">x + ? N = 0, ? = ? ? ? ? ? ? 1 0 0 0 0.2 0 0 0 0.01 ? ? ? ? ? ?</formula><p>We notice that most of the variance in data is along the x-axis. Training a linear autoencoder with latent dimension = 1, results in D = 1 0 0 and E = 1 0 0 where D and E are the decoder and encoder networks respectively.</p><p>Given input x = ,</p><formula xml:id="formula_18">x = DE x = ? ? ? ? ? ? 1 0 0 ? ? ? ? ? ? 1 0 0 ? ? ? ? ? ? ? ? ? ? ? ? = ? ? ? ? ? ? 0 0 ? ? ? ? ? ? , the loss score is L = ?x ?x? 2 = 2 + 2 .</formula><p>Training a LIS-AE on negative samples that have only nonzero values along the z-axis, we end up with the same D and a modified? = 1 0 , where is a large number and the?</p><formula xml:id="formula_19">x = D? x = ? ? ? ? ? ? 1 0 0 ? ? ? ? ? ? 1 0 ? ? ? ? ? ? ? ? ? ? ? ? = ? ? ? ? ? ? + 0 0 ? ? ? ? ? ? , that results inL = ?x ?x? ? 2 + 2 2 with x + has the form + + 0 , x a has the form</formula><p>where , ? 0. The new loss scores for x + and x a are:</p><formula xml:id="formula_20">L(x + ) = 2 + , L(x a ) = 2 + 2 2</formula><p>In the case of regular Linear-AE (PCA), given x + = ( + , + , 0), for each point ( , , ) ? the cylinder: </p><formula xml:id="formula_21">?? 2 + 2 ? + , ? R the</formula><formula xml:id="formula_22">?? 2 + 2 1/ 2 ? + , ? R ,</formula><p>and since is a large number, the cross-section of the cylinder is squashed in the z dimension resulting in heavily penalized loss in the z dimension but a regular loss in the y dimension. In this case, the two samples become indistinguishable only for very small values of . We note that the new? is merely a rotated and stretched version of the old E in the -plane. Thus, we can think of Linear LIS-AE as a regular PCA with its eigenvectors (columns of U r ) stretched and tilted in the directions of the orthogonal complement of the eigenspace. This is done in such a way that keeps the column space of normal examples invariant under the new transformation U E T . By itself, this formulation remains ill-posed since there are infinite number of solutions that do not necessarily help with anomaly detection. More formally, given E := U + U B, we can choose any matrix B such that</p><formula xml:id="formula_23">(B ) = (U T c X + ) since U E T x + = U (U x + + B U x + ) = U U x + ? x + .</formula><p>However, this does not guarantee any advantage for anomaly detection on similar data, even worse in practice, this modification process might result in a slightly worse performance if done arbitrarily since the model usually has to sacrifice some extreme samples from the normal data to balance the two losses. Thus, the negative dataset is used to properly determine the directions of the tilt and hyperparamteres ( and ) determine the importance and amount of stretching (or shrinking) without changing the normal case as much as possible. For deep LIS-AE, the same analogy holds albeit in a latent space.</p><p>Deep architectures are not only useful for learning good representation, but can learn a non-linear transformation with useful properties for our objective such as linear separability of negative and positive samples. By adding a standard binary cross-entropy loss before the non-linear activation of the latent layer during the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>We report results on the following datasets, MNIST <ref type="bibr" target="#b16">[17]</ref>, Fashion-MNIST <ref type="bibr" target="#b32">[33]</ref>, SVHN <ref type="bibr" target="#b18">[19]</ref> and CIFAR-10 <ref type="bibr" target="#b13">[14]</ref>. Results of our approach are compared to baseline models with the same capacity for autoencoder-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Anomaly Detection</head><p>In this section, we test LIS-AE for anomaly detection on image data in unsupervised settings. Given a standard classification dataset, we group a set of classes together into a new dataset and consider it the "normal" dataset. The rest of classes that are not in the normal nor in the negative datasets are considered anomalies. During training, our model is presented only with the normal dataset and the additional negative dataset. We evaluate the performance on test data comprised of both the "normal" and "anomalous" groups. For MNIST and Fashion-MNIST, the encoder network consists of two Convolutional layers with LeakyReLU non-linearities followed by a fully-connected bottleneck layer with tanh activation function. The decoder network consists of a fully-connected layer followed by a LeakyReLU and two Deconvolution layers with LeakyReLU activation functions and a final convolution layer with sigmoid situated at the final output. For SVHN and CIFAR-10 we use latent layer with larger sizes and higher capacity networks with same depth. It is worth noting that the choice of latent layer size has the most effect on performance for all models (compared to other hyper-parameters). We report the best performing latent dimension for all models.</p><p>In table (1), we compare LIS-AE with several autoencoder-based anomaly detection models as baselines, all of which share the exact same architecture. It is worth noting that the most direct comparison is between LIS-AE and AE since not only they have the same architecture, they have the exact same encoder and decoder weights and their performance is merely measured before and after the latent-shaping phase. We use a different variant of RepNN with a sigmoid activation function ( ) = 1/(1 + exp(? )) placed before the tanh staircase function approximation described in section 2. This is mainly used because "squashing" the input between 0 and 1 before passing it to the staircase function gives more robust and easy-to-train network. We only report the best results for Sig-RepNN with 4 activation levels. For anomaly GAN (AnoGAN) <ref type="bibr" target="#b25">[26]</ref>, we follow the implementation described in <ref type="bibr" target="#b24">[25]</ref>. We train a W-GAN <ref type="bibr" target="#b9">[10]</ref> with gradient penalty and report performance for two anomaly scores, namely, encoder-generator reconstruction loss and additional feature-matching distance score in the discriminator feature space (AnoGAN-FM). For AnoGAN, Linear-AE, AE, VAE, RepNN, MemAE, and LIS-A, we use reconstruction error L (x) such that if L (x) &gt; the input is considered an anomaly. Varying the threshold , we are able to compute the area under the curve (AUC) as a measure of performance. Similarly, for OC-SVDD (equivalently OC-SVM with rbf kernel) and OC-DSVDD, we vary inverse length scale and use predicted class label. For Kernel density estimation (KDE) <ref type="bibr" target="#b19">[20]</ref>, we vary the threshold over the log-likelihood scores. For Isolation Forest (IF) <ref type="bibr" target="#b17">[18]</ref>, we vary the threshold over the anomaly score calculated by the Isolation Forest algorithm.</p><p>The datasets tested in table (1) are MNIST and Fashion-MNIST. To train LIS-AE on MNIST we use Omniglot <ref type="bibr" target="#b14">[15]</ref> as our negative dataset since it shares similar compositional characteristics with MNIST. Since Omniglot is a relatively small dataset, we diversify the negative examples with various augmentation techniques, namely, Gaussian blurring, random cropping, horizontal and vertical flipping. We test two settings for MNIST, a 1-class setting where normal dataset is one particular class and the rest of the dataset classes are considered anomalies. The process is repeated for all classes and averge AUC for 10 classes is reported. Another setting is 2-class MNIST where the normal dataset consists of two classes and the remaining classes are considered anomalies. For example, the first task contains digits 0 and 1 and the remaining digits are considered anomalies, the second task contains digit 2 and 3, and so forth. This setting is more challenging since there is more than one class present in the normal dataset. For Fashion-MNIST, the choice of negative example is different. We use the next class as the negative dataset and we do not include it with anomalies (i.e. remaining classes) during test time.</p><p>We note that LIS-AE achieves superior performance to all compared approaches, however, we also notice that these settings are comparatively easy and all tested models performed adequately including classical non-deep approaches. In table <ref type="formula" target="#formula_1">(2)</ref>, we show performance on SVHN and CIFAR-10 which are more complex dataests compared to MNIST and Fashion-MNIST. To train LIS-AE, we split each dataset into two datasets, each split is used as negative examples for the other one. Note that we only test on the remaining classes which are not in the normal nor the negative datasets. For example, the first dataset from CIFAR-10 has five classes, namely, airplane, automobile, bird, cat and deer while the second one has dog, frog, horse, ship and truck. Training on airplane as the first normal task, LIS-AE maximizes the loss for samples drawn from the negative dataset (dog, frog, ship and so forth). We then test its performance on airplane as the normal class and only on automobile, bird, cat and deer as anomalies. Note that we do not test on dog, frog and other classes in the negative dataset. This processes is repeated for all 10 classes and average AUC is reported. As mentioned in section 4.2, We introduce LinSep-LIS-AE as an improvement over base standard LIS-AE. The difference between the two models is only in the first phase where a binary cross-entropy loss is added to ensure that positive and negative examples are linearly seperabable during the second phase. The last two entries of the table are supervised upper bounds for each variant where the negative dataset is the same as outliers. In <ref type="figure" target="#fig_6">figure (4)</ref>, we see that standard AE is prone to generalize so well for other classes which is not a desired property for anomaly detection. In contrast, LIS-AE only reconstructs normal data faithfully which translates to the large performance gap we see in figure <ref type="bibr" target="#b2">(3)</ref>. We also notice that despite CIFAR-10 being more complex than SVHN, most reconstruction-based models are performing better on CIFAR-10 than on SVHN. This is due to the fact that the difference between SVHN classes in terms of reconstruction is not as large since they share similar compositional features and do appear in samples from other classes while for CIFAR-10 classes vary significantly. (e.g. digit-2 and digit-3 on a wall vs truck and bird)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation</head><p>In this section, we investigate the effect of the nature of negative dataset and linear separability of positive and negative examples. In table (3) we train LIS-AE on different negative and positive datasets. Similar to table (2), we split each positive dataset into two datasets and follow the same settings as before with the exception of "None" and "Supervised " cases. The "None" case indicates that no negative examples have been used whereas the Supervised indicates that both outliers and negative datasets share the same classes. Note that this case is different from the case where the positive and negative datasets come from the same dataset. Unless stated otherwise, we only test on classes (outliers) that are not in the positive nor in the negative datasets. For example, when MNIST is used as a source for both positive and negative datasets, the positive data starts with class 0 and negative dataset consists of classes 5 to 9 where the outliers are classes 1 to 4. This process is repeated for all 10 classes present in each dataset and average AUC is reported. Overall, using a negative dataset resulted in a significant increase in performance in every case except for two important cases, namely, when Fashion-MNIST and CIFAR-10 were used as negative datasets for MNIST and SVHN respectively. This could be explained by the fact that the model was not capable of reconstructing Fashion-MNIST and CIFAR-10 classes in the first place. Moreover, shaping the latent layer in such a way that maximizes the loss for Fashion-MNIST and CIFAR-10 classes does not guarantee any advantage for anomaly detection of similar digit classes present in MNIST and SVHN. This coupled with the fact that this process in practice forces the model to ignore some samples from the normal dataset to balance the two losses which results in the performance degradation we observe in these two cases.</p><p>Table <ref type="formula">(4)</ref> is an excerpt of the complete table in the appendices where we examine the effect of each class present in the negative dataset on anomaly detection performance for other test classes from the CIFAR-10 dataset. We split CIFAR-10 into two separate datasets, the first split is used for selecting classes as negative datasets and the other split is used as outliers. For each class in CIFAR-10 we train eight models in different settings, the first setting is None where we train a standard autoencoder with no negative examples as the base model. The remaining seven settings differ in the second phase, we select one class as our negative dataset and test the model performance on each individual class from the outlier dataset. The combined setting is similar to the setting described in section 5.1 where we combine all negative classes in one 5-class negative dataset. Note that these classes are not the same as the classes in the outlier test dataset except for the final setting, which is an upper-bound supervised setting where the negative dataset is comprised of classes that are in the outlier dataset except for the positive class. This process is then repeated for all 10 classes in CIFAR-10. Overall, we observe a significant performance increase over the base model with the general trend of negative classes significantly increasing anomaly detection performance for similar outliers. For example, the dog class drastically improves performance on the cat class but not so much for the plane class. However, we also notice two important exceptions, namely, when the horse class is used as the negative dataset for the car class, we notice a significant performance increase for the relatively similar deer class as expected, however, when the horse class is used as the negative dataset for the same deer class, we notice that the performance does not improve as in the first case and even degrades for the care class. Other notable examples of this observation can be found in the appendices where, for instance, the dog class improves performance on cat outliers, but causes noticeable degradation when used as the negative dataset for the same cat class. The gained performance, in the first case, is due to the fact that these classes share similar compositional features and backgrounds. However in the second case, the same property makes it difficult to balance the minimization and maximization loss during the latent-shaping phase. For example, car and truck images are very similar in this scenario that minimizing and maximizing the loss at the same time becomes contradictory. As posited in section 4.2, we mitigate this issue by adding a binary cross-entropy loss while training in the first phase to ensure that the input of the latent layer is linearly-separable for positive and negative examples. Notice that unlike other approaches <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">22]</ref>, this does not require a labeled positive or negative dataset and relies only on the fact that we have two distinct datasets. This linear separablity makes the second phase of training relatively easier and less contradictory. In table <ref type="bibr" target="#b4">(5)</ref>, we see that LinSep-LIS-AE mitigates this issue for the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper we introduced a novel autoencoder-based model called Latent-Insensitive autoencoder (LIS-AE). With the help of negative samples drawn from a similar domain as the normal data we tune the weights of the bottleneck part of a standrad autoencoder such that the resulting model is able to reconstruct the target task while penalizing anomalous samples. We also presented theoretical justification for the reasoning behind our two-phase training process and the latent-shaping loss function along with a more powerful variant. Multiple ablation studies were conducted to explain the effect of negative classes and highlight other important aspects of our model. We tested our model in a variety of anomaly detection settings with multiple datasets of varying degrees of complexity. Experimental results showed significant performance improvement over compared methods. Future research will focus on possible ways for synthesizing negative examples for domains with limited data.</p><p>We also hope to further study and employ various manifold learning approaches for latent space representation.</p><p>ACKNOWLEDGEMENT Artem Lenskiy was funded by Our Health in Our Hands (OHIOH), a strategic initiative of the Australian National University, which aims to transform healthcare by developing new personalised health technologies and solutions in collaboration with patients, clinicians, and health care providers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A EFFECT OF INDIVIDUAL CLASSES AS NEGATIVE EXAMPLES</head><p>As discussed in section 5.2, we examine the effect of each class present in the negative dataset on anomaly detection performance for other test classes from the CIFAR-10 dataset. The <ref type="table">first table shows results for standard LIS-AE while the second table shows</ref>   Fashion-MNIST classes as Positive datasets. Left, outliers as negative dataset (Supervised). Right, Omniglot as negative dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>The first diagram shows feature extraction phase. ? ! , " # ! , # " ! , " (b) The second phase starts by freezing the model except latent layer. Negative examples x ? are used to fine-tune the latent layer to be only responsive to x + .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>The two phases of training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>where p, q and s are real vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Comparison between Linear AE and LIS-AE. Digit-8 is the normal task. (a) Inputs. (b) Orthogonal vector. (c) AE reconstruction of orthogonal vectors(zero images). (d) LIS-AE reconstruction of orthogonal vectors. (e) AE reconstruction of inputs. (f) LIS-AE reconstruction of inputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>following holds: L(x a ) = L(x + ), making the two samples indistinguishable. In the case of LIS-AE,L(x a ) =L(x + ) holds only for the elliptic cylinder</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 .</head><label>3</label><figDesc>Each line represents a trade-off between accuracy of anomalies and normal data for CIFAR10. The top pane shows accuracies on tasks 0-4 and the bottom shows accuracies on tasks 5-9. Note that as the threshold value increases the model favors accepting anomalies over misclassifying normal examples. LIS-AE gives a significant margin compared to base AE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 .</head><label>4</label><figDesc>Top row is test input from CIFAR-10 and SVHN, middle row is the output of a standard AE (first phase) and the bottom row is the output of LIS-AE. Trained on normal "car" and "digit-0" classes, LIS-AE only reconstructs samples of normal class correctly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>9 MNIST</head><label>9</label><figDesc>Standard LIS-AE trained on CIFAR-10 classes. Left, outliers as negative dataset (Supervised). Right, SVHN as negative dataset. Results of LinSep-LIS-AE variant on SVHN. Left, outliers as negative dataset (Supervised). Right, unsupervised. classes as Positive datasets. Left, outliers as negative dataset (Supervised). Right, Omniglot as negative dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Average AUC for 10 tasks sampled from MNIST, Fashion-MNIST and 5 2-class tasks sampled from MNIST.</figDesc><table><row><cell>Model</cell><cell cols="3">MNIST Fashion-MNIST 2-Class MNIST</cell></row><row><cell>KDE</cell><cell>0.9568</cell><cell>0.9183</cell><cell>0.9206</cell></row><row><cell>IF</cell><cell>0.8624</cell><cell>0.9144</cell><cell>0.73018</cell></row><row><cell>OC-SVM</cell><cell>0.9108</cell><cell>0.8608</cell><cell>0.8741</cell></row><row><cell>OC-DSVDD</cell><cell>0.9489</cell><cell>0.8577</cell><cell>0.8972</cell></row><row><cell>AnoGAN</cell><cell>0.9579</cell><cell>0.9098</cell><cell>0.8406</cell></row><row><cell>AnoGAN-FM</cell><cell>0.9544</cell><cell>0.9072</cell><cell>0.8353</cell></row><row><cell>Linear-AE</cell><cell>0.9412</cell><cell>0.8845</cell><cell>0.8915</cell></row><row><cell>VAE</cell><cell>0.9642</cell><cell>0.9092</cell><cell>0.9263</cell></row><row><cell>Mem-AE</cell><cell>0.9714</cell><cell>0.9131</cell><cell>0.9352</cell></row><row><cell cols="2">Sig-RepNN (N=4) 0.9661</cell><cell>0.9124</cell><cell>0.9261</cell></row><row><cell>AE</cell><cell>0.9601</cell><cell>0.9076</cell><cell>0.9221</cell></row><row><cell>LIS-AE</cell><cell>0.9768</cell><cell>0.9256</cell><cell>0.9457</cell></row></table><note>first phase, we ensure that the input of the latent layer is linearly- separable for positive and negative examples during the second phase. This linearly-separable variant (LinSep-LIS-AE) almost al- ways performs better than directly using LIS-AE. We investigate the effect of this property on the second phase in section 5.2.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Average AUC for 10 anomaly detection tasks sampled from SVHN and CIFAR-10 are shown.</figDesc><table><row><cell>Model</cell><cell cols="2">SVHN CIFAR-10</cell></row><row><cell>KDE</cell><cell>0.5648</cell><cell>0.5752</cell></row><row><cell>IF</cell><cell>0.5112</cell><cell>0.6097</cell></row><row><cell>OC-SVM</cell><cell>0.5047</cell><cell>0.5651</cell></row><row><cell>OC-DSVDD</cell><cell>0.5681</cell><cell>0.6411</cell></row><row><cell>AnoGAN</cell><cell>0.5598</cell><cell>0.5843</cell></row><row><cell>AnoGAN-FM</cell><cell>0.5645</cell><cell>0.5880</cell></row><row><cell>Linear-AE</cell><cell>0.5702</cell><cell>0.5753</cell></row><row><cell>VAE</cell><cell>0.5692</cell><cell>0.5734</cell></row><row><cell>Mem-AE</cell><cell>0.5720</cell><cell>0.5931</cell></row><row><cell>Sig-RepNN (N=4)</cell><cell>0.5684</cell><cell>0.5719</cell></row><row><cell>AE</cell><cell>0.5698</cell><cell>0.5703</cell></row><row><cell>LIS-AE</cell><cell>0.6886</cell><cell>0.8145</cell></row><row><cell>LinSep-LIS-AE</cell><cell>0.7701</cell><cell>0.8858</cell></row><row><cell>Sup. LIS-AE</cell><cell>0.7573</cell><cell>0.8384</cell></row><row><cell cols="2">Sup. LinSep-LIS-AE 0.8479</cell><cell>0.9170</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Average AUC for 10 anomaly detection tasks sampled from two 5class MNIST, Fashion-MNIST, SVHN and CIFAR-10 datasets where a regular LIS-AE is trained with different negative Splits.</figDesc><table><row><cell>Negative</cell><cell></cell><cell cols="2">Positive Data</cell><cell></cell></row><row><cell>Data</cell><cell cols="4">MNIST Fashion SVHN CIFAR-10</cell></row><row><cell>None</cell><cell>0.9485</cell><cell cols="2">0.8740 0.5698</cell><cell>0.5703</cell></row><row><cell>Omni</cell><cell>0.9605</cell><cell>0.9013</cell><cell>-</cell><cell>-</cell></row><row><cell>MNIST</cell><cell>0.9778</cell><cell>0.8942</cell><cell>-</cell><cell>-</cell></row><row><cell>Fashion</cell><cell>0.9482</cell><cell>0.9106</cell><cell>-</cell><cell>-</cell></row><row><cell>SVHN</cell><cell>-</cell><cell>-</cell><cell>0.6886</cell><cell>0.7065</cell></row><row><cell>CIFAR-10</cell><cell>-</cell><cell>-</cell><cell>0.5481</cell><cell>0.8145</cell></row><row><cell cols="2">Same (Sup.) 0.9901</cell><cell cols="2">0.9623 0.7573</cell><cell>0.8384</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>AUC for LIS-AE trained on individual positive and negative classes is reported.</figDesc><table><row><cell>Positive</cell><cell>Negative</cell><cell></cell><cell></cell><cell>Outliers</cell></row><row><cell>Class</cell><cell>Class</cell><cell cols="4">Plane Car Bird Cat Deer avg</cell></row><row><cell></cell><cell>None</cell><cell>0.32</cell><cell>-</cell><cell cols="2">0.34 0.33 0.33 0.330</cell></row><row><cell></cell><cell>Dog 5</cell><cell>0.67</cell><cell>-</cell><cell cols="2">0.89 0.93 0.90 0.848</cell></row><row><cell></cell><cell>Frog 6</cell><cell>0.58</cell><cell>-</cell><cell cols="2">0.90 0.90 0.91 0.823</cell></row><row><cell>Car</cell><cell>Horse 7 Ship 8</cell><cell>0.66 0.83</cell><cell>--</cell><cell cols="2">0.88 0.90 0.92 0.840 0.59 0.51 0.50 0.608</cell></row><row><cell></cell><cell>Truck 9</cell><cell>0.51</cell><cell>-</cell><cell cols="2">0.44 0.49 0.44 0.470</cell></row><row><cell></cell><cell cols="2">Comb. (5-9) 0.81</cell><cell>-</cell><cell cols="2">0.92 0.92 0.94 0.898</cell></row><row><cell></cell><cell>Sup. (0-4)</cell><cell>0.89</cell><cell>-</cell><cell cols="2">0.93 0.90 0.95 0.918</cell></row><row><cell></cell><cell>None</cell><cell>0.56</cell><cell cols="2">0.80 0.52 0.54</cell><cell>-</cell><cell>0.605</cell></row><row><cell></cell><cell>Dog 5</cell><cell>0.72</cell><cell cols="2">0.85 0.63 0.80</cell><cell>-</cell><cell>0.750</cell></row><row><cell></cell><cell>Frog 6</cell><cell>0.66</cell><cell cols="2">0.86 0.60 0.75</cell><cell>-</cell><cell>0.718</cell></row><row><cell>Deer</cell><cell>Horse 7 Ship 8</cell><cell cols="3">0.71 0.93 0.94 0.63 0.72 0.58 0.58 0.71</cell><cell>--</cell><cell>0.645 0.805</cell></row><row><cell></cell><cell>Truck 9</cell><cell cols="3">0.84 0.97 0.62 0.72</cell><cell>-</cell><cell>0.773</cell></row><row><cell></cell><cell cols="2">Comb. (5-9) 0.87</cell><cell cols="2">0.95 0.61 0.73</cell><cell>-</cell><cell>0.790</cell></row><row><cell></cell><cell>Sup. (0-4)</cell><cell>0.93</cell><cell cols="2">0.97 0.63 0.72</cell><cell>-</cell><cell>0.813</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>AUC for LinSep-LIS-AE trained on individual positive and negative classes is reported.</figDesc><table><row><cell>Positive</cell><cell>Negative</cell><cell></cell><cell></cell><cell>Outliers</cell></row><row><cell>Class</cell><cell>Class</cell><cell cols="4">Plane Car Bird Cat Deer</cell><cell>avg</cell></row><row><cell></cell><cell>None</cell><cell>0.32</cell><cell>-</cell><cell cols="2">0.34 0.33 0.33</cell><cell>0.330</cell></row><row><cell></cell><cell>Dog 5</cell><cell>0.67</cell><cell>-</cell><cell cols="2">0.94 0.97 0.95</cell><cell>0.883</cell></row><row><cell></cell><cell>Frog 6</cell><cell>0.58</cell><cell>-</cell><cell cols="2">0.93 0.96 0.96</cell><cell>0.858</cell></row><row><cell>Car</cell><cell>Horse 7 Ship 8</cell><cell>0.69 0.90</cell><cell>--</cell><cell cols="2">0.95 0.97 0.97 0.895 0.78 0.79 0.76 0.808</cell></row><row><cell></cell><cell>Truck 9</cell><cell>0.59</cell><cell>-</cell><cell cols="2">0.77 0.82 0.73</cell><cell>0.728</cell></row><row><cell></cell><cell cols="2">Comb. (5-9) 0.90</cell><cell>-</cell><cell cols="2">0.97 0.97 0.98</cell><cell>0.955</cell></row><row><cell></cell><cell>Sup. (0-4)</cell><cell>0.95</cell><cell>-</cell><cell cols="2">0.98 0.98 0.98 0.9725</cell></row><row><cell></cell><cell>None</cell><cell>0.56</cell><cell cols="2">0.80 0.52 0.54</cell><cell>-</cell><cell>0.605</cell></row><row><cell></cell><cell>Dog 5</cell><cell>0.67</cell><cell cols="2">0.86 0.71 0.89</cell><cell>-</cell><cell>0.783</cell></row><row><cell></cell><cell>Frog 6</cell><cell>0.68</cell><cell cols="2">0.87 0.62 0.79</cell><cell>-</cell><cell>0.740</cell></row><row><cell>Deer</cell><cell>Horse 7 Ship 8</cell><cell cols="3">0.70 0.94 0.95 0.63 0.73 0.84 0.61 0.72</cell><cell>--</cell><cell>0.718 0.813</cell></row><row><cell></cell><cell>Truck 9</cell><cell cols="3">0.84 0.97 0.61 0.76</cell><cell>-</cell><cell>0.795</cell></row><row><cell></cell><cell cols="2">Comb. (5-9) 0.90</cell><cell cols="2">0.97 0.66 0.80</cell><cell>-</cell><cell>0.833</cell></row><row><cell></cell><cell>Sup. (0-4)</cell><cell>0.97</cell><cell cols="2">0.98 0.76 0.83</cell><cell>-</cell><cell>0.885</cell></row><row><cell cols="6">aforementioned cases and gives the AUC increase we observed in</cell></row><row><cell>table (2).</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>results for LinSep-LIS-AE.The following graphs are detailed results for some experiments in various settings described in section 5.1 and 5.2. Each curve represents a trade-off between accuracy on anomalies and on normal data for each dataset. The two left panes are an upper-bound supervised setting where the negative dataset is the same as outliers. The top pane shows accuracies on tasks 0 to 4 and the bottom shows accuracies on tasks 5 to 9. Note that as the threshold value increases the model favors accepting anomalies over misclassifying normal examples. In almost all cases, we observe that LIS-AE gives a significant margin compared to normal AE.</figDesc><table><row><cell cols="10">Positive B DETAILED RESULTS FOR ALL 10 TASKS Negative Outliers</cell><cell></cell><cell></cell><cell></cell><cell>Positive</cell><cell>Negative</cell><cell>Outliers</cell></row><row><cell>Class</cell><cell>Class</cell><cell cols="11">Plane Car Bird Cat Deer avg</cell><cell>Class</cell><cell>Class</cell><cell>Dog Frog Horse Ship Truck</cell><cell>avg</cell></row><row><cell></cell><cell>None</cell><cell></cell><cell>-</cell><cell></cell><cell cols="8">0.78 0.58 0.62 0.61 0.648</cell><cell></cell><cell>None</cell><cell>-</cell><cell>0.69</cell><cell>0.66</cell><cell>0.57</cell><cell>0.77</cell><cell>0.673</cell></row><row><cell></cell><cell>Dog 5</cell><cell></cell><cell>-</cell><cell></cell><cell cols="8">0.84 0.88 0.97 0.93 0.905</cell><cell></cell><cell>Plane 0</cell><cell>-</cell><cell>0.62</cell><cell>0.68</cell><cell>0.98</cell><cell>0.94</cell><cell>0.805</cell></row><row><cell>Positive</cell><cell>Negative Frog 6</cell><cell></cell><cell>-</cell><cell></cell><cell cols="8">Outliers 0.84 0.86 0.95 0.93 0.895</cell><cell>Positive</cell><cell>Negative Car 1</cell><cell>-</cell><cell>0.67</cell><cell>Outliers 0.7</cell><cell>0.96</cell><cell>0.97</cell><cell>0.825</cell></row><row><cell>Class Plane</cell><cell>Class None Horse 7 Ship 8</cell><cell cols="11">Plane Car Bird Cat Deer avg -0.78 0.58 0.62 0.61 0.648 -0.83 0.85 0.95 0.92 0.888 -0.80 0.57 0.74 0.56 0.668</cell><cell>Class Dog</cell><cell>Class None Bird 2 Cat 3</cell><cell>Dog Frog Horse Ship Truck avg -0.69 0.66 0.57 0.77 0.673 -0.73 0.68 0.94 0.86 0.803 -0.65 0.68 0.8 0.85 0.745</cell></row><row><cell></cell><cell>Dog 5 Truck 9</cell><cell>-</cell><cell>-</cell><cell cols="9">0.83 0.87 0.95 0.91 0.890 0.92 0.71 0.87 0.74 0.810</cell><cell></cell><cell>Plane 0 Deer 4</cell><cell>-</cell><cell>-</cell><cell>0.53 0.81</cell><cell>0.66 0.74</cell><cell>0.92 0.89</cell><cell>0.89 0.81</cell><cell>0.750 0.8125</cell></row><row><cell></cell><cell>Frog 6 Comb. (0-4)</cell><cell>-</cell><cell>-</cell><cell cols="9">0.83 0.86 0.94 0.90 0.883 0.90 0.85 0.96 0.94 0.913</cell><cell></cell><cell>Car 1 Comb. (0-4)</cell><cell>-</cell><cell>-</cell><cell>0.56 0.80</cell><cell>0.68 0.76</cell><cell>0.95 0.97</cell><cell>0.91 0.96</cell><cell>0.775 0.874</cell></row><row><cell>Plane</cell><cell>Horse 7 Ship 8 Sup. (5-9) None</cell><cell>--</cell><cell>-0.32</cell><cell cols="9">0.82 0.86 0.94 0.91 0.883 0.83 0.86 0.92 0.90 0.878 0.93 0.90 0.96 0.95 0.935 -0.34 0.33 0.33 0.330</cell><cell>Dog</cell><cell>Bird 2 Sup. (5-9) Cat 3 None</cell><cell>--</cell><cell>-0.40</cell><cell>0.63 0.90 0.67 -</cell><cell>0.63 0.85 0.65 0.53</cell><cell>0.77 0.97 0.66 0.49</cell><cell>0.78 0.98 0.81 0.67</cell><cell>0.925 0.703 0.698 0.523</cell></row><row><cell></cell><cell>Truck 9 Dog 5</cell><cell>-</cell><cell>0.67</cell><cell cols="9">0.84 0.86 0.95 0.91 0.890 -0.94 0.97 0.95 0.883</cell><cell></cell><cell>Deer 4 Plane 0</cell><cell>-</cell><cell>0.7</cell><cell>0.73</cell><cell>-</cell><cell>0.69 0.58</cell><cell>0.70 0.98</cell><cell>0.76 0.96</cell><cell>0.720 0.841</cell></row><row><cell></cell><cell>Comb. (5-9) Frog 6</cell><cell>-</cell><cell>0.58</cell><cell cols="9">0.83 0.85 0.93 0.91 0.880 -0.93 0.96 0.96 0.858</cell><cell></cell><cell>Comb. (0-4) Car 1</cell><cell>-</cell><cell>0.7</cell><cell>0.58</cell><cell>-</cell><cell>0.67 0.83</cell><cell>0.95 0.98</cell><cell>0.94 0.98</cell><cell>0.785 0.930</cell></row><row><cell>Car</cell><cell>Sup. (0-4) None Horse 7 Ship 8</cell><cell cols="2">-0.32 0.69 0.90</cell><cell cols="9">0.81 0.88 0.94 0.92 0.888 -0.34 0.33 0.33 0.330 -0.95 0.97 0.97 0.895 -0.78 0.79 0.76 0.808</cell><cell>Frog</cell><cell>Sup. (5-9) None Bird 2 Cat 3</cell><cell>-0.40 0.86 0.88</cell><cell>0.56 -</cell><cell>--</cell><cell>0.73 0.53 0.91 0.87</cell><cell>0.95 0.49 0.96 0.94</cell><cell>0.95 0.67 0.93 0.92</cell><cell>0.798 0.523 0.933 0.912</cell></row><row><cell></cell><cell>Dog 5 Truck 9</cell><cell cols="2">0.67 0.59</cell><cell>-</cell><cell>-</cell><cell cols="7">0.89 0.93 0.77 0.82 0.73 0.728 0.9 0.848</cell><cell></cell><cell>Plane 0 Deer 4</cell><cell>0.73 0.81</cell><cell>-</cell><cell>-</cell><cell>0.81 0.92</cell><cell>0.96 0.95</cell><cell>0.93 0.92</cell><cell>0.858 0.929</cell></row><row><cell></cell><cell cols="3">Frog 6 Comb. (0-4) 0.90 0.58</cell><cell>-</cell><cell>-</cell><cell cols="7">0.9 0.97 0.97 0.98 0.955 0.9 0.91 0.823</cell><cell></cell><cell>Car 1 Comb. (0-4) 0.91 0.74</cell><cell>-</cell><cell>-</cell><cell>0.83 0.95</cell><cell>0.96 0.98</cell><cell>0.97 0.98</cell><cell>0.875 0.956</cell></row><row><cell>Car</cell><cell>Horse 7 Ship 8 Sup. (5-9) None</cell><cell cols="4">0.66 0.83 0.95 0.52 0.78 ---</cell><cell cols="7">0.88 0.9 0.59 0.51 0.98 0.98 0.98 0.973 0.92 0.840 0.5 0.608 -0.54 0.52 0.590</cell><cell>Frog</cell><cell>Bird 2 Sup. (5-9) Cat 3 None</cell><cell>0.80 0.94 0.84 0.41 0.58 ---</cell><cell>0.84 0.97 0.80 -</cell><cell>0.91 0.98 0.87 0.46</cell><cell>0.85 0.98 0.86 0.66</cell><cell>0.968 0.850 0.843 .0531</cell></row><row><cell></cell><cell>Truck 9 Dog 5</cell><cell cols="4">0.51 0.56 0.78 -</cell><cell cols="7">0.44 0.49 0.44 0.470 -0.81 0.55 0.675</cell><cell></cell><cell>Deer 4 Plane 0</cell><cell>0.75 0.53 0.59 -</cell><cell>0.86</cell><cell>-</cell><cell>0.88 0.96</cell><cell>0.87 0.87</cell><cell>0.840 0.806</cell></row><row><cell></cell><cell cols="5">Comb. (5-9) 0.81 Frog 6 0.53 0.80 -</cell><cell cols="7">0.92 0.92 0.94 0.898 -0.71 0.56 0.650</cell><cell></cell><cell>Comb. (0-4) 0.75 Car 1 0.57 0.67 -</cell><cell>0.84</cell><cell>-</cell><cell>0.97 0.96</cell><cell>0.95 0.95</cell><cell>0.877 0.860</cell></row><row><cell>Bird</cell><cell>Sup. (0-4) None Horse 7 Ship 8</cell><cell cols="4">0.89 0.52 0.78 -0.63 0.81 0.86 0.93</cell><cell cols="7">0.93 0.90 0.95 0.918 -0.54 0.52 0.590 -0.72 0.59 0.688 -0.64 0.47 0.725</cell><cell>Horse</cell><cell>Sup. (5-9) None Bird 2 Cat 3</cell><cell>0.82 0.41 0.58 -0.33 0.75 0.78 0.81</cell><cell>0.88 -</cell><cell>--</cell><cell>0.97 0.46 0.93 0.9</cell><cell>0.96 0.66 0.79 0.77</cell><cell>0.907 0.528 0.823 0.826</cell></row><row><cell></cell><cell>Dog 5 Truck 9</cell><cell cols="4">0.59 0.75 0.74 0.95</cell><cell>-</cell><cell>-</cell><cell cols="5">0.71 0.49 0.635 0.68 0.48 0.713</cell><cell></cell><cell>Plane 0 Deer 4</cell><cell>0.55 0.50 0.67 0.85</cell><cell>-</cell><cell>-</cell><cell>0.93 0.87</cell><cell>0.83 0.68</cell><cell>0.703 0.800</cell></row><row><cell></cell><cell cols="5">Frog 6 Comb. (0-4) 0.82 0.95 0.53 0.78</cell><cell>-</cell><cell>-</cell><cell cols="5">0.69 0.54 0.635 0.76 0.60 0.783</cell><cell></cell><cell>Car 1 Comb. (0-4) 0.75 0.91 0.56 0.58</cell><cell>-</cell><cell>-</cell><cell>0.90 0.97</cell><cell>0.93 0.93</cell><cell>0.743 0.891</cell></row><row><cell>Bird</cell><cell>Horse 7 Ship 8 Sup. (5-9) None</cell><cell cols="6">0.63 0.80 0.86 0.89 0.89 0.97 0.55 0.76 0.50 ---</cell><cell cols="5">0.66 0.57 0.665 0.61 0.45 0.703 0.70 0.56 0.773 -0.50 0.578</cell><cell>Horse</cell><cell>Bird 2 Sup. (5-9) Cat 3 None</cell><cell>0.62 0.73 0.82 0.96 0.77 0.76 062 0.74</cell><cell>--</cell><cell>-0.73</cell><cell>0.80 0.98 0.65 -</cell><cell>0.71 0.96 0.66 0.77</cell><cell>0.930 0.715 0.710 0.717</cell></row><row><cell></cell><cell>Truck 9 Dog 5</cell><cell cols="6">0.76 0.94 0.50 0.71 0.51 -</cell><cell cols="5">0.64 0.72 0.765 -0.46 0.545</cell><cell></cell><cell>Deer 4 Plane 0</cell><cell>0.62 0.83 0.84 0.89</cell><cell>-</cell><cell>0.93</cell><cell>0.57</cell><cell>-</cell><cell>0.60 0.85</cell><cell>0.655 0.890</cell></row><row><cell></cell><cell cols="7">Comb. (5-9) 0.78 0.90 Frog 6 0.52 0.76 0.58 -</cell><cell cols="5">0.62 0.49 0.698 -0.64 0.625</cell><cell></cell><cell>Comb. (0-4) 0.51 0.57 Car 1 0.86 0.91</cell><cell>-</cell><cell>0.93</cell><cell>0.89</cell><cell>-</cell><cell>0.88 0.92</cell><cell>0.713 0.920</cell></row><row><cell>Cat</cell><cell>Sup. (0-4) None Horse 7 Ship 8</cell><cell cols="6">0.82 0.94 0.55 0.76 0.50 -0.65 0.79 0.56 0.93 0.92 0.56</cell><cell cols="5">0.60 0.48 0.710 -0.50 0.578 -0.64 0.660 -0.48 0.723</cell><cell>Ship</cell><cell>Sup. (5-9) None Bird 2 Cat 3</cell><cell>0.59 0.66 0.62 0.74 0.96 0.97 0.97 0.98</cell><cell>-0.73 0.97 0.97</cell><cell>0.95 -</cell><cell>--</cell><cell>0.92 0.77 0.85 0.85</cell><cell>0.780 0.715 0.930 0.933</cell></row><row><cell></cell><cell>Dog 5 Truck 9</cell><cell cols="6">0.54 0.73 0.52 0.81 0.96 0.52</cell><cell>-</cell><cell>-</cell><cell cols="3">0.52 0.578 0.48 0.693</cell><cell></cell><cell>Plane 0 Deer 4</cell><cell>0.75 0.75 0.95 0.97</cell><cell>0.82 0.97</cell><cell>-</cell><cell>-</cell><cell>0.74 0.84</cell><cell>0.765 0.927</cell></row><row><cell></cell><cell cols="7">Frog 6 Comb. (5-9) 0.88 0.95 0.62 0.56 0.72 0.60</cell><cell>-</cell><cell>-</cell><cell cols="3">0.62 0.625 0.68 0.783</cell><cell></cell><cell>Car 1 Comb. (0-4) 0.97 0.98 0.84 0.89</cell><cell>0.90 0.98</cell><cell>-</cell><cell>-</cell><cell>0.88 0.90</cell><cell>0.878 0.956</cell></row><row><cell>Cat</cell><cell>Horse 7 Ship 8 Sup. (5-9) None</cell><cell cols="8">0.70 0.75 0.59 0.91 0.89 0.53 0.95 0.97 0.75 0.56 0.80 0.52 0.54 ---</cell><cell cols="3">0.68 0.680 0.46 0.678 0.76 0.860 -0.605</cell><cell>Ship</cell><cell>Bird 2 Sup. (5-9) Cat 3 None</cell><cell>0.94 0.96 0.97 0.98 0.95 0.95 0.35 0.53</cell><cell>0.94 0.98 0.93 0.46</cell><cell>--</cell><cell>-0.30</cell><cell>0.78 0.94 0.8 -</cell><cell>0.968 0.905 0.908 0.412</cell></row><row><cell></cell><cell>Truck 9 Dog 5</cell><cell cols="8">0.82 0.94 0.50 0.67 0.86 0.71 0.89 -</cell><cell cols="3">0.48 0.685 -0.783</cell><cell></cell><cell>Deer 4 Plane 0</cell><cell>0.92 0.96 0.69 0.70</cell><cell>0.94 0.67</cell><cell>-</cell><cell>0.87</cell><cell>0.78</cell><cell>-</cell><cell>0.900 0.733</cell></row><row><cell></cell><cell cols="9">Comb. (5-9) 0.89 0.91 0.55 Frog 6 0.68 0.87 0.62 0.79 -</cell><cell cols="3">0.52 0.718 -0.740</cell><cell></cell><cell>Comb. (0-4) 0.95 0.97 Car 1 0.54 0.61</cell><cell>0.96 0.53</cell><cell>-</cell><cell>0.61</cell><cell>0.83</cell><cell>-</cell><cell>0.928 0.573</cell></row><row><cell>Deer</cell><cell>Sup. (0-4) None Horse 7 Ship 8</cell><cell cols="8">0.93 0.93 0.58 0.56 0.80 0.52 0.54 -0.70 0.84 0.61 0.72 0.94 0.95 0.63 0.73</cell><cell cols="3">0.54 0.745 -0.605 -0.718 -0.813</cell><cell>Truck</cell><cell>Sup. (5-9) None Bird 2 Cat 3</cell><cell>0.94 0.96 0.35 0.53 0.93 0.89 0.96 0.91</cell><cell>0.96 0.46 0.88 0.88</cell><cell>-0.30 0.73 0.63</cell><cell>0.88 -</cell><cell>--</cell><cell>0.935 0.41 0.858 0.845</cell></row><row><cell></cell><cell>Dog 5 Truck 9</cell><cell cols="8">0.72 0.85 0.63 0.80 0.84 0.97 0.61 0.76</cell><cell>-</cell><cell>-</cell><cell>0.750 0.795</cell><cell></cell><cell>Plane 0 Deer 4</cell><cell>0.61 0.52 0.90 0.88</cell><cell>0.58 0.91</cell><cell>0.80 0.67</cell><cell>-</cell><cell>-</cell><cell>0.628 0.840</cell></row><row><cell></cell><cell cols="9">Frog 6 Comb. (0-4) 0.90 0.97 0.66 0.80 0.66 0.86 0.60 0.75</cell><cell>-</cell><cell>-</cell><cell>0.718 0.833</cell><cell></cell><cell>Car 1 Comb. (0-4) 0.97 0.96 0.51 0.57</cell><cell>0.53 0.91</cell><cell>0.47 0.82</cell><cell>-</cell><cell>-</cell><cell>0.520 0.914</cell></row><row><cell>Deer</cell><cell>Horse 7 Ship 8 Sup. (5-9)</cell><cell cols="8">0.71 0.58 0.58 0.71 0.93 0.94 0.63 0.72 0.97 0.98 0.76 0.83</cell><cell>--</cell><cell>-</cell><cell>0.645 0.805 0.885</cell><cell>Truck</cell><cell>Cat 3 Bird 2 Sup. (5-9)</cell><cell>0.95 0.90 0.92 0.90 0.98 0.98</cell><cell>0.82 0.84 0.96</cell><cell>0.61 0.73 0.89</cell><cell>--</cell><cell>-</cell><cell>0.820 0.848 0.953</cell></row><row><cell></cell><cell>Truck 9</cell><cell cols="7">0.84 0.97 0.62 0.72</cell><cell></cell><cell>-</cell><cell></cell><cell>0.773</cell><cell></cell><cell>Deer 4</cell><cell>0.91 0.92</cell><cell>0.86</cell><cell>0.63</cell><cell>-</cell><cell>0.830</cell></row><row><cell></cell><cell cols="8">Comb. (5-9) 0.87 0.95 0.61 0.73</cell><cell></cell><cell>-</cell><cell></cell><cell>0.790</cell><cell></cell><cell>Comb. (0-4) 0.93 0.91</cell><cell>0.83</cell><cell>0.76</cell><cell>-</cell><cell>0.858</cell></row><row><cell></cell><cell>Sup. (0-4)</cell><cell cols="7">0.93 0.97 0.62 0.72</cell><cell></cell><cell>-</cell><cell></cell><cell>0.810</cell><cell></cell><cell>Sup. (5-9)</cell><cell>0.94 0.95</cell><cell>0.90</cell><cell>0.78</cell><cell>-</cell><cell>0.893</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">? Muhammad S. Battikh and Artem A. Lenskiy</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">? Muhammad S. Battikh and Artem A. Lenskiy</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">? Muhammad S. Battikh and Artem A. Lenskiy</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">? Muhammad S. Battikh and Artem A. Lenskiy</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised real-time anomaly detection for streaming data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subutai</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Lavin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Purdy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuha</forename><surname>Agha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">262</biblScope>
			<biblScope unit="page" from="134" to="147" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Variational autoencoder based anomaly detection using reconstruction probability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwon</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungzoon</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Special Lecture on IE</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">153</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Auto-association by multilayer perceptrons and singular value decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>Bourlard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Kamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological cybernetics</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="291" to="294" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Robust principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Emmanuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Cand?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust, deep and inductive anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghavendra</forename><surname>Chalapathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="36" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Anomaly detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Chandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arindam</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vipin</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM computing surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1" to="58" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Memorizing normality to detect anomaly: Memory-augmented deep autoencoder for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vuong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Budhaditya</forename><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1705" to="1714" />
		</imprint>
	</monogr>
	<note>Moussa Reda Mansour, Svetha Venkatesh, and Anton van den Hengel</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00028</idno>
		<title level="m">Improved training of wasserstein gans</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning temporal regularity in video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmudul</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghyun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="733" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Outlier detection using replicator neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Baxter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Data Warehousing and Knowledge Discovery</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="170" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04606</idno>
		<title level="m">Deep anomaly detection with outlier exposure</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Humanlevel concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Brenden M Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Kernel methods in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Christoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lampert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Now Publishers Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<title level="m">MNIST handwritten digit database</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Isolation forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><forename type="middle">Ming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">eighth ieee international conference on data mining</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="413" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">On estimation of a probability density function and mode. The annals of mathematical statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Parzen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1962" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1065" to="1076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">On lines and planes of closest fit to systems of points in space. The London, Edinburgh, and Dublin philosophical magazine and journal of science</title>
		<editor>Karl Pearson. 1901. LIII</editor>
		<imprint>
			<date type="published" when="1901" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="559" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning deep features for one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pramuditha</forename><surname>Perera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="5450" to="5463" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Self-taught learning: transfer learning from unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Battle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
		<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="759" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Goernitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Shoaib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kloft</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4393" to="4402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">f-AnoGAN: Fast unsupervised anomaly detection with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Seeb?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Sebastian M Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ursula</forename><surname>Langs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidt-Erfurth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="30" to="44" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly detection with generative adversarial networks to guide marker discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Seeb?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ursula</forename><surname>Sebastian M Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Langs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on information processing in medical imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Estimating the support of a high-dimensional distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert C</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1443" to="1471" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Vehicle detection in aerial images based on region convolutional neural networks and hard negative example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanxin</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">336</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Support vector data description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="45" to="66" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Replicator neural networks for outlier modeling in segmental speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?szl?</forename><surname>T?th</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?bor</forename><surname>Gosztolya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="996" to="1001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">L?on Bottou, and Vladimir Vapnik</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Sinz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1009" to="1016" />
		</imprint>
	</monogr>
	<note>Inference with the universum</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A comparative study of RNN for outlier detection in data mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Baxter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifang</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2002 IEEE International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="709" to="712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Anomaly detection with robust deep autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Randy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paffenroth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 23rd ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="665" to="674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A survey on unsupervised outlier detection in high-dimensional numerical data. Statistical Analysis and Data Mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Zimek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The ASA Data Science Journal</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="363" to="387" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep autoencoding gaussian mixture model for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Martin Renqiang Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daeki</forename><surname>Lumezanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
