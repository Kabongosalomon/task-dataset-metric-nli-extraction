<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ASMNet: a Lightweight Deep Neural Network for Face Alignment and Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><forename type="middle">Pourramezan</forename><surname>Fard</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Denver</orgName>
								<address>
									<settlement>Denver</settlement>
									<region>CO</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hojjat</forename><surname>Abdollahi</surname></persName>
							<email>hojjat.abdollahi@du.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Denver</orgName>
								<address>
									<settlement>Denver</settlement>
									<region>CO</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Mahoor</surname></persName>
							<email>mohammad.mahoor@du.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Denver</orgName>
								<address>
									<settlement>Denver</settlement>
									<region>CO</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ASMNet: a Lightweight Deep Neural Network for Face Alignment and Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Active Shape Model (ASM) is a statistical model of object shapes that represents a target structure. ASM can guide machine learning algorithms to fit a set of points representing an object (e.g., face) onto an image. This paper presents a lightweight Convolutional Neural Network (CNN) architecture with a loss function being assisted by ASM for face alignment and estimating head pose in the wild. We use ASM to first guide the network towards learning a smoother distribution of the facial landmark points. Inspired by transfer learning, during the training process, we gradually harden the regression problem and guide the network towards learning the original landmark points distribution. We define multi-tasks in our loss function that are responsible for detecting facial landmark points as well as estimating the face pose. Learning multiple correlated tasks simultaneously builds synergy and improves the performance of individual tasks. We compare the performance of our proposed model called ASMNet with MobileNetV2 (which is about 2 times bigger than ASMNet) in both the face alignment and pose estimation tasks. Experimental results on challenging datasets show that by using the proposed ASM assisted loss function, the ASMNet performance is comparable with MobileNetV2 in the face alignment task. In addition, for face pose estimation, ASMNet performs much better than MobileNetV2. ASMNet achieves an acceptable performance for facial landmark points detection and pose estimation while having a significantly smaller number of parameters and floating-point operations compared to many CNN-based models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Facial Landmark Points Detection is an essential task in many facial image analyses and applications. It is crucial for facial image alignment, face recognition <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b34">35]</ref>, pose estimation <ref type="bibr" target="#b40">[41]</ref>, and facial expression recogni- tion <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b55">56]</ref>. Active Shape Model (ASM) introduced by Tim Cootes <ref type="bibr" target="#b8">[9]</ref> is among the first methods designed for facial landmark points detection. ASM is a statistical shape model made out of object samples. ASM and its variant, Active Appearance Models (AAM) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b24">25]</ref>, can guide learning algorithms to fit a set of points (e.g., facial points) representing an object into an image containing the object instance. In better words, ASM guides the learning algorithm to iteratively deforms the model to find the best match po-sition between the model and the data in a new image. AS-M/AAM and their predecessors' deformable models <ref type="bibr" target="#b33">[34]</ref> have been studied well for landmark point detection in facial image analysis and human body joint tracking. We propose to use ASM in a deep convolutional neural network (CNN) for facial landmark point detection and head pose estimation.</p><p>Although most of the existing computer vision methods have focused on facial landmark points detection and pose estimation as separate tasks, some recent works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56]</ref> show that learning correlated tasks simultaneously can improve the system accuracy. For instance, the authors of <ref type="bibr" target="#b51">[52]</ref> explain that information contained in the features is distributed throughout deep neural networks hierarchically. More specifically, while lower layers contain information about edges, and corners and hence are more appropriate for localization tasks such as facial landmark points detection and pose estimation, deeper layers contain more abstract information which is more suitable for classification tasks <ref type="bibr" target="#b27">[28]</ref>. Inspired by the idea of multi-task learning, we design our CNN model and hence the associated loss function to learn multiple correlated tasks simultaneously.</p><p>Several methods have been proposed for facial landmark points detection such as Constrained Local Model-Based Methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11]</ref>, AAM <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b24">25]</ref>, part models <ref type="bibr" target="#b59">[60]</ref>, and Deep Learning (DL) based methods <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b53">54]</ref>. Although DL-based methods are considered as state-of-the-art methods, facial landmark points detection is still considered a challenging task specifically for faces with large pose variations <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b43">44]</ref>. Accordingly, the price to pay to achieve a high accuracy is the rise in computational complexity and the fall in efficiency. Recent methods have focused on improving the accuracy and this is normally achieved by introducing new layers and consequently increasing the number of parameters as well as longer inference time. These methods prove to be accurate and successful in desktop and server applications, but with the growth of IoT, mobile devices, and robotics, there is a growing need for more accurate and efficient algorithms. There are a few networks that are designed for low-power devices. One of the most popular ones is MobileNetV2 <ref type="bibr" target="#b32">[33]</ref> which has proven to be a good feature extractor <ref type="bibr" target="#b16">[17]</ref>.</p><p>In this paper, we propose a new network structure that is inspired by MobileNetV2 <ref type="bibr" target="#b32">[33]</ref> and is specifically designed for facial landmark points detection with the focus on making the network shallow and small without losing much accuracy. To achieve this goal we propose a new loss function that employs ASM as an assistant loss and uses multi-task learning to improve the accuracy. <ref type="figure" target="#fig_0">Fig. 1</ref> depicts a general framework of our proposed idea. We tested our proposed method with the challenging 300W <ref type="bibr" target="#b30">[31]</ref> dataset, and the Wider Facial Landmarks in the Wild (WFLW) <ref type="bibr" target="#b43">[44]</ref> dataset. Our experimental results show that the accuracy of facial landmark points detection and pose estimation is comparable with the state-of-the-art methods while the size of the network is 2 times smaller than MobileNetV2 <ref type="bibr" target="#b32">[33]</ref>.</p><p>The remainder of this paper is organized as follows. Sec. 2 reviews the related work in facial landmark points detection, pose detection, and small neural networks. Sec. 3 describes the architecture of our proposed network, the ASM assisted loss function and the training method. Experimental results are provided in Sec. 4. Finally, Sec. 5 concludes the paper with some discussions on the proposed method and future research directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Automated facial landmark points detection has been studied extensively by the computer vision community. Zhou etal. <ref type="bibr" target="#b56">[57]</ref> classified facial landmark points detection methods into two categories: regression-based and template fitting methods. Regression-based methods consider a facial image as a vector and use a transformation such as Principle Component Analysis (PCA), Discrete Cosine Transform (DCT) <ref type="bibr" target="#b31">[32]</ref>, or Gabor Wavelets <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b42">43]</ref> to transform the image into another domain. Then a classification algorithm such as SVM <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14]</ref> or boosted cascade detector <ref type="bibr" target="#b41">[42]</ref> is used to detect facial landmarks. In contrast, template fitting methods such as Active Shape Models (ASM) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27]</ref> and Active Appearance Models (AAM) <ref type="bibr" target="#b9">[10]</ref> constrain the search for landmark positions by using prior knowledge. Inspired by the ASM method, we define a loss term that applies a constraint to the shapes learned during the training process.</p><p>Recently, Deep Learning techniques have dominated state-of-the-art results in terms of performance and robustness. There have been several new CNN-based methods for facial landmark points detection. Sun Y. etal. <ref type="bibr" target="#b35">[36]</ref> proposed a deep CNN cascade to extract the facial key-points back in 2013. Zhang Z. etal. <ref type="bibr" target="#b53">[54]</ref> proposed a multi-task approach in which instead of solving FLP detection as an isolated task, they bundle it with similar tasks such as head pose estimation into a deep neural network to increase robustness and accuracy. Ranjan R. etal. <ref type="bibr" target="#b27">[28]</ref> also uses deep multi-task learning and achieves high accuracy when detecting facial landmark. Several studies fit a 3D model onto the face and then infer the landmark positions <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b58">59]</ref>.</p><p>One related task that can be trained simultaneously with facial landmark points detection, is head pose estimation. Detecting facial landmarks and estimating head pose can be made easier using 3D information <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b58">59]</ref>, however, this information is not always available. Wu etal. <ref type="bibr" target="#b45">[46]</ref> propose a unified model for simultaneous facial landmark points detection, head pose estimation, and facial deformation analysis. This approach is robust to occlusion which is the result of the interaction between these tasks. One approach to estimate the head pose is to use the facial landmark point and head pose estimator sequentially <ref type="bibr" target="#b40">[41]</ref>. We proposed a multi-task learning approach to tackle the problem of facial landmark points detection and pose estimation using a loss function assisted by ASM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed ASM Network</head><p>We first review the Active Shape Model (ASM) algorithm and then introduce our proposed network architecture for landmark point localization and pose estimation. Finally, we explain our customized loss function based on ASM that improves the accuracy of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Active Shape Model Review</head><p>Active Shape Model is a statistical model of shape objects.</p><p>Each shape is represented as n points, S set = {(S 1</p><p>x , S 1 y ), ..., (S n x , S n y )} that are aligned into a common coordinate system. To simplify the problem and learn shape components, Principal Component Analysis (PCA) is applied to the covariance matrix calculated from a set of K training shape samples. Once the model is built, an approximation of any training sample (S) is calculated using Eq. 1:</p><formula xml:id="formula_0">S ? S + P b<label>(1)</label></formula><p>where S is the sample mean, P = (p 1 , p 2 , ..., p t ) contains t eigenvectors of the covariance matrix and b is a t dimensional vector given by Eq. 2:</p><formula xml:id="formula_1">b = P (S ? S)<label>(2)</label></formula><p>Consequently, a set of parameters of a deformable model is defined by vector b, so that by varying the elements of the vector, the shape of the model is changed. Consider that the statistical variance (i.e., eigenvalue) of the i th parameter of b is ? i . To make sure the generated image after applying ASM is relatively similar to the ground truth, the parameter b i of vector b is usually limited to ?3 ? ? i <ref type="bibr" target="#b6">[7]</ref>. This constraint ensures that the generated shape is similar to those in the original training set. Hence, we create a new shape S N ew after applying this constraint, according to Eq. 3:</p><formula xml:id="formula_2">S N ew = S + Pb<label>(3)</label></formula><p>whereb is the constrained b. We also define ASM operator according to Eq. 4:</p><formula xml:id="formula_3">ASM : (P i x , P i y ) ? (A i x , A i y )<label>(4)</label></formula><p>ASM transforms each input point (P i x , P i y ) to a new point (A i x , A i y ) using Eqs. 1, 2, and 3. In this paper, we propose a deep convolutional neural network architecture that utilizes ASM in the training loss function. Our proposed network (ASMNet) is significantly smaller than its predecessor, MobileNetV2 <ref type="bibr" target="#b32">[33]</ref>, while its performance is comparable with MobileNetV2 <ref type="bibr" target="#b32">[33]</ref> in localizing landmark points and much better in pose estimation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Proposed ASMNet Architecture</head><p>MobileNet and its variants <ref type="bibr" target="#b32">[33]</ref> have received great attention as one of the most well-known deep neural networks for operation on embedded and mobile devices. Especially, MobileNetV2 <ref type="bibr" target="#b32">[33]</ref> is shown to cope well with complex tasks such as image classification, object detection, and semantic segmentation. We have designed a network that is about two times smaller than MobileNetV2 <ref type="bibr" target="#b32">[33]</ref>, both in terms of the number of parameters, and FLOPs. In designing ASMNet, we only use the first 15 blocks of MobileNetV2 <ref type="bibr" target="#b32">[33]</ref> while the main architecture which has 16 blocks. Nevertheless, creating a shallow network would eventually lower the final accuracy of the system. To avoid this problem we purposefully add a few new layers. <ref type="figure" target="#fig_1">Fig. 2</ref> shows the architecture of ASMNet.</p><p>According to <ref type="bibr" target="#b51">[52]</ref> the features in a Convolutional Neural Network (CNN) are distributed hierarchically. In other words, lower layers have features such as edges, and corners which are more suitable for tasks like landmark localization and pose estimation, and deeper layers contain more abstract features that are more suitable for tasks like image classification and image detection. Training a network for correlated tasks simultaneously builds a synergy that can improve the performance of each task <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b53">54]</ref>.</p><p>Motivated by the approach in <ref type="bibr" target="#b27">[28]</ref>, we designed a multi-task CNN to detect facial landmarks as well as estimating the pose of the faces (pitch, roll, and yaw) simultaneously. In order to use features from different layers, we have created shortcuts from block-1-batch-normalization, block-3-batch-normalization, block-6-batch-normalization, block-10-batch-normalization, and finally block-13-batchnormalization. We connect each of these shortcuts to the output of block 15 of MobileNetV2 <ref type="bibr" target="#b32">[33]</ref>, block-15-add, using a global average pooling layer. Finally, we concatenate all the global average pooling layers. Such architecture enables us to use features that are available in different layers of the network while keeping the number of the FLOPs small. In other words, since the original MobileNetV2 <ref type="bibr" target="#b32">[33]</ref> is designed for image classification task -where the more abstract features are required -it might not be suitable for face alignment task -which needs both abstract features that are available in the deeper layers as well as features that are available in the lower layers such as edges and corners -. We Designed ASMNet (see <ref type="figure" target="#fig_1">Fig. 2</ref>), by fusing the features that are available if different layers of the model. Furthermore, by concatenating the features that are collected after each global average pooling layer in the backpropagation process, it will be possible for the network to evaluate the effect of each shortcut path.</p><p>Moreover, we add another correlated task to the network. As <ref type="figure" target="#fig_1">Fig. 2</ref> shows, the proposed network predicts 2 different outputs: the facial landmark points (the main output of the network), as well as the face pose. While the correlation and the synergy between these two tasks can result in more accurate results, we also wanted our lightweight ASMNet to be able to predict face pose as well so that it might be used in more applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">ASM Assisted Loss Function</head><p>In the following, we describe the loss functions for two different tasks. These tasks are responsible for facial landmark points detection, and pose estimation. Facial landmark points detection task: The common loss function for facial landmark points detection is Mean Square Error (MSE). We propose a new loss function that including MSE, as the main loss as well an the assistant loss which utilizes ASM to improve the accuracy of the network called ASM-LOSS.</p><p>The proposed ASM-LOSS guides the network to first learn the smoothed distribution of the facial landmark points. In other words, during the training process, the loss function compares the predicted facial landmark points with their corresponding ground truth as well as the smoothed version the ground truth which is generated using ASM. Given this, in the early stage of training, we set a bigger weight to the ASM-LOSS in comparison to the main loss -which is MSE -, since the variation of the smoothed facial landmark points are much lower than the original land-mark points, and as a rule of thumb, easier to be learned by a CNN. Then, by gradually decrease the weight of the ASM-LOSS, we lead the network to focus more on the original landmark points. In practice, we figured out that this method, which is also can be taken to account as transfer learning, works out well and results in more accurate models.</p><p>We also discover that although face pose estimation has a heavy reliance on face alignment, it can achieve good accuracy with the assistant of smoothed facial landmark points as well. In other words, if the performance of facial landmark point detection task is acceptable, which means network can predict facial landmark such that the whole shape of the face is correct, the pose estimation can achieve a good accuracy. Accordingly, using smoothed landmark points and training network using ASM-LOSS will results in a more accuracy in pose estimation task.</p><p>Consider that for each image in the training set, there exists n landmark points in a set called G such that (G i x , G i y ) is the coordinates for the i th landmark point. Similarly, a predicted set P contains n points such that (P i x , P i y ) is the predicted coordinates for the i th landmark point.</p><formula xml:id="formula_4">G set = {(G 1 x , G 1 y ), ..., (G n x , G n y )} P set = {(P 1 x , P 1 y ), ..., (P n x , P n y )}<label>(5)</label></formula><p>We apply PCA on the training set and calculate eigenvectors and eigenvalues. Then, we calculate set A, which contains n points and each point is the transformation of the corresponding point in G, by applying the ASM operator according to Eq. 4:</p><formula xml:id="formula_5">A set = {(A 1 x , A 1 y ), ..., (A n x , A n y )} ASM : (G i x , G i y ) ? (A i x , A i y )<label>(6)</label></formula><p>We define the main facial landmark point loss, Eq. 7, as the Mean Square Error between the ground truth (G) and the predicted landmark points (P).</p><formula xml:id="formula_6">L mse = 1 N 1 n N j=1 n i=1 G i j ? P i j 2<label>(7)</label></formula><p>where N is the total number of images in the training set and</p><formula xml:id="formula_7">G i j = (G i x , G i y )</formula><p>shows the i th landmark of the j th sample in the training set. We calculate ASM-LOSS as the error between ASM points (A set ), and predicted landmark points (P set ) using Eq. 8:</p><formula xml:id="formula_8">L asm = 1 N 1 n N j=1 n i=1 A i j ? P i j 2<label>(8)</label></formula><p>Finally, we calculate the total loss for the facial landmark task with according to Eq. 9:</p><formula xml:id="formula_9">L f acial = L mse + ? ? L asm<label>(9)</label></formula><p>The accuracy of PCA have a heavy reliance on the ASM points (A set ), which means that the more accurate the PCA, the less the discrepancy between the ground truth (G) and the ASM points (A set ). To be more detailed, by reducing the accuracy of PCA, the generated ASM points (A set ), will be more similar to the average point set, which is the average of all the ground truth face objects in the training sets. Consequently, predicting points in A set is easier than the points in the G set since the variation of latter is lower than the variation of the former. We use this feature to design our loss function such that we first guide the network towards learning the distribution of the smoothed landmark points -which is easier to be learned -and gradually harden the problem by decreasing the weight of ASM-LOSS. We define ? as ASM-LOSS weight using Eq. 10:</p><formula xml:id="formula_10">? = ? ? ? 2 i &lt; l 3 1 l 3 &lt; i &lt; 2l 3 0.5 i &gt; 2l 3<label>(10)</label></formula><p>where i is the epoch number and l is the total number of training epochs. As shown in Eqs. 9, at the beginning of the training, the value of ? is higher, which means we put more emphasize on ASM-LOSS. Hence, the network focuses more on predicting a simpler task and converges faster. Then after one-third of total epochs, we reduce ? to 1, and put equal emphasis on the main MSE loss ASM-LOSS. Finally, after two-third of total epochs, by reducing ? to 0.5, we direct the network toward predicting the main ground truths, while considering the smoothed points generated using ASM as an assistant. We also show experimentally in Sec. 4, that such technique leads to more accurate results, specifically when it comes to a lightweight network like ASMNet. Pose estimation task: We use mean square error to calculate the loss for the head pose estimation task. Eq. 11 defines the loss function L pose , where yaw(y p ), pitch(p p ), and roll(r p ) are the predicted poses and y t , p t , and r t are the corresponding ground truths.</p><formula xml:id="formula_11">L pose = 1 N N j=1 (y p j ? y t j ) 2 + (p p j ? p t j ) 2 + (r p j ? r t j ) 2 3 (11)</formula><p>Finally, we calculate the total loss as the total weighted loss of the 2 individual losses using Eq. 12:</p><formula xml:id="formula_12">L = 2 i=1 ? taski L taski<label>(12)</label></formula><p>such that task i is the i th element of the task set T = { L f acial , L pose } and the value of ? taski corresponds to the importance of the i th task. Since we define facial landmark points detection task to be more important than pose estimation, we choose ? task = {1, 0.5}. <ref type="figure" target="#fig_0">Fig. 1</ref> illustrates the process of calculating the total loss value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Training Phase</head><p>300W.We followed the protocol described in <ref type="bibr" target="#b28">[29]</ref> to train our networks on the 300W <ref type="bibr" target="#b30">[31]</ref> dataset. We use 3,148 faces consisting of 2,000 images from the training subset of HELEN <ref type="bibr" target="#b22">[23]</ref> dataset, 811 images from the training subset of LFPW <ref type="bibr" target="#b3">[4]</ref> dataset, and 337 images from the full set of AFW <ref type="bibr" target="#b59">[60]</ref>  WFLW. WFLW <ref type="bibr" target="#b43">[44]</ref>, containing 7500 images for training and 2500 images for testing, is another widely used dataset, recently has been proposed based on WIDER FACE <ref type="bibr" target="#b50">[51]</ref>. Each image in this dataset contains 98 manual annotated landmarks. In order to be able to evaluate the models under different circumstances, WFLW <ref type="bibr" target="#b43">[44]</ref> provides 6 different subsets including 314 expression images, 326 large pose images, 206 make-up images, 736 occlusion images, 698 illumination images, and 773 blur images.</p><p>We use the method and algorithm in <ref type="bibr" target="#b29">[30]</ref> to calculate the yaw, roll, and, pitch for each image in the dataset since to the best of our knowledge, no dataset provides the annotation for facial landmark points and face pose jointly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>For the training set in each dataset, we crop all the images and extract the face region. Then the face images are scaled to 224 ? 224 pixels. We augment the images (in terms of contrast, brightness, and color) to add robustness of data variation to the network. We use Adam optimizer for training the networks with learning rate 10 ?2 , ? 1 = 0.9, ? 2 = 0.999, and decay = 10 ?5 . Then we train networks for about 150 epochs with a batch size of 50. We implemented our codes using the TensorFlow library and run them on a NVidia 1080Ti GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation Metrics</head><p>We follow the previous works and employ normalized mean error (NME) to measure the accuracy of our model. We define the normalising factor, followed by MDM <ref type="bibr" target="#b37">[38]</ref> and <ref type="bibr" target="#b30">[31]</ref> as "inter-ocular" distance (the distance between the outer-eye-corners). Furthermore, we calculate failure rate (FR), defined as the proportion of failed detected faces, for a maximum error of 0.1. Cumulative Errors Distribution (CED) curve as well as the area-under-thecurve (AUC) <ref type="bibr" target="#b48">[49]</ref> is also reported. Besides, we use mean absolute error (MAE) for evaluating the pose estimation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with Other Models</head><p>We conducted four different experiments to evaluate the effectiveness of the proposed ASM assisted loss function. These experiments are designed to assess the performance of MobileNetV2 <ref type="bibr" target="#b32">[33]</ref> and ASMNet, with and without the proposed ASM assisted loss function. <ref type="table" target="#tab_1">Table 1</ref> shows the results of the experiments on Full subsets of 300W <ref type="bibr" target="#b30">[31]</ref> (full), and WFLW <ref type="bibr" target="#b43">[44]</ref>, as well as the number of network parameters(#Params) and the sum of the FLOPs. For simplicity, we name our model as "mnv2" (MobileNetV2 <ref type="bibr" target="#b32">[33]</ref> trained using standard MSE loss function), "mnv2 r" (MobileNetV2 <ref type="bibr" target="#b32">[33]</ref> trained using our ASM assisted loss function), "ASMNet nr" (ASMNet trained using standard MSE loss function), and "ASMNet" ( ASMNet trained using our ASM assisted loss function). <ref type="table" target="#tab_1">Table 1</ref> shows that the proposed ASM assisted loss function has a lower NME in both cases. Furthermore, while our proposed network architecture is about two times smaller than MobileNetV2 <ref type="bibr" target="#b32">[33]</ref>, its performance is comparable with it after applying our proposed ASM assisted loss function. It means that without sacrificing accuracy, we have created a network that is smaller and faster in comparison to Mobile-NetV2 <ref type="bibr" target="#b32">[33]</ref>. Such characteristics make the ASMNet suitable for running on mobile and embedded devices.</p><p>Evaluation on 300W. The 300W <ref type="bibr" target="#b30">[31]</ref> dataset is a very challenging benchmark in facial landmark detection task. <ref type="table" target="#tab_2">Table 2</ref> shows a comparison between ASMNet and the state-of-the-art methods. Although the performance of ASMNet does not outperform the state-of-the-art methods, comparing the number of the parameters, and FLOPs of the models (see <ref type="table" target="#tab_6">Table 6</ref>), the accuracy of our proposed model is comparable and accurate in the context of small networks such as MobileNetV2 <ref type="bibr" target="#b32">[33]</ref>. Furthermore, As the table 2 shows, the performance of the ASMNet with ASM assisted loss function on 300W <ref type="bibr" target="#b30">[31]</ref> is better than the performance of ASMNet without the assisted loss. <ref type="figure" target="#fig_2">Fig. 3</ref> shows the some example of facial landmark detection using ASMNet on the Challenging subset of 300W <ref type="bibr" target="#b30">[31]</ref> dataset. As we can see, ASMNet performs well, even in challenging face images.   Evaluation on WFLW. <ref type="table" target="#tab_3">Table 3</ref> shows the performance of the state-of-the-art method and our proposed method over WFLW <ref type="bibr" target="#b43">[44]</ref> and its 6 subsets. The performance of ASMNet is comparable to the performance of MobileNetV2 <ref type="bibr" target="#b32">[33]</ref>. In other words, using the proposed ASM assisted loss function improves the model accuracy. <ref type="figure">Fig. 4</ref> shows the some example of facial landmark detection using ASMNet on WFLW <ref type="bibr" target="#b43">[44]</ref> dataset. While ASMNet can be taken as a very lightweight model, its performance is acceptable under different circumstances such as occlusion, extreme pose, expression, illumination, blur, and make-up.</p><p>Pose Evaluation. Neither 300W <ref type="bibr" target="#b30">[31]</ref> nor WFLW <ref type="bibr" target="#b43">[44]</ref> dataset do not provide the head pose information. Accordingly, we followed the method used by <ref type="bibr" target="#b47">[48]</ref> and used another application to synthesizes the pose information. Although <ref type="bibr" target="#b47">[48]</ref> used <ref type="bibr" target="#b2">[3]</ref> for synthesizing the pose information, we used HopeNet <ref type="bibr" target="#b29">[30]</ref> which is a state-of-the-art pose estimation method. Using HopeNet we acquired the yaw, pitch, and roll values of the 300W <ref type="bibr" target="#b30">[31]</ref>, and WFLW <ref type="bibr" target="#b43">[44]</ref> images and used them as the ground truths for our network. <ref type="table" target="#tab_4">Table 4</ref> shows the mean absolute error (MAE) between HopeNet <ref type="bibr" target="#b29">[30]</ref> results and our ASMNet.  <ref type="figure">Figure 4</ref>: Facial Landmark detection using ASMNet over WFLW <ref type="bibr" target="#b43">[44]</ref> dataset.</p><p>In addition, we compare the performance of our proposed method with <ref type="bibr" target="#b47">[48]</ref> as well as <ref type="bibr" target="#b49">[50]</ref> in <ref type="table" target="#tab_5">Table 5</ref> using Full subset of 300W <ref type="bibr" target="#b30">[31]</ref> dataset. As the results show, the performance of our lightweight ASMNet is comparable to HopeNet <ref type="bibr" target="#b29">[30]</ref>, which is a state-of-the-art method and outperforms the other methods as well. Besides, the  performance of ASMNet is better than MobileNetV2 <ref type="bibr" target="#b32">[33]</ref>, even when it utilizes the ASM-LOSS function. Since in pose estimation task aligning the whole shape of the face is more crucial than aligning each landmark point, using ASM-LOSS function will lead to better performance.  Moreover, ASMNet is designed to use features generated in different layers of the neural network which enables it to outperforms MobileNetV2 <ref type="bibr" target="#b32">[33]</ref> in pose estimation task. <ref type="figure" target="#fig_3">Fig. 5</ref> shows the output of the pose estimation task.</p><p>Ablation Study. In <ref type="table" target="#tab_7">Table 7</ref> we study the ASM assisted loss by calculating the difference between normalized mean errors with and without ASM assisted loss both on ASMNet and MobileNetV2 <ref type="bibr" target="#b32">[33]</ref>. As shown, using ASMNet utilized with ASM-LOSS function results in 0.96%, and 1.19% reduction in NME on 300W <ref type="bibr" target="#b30">[31]</ref>, and WFLW <ref type="bibr" target="#b43">[44]</ref> respectively. Theses numbers are 0.11%, and 0.16% for Mobile-NetV2 <ref type="bibr" target="#b32">[33]</ref>. According to the <ref type="table" target="#tab_7">Table 7</ref>, and <ref type="figure">Fig. 6</ref>, using the ASM assisted loss function resulted in more accuracy improvement for ASMNet compared to MobileNetV2 <ref type="bibr" target="#b32">[33]</ref>. Hence, it can be concluded that the ASM-LOSS function is capable of helping the lightweight CNN much more. In other words, when a lightweight network does not perform accurately enough, using the proposed ASM-LOSS function will play a vital role in improving the performance.</p><p>Model Size and Computational Cost Analysis. We calculate the number of network parameters as well as FLOPs to evaluate the model size and computational complexity. We calculate the FLOPs over the resolution of 224 ? 224. As <ref type="table" target="#tab_6">Table 6</ref>, although ASMNet is the smallest, its performance is comparable with MobileNetV2 <ref type="bibr" target="#b32">[33]</ref>, one of the best in compact-class models. Furthermore, since the idea behind ASMNet is to put a trade-off between accuracy and model performance, as we can see in <ref type="table" target="#tab_6">Table 6</ref>, adding ASM assisted loss to a lightweight model such as <ref type="figure">Figure 6</ref>: Comparing the performance of ASMNet, as well as MobileNetV2 <ref type="bibr" target="#b32">[33]</ref> with and without the proposed ASM assisted loss function on 300W <ref type="bibr" target="#b30">[31]</ref>. ASMNet, and MobileNetV2 <ref type="bibr" target="#b32">[33]</ref>, results in the accuracy improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>In this paper, we proposed ASMNet, a lightweight CNN architecture with multi-task learning for facial landmark points detection and pose estimation. We proposed a loss function that is assisted using ASM <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27]</ref> that increases the network accuracy. We built our network (called ASMNet) using a small portion of MobileNetV2 <ref type="bibr" target="#b32">[33]</ref>. The proposed ASMNet architecture is about 2 times smaller than Mobile-NetV2 <ref type="bibr" target="#b32">[33]</ref>, while the accuracy remains at the same rate. The results of evaluating ASMNet and our proposed ASM assisted loss on widely used 300W <ref type="bibr" target="#b30">[31]</ref>, and WFLW <ref type="bibr" target="#b43">[44]</ref> datasets show that the accuracy of ASMNet is acceptable in detecting facial landmark points and estimating head pose. The proposed method has the potential to be used in other computer vision tasks such as human body joint tracking or other shape objects that can be modeled using ASM. Hence, as a future research direction, we will investigate using ASMNet for such applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The proposed loss function (Loss total ) learns two main tasks simultaneously and uses ASM as assistant loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The architecture of the ASMNet network. ASM-Net designed using first 15 blocks of MobileNetV2<ref type="bibr" target="#b32">[33]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Facial Landmark detection using ASMNet over 300W<ref type="bibr" target="#b30">[31]</ref> Challenging subset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>ASMNet can also estimate the head pose even in challenging conditions. The input images are from 300W<ref type="bibr" target="#b30">[31]</ref> Challenging set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Number of parameters in million (M) andFLOPs in billion (B), as well as Normalized Mean Error (NME in %) of landmarks localization on 300W<ref type="bibr" target="#b30">[31]</ref>, and WFLW<ref type="bibr" target="#b43">[44]</ref> datasets.</figDesc><table><row><cell>Method</cell><cell></cell><cell>NME</cell><cell cols="2">Params (M) FLOPs (B)</cell></row><row><cell></cell><cell cols="2">300W WFLW</cell><cell></cell><cell></cell></row><row><cell>mnv2 mnv2 r</cell><cell>4.70 4.59</cell><cell>9.57 9.41</cell><cell>2.42</cell><cell>0.60</cell></row><row><cell>ASMNet nr ASMNet</cell><cell>6.49 5.50</cell><cell>11.96 10.77</cell><cell>1.43</cell><cell>0.51</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Normalized</figDesc><table><row><cell></cell><cell cols="3">Mean Error (in %) of 68-point land-</cell></row><row><cell cols="3">marks localization on 300W [31] dataset.</cell><cell></cell></row><row><cell>Method</cell><cell cols="3">Normalized Mean Error Common Challenging Fullset</cell></row><row><cell>RCN [16]</cell><cell>4.67</cell><cell>8.44</cell><cell>5.41</cell></row><row><cell>DAN [21]</cell><cell>3.19</cell><cell>5.24</cell><cell>3.59</cell></row><row><cell>PCD-CNN [22]</cell><cell>3.67</cell><cell>7.62</cell><cell>4.44</cell></row><row><cell>CPM [13]</cell><cell>3.39</cell><cell>8.14</cell><cell>4.36</cell></row><row><cell>DSRN [26]</cell><cell>4.12</cell><cell>9.68</cell><cell>5.21</cell></row><row><cell>SAN [12]</cell><cell>3.34</cell><cell>6.60</cell><cell>3.98</cell></row><row><cell>LAB [44]</cell><cell>2.98</cell><cell>5.19</cell><cell>3.49</cell></row><row><cell>DCFE [40]</cell><cell>2.76</cell><cell>5.22</cell><cell>3.24</cell></row><row><cell>mnv2</cell><cell>3.93</cell><cell>7.52</cell><cell>4.70</cell></row><row><cell>mnv2 r</cell><cell>3.88</cell><cell>7.35</cell><cell>4.59</cell></row><row><cell>ASMNet nr</cell><cell>5.86</cell><cell>8.80</cell><cell>6.46</cell></row><row><cell>ASMNet</cell><cell>4.82</cell><cell>8.2</cell><cell>5.50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Normalized Mean Error (in %), failure rate (in %), and AUC of 98-point landmarks localization on WFLW<ref type="bibr" target="#b43">[44]</ref> dataset.</figDesc><table><row><cell cols="2">Metric Method</cell><cell>Test set</cell><cell>Pose</cell><cell cols="3">Expression Illumination Make-Up</cell><cell>Occlusion</cell><cell>Blur</cell></row><row><cell></cell><cell>ESR [5]</cell><cell>11.13</cell><cell>25.88</cell><cell>11.47</cell><cell>10.49</cell><cell>11.05</cell><cell>13.75</cell><cell>12.20</cell></row><row><cell></cell><cell>SDM [47]</cell><cell>10.29</cell><cell>24.10</cell><cell>11.45</cell><cell>9.32</cell><cell>9.38</cell><cell>13.03</cell><cell>11.28</cell></row><row><cell>Mean Error (%)</cell><cell>CFSS [58] DVLN [45] LAB [44] ResNet50(Wing+PDB) [15] mnv2 mnv2 r</cell><cell>9.07 6.08 5.27 5.11 9.57 9.41</cell><cell>21.36 11.54 10.24 8.75 18.18 17.86</cell><cell>10.09 6.78 5.51 5.36 9.93 9.78</cell><cell>8.30 5.73 5.23 4.93 8.98 8.90</cell><cell>8.74 5.98 5.15 5.41 9.92 9.67</cell><cell>11.76 7.33 6.79 6.37 11.38 11.25</cell><cell>9.96 6.88 6.32 5.81 10.79 10.66</cell></row><row><cell></cell><cell>ASMNet nr</cell><cell>11.96</cell><cell>21.95</cell><cell>13.08</cell><cell>11.02</cell><cell>11.84</cell><cell>13.24</cell><cell>12.60</cell></row><row><cell></cell><cell>ASMNet</cell><cell>10.77</cell><cell>21.11</cell><cell>12.02</cell><cell>9.93</cell><cell>10.55</cell><cell>12.34</cell><cell>11.62</cell></row><row><cell></cell><cell>ESR [5]</cell><cell>35.24</cell><cell>90.18</cell><cell>42.04</cell><cell>30.80</cell><cell>38.84</cell><cell>47.28</cell><cell>41.40</cell></row><row><cell></cell><cell>SDM [47]</cell><cell>29.40</cell><cell>84.36</cell><cell>33.44</cell><cell>26.22</cell><cell>27.67</cell><cell>41.85</cell><cell>35.32</cell></row><row><cell>Failure Rate</cell><cell>CFSS [58] DVLN [45] LAB [44] ResNet50(Wing+PDB) [15] mnv2</cell><cell>20.56 10.84 7.56 6.00 30.64</cell><cell>66.26 46.93 28.83 22.70 88.03</cell><cell>23.25 11.15 6.37 4.78 34.07</cell><cell>17.34 7.31 6.73 4.30 25.39</cell><cell>21.84 11.65 7.77 7.77 32.03</cell><cell>32.88 16.30 13.72 12.50 41.84</cell><cell>23.67 13.71 10.74 7.76 38.80</cell></row><row><cell></cell><cell>mnv2 r</cell><cell>30.04</cell><cell>88.65</cell><cell>31.52</cell><cell>24.67</cell><cell>30.09</cell><cell>41.44</cell><cell>37.25</cell></row><row><cell></cell><cell>ASMNet nr</cell><cell>50.2</cell><cell>98.46</cell><cell>70.38</cell><cell>43.68</cell><cell>50.0</cell><cell>59.78</cell><cell>56.14</cell></row><row><cell></cell><cell>ASMNet</cell><cell>39.12</cell><cell>98.41</cell><cell>59.87</cell><cell>33.38</cell><cell>38.34</cell><cell>48.64</cell><cell>46.31</cell></row><row><cell></cell><cell>ESR [5]</cell><cell>0.2774</cell><cell>0.0177</cell><cell>0.1981</cell><cell>0.2953</cell><cell>0.2485</cell><cell>0.1946</cell><cell>0.2204</cell></row><row><cell></cell><cell>SDM [47]</cell><cell>0.3002</cell><cell>0.0226</cell><cell>0.2293</cell><cell>0.3237</cell><cell>0.3125</cell><cell>0.2060</cell><cell>0.2398</cell></row><row><cell>AUC</cell><cell>CFSS [58] DVLN [45] LAB [44]</cell><cell>0.3659 0.4551 0.5323</cell><cell>0.0632 0.1474 0.2345</cell><cell>0.3157 0.3889 0.4951</cell><cell>0.3854 0.4743 0.5433</cell><cell>0.3691 0.4494 0.5394</cell><cell>0.2688 0.3794 0.4490</cell><cell>0.3037 0.3973 0.4630</cell></row><row><cell></cell><cell>ResNet50(Wing+PDB) [15]</cell><cell>0.5504</cell><cell>0.3100</cell><cell>0.4959</cell><cell>0.5408</cell><cell>0.5582</cell><cell>0.4885</cell><cell>0.4918</cell></row><row><cell></cell><cell>mnv2</cell><cell>0.2388</cell><cell>0.0096</cell><cell>0.1812</cell><cell>0.2510</cell><cell>0.2147</cell><cell>0.1719</cell><cell>0.1852</cell></row><row><cell></cell><cell>mnv2 reg</cell><cell>0.2447</cell><cell>0.0099</cell><cell>0.1836</cell><cell>0.2563</cell><cell>0.2282</cell><cell>0.1779</cell><cell>0.1880</cell></row><row><cell></cell><cell>ASMNet nr</cell><cell>0.1024</cell><cell>0.0008</cell><cell>0.0414</cell><cell>0.1129</cell><cell>0.0941</cell><cell>0.0729</cell><cell>0.0797</cell></row><row><cell></cell><cell>ASMNet</cell><cell>0.1637</cell><cell>0.0010</cell><cell>0.0714</cell><cell>0.1826</cell><cell>0.1653</cell><cell>0.1202</cell><cell>0.1268</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Mean Absolute Error of pose estimation on 300W<ref type="bibr" target="#b30">[31]</ref>, WFLW<ref type="bibr" target="#b43">[44]</ref> datasets compared to HopeNet<ref type="bibr" target="#b29">[30]</ref>.</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="4">ASMNet nr ASMNet mnv2 mnv2 r</cell></row><row><cell></cell><cell>yaw</cell><cell>2.41</cell><cell>1.62</cell><cell>1.75</cell><cell>1.71</cell></row><row><cell>300W [31]</cell><cell>pitch</cell><cell>1.87</cell><cell>1.80</cell><cell>1.93</cell><cell>1.89</cell></row><row><cell></cell><cell>roll</cell><cell>2.115</cell><cell>1.24</cell><cell>1.32</cell><cell>1.30</cell></row><row><cell></cell><cell>yaw</cell><cell>3.14</cell><cell>2.97</cell><cell>3.06</cell><cell>3.08</cell></row><row><cell>WFLW [44]</cell><cell>pitch</cell><cell>2.99</cell><cell>2.93</cell><cell>3.03</cell><cell>2.94</cell></row><row><cell></cell><cell>roll</cell><cell>2.23</cell><cell>2.21</cell><cell>2.26</cell><cell>2.22</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Mean Absolute Error of pose estimation on using ASMNet, JFA<ref type="bibr" target="#b47">[48]</ref>, and Yanget. al<ref type="bibr" target="#b49">[50]</ref> on 300W<ref type="bibr" target="#b30">[31]</ref>.</figDesc><table><row><cell>Method</cell><cell cols="3">Pitch Yaw Roll</cell></row><row><cell>Yanget. al [50]</cell><cell>5.1</cell><cell>4.2</cell><cell>2.4</cell></row><row><cell>JFA [48]</cell><cell>3.0</cell><cell>2.5</cell><cell>2.6</cell></row><row><cell>ASMNet</cell><cell cols="3">1.80 1.62 1.24</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Model size (the number of model parameters) and computational cost (FLOPs) analysis of different networks.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="2">#Params (M) FLOPs (B)</cell></row><row><cell>DVLN [45]</cell><cell>VGG-16</cell><cell>132.0</cell><cell>14.4</cell></row><row><cell>SAN [12]</cell><cell>ResNet-152</cell><cell>57.4</cell><cell>10.7</cell></row><row><cell>LAB [44]</cell><cell>Hourglass</cell><cell>25.1</cell><cell>19.1</cell></row><row><cell>ResNet50 (Wing + PDB) [15]</cell><cell>ResNet-50</cell><cell>25</cell><cell>3.8</cell></row><row><cell>ASMNet</cell><cell>MobileNetV2 [33]</cell><cell>1.4</cell><cell>0.5</cell></row><row><cell>MobileNetV2 [33]</cell><cell>-</cell><cell>2.4</cell><cell>0.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Investigating the effect of using ASM assisted loss function both on MobileNetV2<ref type="bibr" target="#b32">[33]</ref> and ASMNet.</figDesc><table><row><cell>Method</cell><cell cols="3">NME reduction (in %) ASMNet mnv2</cell></row><row><cell></cell><cell>Full</cell><cell>0.96</cell><cell>0.11</cell></row><row><cell>300W [31]</cell><cell>Common</cell><cell>1.58</cell><cell>0.05</cell></row><row><cell></cell><cell>Challenging</cell><cell>0.60</cell><cell>0.17</cell></row><row><cell></cell><cell>Full</cell><cell>1.19</cell><cell>0.16</cell></row><row><cell></cell><cell>Large pose</cell><cell>0.84</cell><cell>0.32</cell></row><row><cell></cell><cell>Expression</cell><cell>1.06</cell><cell>0.15</cell></row><row><cell>WFLW [44]</cell><cell>Illumination</cell><cell>1.09</cell><cell>0.08</cell></row><row><cell></cell><cell>Makeup</cell><cell>1.29</cell><cell>0.25</cell></row><row><cell></cell><cell>Occlusion</cell><cell>0.13</cell><cell>0.90</cell></row><row><cell></cell><cell>Blur</cell><cell>0.98</cell><cell>0.13</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Independent component analysis and support vector machine for face feature extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Antonini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Thiran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Audio-and Video-Based Biometric Person Authentication</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="111" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A face recognition system based on automatically determined facial fiducial points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Campadelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lanzarotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="432" to="443" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Robust discriminative response map fitting with constrained local models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Asthana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3444" to="3451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Localizing parts of faces using a consensus of exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2930" to="2940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Face alignment by explicit shape regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="177" to="190" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Joint cascade face detection and alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="109" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">An introduction to active shape models. Image processing and analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Baldock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Graham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="223" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="484" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Active shape models-their training and application. Computer vision and image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Graham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="38" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Statistical models of appearance for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Feature detection and tracking with constrained local models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cristinacce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bmvc</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Style aggregated network for facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="379" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Supervision-by-registration: An unsupervised approach to improve the precision of facial landmark detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-I</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="360" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Svm based asm for facial landmarks location</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th IEEE International Conference on Computer and Information Technology</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="321" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Wing loss for robust facial landmark localisation with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2235" to="2245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recombinator networks: Learning coarse-to-fine feature aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5743" to="5752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Speed/accuracy trade-offs for modern convolutional object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7310" to="7311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Coupling alignments with recognition for still-to-video face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3296" to="3303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pose-invariant 3d face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3694" to="3702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pose-invariant face alignment with a single cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3200" to="3209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep alignment network: A convolutional neural network for robust face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kowalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Naruniec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Trzcinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="88" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Disentangling 3d pose in a dendritic cnn for unconstrained 2d face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="430" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Interactive facial feature localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="679" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Surpassing human-level face verification performance on lfw with gaussianface</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-ninth AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Generative face alignment through 2.5 d active appearance models. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="250" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Direct shape regression networks for end-toend face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Athitsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5040" to="5049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Active shape models with invariant optimal features (iof-asm) application to cardiac mri segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ordas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Boisrobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huguet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frangi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computers in Cardiology</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="633" to="636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hyperface: A deep multi-task learning framework for face detection, landmark localization, pose estimation, and gender recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="121" to="135" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Face alignment at 3000 fps via regressing local binary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1685" to="1692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fine-grained head pose estimation without keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">300 faces in-the-wild challenge: The first facial landmark localization challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="397" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Robust facial landmarking for registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Salah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cinar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Akarun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sankur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annales des T?l?communications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deformable model fitting by regularized landmark mean-shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="200" to="215" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A survey of local feature methods for 3d face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soltanpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boufama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="391" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep convolutional network cascade for facial point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3476" to="3483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep learning face representation from predicting 10,000 classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1891" to="1898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mnemonic descent method: A recurrent process applied for end-to-end face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Snape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4177" to="4187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Regressing a 3d face shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3748" to="3755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A deeply-initialized coarse-to-fine ensemble of regression trees for face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vald?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Baumela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="585" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Driver gaze tracking and eyes off the road detection system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Levi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2014" to="2027" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Robust real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fully automatic facial feature point detection using gabor feature based boosted classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vukadinovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2005 IEEE International Conference on Systems, Man and Cybernetics</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1692" to="1698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Look at boundary: A boundary-aware face alignment algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2129" to="2138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Leveraging intra and inter-dataset variations for robust face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="150" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Simultaneous facial landmark detection, pose and deformation estimation under facial occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3471" to="3480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="532" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Joint head pose estimation and face alignment framework using global and local cnn features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 12th IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG 2017)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="642" to="649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">An empirical study of recent face alignment methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Robinson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05049</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gunes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Robinson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.03148</idno>
		<title level="m">Face alignment assisted by head pose estimation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Wider face: A face detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5525" to="5533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Coarse-to-fine autoencoder networks (cfan) for real-time face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Facial landmark detection by deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="94" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Hierarchical facial landmark localization via cascaded random binary patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1277" to="1288" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Face recognition: A literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM computing surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="399" to="458" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Automatic landmark location with a combined active shape model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Petrovska-Delacr?taz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dorizzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 3rd International Conference on Biometrics: Theory, Applications, and Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Face alignment by coarse-to-fine shape searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4998" to="5006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Face alignment across large poses: A 3d solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="146" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Face detection, pose estimation, and landmark localization in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="2879" to="2886" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
