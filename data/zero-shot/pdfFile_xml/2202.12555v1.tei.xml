<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">6D ROTATION REPRESENTATION FOR UNCONSTRAINED HEAD POSE ESTIMATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Hempel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Electrical Engineering and Information Technology</orgName>
								<orgName type="institution">Neuro-Information Technology Otto von Guericke University</orgName>
								<address>
									<settlement>Magdeburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">A</forename><surname>Abdelrahman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Electrical Engineering and Information Technology</orgName>
								<orgName type="institution">Neuro-Information Technology Otto von Guericke University</orgName>
								<address>
									<settlement>Magdeburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayoub</forename><surname>Al-Hamadi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Electrical Engineering and Information Technology</orgName>
								<orgName type="institution">Neuro-Information Technology Otto von Guericke University</orgName>
								<address>
									<settlement>Magdeburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">6D ROTATION REPRESENTATION FOR UNCONSTRAINED HEAD POSE ESTIMATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-head pose estimation</term>
					<term>orientation regres- sion</term>
					<term>rotation matrix</term>
					<term>geodesic loss</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present a method for unconstrained endto-end head pose estimation. We address the problem of ambiguous rotation labels by introducing the rotation matrix formalism for our ground truth data and propose a continuous 6D rotation matrix representation for efficient and robust direct regression. This way, our method can learn the full rotation appearance which is contrary to previous approaches that restrict the pose prediction to a narrowangle for satisfactory results. In addition, we propose a geodesic distance-based loss to penalize our network with respect to the SO(3) manifold geometry. Experiments on the public AFLW2000 and BIWI datasets demonstrate that our proposed method significantly outperforms other state-ofthe-art methods by up to 20%. We open-source our training and testing code along with our pre-trained models: https://github.com/thohemp/6DRepNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Head pose estimation from a single image is a key task in facial analysis that is used for a wide range of applications such as driver assistance <ref type="bibr" target="#b0">[1]</ref>, augmented reality <ref type="bibr" target="#b1">[2]</ref>, and human-robot interaction <ref type="bibr" target="#b2">[3]</ref>. Current methods are commonly divided into landmark-based and landmark-free approaches. Landmark-based methods <ref type="bibr" target="#b3">[4]</ref> detect facial landmarks in an initial step and, in a subsequent step, recover the 3D head pose by establishing correspondence between these landmarks and a 3D head model. While this approach can lead to very accurate results, it is highly dependent on the correct prediction of the landmark positions. Hence, inferior landmark localization caused by occlusion and extreme rotation can consequently impair an accurate head pose estimation.</p><p>Landmark-free approaches overcome this problem by directly estimating the head pose. These methods commonly facilitate deep neural networks to formulate the orientation prediction as an appearance-based task. HopeNet <ref type="bibr" target="#b4">[5]</ref> presented a This research was funded by the Federal Ministry of Education and Research of Germany (BMBF) RoboAssist no. 03ZZ0448L. multi-loss approach by binning the target angle range to combine a cross-entropy and a mean squared error loss function for Euler angle prediction. Similarly, QuatNet <ref type="bibr" target="#b5">[6]</ref> adapts the cross-entropy paradigm but splits classification and regression into separate network branches. One branch is used for classifying the Euler angles and the second one for regressing the pose in quaternion representation. Similarly, HPE <ref type="bibr" target="#b6">[7]</ref> treats classification and regression separately and then averages the outputs as a pose regression subtask. WHENet <ref type="bibr" target="#b7">[8]</ref> keeps the single branch strategy but increases the number of bins to extend the predictable yaw Euler angle range together with an EfficientNet backbone. Whereas, FSA-Net <ref type="bibr" target="#b8">[9]</ref> proposes a network with a stage-wise regression and feature aggregation scheme for predicting Euler angles. TriNet <ref type="bibr" target="#b9">[10]</ref> adapts this method, but estimates the three unit vectors of the rotation matrix instead of Euler angles and incorporates an additional orthogonality loss to stabilize the predictions. In another approach, FDN <ref type="bibr" target="#b10">[11]</ref> proposes a feature decoupling method to explicitly learn discriminative features of different head orientations.</p><p>It has become a common convention to split up the continuous rotation variables into bins for classification to stabilize the predictions <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11]</ref>. However, this is problematic as pruning segments of angles into bins will consequently lead to a loss of information. Additionally, most of the current methods use the Euler angle or quaternion representation to train their networks. However, Zhou et al. <ref type="bibr" target="#b11">[12]</ref> demonstrated that any representation of rotation with four or fewer dimensions is discontinuous and therefore not ideal to use in a learning task for neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions:</head><p>We propose a landmark-free head pose estimation method that uses the rotation matrix representation for regressing accurate head orientations. The nine-parameter matrix enables full pose regression without suffering ambiguity problems. Additionally, it allows simplifying the network by detaching unnecessary performance stabilizing measures used by other methods, e.g. discretization of the rotation variables into a classification problem. This simplicity makes our network easily to be transferred into other rotation-related problems. Instead of predicting the entire nine-parameter rotation matrix, we efficiently regress a compressed 6D form that is transformed into the rotation matrix in a subsequent ?2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.  task.</p><p>Further, we propose to use the geodesic loss instead of the commonly used mean squared error loss. This way, we can use the distance angle to penalize our network in the training process that encapsulates the Special Orthogonal Group SO(3) manifold geometry. <ref type="figure" target="#fig_1">Fig 1 shows</ref> an overview of your proposed method. Each component will be explained in detail in the following sections. Inspired by the 6D representation that is used in our approach, we call our network 6DRepNet.</p><p>Our training, testing code, and pre-trained CNN models are made publicly available to facilitate research experimentation and practical application development.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">METHOD</head><p>A key aspect for tackling direct orientation prediction is the use of an appropriate rotation representation. A popular and convenient representation is the Euler angle. However, this representation is not optimal as it suffers from the gimbal lock in which case there are multiple rotation parameterizations for the same visual head pose appearance. As a consequence, neural networks have difficulties to learn the accurate pose. Whereas, on the contrary, the quaternion representation doesn't suffer from the gimbal lock, it still has an ambiguity caused by its antipodal symmetry. Especially when learning the full range of head poses, this can lead to decreased estimation performance. A more favorable rotation representation is the rotation matrix, which is a continuous representation with a distinct parameterization for each rotation. In SO(3) the matrix representation R is sized 3 ? 3 with orthogonality constraint RR T = I, where R T is the transposed matrix and I the identity matrix. One could now try to regress the rotation matrix directly, but this would require finding all nine parameters and enforcing the orthogonality constraint, either via the Gram-Schmidt process or by finding the nearest optimal solution using SVD. Instead, we follow the approach by Zhou et al. <ref type="bibr" target="#b11">[12]</ref> and perform the Gram-Schmidt mapping inside the representation itself by simply dropping the last column vector of the rotation matrix. This reduces the 3 ? 3 matrix into a 6D rotation representation that has been reported to introduce smaller errors for direct regression <ref type="bibr" target="#b11">[12]</ref>.</p><formula xml:id="formula_0">g GS = ? ? ? ? | | | a 1 a 2 a 3 | | | ? ? ? ? = ? ? | | a 1 a 2 | | ? ?<label>(1)</label></formula><p>The predicted 6D representation matrix can be then mapped back into SO <ref type="formula" target="#formula_2">(3)</ref>.</p><formula xml:id="formula_1">f GS = ? ? ? ? | | a 1 a 2 | | ? ? ? ? = ? ? | | | b 1 b 2 b 3 | | | ? ? (2)</formula><p>Hereby, the remaining column vector is simply determined by the cross product that ensures that the orthogonality constraint is satisfied for the resulting 3 ? 3 matrix.</p><formula xml:id="formula_2">b 1 = a 1 ||a 1 || b 2 = u 2 ||u 2 || , u 2 = a 2 ? (b 1 ? a 2 )b 1 b 3 = b 1 ? b 2<label>(3)</label></formula><p>As a result, our network has only to predict 6 parameters that are mapped into a 3 ? 3 rotation matrix in a subsequent transformation which at the same time also satisfies the orthogonality constraint.</p><p>A commonly used loss function for head pose related tasks is the l2-norm. However, using the Frobenius norm for measuring distances between two matrices would break with the SO(3) manifold geometry. Instead, the shortest path between two 3D rotations is geometrically interpreted as the geodesic distance. Let R p and R gt ? SO(3) be the estimated and the ground truth rotation matrices, respectively, then the geodesic distance between both rotation matrices is defined as:</p><formula xml:id="formula_3">L g = cos ?1 tr(R p R T gt ) ? 1 2<label>(4)</label></formula><p>In the following, we will use this metric as the loss function for our neural network to compute accurate distance information between the predicted and ground truth orientation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head><p>We implement our proposed network using Pytorch. As backbone, we choose RepVGG <ref type="bibr" target="#b12">[13]</ref>. For training, the RepVGG is designed as a multi-branch model just like ResNet <ref type="bibr" target="#b13">[14]</ref> or Inception <ref type="bibr" target="#b14">[15]</ref>. For deployment, the model can be converted into a VGG-like architecture using a re-parameterization scheme. The trimmed model yields the same accuracy but with a shorter inference time. This way, RepVGG combines the accuracy of multi-branch models with the efficiency of single branch architectures. RepVGG provides multiple architectures sizes, where we use the RepVGG-B1g2 that acts as the equivalent to the ResNet50. For the final layers, we choose a single fully connected layer with 6 outputs. As groundwork for this choice, we tested multiple configurations for the final layers, including one layer up to three sequential fully connected layers, single final layers with 6 output neurons, and separated branches with one output neuron each. In our experiments, a single final layer with 6 output neurons performed the best. The network is trained for 30 epochs using the Adam optimizer with initial learning of 1e ?5 for the backbone and 1e ?4 for the final fully connected layer. Both learning rates are halved every 10 epochs. We use a batch size of 64.</p><p>Datasets: In general, our network is able to learn the full range of rotation. Unfortunately, due to the type of annotation technique, the most popular datasets for head pose estimation contain mainly frontal face samples. For evaluation we use three public available datasets: 300W-LP <ref type="bibr" target="#b15">[16]</ref>, ALFW2000 <ref type="bibr" target="#b16">[17]</ref> and BIWI <ref type="bibr" target="#b17">[18]</ref>. 300W-LP consists of 66,225 face samples collected from multiple databases that are further enhanced to 122,450 samples by image flipping. It is based on around 4000 real images. The ground truth is provided in the Euler angle format. For training, we convert them into the matrix form.</p><p>The ALFW2000 dataset contains the first 2,000 images from the ALFW dataset annotated with the ground truth 3D faces and the corresponding 68 landmarks. It contains samples with large variations, different illumination, and occlusion conditions.</p><p>The BIWI dataset includes 15,678 images that were created in a lab environment with 20 participants. In this dataset, the head takes up only a small area in the images. Hence, we use the MTCNN <ref type="bibr" target="#b18">[19]</ref> face detector to loosely crop the heads from the images.</p><p>For a fair comparison, we follow the preprocessing strategy from the other methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10]</ref> and only keep samples that have Euler angles between -99?and 99?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 1:</head><p>In our first experiment, we follow the convention by using the synthetic 300W-LP dataset for training and the two real-world datasets ALFW2000 and BIWI for testing. The standard evaluation metric is the Mean Absolute Error (MAE) of the Euler angles, so we convert our rotation matrix predictions into Euler angles for better comparison. <ref type="table" target="#tab_0">Table 1</ref> shows the results from our first experiment setup. We compare our methods against the results reported by other state-of-the-art landmark-free approaches for head pose estimation. It demonstrates that our method outperforms the current state of the art by almost 20% on the AFLW2000 test dataset and achieves the lowest error rate on all three kinds of rotation angles yaw, , and roll. On the BIWI dataset, our method achieves state-of-the-art results for the MAE. In contrast to other methods that report very diverging results of the single angle errors, or approach reports overall very balanced errors. This indicates that our network is able to learn in a consistent and robust manner.</p><p>For better interpretation, we added an extra column to show which methods are in general able to predict full-range rotations and which restrict their predictions within a certain range of angles. It shows that besides our approach, only two other methods target full range regression. The remaining methods have a special network architecture dedicated BIWI Yaw Pitch Roll MAE HopeNet (? = 1) <ref type="bibr" target="#b4">[5]</ref> 3.29 3.39 3.00 3.23 FSA-Net <ref type="bibr" target="#b8">[9]</ref> 2.89 4.29 3.60 3.60 TriNet <ref type="bibr" target="#b9">[10]</ref> 2.93 3.04 2.44 2.80 FDN <ref type="bibr" target="#b10">[11]</ref> 3.00 3.98 2.88 3.29 6DRepNet 2.69 2.92 2.36 2.66  to narrow range head pose predictions. Most similar to our method, TriNet <ref type="bibr" target="#b9">[10]</ref> uses the matrix representation and directly predicts the complete matrix. As this prediction cannot be assumed to satisfy the orthogonality constraint, it needs an excessive post-process by finding an appropriate rotation matrix that is close to the prediction and at the same time has orthogonal column norm vectors. The second method marked with wide range prediction probabilities, WHENet <ref type="bibr" target="#b7">[8]</ref>, only allows the full rotation for the yaw (but not for pitch and roll) by adding more classes to their classification problem. It is noticeable that this network performs worse compared to their similar network adaptation WHENet-V that has been restricted to be only capable of predicting angles between -90?and 90?. We argue that this decline in accuracy could be caused by the introduced label ambiguities as they use the Euler representation for training. <ref type="figure" target="#fig_2">Fig. 2</ref> shows qualitative results from the AFLW2000 dataset. We visualized the predicted Euler angles to demonstrate how the head poses are estimated in the image samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 2:</head><p>For our second experiment, we follow the convention by FSA-Net <ref type="bibr" target="#b8">[9]</ref> and randomly split the BIWI dataset in a ratio of 7:3 for training and testing, respectively. <ref type="table" target="#tab_1">Table 2</ref> shows our results compared with other state-of-the-art methods that followed the same testing strategy. It demonstrates that our method outperforms all other methods not only in the overall MAE but at the same time also in predicting the yaw, pitch, and roll equally to our experiment on the AFLW2000 dataset. This supports the observed robustness in experiment 1, that achieving stable accurate results for all three angles does not only depend on the trained dataset, but rather on our proposed method itself.  Experiment 3: Most current methods use the 2 -norm for calculating the loss in the training procedure. We argue that the Geodesic distance is a better distance metric to measure the prediction accuracy for head pose orientation. To prove this, we conduct another experiment where we repeat our previous tests, but this time train our network with the 2 distance loss. <ref type="table" target="#tab_2">Table 3</ref> shows these results compared to our models trained with geodesic distance loss. It states that the networks with geodesic loss penalty performed slightly better than those that were trained with 2 -norm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 4:</head><p>In a final experiment, we analyze the impact of the chosen backbone on the results. ResNet50 is a popular standard network that is also used by HopeNet and TriNet, so we use it as the comparison backbone. <ref type="table" target="#tab_4">Table 4</ref> shows that our method with RepVGG backbone is capable to perform about 7% better on all test scenarios against the 2 loss. However, with the aid of the ResNet50 backbone, our method would still achieve state-of-the-art results on the AFLW2000 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>In this paper, we presented a method for unconstrained endto-end head pose estimation from single images. We follow the argument that the rotation matrix is more suitable for orientation learning tasks and propose a continuous 6D rotation matrix representation for efficient direct regression. Additionally, we introduce the geodesic loss instead of the commonly used MSE for the robust training. Also different from previous approaches, our method can regress full rotations and does not utilize an angle restricting binning principle. Nonetheless, our method outperforms other state-of-theart methods on multiple datasets by up to 20%. In additional experiments, we analyze the impact of the backbone and loss function on our results. In the future, we aim to draw on our method's full potential by training it on a dataset that provides full rotation samples. To this end, a potential dataset has been introduced by WHENet [8] that presented a method for recovering full head pose annotations for the CMU Panoptic dataset <ref type="bibr" target="#b19">[20]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>arXiv:2202.12555v1 [cs.CV] 25 Feb 2022</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>Overview of the proposed method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Example images with converted Euler angle visualization from the AFLW2000 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparisons with the state-of-the-art methods on the AFLW2000 and BIWI dataset. All models are trained on the 300W-LP dataset.</figDesc><table><row><cell></cell><cell>AFLW2000</cell><cell>BIWI</cell></row><row><cell>Full Range 1</cell><cell>Yaw Pitch Roll MAE</cell><cell>Yaw Pitch Roll MAE</cell></row><row><cell>HopeNet (? = 2) [5]</cell><cell>6.47 6.56 5.44 6.16</cell><cell>5.17 6.98 3.39 5.18</cell></row><row><cell>HopeNet (? = 1) [5]</cell><cell>6.92 6.64 5.67 6.41</cell><cell>4.81 6.61 3.27 4.90</cell></row><row><cell>FSA-Net [9]</cell><cell>4.50 6.08 4.64 5.07</cell><cell>4.27 4.96 2.76 4.00</cell></row><row><cell>HPE [7]</cell><cell>4.80 6.18 4.87 5.28</cell><cell>3.12 5.18 4.57 4.29</cell></row><row><cell>QuatNet [6]</cell><cell>3.97 5.62 3.92 4.50</cell><cell>2.94 5.49 4.01 4.15</cell></row><row><cell>WHENet-V [8]</cell><cell>4.44 5.75 4.31 4.83</cell><cell>3.60 4.10 2.73 3.48</cell></row><row><cell>WHENet [8]</cell><cell>5.11 6.24 4.92 5.42</cell><cell>3.99 4.39 3.06 3.81</cell></row><row><cell>TriNet [10]</cell><cell>4.04 5.77 4.20 4.67</cell><cell>4.11 4.76 3.05 3.97</cell></row><row><cell>FDN [11]</cell><cell>3.78 5.61 3.88 4.42</cell><cell>4.52 4.70 2.56 3.93</cell></row><row><cell>6DRepNet</cell><cell>3.63 4.91 3.37 3.97</cell><cell>3.24 4.48 2.68 3.47</cell></row></table><note>1 These methods allow full range predictions.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparisons with the state-of-the-art methods on the BIWI dataset. 70% of the BIWI dataset is used for training and the remaining 30% for testing.</figDesc><table><row><cell></cell><cell cols="3">AFLW2000 BIWI 70/30 BIWI</cell></row><row><cell></cell><cell>MAE</cell><cell>MAE</cell><cell>MAE</cell></row><row><cell>2</cell><cell>4.13</cell><cell>3.70</cell><cell>2.84</cell></row><row><cell>Geodesic</cell><cell>3.97</cell><cell>3.47</cell><cell>2.66</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison of the MAE between 2 and geodesic loss.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison of the MAE between the different backbones.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Head pose estimation for driver assistance systems: A robust algorithm and experimental evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Murphy-Chutorian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anup</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan Manubhai</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2007 IEEE Intelligent Transportation Systems Conference</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="709" to="714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Head pose estimation and augmented reality tracking: An integrated system and evaluation for monitoring driver awareness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Murphy-Chutorian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan Manubhai</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="300" to="311" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Robo-hud: Interaction concept for contactless operation of industrial cobotic systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominykas</forename><surname>Strazdas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayoub</forename><surname>Al-Hamadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Landmark based head pose estimation benchmark and method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Werner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frerk</forename><surname>Saxen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayoub</forename><surname>Al-Hamadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3909" to="3913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Finegrained head pose estimation without keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nataniel</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunji</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2155" to="215509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Quatnet: Quaternion-based head pose estimation with multiregression loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng-Wei</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tung-Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung</forename><surname>Wing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yi</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1035" to="1046" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improving head pose estimation using two-stage ensembles with top-k regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinbang</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page">103827</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Whenet: Real-time fine-grained estimation for wide range head pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Gregson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">31st British Machine Vision Conference 2020</title>
		<meeting><address><addrLine>UK</addrLine></address></meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fsa-net: Learning fine-grained structure aggregation for head pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsun-Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Yu</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A vector-based representation to enhance head pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwen</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongcheng</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongfang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingjie</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2021-01" />
			<biblScope unit="page" from="1188" to="1197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Fdn: Feature decoupling network for head pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On the continuity of rotation representations in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5738" to="5746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Repvgg: Making vggstyle convnets great again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiaohan Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13728" to="13737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Face alignment across large poses: A 3d solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="146" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">High-fidelity pose and expression normalization for face recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="787" to="796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Random forests for real time 3d face analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dantone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fossati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="437" to="458" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Panoptic studio: A massively multiview system for social motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohei</forename><surname>Nobuhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
