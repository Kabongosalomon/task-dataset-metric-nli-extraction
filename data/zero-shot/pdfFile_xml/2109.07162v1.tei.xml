<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MISSFormer: An Effective Medical Image Segmentation Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohong</forename><surname>Huang</surname></persName>
							<email>huangxh@bupt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Deng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandan</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueguang</forename><surname>Yuan</surname></persName>
							<email>yuanxg@bupt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MISSFormer: An Effective Medical Image Segmentation Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The CNN-based methods have achieved impressive results in medical image segmentation, but it failed to capture the long-range dependencies due to the inherent locality of convolution operation. Transformer-based methods are popular in vision tasks recently because of its capacity of long-range dependencies and get a promising performance. However, it lacks in modeling local context, although some works attempted to embed convolutional layer to overcome this problem and achieved some improvement, but it makes the feature inconsistent and fails to leverage the natural multi-scale features of hierarchical transformer, which limit the performance of models. In this paper, taking medical image segmentation as an example, we present MISSFormer, an effective and powerful Medical Image Segmentation tranSFormer. MISSFormer is a hierarchical encoder-decoder network and has two appealing designs: 1) A feed forward network is redesigned with the proposed Enhanced Transformer Block, which makes features aligned adaptively and enhances the long-range dependencies and local context. 2) We proposed Enhanced Transformer Context Bridge, a context bridge with the enhanced transformer block to model the long-range dependencies and local context of multi-scale features generated by our hierarchical transformer encoder. Driven by these two designs, the MISSFormer shows strong capacity to capture more valuable dependencies and context in medical image segmentation. The experiments on multi-organ and cardiac segmentation tasks demonstrate the superiority, effectiveness and robustness of our MISSFormer, the exprimental results of MISSFormer trained from scratch even outperforms state-of-the-art methods pretrained on ImageNet, and the core designs can be generalized to other visual segmentation tasks. The code will be released in Github.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the improvement of medical treatment and the people's health awareness, the requirements of highly accurate medical image analysis (such as preoperative evaluation, auxiliary diagnosis) have become more prominent, and the medical image segmentation, as a crucial step of them, the precise and robust segmenation results will provide a good foundation for subsequent analysis and treatment.</p><p>Since the fully convolutional networks (FCNs) <ref type="bibr" target="#b19">(Long, Shelhamer, and Darrell 2015)</ref> opened a door for seman-* Corresponding author tic segmentation, one of its variants, the U-shaped networks <ref type="bibr" target="#b21">(Ronneberger, Fischer, and Brox 2015;</ref><ref type="bibr">iek et al. 2016</ref>) got a promising performance in medical image segmentation by the improvement of skip connection, which provided more detailed information. According to this elegant architecture, the variants of U-Net <ref type="bibr">(Isensee et al. 2021;</ref><ref type="bibr" target="#b36">Zhou et al. 2018;</ref><ref type="bibr" target="#b14">Huang et al. 2020</ref>) have been achieved excellent performance and impressive results. Although their awesome performance and prevalence, the CNN-based methods suffer from a limitation in modeling the long-range dependencies because of the locality of convolution operation <ref type="bibr" target="#b30">Xie et al. 2021b)</ref>, and they failed to achieve the goal of precise medical image analysis. To overcome the limitation, some works proposed dilated convolution <ref type="bibr" target="#b8">Feng et al. 2020</ref>) and pyramid pooling <ref type="bibr" target="#b33">(Zhao et al. 2017)</ref> to enlarge the receptive field as much as possible. And some recent works <ref type="bibr" target="#b21">Mou et al. 2019;</ref><ref type="bibr" target="#b2">Chen et al. 2021;</ref><ref type="bibr" target="#b22">Sinha and Dolz 2020)</ref> tried to employ few selfattention layers or transformer layers <ref type="bibr" target="#b24">(Vaswani et al. 2017)</ref> in high-level semantic feature maps due to the quadratic relationship between self-attention computational complexity and feature map size, which makes these methods insufficient to capture the abundant long-range dependencies.</p><p>Recently, the success of transformers that capture longrange dependencies makes it possible to solve the above problems. Especially, the researches on visual transformer <ref type="bibr" target="#b18">(Liu et al. 2021;</ref><ref type="bibr" target="#b6">Dosovitskiy et al. 2020;</ref><ref type="bibr" target="#b27">Wang et al. 2021c;</ref><ref type="bibr" target="#b9">Graham et al. 2021;</ref><ref type="bibr" target="#b5">Chu et al. 2021a;</ref><ref type="bibr" target="#b29">Xie et al. 2021a;</ref><ref type="bibr" target="#b34">Zheng et al. 2021a</ref>) are in full swing and have got a promising performance in vision tasks, encouraged by the great success of transformer in natural language processing (NLP). Corresponding to the transformer in NLP, vision transformer(Dosovitskiy et al. 2020) fed the image into standard transformer with positional embeddings by dividing an image into non-overlapping patches and achieved comparable performance with CNN-based methods. Pyramid vision transformer (PVT) <ref type="bibr" target="#b27">(Wang et al. 2021c</ref>) and Swin transformer <ref type="bibr" target="#b18">(Liu et al. 2021)</ref> proposed hierarchical transformer to explore the vision transformer with spatial reduction attention (SRA) and window-based attention respectively, which are responsible for reducing computational complexity. Besides, the attempts of SETR <ref type="bibr" target="#b34">(Zheng et al. 2021a)</ref> in semantic segmentation proved the potential of transformer in visual tasks once again.</p><p>However, some recent works <ref type="bibr" target="#b15">(Islam, Jia, and Bruce 2020;</ref><ref type="bibr" target="#b6">Chu et al. 2021b;</ref><ref type="bibr" target="#b16">Li et al. 2021)</ref> showed the limitation of self-attention on local context, inspired by this, Uformer <ref type="bibr" target="#b28">(Wang et al. 2021d</ref>), SegFormer <ref type="bibr" target="#b29">(Xie et al. 2021a)</ref> and PVTv2  tried to embed convolutional layer between fully-connected layers of feed forward network in transformer block to overcome this problem. Despite it captured local context to some extent, but there are some limitations: 1) the convolutional layer is embeded between fully-connected layers of feed forward network directly, which destroys the consistency and delivery between features, although supplement some local context and get better performance. 2) it did not consider the integration of multi-scale information generated by hierarchical encoder. Both limitations lead to the inferior learning of networks.</p><p>In this paper, MISSFormer, an effective and powerful Medical Image Segmentation tranSFormer, is proposed to leverage the powerful long-ranged dependencies capability of self-attention to produce accurate medical image segmentation. MISSFormer is based on the U-shaped architecture, whose redesigned transfomer block, named Enhanced Transformer Block, makes feature consistent and enhance the feature representations. The MISSFormer consists of encoder, bridge, decoder and skip connection, these components are all builded from the enhanced tansformer block. The encoder extracts hierarchical features through the overlapped image patches, dependencies between different scale features are modeled via bridge, and decoder is responsible for pixel-wise segmentation prediction with skip connection. The main contributions of this paper can be summarized as follows:</p><p>? We propose MISSFormer, a position-free and hierarchical U-shaped transformer for medical image segmentation. ? We redesign a powerful feed forward network, Enhanced</p><p>Mix-FFN, with better feature consistensy, long-range dependencies and local context, based on this, we expand it and get an Enhanced Transformer Block to make a strong representation. ? We propose an Enhanced Transformer Context Bridge based on the Enhanced Transformer Block to capture the correlations of hierarchical multi-scale features. ? The superior experimental results on medical image segmentation datasets demonstrate the effectiveness, superiority and robustness of the proposed MISSFormer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Medical image segmentation. Medical image segmentation is a pixel level task of seperating the lesions or organs pixels in a given medical image. U-shaped network(Ronneberger, Fischer, and Brox 2015) played a cornerstone role in medical image segmentation tasks because of its superior performance and elegant structure. Benefiting from the rapid development of computer vision tasks <ref type="bibr" target="#b12">(He et al. 2016;</ref><ref type="bibr" target="#b3">Chen et al. 2017)</ref>, the medical image segmentation drew lessons from its key insight, for example, resnet architecture became a general encoder backbone for medical image segmentation network, the dilated convolution and pyramid pooling were utilized to enlarge the receptive field for lesion and organ segmentation <ref type="bibr" target="#b8">Feng et al. 2020)</ref>. Besides, various attention mechanisms were effective to promote segmentation performance, reverse attention <ref type="bibr" target="#b4">(Chen et al. 2018)</ref> was applied to accurate polyp segmentation <ref type="bibr" target="#b7">(Fan et al. 2020)</ref>, squeeze-and-excitation attention <ref type="bibr" target="#b13">(Hu, Shen, and Sun 2018)</ref> was integrated into module to refine the channel information to segment vessel in retinal images <ref type="bibr" target="#b10">(Zhang et al. 2019)</ref>, and some works <ref type="bibr" target="#b21">(Mou et al. 2019;</ref><ref type="bibr" target="#b22">Sinha and Dolz 2020)</ref> employed self-attention mechanism to supplement the longrange dependencies for segmentation tasks. Vision transformers. <ref type="bibr">ViT(Dosovitskiy et al. 2020)</ref> introduced transformer <ref type="bibr" target="#b24">(Vaswani et al. 2017)</ref> into visual tasks for the first time and achieved impressive performance because of the capacity for global dependencies of transformer. Vision tasks developed a new stage inspired by ViT, for example, DeiT ) explored the efficient training strategies for ViT, PVT <ref type="bibr" target="#b27">(Wang et al. 2021c)</ref> proposed a pyramid transformer with SRA to reduce the computational complexity, and Swin transformer <ref type="bibr" target="#b18">(Liu et al. 2021)</ref> was an efficient and effective hierarchical vision transformer, whose window-based mechanism enhances the locality of features, which was also the improvement of some excellent transformer works <ref type="bibr" target="#b15">(Islam, Jia, and Bruce 2020;</ref><ref type="bibr" target="#b6">Chu et al. 2021b;</ref><ref type="bibr" target="#b16">Li et al. 2021)</ref>. For other specific tasks, SETR <ref type="bibr" target="#b35">(Zheng et al. 2021b</ref>) was a semantic segmentation network based on transformer and made ViT as backbone, Seg-Former <ref type="bibr" target="#b29">(Xie et al. 2021a</ref>) introduced a simple and efficient design for semantic segmentation powered by transformer, DETR <ref type="bibr" target="#b1">(Carion et al. 2020)</ref> proposed an end-to-end object detection framework with transformer, Uformer <ref type="bibr" target="#b28">(Wang et al. 2021d</ref>) builded a general U-shaped transformer for image restoration.</p><p>Transformers for medical image segmentation. Researchers borrowed the transformer to medical image segmentation inspired by the rapid development of vision transformers. Transunet ) employed some transformer layers into the low resolution encoder feature maps to capture the long-range dependencies, UNETR <ref type="bibr" target="#b11">(Hatamizadeh et al. 2021</ref>) applied transformer to make a powerful encoder for 3d medical image segmentation with CNN decoder, CoTr ) and TransBTS  bridged the CNN-based encoder and decoder with the transformer to improve the segmentation performance in low resolution stage. Besides these methods which are the combination of CNN and transformer,  proposed Swin-Unet, based on Swin transformer <ref type="bibr" target="#b18">(Liu et al. 2021)</ref>, to demonstrate the application potential of pure transformer in medical image segmentations. However, Swin-Unet, whose encoder backbone is Swin transformer pretrained on Ima-geNet, requires pre-training on large-scale datasets, different from it, the proposed MISSFormer is trained on the medical image datasets from scratch, and achieves better performance because of the discriminative feature representations by Enhanced Transformer Block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>This section describes the overall pipeline and the specific structure of MISSFormer first, and then we show the details of the improved transformer block, Enhanced Transformer Block, which is the basic unit of MISSFormer. After that, we introduce the proposed Enhanced Transformer Context Bridge, which models the correlations of hierarchical multiscale information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overall Pipline</head><p>The proposed MISSFormer is shown in <ref type="figure" target="#fig_0">Fig.1(a)</ref>, which is a hierarchical encoder-decoder architecture, with enhanced transformer context bridge module appended between encoder and decoder. Specifically, given an input image, MISSFormer first divides it into overlapping patches of size 4*4 to preserve its local continuity with convolutional layers. Then, the overlapping patches are fed into encoder to produce the multi-scale features. Here, the encoder is hierarchical and each stage includes enhanced transformer blocks and patch merging layer, enhanced transformer block learns the long-range dependencies and local context with limited computational complexity, patch merging layer is applied to generate the downsampling features. After that, MISSFormer makes the generated multi-scale features pass through the Enhanced Transformer Context Bridge to caputure the correlations of different scale features. In practical, different level features is flattened in spatial dimension and reshape them to make consistent in channel dimension, then concatenate them in flattened spatial dimention and feed into the enhanced transformer context bridge with d-depth. After that, we split and restore them to their original shape and obtain the discriminative hierarchical multi-scale fetures.</p><p>For the segmentation prediction, MISSFormer takes the descriminative features and skip connections as inputs of decoder. Each decoder stage includes Enhanced Transformer Blocks and patch expanding layer ). Contrary to patch merging layer, the patch expanding layer upsample the adjacent feature maps to twice the original resolution except that the last one is four times, and last, the pixel-wise segmentation prediction is output by a linear projection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Enhanced Transformer Block</head><p>Long-range dependencies and local context are effective for accurate medical image segmentation, transformer and convolution are better for long-range dependencies and locality in present, respectively. While computational complexity the original transformer block is quadratic with the feature map resolution, which makes it unsuitable for high resolution feature maps. Second, transformer lacks the ability to extract the local context <ref type="bibr" target="#b15">(Islam, Jia, and Bruce 2020;</ref><ref type="bibr" target="#b6">Chu et al. 2021b;</ref><ref type="bibr" target="#b16">Li et al. 2021)</ref>, although Uformer, SegFormer and PVTv2 tried to overcome the limitation by embedding a convolutional layer in feed forward netwok directly, we argue that this approach makes the feature consistency broken and and hinder the feature delivery, even some improved performance is achieved by them.</p><p>In order to solve the above problems, we proposed Enhanced Transformer Block. As is shown in <ref type="figure" target="#fig_0">Fig.1(b)</ref>, the En- Efficient Self-Attention. Efficient self-attention is a spatial reduction self-attention <ref type="bibr" target="#b27">(Wang et al. 2021c</ref>), which can be applied to high resolution feature map. Given a feature map F ? R H?W ?C , and H, W, C is the height, width and channel depth respectively. For the original standard multi-head self-attention, it makes Q, K, V have same shape N ? C, where N = H ? W , which can be formulated as:</p><formula xml:id="formula_0">Attention(Q, K, V ) = Sof tM ax( QK T ? d head )V<label>(1)</label></formula><p>and its computational complexity is O(N 2 ). While for the efficient self-attention, it applied a spatial reducation ratio R to reduce the spatial resolution as follows:</p><formula xml:id="formula_1">new K = Reshape( N R , C ? R)W (C ? R, C)<label>(2)</label></formula><p>it first reshapes K and V to N R ? (C ? R), and then a linear projection W is used to make channel depth restore to C. After that, the computational complexity of self-attention reduces to O( N 2 R ), and can be applied to high resolution feature maps. The spatial reduction operation is convolution or pooling in common.</p><p>Enhanced Mix-FFN. Different from previous methods in <ref type="figure" target="#fig_1">Fig.2(a)</ref> and (b), we redesigned the structure of Mix-FFN to align feature and make discriminative representations. As shown in <ref type="figure" target="#fig_1">Fig.2(c)</ref>, First we add a skip connection before the depth-wise convolution for the feature diversity and delivery, we will show that the skip connection is essential for Mix-FFN in Section 4.2. Then, we applied layer norm after the skip connection for aligning features and better consistensy and convergency, which can be formulated as:</p><formula xml:id="formula_2">y 1 = (LN (Conv 3?3 (F C(x in )) + F C(x in )) x out = F C(GELU (y 1 )) + x in<label>(3)</label></formula><p>where, x in is the output of efficient self-attention, Conv 3?3 is convolution with kernel 3 ? 3, we applied depth-wise convolution for efficiency in this paper actually.  <ref type="bibr" target="#b17">(Liu et al. 2020)</ref>, we extend our design to a general form with the help of layer norm, which facilitate the optimization of skip connection <ref type="bibr" target="#b24">(Vaswani et al. 2017)</ref>. As shown in <ref type="figure" target="#fig_1">Fig.2(d)</ref>, we make a Enhanced Mix block embeded in the original feed-forward network. We introduced recursive skip connection in Enhanced Mix block, given an input feature map x in , a depth-wise convolution layer is applied to capture the local context, and then a recursive skip connection follwed, and it can be defined as:</p><formula xml:id="formula_3">y i = LN (x in + y i?1 ) x out = F C(GELU (y i )) + x in<label>(4)</label></formula><p>where y 1 = LN (x in + F (x in , W )). After that, the model makes more expressive power due to the construction of different feature distribution and consistensy by each recursive step.</p><formula xml:id="formula_4">token F i = Reshape(F i , [B, ?1, C]) merge token = Concatenate(token F i , dim = 1)</formula><p>Atten token = Ef f icient Atten(LN (merge token)) res token = LN (Atten token + merge token) split token = Split(res token, dim = 1) F F N i = EnhancedM ix ? F F N (split token) output = Concatenate(F F N i , dim = 1) + res token (5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Enhanced Transformer Context Bridge</head><p>Multi-scale information fusion has been proved to be crucial for accurate semantic segmentation in CNN-based   method <ref type="bibr" target="#b22">(Sinha and Dolz 2020;</ref><ref type="bibr" target="#b3">Chen et al. 2017)</ref>. In this part, we explore the multi-scale feature fusion for the Transformer-based method by the aid of hierarchical structure of MISSFormer. The multiple stage feature maps are obtained after feeding the patches into encoder, whose settings of patch merging and channel depth in every stage keep same with SegFormer. Given multi-level features F 1 , F 2 , F 3 , F 4 , which are generated by hierarchical encoder, we flatten them in spatial dimension and reshape them to keep the channel depth same with each other, then we concatenate them in flattened spatial dimension, after that, the concatenated token is fed into enhanced transformer block to construct the long-range dependencies and local context correlation. The process can be summarized as formula <ref type="formula">(5)</ref>. After the feature passed through d enhanced transformer block, we split tokens and restore them to original shape of features in every stage, and feed them into transformer-based decoder with corresponding skip connection to predict the pixel-wise segmentation map. The depth of Context Bridge is set to 4 in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we first conduct the experiment of ablation studies to validate the effectiveness of each component in MISSFormer, and then the comparation results with previous state-of-the-art methods are reported to demonstrate the superiority of the proposed MISSFormer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiments Settings</head><p>Datasets. We perform experiments on two different formats of datasets: Synapse multi-organ segmentation dataset (Synapse) and Automated cardiac diagnosis challenge dataset (ACDC). The Synapse dataset includes 30 abdominal CT scans with 3779 axial abdominal clinical CT images, and the dataset is divided into 18 scans for training and 12 for testing randomly, follow the <ref type="bibr" target="#b2">Chen et al. 2021)</ref>. We evaluate our method with the average Dice-S?rensen Coefficient (DSC) and average Hausdorff Distance (HD) on 8 abdominal organs (aorta, gallbladder, spleen, left    <ref type="bibr" target="#b2">Chen et al. 2021)</ref>, whose model is initialized by the pretrained model on ImageNet, the MISS-Former is initialized randomly and trained from scratch, so, the moderate data augmentation is conducted for all datasets, the initial learning rate is 0.05 and poly learning rate policy is used, the max training epoch is 400 with batchsize of 24. SGD optimizer with momentum 0.9 and weight decay 1e-4 is adopted for MISSFormer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Studies</head><p>We conduct ablation studies on Synapse dataset to varify the effectiveness of the essential component in our approach. We set the SegFormer B1 as baseline method, and the number of transformer block in every stage of encoder and decoder is set to 2 to keep the same with other methods for fair comparison. All experiments are performed with the same super parameter settings and trained from scratch.</p><p>Architecture selection. We replace the SegFormer B1 MLP decoder with its transformer block and patch expand-  ing to make it U-shaped SegFormer, called "U-SegFormer", and the results are shown in table 1. As we can see, the U-SegFormer achieved better performance than SegFormer due to the U-shaped model can fuse more corresponding details information with skip connection in each stage, although the SegFormer integrate multi-level information. The skip-connection is applicable for U-shaped network based transformer as before.</p><p>Effectiveness of feature consistensy and delivery. Based on U-shaped transformer, we further perform the experiments to validate the impact of feature consistensy and delivery. <ref type="table" target="#tab_2">Table 2</ref> reports the comparison results. We first design different skip connections: concatenation and summation. <ref type="table" target="#tab_2">Table 2</ref> shows that both skip connections boost the model performance greately, and the summation skip connection even improved more than 2.6%, which provide strong support for our above analysis, and prove that effectiveness of skip connection and importance of enhancing feature delievery. Then, we explore the gap caused by the direct embedding of convolution, a layer norm is applied to align the feature and distribution, we integrate it after the skip connection, which is called Simple MISSFormer, and it has even 1% improvements based on U-SegFormer w/skip. Finally, with the help of redesigned feed-forward network, we improved feature distributions and enhanced feature representations to generate an increasing promotion of 3.63 DSC, compared with U-SegFormer baseline.</p><p>Impact of further feature consistensy in Enhanced Mix Block. Inspired by above exploration and <ref type="bibr" target="#b17">(Liu et al. 2020)</ref>, we extend redesigned FFN of Simple MISSFormer to make it more general, we call it MISSFormer S due to the absence of multi-scale integration. We design experiments to assess the influence of further consistensy and distribution caused by different recursive steps, and its results are recoreded in table 3. The results become better with the increasement of recursive step, which further improved the necessity and effectiveness of feature consistensy and distribution when convolution is embedded in FFN.</p><p>Influence of Enhanced Transformer Context Bridge. We conducted experiments to explore the role of multi-scale information in transformer-based methods on acount of the hierarchical features generated by MISSFormer encoder. As  We observe that the model achieved best performance to have a 2.26% DSC improvement when step is 1 and the growth rate is gradually decreasing with the increasement of recrsive step, even negative. We guess that there is a balance between the recursive step and Enhanced Transformer Context Bridge or between the number of layernorm and model capacity, which will be discussed in our future work. Besides, we also investigated how bridge depth and multi-scale information integration affect model performance, and the results are saved to table 5. For the exploration of bridge depth, 4 is a suitable depth in MISSFormer beacause of the limited medical data. For transformer-based hierarchical features, the more scale features are fed into the enhanced transformer context bridge, the more comprehensive the model can be learned for longrange dependencies and local context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with state-of-the-art methods</head><p>This section reports the comparision results of MISSFormer and previous state-of-the-art methods on Synapse dataset and ACDC dataset. Experiment results on Synapse dataset. <ref type="table" target="#tab_8">Table 6</ref> presents the comparision results of proposed MISSFormer and previous state-of-the-art methods. As shown, the proposed method achieved state-of-the-art performance in al- <ref type="figure">Figure 3</ref>: The visual comparison with previous state-of-the-art methods on Synapse dataset. Above the red line is good cases, and below it is a failed case, Our MISSFormer shows a better performance than other method . most all measures, and it is worth mentioning that the encoder of Transunet and Swin-Unet is pretrained on Ima-geNet, while the MISSFormer trained on Synapse dataset from scratch, which indicates that MISSFormer capture the better long-range dependencies and local context to make strong feature representations. It can be seen from <ref type="figure">Fig.3</ref> that our MISSFormer achieve better edge predictions and hard example segmentations compared to Tranunet and Swin-Unet, even in bad case. Compared MISSFormer and MISS-Former S, MISSFormer has precise result and less missegmentation because of the integration of multi-scale information.</p><p>Experiment results on ACDC dataset. We evaluate our method on ACDC dataset, which is in the form of MRI. <ref type="table" target="#tab_10">Table 7</ref> presents the segmentation accuracy on our divided ACDC dataset, MISSFormer maintain the first position because of the powerful feature extraction, which indicates the outstanding generalization and robustness of MISSFormer.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The overall structure of proposed MISSFormer. (a) The proposed MISSFormer framework. (b) The structure of Enhanced Transformer Block..</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The various exploration of locality in feed forward nerual network, from left to right: (a) LeFF in Uformer, (b) Mix-FFN in SegFormer and PVTv2, (c) proposed Simple Enhanced Mix-FFN, (d) proposed Enhanced Mix-FFN hanced Transformer Block is composed of LayerNorm, Efficient Self-Attention and Enhanced Mix-FFN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Effectiveness of feature consistensy and delivery in Simple Enhanced Mix-FFN, cat indicates concatenation operation for skip connection, add means summation.</figDesc><table><row><cell>Achitecture</cell><cell cols="2">step DSC? HD?</cell></row><row><cell></cell><cell>1</cell><cell>79.73 20.14</cell></row><row><cell>MISSFormer S</cell><cell>2</cell><cell>79.91 21.33</cell></row><row><cell></cell><cell>3</cell><cell>80.74 19.65</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Impact of recursive skip connection in Enhanced Mix-FFN, step means recursive step.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Impact of Enhance Transformer Context Bridge on recursive skip connection of MISSFormer.</figDesc><table><row><cell>Achitecture</cell><cell cols="3">depth stage DSC? HD?</cell></row><row><cell></cell><cell>2</cell><cell cols="2">4321 80.19 18.88</cell></row><row><cell></cell><cell>4</cell><cell cols="2">4321 81.96 18.20</cell></row><row><cell>MISSFormer</cell><cell>6 4</cell><cell cols="2">4321 81.03 21.36 432 80.65 18.39</cell></row><row><cell></cell><cell>4</cell><cell>43</cell><cell>79.86 20.33</cell></row><row><cell></cell><cell>4</cell><cell>4</cell><cell>79.56 20.95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: Exploration of bridge depth and multi-scale infor-</cell></row><row><cell>mation in MISSFormer.</cell></row><row><cell>kidney, right kidney, liver, pancreas, spleen, stomach). The</cell></row><row><cell>ACDC dataset includes 100 MRI scans collected from dif-</cell></row><row><cell>ferent patients, and each scan labeled three organs, left ven-</cell></row><row><cell>tricle (LV), right ventricle (RV) and myocardium (MYO).</cell></row><row><cell>Consistent with the previous method(Cao et al. 2021; Chen</cell></row><row><cell>et al. 2021), 70 cases are used for training, 10 for valida-</cell></row><row><cell>tion and 20 for testing, and the average DSC is applied to</cell></row><row><cell>evaluate the method. It should be noted that we split the</cell></row><row><cell>dataset randomly because we can not get the divided ACDC</cell></row><row><cell>data used in the previous method, so we reimplement the</cell></row><row><cell>TransUnet(Chen et al. 2021) and SwinUnet(Cao et al. 2021)</cell></row><row><cell>on our divided dataset and keep same settings with original</cell></row><row><cell>methods.</cell></row><row><cell>Implementation details. The MISSFormer is imple-</cell></row><row><cell>mented based on PyTorch and trained on Nvidia GeForce</cell></row><row><cell>RTX 3090 GPU with 24 GB memory. Different from previ-</cell></row><row><cell>ous work</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Comparison to state-of-the-art methods on Synapse dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>table 4</head><label>4</label><figDesc></figDesc><table><row><cell>shows, we list the results of MISSFormer S for intu-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Comparison to state-of-the-art methods on ACDC dataset. means that we keep the original settings and reimplement the method on our divided ACDC dataset itionistic comparison, and the performance of the model has been improved to varying degrees except step equals 3 after embedding the Enhanced Transformer Context Bridge into MISSFormer S, we call it as MISSFormer.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">ConclusionIn this paper, we presented MISSFormer, a postion-free and hierarchical U-shaped medical image segmentation transformer, which explored the global dependencies and local context capture. The proposed Enhanced Mix Block can overcome the problem of feature inconsistency caused by the direct embedding of convolution in feed forward neural network effectively and make discriminative feature representations. Based on these core designs, we further investigated the integration of muti-scale features generated by our hierarchical transformer encoder, which is essential for accurate sementation. We evaluated our method on two different forms of datasets, the superior results demonstrated the effectiveness and robustness of MISSFormer.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.05537</idno>
		<title level="m">Swin-Unet: Unet-like Pure Transformer for Medical Image Segmentation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Transunet: Transformers make strong encoders for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04306</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Reverse attention for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="234" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Twins: Revisiting the design of spatial attention in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.13840</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.10882</idno>
		<idno>arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Conditional positional encodings for vision transformers</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pranet: Parallel reverse attention network for polyp segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="263" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">CPFNet: Context pyramid fusion network for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3008" to="3018" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01136</idno>
		<title level="m">LeViT: a Vision Transformer in ConvNet&apos;s Clothing for Faster Inference</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ce-net: Context encoder network for 2d medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2281" to="2292" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Unetr: Transformers for 3d medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hatamizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10504</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">2021. nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Iwamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdulkadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Lienkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<editor>Springer, Cham. Isensee, F.</editor>
		<editor>Jaeger, P. F.</editor>
		<editor>Kohl, S. A.</editor>
		<editor>Petersen, J.</editor>
		<editor>and Maier-Hein, K. H.</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="203" to="211" />
		</imprint>
	</monogr>
	<note>3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation. Nature methods</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Bruce</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08248</idno>
		<title level="m">How much position information do convolutional neural networks encode? arXiv preprint</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.05707</idno>
		<title level="m">Localvit: Bringing locality to vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rethinking skip connection with layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3586" to="3598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<title level="m">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">V-net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-A</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 fourth international conference on 3D vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">CS-Net: channel and spatial attention network for curvilinear structure segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">U-Net: Convolutional Networks for Biomedical Image Segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="721" to="730" />
		</imprint>
	</monogr>
	<note>International Conference on Medical Image Computing and Computer-Assisted Intervention</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-scale self-guided attention for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE journal of biomedical and health informatics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="121" to="130" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.04430</idno>
		<title level="m">TransBTS: Multimodal Brain Tumor Segmentation Using Transformer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13797</idno>
		<title level="m">Pvtv2: Improved baselines with pyramid vision transformer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12122</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Uformer: A General U-Shaped Transformer for Image Restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03106</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.15203</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03024</idno>
		<title level="m">CoTr: Efficiently Bridging CNN and Transformer for 3D Medical Image Segmentation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A generic edge-attention guidance network for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Et-Net</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="442" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6881" to="6890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6881" to="6890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Unet++: A nested u-net architecture for medical image segmentation. In Deep learning in medical image analysis and multimodal learning for clinical decision support</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M R</forename><surname>Siddiquee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
