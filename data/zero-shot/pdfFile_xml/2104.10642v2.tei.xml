<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Temporal Modulation Network for Controllable Space-Time Video Super-Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Nankai University</orgName>
								<address>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Statistics and Data Science</orgName>
								<orgName type="institution">Nankai University</orgName>
								<address>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Nankai University</orgName>
								<address>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Lab of Pattern Recognition</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Sun</surname></persName>
							<affiliation key="aff3">
								<orgName type="laboratory">Youtu Lab</orgName>
								<address>
									<settlement>Tencent, Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Nankai University</orgName>
								<address>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Temporal Modulation Network for Controllable Space-Time Video Super-Resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Space-time video super-resolution (STVSR) aims to increase the spatial and temporal resolutions of lowresolution and low-frame-rate videos. Recently, deformable convolution based methods have achieved promising STVSR performance, but they could only infer the intermediate frame pre-defined in the training stage. Besides, these methods undervalued the short-term motion cues among adjacent frames. In this paper, we propose a Temporal Modulation Network (TMNet) to interpolate arbitrary intermediate frame(s) with accurate high-resolution reconstruction. Specifically, we propose a Temporal Modulation Block (TMB) to modulate deformable convolution kernels for controllable feature interpolation. To well exploit the temporal information, we propose a Locally-temporal Feature Comparison (LFC) module, along with the Bi-directional Deformable ConvLSTM, to extract short-term and long-term motion cues in videos. Experiments on three benchmark datasets demonstrate that our TMNet outperforms previous STVSR methods. The code is available at https: //github.com/CS-GangXu/TMNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Nowadays, flat-panel displays using liquid-crystal display (LCD) or light-emitting diode (LED) technologies can broadcast Ultra High Definition Television (UHD TV) videos with 4K (3840 ? 2160) or 8K (7680 ? 4320) fullcolor pixels, at the frame rate of 120 frames per second (FPS) or 240 FPS <ref type="bibr" target="#b38">[39]</ref>. However, currently available videos are commonly in Full High Definition (FHD) with a resolution of 2K (1920 ? 1080) at 30 FPS <ref type="bibr" target="#b44">[45]</ref>. To broadcast FHD videos on UHD TVs, it is necessary to increase their Given input frames at moments 0 (begin) and 1 (end), <ref type="bibr" target="#b44">[45]</ref> could only interpolate pre-defined intermediate frame at moment 0.5 (a), while our TMNet can generate intermediate frames at arbitrary moments (e.g., 0.3, 0.5, 0.7) (b). space-time resolutions comfortably with the broadcasting standard of UHD TVs. Although it is possible to increase the spatial resolution of videos frame-by-frame via single image super-resolution methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b21">22]</ref>, the perceptual quality of the enhanced videos would be deteriorated by temporal distortion <ref type="bibr" target="#b16">[17]</ref>. To this end, the space-time video superresolution (STVSR) methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b44">45]</ref> are developed to simultaneously increase the spatial and temporal resolutions of low-frame-rate and low-resolution videos.</p><p>Previous model-based STVSR methods <ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref> rely heavily on precise spatial and temporal registration <ref type="bibr" target="#b37">[38]</ref>, and would produce inferior reconstruction results when the registration is inaccurate. Besides, they usually require huge computational costs on solving complex optimization problems, resulting in low inference efficiency <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b24">25]</ref>. Later, deep convolutional neural networks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b43">44]</ref> have been widely employed in video restoration tasks such as video super-resolution (VSR) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b36">37]</ref>, video frame interpretation (VFI) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b45">46]</ref>, and the more challenging STVSR <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b44">45]</ref>. A straightforward solution for STVSR is to perform VFI and VSR successively on low-resolution and low-frame-rate videos, to increase their spatial resolutions and frame rates <ref type="bibr" target="#b44">[45]</ref>. However, these two-stage methods ignore the inherent correlation between temporal and spatial dimensions. That is, the videos with high-resolution frames contain richer details on moving object(s) and back-ground, while those in high-frame-rate provide finer pixel alignment between adjacent frames <ref type="bibr" target="#b7">[8]</ref>. Therefore, these two-stage STVSR methods would suffer from the temporal inconsistency problem <ref type="bibr" target="#b44">[45]</ref> and produce artifacts, e.g., "the attentional blink phenomenon" <ref type="bibr" target="#b35">[36]</ref> on STVSR.</p><p>To well exploit the correlation between the temporal and spatial dimensions in videos, several one-stage STVSR methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b44">45]</ref> have been proposed to simultaneously perform VFI and VSR reconstruction on low-frame-rate and low-resolution videos. The work of STARnet <ref type="bibr" target="#b7">[8]</ref> estimates the motion cues with an additional optical flow branch <ref type="bibr" target="#b4">[5]</ref>, and performs feature warping of two adjacent frames to interpolate the intermediate frame. But this flow-based method <ref type="bibr" target="#b7">[8]</ref> needs to learn an extra branch for optical flow estimation, which consumes expensive costs on computation and memory. To alleviate this problem, Xiang et al. <ref type="bibr" target="#b44">[45]</ref> employed the deformable convolution backbone <ref type="bibr" target="#b40">[41]</ref>, and directly performed STVSR on the feature space. Though with promising performance, current STVSR networks could only generate the intermediate frames pre-defined in the network architecture, and thus are restricted to highly-controlled application scenarios with fixed frame-rate videos. However, in many commercial scenarios, such as sports events, it is very common for the user to flexibly adjust the intermediate video frames for better visualization. Thus, it is necessary to develop controllable STVSR methods for smooth motion synthesizing.</p><p>To fulfill the versatile requirements of broadcasting scenarios, in this paper, we propose a Temporal Modulation Network (TMNet) to interpolate an arbitrary number of intermediate frames for STVSR, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. But current deformable convolution based methods <ref type="bibr" target="#b44">[45]</ref> could only generate pre-defined intermediate frame(s). To tackle this problem, we propose a Temporal Modulation Block (TMB) to incorporate motion cues into the feature interpolation of intermediate frames. Specifically, we first estimate the motion between two adjacent frames under the deformable convolution framework <ref type="bibr" target="#b40">[41]</ref>, and learn controllable interpolation at an arbitrary moment defined by a temporal parameter. In addition, we also propose a Locallytemporal Feature Comparison module to fuse multi-frame features for effective spatial alignment and feature warping, and a globally-temporal feature fusion to explore the longterm variations of the whole video. This two-stage temporal feature fusion scheme accurately interpolates the intermediate frames for STVSR. Extensive experiments on three benchmarks <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b45">46]</ref> demonstrate that our TMNet is able to interpolate an arbitrary number of intermediate frames, and achieves state-of-the-art performance on STVSR.</p><p>The contribution of this work are three-fold:</p><p>? We propose a Temporal Modulation Network (TM-Net) to perform controllable interpolation of arbitrary frame-rates for flexible STVSR performance. This is achieved by our Temporal Modulation Block under the deformable convolution framework.</p><p>? We present a two-stage temporal feature fusion scheme for effective STVSR. Specifically, we propose a locally-temporal feature comparison module to exploit the short-term motion cues of adjacent frames, and perform globally-temporal feature fusion by exploring the long-term variations over the whole video.</p><p>? Experiments on three benchmarks show that our TM-Net is able to perform controllable frame interpolation at arbitrary frame-rate, and outperforms stateof-the-art STVSR methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Video frame interpolation (VFI) aims to synthesize new intermediate frames between adjacent frames <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20]</ref>. Early VFI methods mainly resort to optical flow techniques for motion estimation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b26">27]</ref>. Jiang et al. <ref type="bibr" target="#b14">[15]</ref> modeled motion interpretation for arbitrary frame-rate VFI.</p><p>Niklaus et al. <ref type="bibr" target="#b25">[26]</ref> warped the input frames with contextual information, and interpolated context-aware intermediate frames. Bao et al. employed motion estimation and compensation for VFI in <ref type="bibr" target="#b1">[2]</ref>, and obtained improved performance by further exploring the depth information <ref type="bibr" target="#b0">[1]</ref>. Niklaus et al. <ref type="bibr" target="#b26">[27]</ref> tackled the conflicts of mapping multiple pixels to the same location in VFI by softmax splatting. However, these optical flow based methods need huge computational costs on motion estimation. Therefore, recently researchers exploited to learn spatially-adaptive convolution kernels <ref type="bibr" target="#b27">[28]</ref> or deformable ones for VFI <ref type="bibr" target="#b19">[20]</ref>. Video super-resolution (VSR) is the task of increasing the spatial resolutions of low-resolution (LR) videos <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b40">41]</ref>. Existing VSR methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b40">41]</ref> mainly aggregate spatial information of multiple frames for high-resolution (HR) reconstruction, with the help of optical flow techniques <ref type="bibr" target="#b4">[5]</ref>. Jo et al. <ref type="bibr" target="#b15">[16]</ref> generated dynamic upsampling filters to enhance the LR frames with residual learning <ref type="bibr" target="#b11">[12]</ref>. Wang et al. <ref type="bibr" target="#b40">[41]</ref> proposed the Pyramid, Cascading and Deformable (PCD) module to perform frame alignment, and then fused multiple frames into a single one by spatial and temporal attention. Haris et al. <ref type="bibr" target="#b6">[7]</ref> designed an iterative refinement framework by integrating the spatial and temporal contexts of multiple frames. Tian et al. <ref type="bibr" target="#b36">[37]</ref> utilized the learned sampling offsets of deformable convolution kernels to align the supporting frames with the reference ones, which are both used to reconstruct the HR frames. Space-time video super-resolution (STVSR) aims to increase the spatial and temporal dimensions of the lowframe-rate and low-resolution videos <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b44">45]</ref>. Shechtman et al. <ref type="bibr" target="#b30">[31]</ref>   <ref type="bibr" target="#b44">[45]</ref>. Finally, we employ 40 residual blocks to reconstruct high-resolution feature maps F H , and two Pixel-Shuffle layers to output the high-frame-rate and high-resolution video I H . mulated their STVSR method under the Markov Random Field framework <ref type="bibr" target="#b5">[6]</ref>. STARnet <ref type="bibr" target="#b7">[8]</ref> leveraged inherent motion relationship between spatial and temporal dimensions with an extra optical flow branch <ref type="bibr" target="#b4">[5]</ref>, and perform feature warping of two adjacent frames to interpolate the intermediate frame. Xiang et al. <ref type="bibr" target="#b44">[45]</ref> developed a unified framework to interpolate the multi-frame features via PCD alignment modules <ref type="bibr" target="#b40">[41]</ref>, the intermediate features by bidirectional deformable ConvLSTM <ref type="bibr" target="#b33">[34]</ref>, and finally performed STVSR by multi-frame feature fusion. In this work, our goal is to develop a temporally controllable network for powerful and flexible STVSR. Though built upon <ref type="bibr" target="#b44">[45]</ref>, our TMNet arrives at better performance on benchmark datasets, owing to the proposed locally temporal feature comparison module. Modulation networks. Recently, researchers proposed to control the restoration intensity of the main network by additional modulation branches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42]</ref>. These modulation networks are trained to trade-off the restoration quality and flexibility, which are controlled by hyper-parameters. He et al. <ref type="bibr" target="#b8">[9]</ref> put feature modulation filters after each convolution layer to modulate the output according to user's preference. Later, He et al. <ref type="bibr" target="#b9">[10]</ref> expanded this design to multiple dimensions, and modulated the output according to the levels of multiple degradation types. Wang et al. <ref type="bibr" target="#b39">[40]</ref> learned the features from tuning blocks and residual ones with different objectives, to control the trade-off between noise reduction and detail preservation. In this work, we consider the modulation on temporal dimension, instead of on restoration intensity as in these modulation networks. As far as we know, our work is among the first to implement temporal modulation in the STVSR problem. As will be shown in ?4, our TMNet can explore the potential of tem-poral modulation for controllable STVSR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>In this section, we first overview our Temporal Modulation Network (TMNet) for STVSR in ?3.1. Then, we introduce our Temporal Modulation Block for controllable feature interpolation in ?3.2. We present temporal feature fusion in ?3.3, and high-resolution reconstruction in ?3.4. Finally, the training details are given in ?3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network Overview</head><p>As illustrated in <ref type="figure">Fig. 2</ref>, our TMNet consists of three seamless stages: controllable feature interpolation, temporal feature fusion, and high-resolution reconstruction. Controllable feature interpolation. Given a sequence of low-frame-rate and low-resolution video I L = {I L 2i?1 } n i=1 , our TMNet firstly extracts the corresponding initial feature maps {F L 2i?1 } n i=1 through 5 residual blocks. To perform temporally controllable feature interpolation, we propose a Temporal Modulation Block (TMB) to modulate the deformable convolution kernels with a temporal hyperparameter t. Here, t?(0, 1) indicates the (arbitrary) moment at which we plan to interpolate a feature map F L 2i,t from the feature maps F L 2i?1 and F L 2i+1 of two adjacent frames I L 2i?1 and I L 2i+1 , respectively. Finally, we obtain a feature se-</p><formula xml:id="formula_0">quence F L = {F L 1 , F L 2,t , F L 3 , ..., F L 2n?2,t , F L 2n?1 }</formula><p>of highframe-rate and low-resolution video frames. Temporal feature fusion. The extracted (or interpolated) feature maps in F L are often of low-quality, since they are extracted from individual LR frames (or interpolated by the initial feature maps of adjacent LR frames). Thus, we pro-  <ref type="figure">Figure 3</ref>: Proposed Temporal Modulation Block (TMB) modulated Pyramid, Cascading and Deformable (PCD) module <ref type="bibr" target="#b40">[41]</ref> for controllable feature interpolation. f 0?1 TMB (f 1?0 TMB ) is the PCD module modulated by our TMB block to model the forward (backward) motion. Our TMB modulates all the three levels of the PCD module, by transforming the temporal hyper-parameter t into a modulation vector v t via a fully connected network (FCN) consisted of three convolutional layers. pose a Locally-temporal Feature Comparison (LFC) module, to refine every feature map in F L with the help of the feature maps of adjacent frames. After the local feature refinement, we further improve the feature maps in F L by performing globally-temporal feature fusion (GFF). This is implemented by employing a Bi-directional Deformable ConvLSTM (BDConvLSTM) network <ref type="bibr" target="#b44">[45]</ref>, to consecutively aggregate the useful information from individual feature maps along the temporal direction. Both the LFC and GFF fusion modules well exploit the intra-correlation between spatial and temporal dimensions to improve the quality of feature maps in F L . In the end, we obtain the sequence of improved feature maps F L GF F . High-resolution reconstruction. Here, we feed the sequence of feature maps F L GF F into 40 residual blocks to improve their quality along the spatial dimension. Next, we increase the spatial resolution of these improved feature maps via the widely used Pixel-Shuffle layers <ref type="bibr" target="#b32">[33]</ref>, and output the final high-frame-rate and high-resolution video sequence</p><formula xml:id="formula_1">2 1 L i F ? 2 1 L i F + Down t C C C Up Conv LReLU C Conv FCN TMB DCN C 2 ,0 L i t F ? LReLU C DCN C TMB Up t Up Conv LReLU DCN TMB Up t Down offset 1 0 TMB f ? 0 1 TMB f ? 2 1 L i F ? 2 1 L i F + 2 ,0 L i t F ? 2 ,1 L i t F ? 2 , L i t F Controllable Feature Interpolation 1-t t C C</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TMB Modulated PCD Module ( )</head><formula xml:id="formula_2">I H = {I H 1 , I H 2,t , ..., I H 2n?2,t , I H 2n?1 }.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Controllable Feature Interpolation</head><p>Given a sequence of low-frame-rate and low-resolution video frames</p><formula xml:id="formula_3">I L = {I L 2i?1 } n i=1 , we first extract the corre- sponding initial features F L = {F L 2i?1 } n i=1</formula><p>via five residual blocks. Each residual block contains a sequence of "Conv-ReLU-Conv" operations with a skip connection. For any two adjacent frames I L 2i?1 and I L 2i+1 (i ? {1, ..., n ? 1}), our goal here is to interpolate the feature of intermediate frame at an arbitrary moment t ? (0, 1). To this end, we need to estimate the motion cues from I L 2i?1 to the intermediate frame (forward) and that from I L 2i+1 to the intermedi-ate frame (backward). Previous STVSR methods <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b44">45]</ref> utilized the Pyramid, Cascading and Deformable (PCD) module to estimate the offset between F L 2i?1 and F L 2i+1 as the motion cues, to align and interpolate the features of the intermediate frame under the deformable convolutional framework <ref type="bibr" target="#b47">[48]</ref>. However, the vanilla PCD module could only estimate the motion to a predefined moment, which is fixed in both training and inference stages.</p><p>To overcome this limitation, we propose a Temporal Modulation Block (TMB) to modulate the learned offset between F L 2i?1 and F L 2i+1 . The modulation is controlled by a hyper-parameter t ? (0, 1), indicating an arbitrary moment that we plan to interpolate a new frame. This enables our TMNet to control the feature interpolation process upon the initial feature maps F L 2i?1 and F L 2i+1 of two adjacent frames I L 2i?1 and I L 2i+1 in the input video. The PCD module modulated by our TMB block can estimate the forward and backward motions and interpolate the feature map F L 2i,t of a new frame at the arbitrary moment t ? (0, 1).</p><p>Denote by f 0?1 TMB and f 1?0 TMB the PCD modules modulated by our TMB block, to model the forward and backward motions, respectively. Here, we perform modulated feature interpolation from the forward and backward directions:</p><formula xml:id="formula_4">F L 2i,0?t = f 0?1 TMB (F L 2i?1 , F L 2i+1 , t), F L 2i,1?t = f 1?0 TMB (F L 2i?1 , F L 2i+1 , 1 ? t),<label>(1)</label></formula><p>where F L 2i,0?t and F L 2i,1?t are the interpolated features aligned from the feature maps F L 2i?1 and F L 2i+1 of the adjacent frames. Note that the two TMB-modulated PCD modules share the same network structure but have different weights. Here we only take the f 0?1 TMB as an example to explain how the PCD modules modulated by our TMB work on modeling the forward motion. The PCD module f 1?0 TMB modeling the backward motion can be similarly explained.</p><p>As shown in <ref type="figure">Fig. 3</ref>, the PCD module has three levels to estimate the motion in different scales. To realize flexible modulation on the temporal dimension, we embed our TMB block into each level of the vanilla PCD module independently, to modulate the offset before the deformable convolutional network (DCN). The benefits of adding our TMB block to all three levels of the PCD module will be verified in ?4. To adaptively modulate the offset by our TMB at different levels of PCD, we use three convolutional layers to map the temporal hyper-parameter t onto a modulation vector v t of size 1 ? 1 ? 64. To well exploit the motion cues for precise modulation, we feed the features in each vanilla PCD level into two convolutional layers to enlarge their receptive fields. Then, the generated feature is multiplied with the modulation vector v t along the channel dimension, to produce the TMB-modulated features. For robustness, we add the TMB-modulated features with the corresponding pre-modulated features before DCN.</p><p>Once obtaining the modulated feature maps F L 2i,0?t and F L 2i,1?t , we interpolate the intermediate feature F L 2i,t via channel-wise concatenation "[?, ?]" followed by a 1 ? 1 convolution layer f 1?1 as:</p><formula xml:id="formula_5">F L 2i,t = f 1?1 ([F L 2i,0?t , F L 2i,1?t ]).<label>(2)</label></formula><p>Now, we obtain the features of the interpolated sequence</p><formula xml:id="formula_6">F L = {F L 1 , F L 2,t , F L 3 , ..., F L 2n?2,t , F L 2n?1 }</formula><p>for the highframe-rate and low-resolution video. Next, we perform feature fusion along the temporal dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Temporal Feature Fusion</head><p>Here, the initial features are extracted (or interpolated) from individual (or adjacent) frames. There is considerable leeway to improve their quality. But we also feed the initial features into the Pixel-Shuffle part of our TMNet. Locally-temporal feature comparison. It is essential to maintain short-term temporal consistency for each current frame. For this purpose, we propose a Locally-temporal Feature Comparison (LFC) module to exploit the complementary information (e.g., motion cues) from adjacent frames. As illustrated in <ref type="figure">Fig. 4</ref>, to refine the feature map F L 2i,t of current frame from adjacent feature maps F L 2i?1 and F L 2i+1 , we concatenate current frame (F L 2i,t ) and adjacent frames (F L 2i?1 , F L 2i+1 ), and employ two convolutional layers to learn the offset in the deformable convolutional framework <ref type="bibr" target="#b47">[48]</ref>. Note that we learn two offsets to describe the motion cues in the forward (from I L 2i?1 to current frame) and the backward (from I L 2i+1 to current frame) directions. Then, the learned offset from forward (or backward) direction is used to align the feature map F L 2i?1 of previous (or F L 2i+1 of next) frame with that of the current frame, via one deformable convolutional layer. After the alignment,  <ref type="figure">Figure 4</ref>: Proposed Locally-temporal Feature Comparison (LFC) module refines the interpolated feature F L 2i,t by exploiting short-term motion cues among adjacent frames.</p><p>we concatenate the aligned feature maps of two adjacent frames with that of the current frame, and perform feature comparison via four 1 ? 1 convolutional layers and an addition operation. For the first (or last) frame, the previous (or next) adjacent frame is just itself. Now we get a refined feature sequence F L LF C . Globally-temporal feature fusion. The feature sequence refined by our LFC module is able to maintain short-term consistency in the interpolated video. But it would fail on large or fast motions, since LFC lacks the capability of modeling the motions over the whole video. To tackle this problem, we propose to exploit the long-term information in videos by globally-temporal feature fusion. Inspired by <ref type="bibr" target="#b44">[45]</ref>, we feed the feature sequence F L LF C generated by our LFC into the BDConvLSTM network, and obtain the features F L GF F in long-term temporal consistency. As will be illustrated in the experimental section, our short-term LFC module and the long-term BDConvLSTM indeed boost the performance of our TMNet on STVSR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">High-Resolution Reconstruction</head><p>Until now, the intra-correlation of temporal and spatial dimensions is well explored to obtain the high-quality feature sequence F L GF F of the whole video. Then, we perform spatial refinement for the feature maps via 40 residual blocks, and get the refined feature maps F H . Then we add the features F H with the corresponding initial feature maps in F L , and obtain the reconstructed feature maps F H f inal . Finally, we feed the reconstructed feature maps F H f inal into two Pixel-Shuffle layers, followed by a sequence of "Conv-LeakyReLU-Conv" operations, to output the reconstructed HR video frames</p><formula xml:id="formula_7">I H = {I H 1 , I H 2,t , ..., I H 2n?2,t , I H 2n?1 }.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Training Details</head><p>Implementation details. We employ the Adam optimizer <ref type="bibr" target="#b17">[18]</ref> with ? 1 = 0.9 and ? 2 = 0.999 to optimize our TMNet with the Charbonnier loss function <ref type="bibr" target="#b18">[19]</ref>, as suggested in <ref type="bibr" target="#b44">[45]</ref>. The learning rate is initialized as 4 ? 10 ?4 , and is decayed to 1 ? 10 ?7 with a cosine annealing <ref type="bibr" target="#b23">[24]</ref> for <ref type="table">Table 1</ref>: Comparison of PSNR, SSIM <ref type="bibr" target="#b42">[43]</ref>, speed (in fps), and parameters (in million) by different STVSR methods on Vid4 <ref type="bibr" target="#b34">[35]</ref>, Vimeo-Fast, Vimeo-Medium, Vimeo-Slow <ref type="bibr" target="#b45">[46]</ref>. "?" means that larger is better. The speed is evaluated on Vid4 <ref type="bibr" target="#b34">[35]</ref>. The best, second best and third best results are highlighted in red, blue and bold, respectively. every 150,000 iterations. We initialize the parameters of our TMNet by Kaiming initialization <ref type="bibr" target="#b10">[11]</ref> without pre-trained weights. The batch size is 24. Our TMNet, implemented in PyTorch <ref type="bibr" target="#b28">[29]</ref> and Jittor <ref type="bibr" target="#b13">[14]</ref>, is trained in a total of 600,000 iterations on four RTX 2080Ti GPUs, which takes about 8.71 days (209.04 hours). For each input video clip, we randomly crop it into a sequence of downsampled patches of size 32 ? 32. For data argumentation, we horizontal-flip each frame, and randomly rotate it with 90 ? , 180 ? , or 270 ? . Network training. When directly trained with the proposed TMB block, our TMNet suffers from clear performance drops on STVSR, as shown in our experiments. One possible reason is that our TMNet can not accurately estimate the motion cues to interpolate an intermediate frame at the arbitrary moment t ? (0, 1) since our TMB does not know the moment before the training modulated feature. To resolve this problem, we propose to train our TMNet by a two-step strategy:</p><p>Step 1, we train our main TMNet without the proposed TMB block;</p><p>Step 2, we only train our TMB block while fixing the trained main network.</p><p>In Step 1, we train our TMNet on the Vimeo-90K dataset <ref type="bibr" target="#b45">[46]</ref>, which will be introduce in ?4.1. This dataset consists of 7-frame video clips. For each clip, the 1-st, 3rd, 5-th, and 7-th LR frames are input into our TMNet as low-frame-rate and low-resolution video. We set t = 0.5 to get rid of the TMB block from our TMNet, and learn to generate the 7-frame high-resolution and high-frame-rate video. This enables our TMNet to fairly compare with previous STVSR methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b44">45]</ref>. For supervision, we calculate the loss function over the corresponding 7-frame HR video clip in the Vimeo-90K dataset <ref type="bibr" target="#b45">[46]</ref>.</p><p>In Step 2, we fix the learned weights of our main network, and only train our TMB block for temporal mod-ulation. The training is performed on the Adobe240fps dataset <ref type="bibr" target="#b34">[35]</ref>, which is in high-frame-rate and suitable for training our TMB block. We also split it into groups of 7frame video clips. For each clip, the 1-st and 7-th HR frames are downsampled as the inputs of our TMNet. We set the temporal hyper-parameter t?{ <ref type="bibr" target="#b0">1</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Dataset. We use Vimeo-90K septuplet dataset <ref type="bibr" target="#b45">[46]</ref> as the training set. It contains 91,701 video sequences, extracted from 39K video clips selected from Vimeo-90K. Each sequence contains 7 continuous frames of resolution 448 ? 256. The Vid4 <ref type="bibr" target="#b22">[23]</ref> and Vimeo-90K test set are used as evaluation datasets. As suggested in <ref type="bibr" target="#b44">[45]</ref>, we split the Vimeo-90K septuplet test set into three subsets of Fast motion, Medium motion, and Slow motion, which include 1225, 4977, and 1613 video clips, respectively. We also remove 5 video clips from the original Medium motion set and 3 clips from the Slow motion set, which contain only all-black backgrounds.</p><p>To make our TMNet feasible for controllable feature interpolation, we train our TMB block individually on the Adobe240fps dataset <ref type="bibr" target="#b34">[35]</ref>. It has 133 videos (in 720P) taken with hand-held cameras, and is randomly split into the train, val, and test subsets with 100, 16, and 17 videos, respectively. For each video, we split it into groups of 7-frame video clips. We feed the 1-st and 7-th frames in each clip into our TMNet to generate 5 intermediate frames.</p><p>We downsample the HR frames to create the LR frames by bicubic interpolation, with a factor of 4.  Evaluation metric. We employ the widely used Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM) <ref type="bibr" target="#b42">[43]</ref> to evaluate different methods on the STVSR task. The PSNR and SSIM metrics are calculated on the Y channel of the YCbCr color space, as favored by previous VSR <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b40">41]</ref> and STVSR <ref type="bibr" target="#b44">[45]</ref> methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison to State-of-the-arts</head><p>Comparison methods. We compare our TMNet with stateof-the-art two-stage and one-stage STVSR methods. For the two-stage STVSR methods, we perform video frame interpolation (VFI) by SuperSloMo <ref type="bibr" target="#b14">[15]</ref>, DAIN <ref type="bibr" target="#b0">[1]</ref> or SepConv <ref type="bibr" target="#b27">[28]</ref>, and perform video super-resolution (VSR) by Bicubic Interpolation (BI), RCAN <ref type="bibr" target="#b46">[47]</ref>, RBPN <ref type="bibr" target="#b6">[7]</ref> or EDVR <ref type="bibr" target="#b40">[41]</ref>. For one-stage STVSR methods, we compare our TMNet with the recently developed Zooming SlowMo <ref type="bibr" target="#b44">[45]</ref> and STARnet <ref type="bibr" target="#b7">[8]</ref>. To fairly compare with these competitors, we set t = 0.5 in our TMNet to generate the frame at the middle moment of any two adjacent frames. That is, the 1-st, 3-rd, 5-th, and 7-th LR frames of each clip in Vimeo-90K are fed into our TMNet to reconstruct the 7 HR frames. All these methods are trained on the Vimeo-90K septuplet dataset <ref type="bibr" target="#b45">[46]</ref>, evaluated on the Vimeo-90K test set <ref type="bibr" target="#b45">[46]</ref> and the Vid4 <ref type="bibr" target="#b34">[35]</ref> dataset. Objective results. We list the quantitative comparison results in <ref type="table">Table 1</ref>. As suggested in <ref type="bibr" target="#b44">[45]</ref>, we omit the baseline models with Bicubic Interpolation when comparing the speed. One can see that our TMNet outperforms the Zooming SlowMo <ref type="bibr" target="#b44">[45]</ref> by 0.12dB, 0.23dB, 0.19dB, and 0.15dB on the Vid4, Vimeo-Fast, Vimeo-Medium, and Vimeo-Slow datasets in terms of PSNR. On SSIM <ref type="bibr" target="#b42">[43]</ref>, our TMNet achieves better results than the competitors in most cases, but is only slightly inferior to STARnet <ref type="bibr" target="#b7">[8]</ref> on Vid4 <ref type="bibr" target="#b22">[23]</ref> and Vimeo-Slow. However, our TMNet needs only one-ninth of the parameters in STARnet. On speed, one-stage methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b44">45]</ref> run much faster than two-stage ones <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b40">41]</ref>. Our TMNet runs at 14.69fps, and is only slower than Zooming Slow-Mo <ref type="bibr" target="#b44">[45]</ref>. All these results validate the effectiveness of our TMNet on STVSR. Visualization. In the 1-st row of <ref type="figure" target="#fig_2">Figure 5</ref>, we present the 5 intermediate frames (Frame 1550 to Frame 1554) interpolated by our TMNet on the sequence "0056" from the Adobe240fps test set <ref type="bibr" target="#b34">[35]</ref>, given the Frame 1549 and Frame 1555 as inputs. It can be seen that our TMNet is able to perform flexible frame interpolation for STVSR. In the 2-nd row of <ref type="figure" target="#fig_2">Figure 5</ref>, we show the reconstructed frames by different STVSR methods from Vimeo-Fast and and Vimeo-Slow datasets <ref type="bibr" target="#b45">[46]</ref> generated by the competing methods respectively. We observe that our TMNet, with the proposed LFC module, can restore more clearly the structures and textures than the competitors. For example, on the Clip "0001" in the sequence "0070" of Vimeo-Slow datasets, our TMNet reconstructs clearly the texture pattern on the bag. In summary, our TMNet demonstrates flexible and powerful STVSR ability quantitatively and qualitatively. More visual comparison on the Vid4 <ref type="bibr" target="#b22">[23]</ref>, Vimeo-90K test set <ref type="bibr" target="#b34">[35]</ref>, and Adobe240fps <ref type="bibr" target="#b45">[46]</ref> datasets are provided in the Supplementary File, because of page limitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>Here, we conduct detailed examinations of our TMNet on STVSR. Specifically, we assess 1) the importance of our Temporal Modulation Block (TMB) for controllable feature interpolation; 2) different strategies that our TMB block modulates the PCD module; 3) how to design our TMB block; 4) how our Locally-temporal Feature Comparison (LFC) module contribute to the temporal feature fusion in our TMNet; 5) the combination of high-quality feature maps F H and initial feature maps F L for STVSR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Does our TMB block contribute to controllable feature interpolation?</head><p>To answer this question, we compare our TMNet with previous STVSR methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b44">45]</ref> on generating intermediate frames from two adjacent frames. Due to limited space, we provide the comparison of visual results on Adobe240fps test set <ref type="bibr" target="#b34">[35]</ref> in the Supplementary File. We observe that our TMNet with TMB block indeed exhibits temporally controllable STVSR performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">How different strategies that our TMB block modulate the PCD module influence our TMNet on STVSR?</head><p>The PCD module <ref type="bibr" target="#b40">[41]</ref> has a three-level pyramid structure: the 1-st level L1; the 2-nd level L2 is downsampled from the features in L1 by convolution filters at a stride of 2; similarly, the 3-rd level L3 is downsampled from L2 by a stride of 2. In our TMNet, the proposed TMB modulates all the three levels of the PCD module. But our TMB can also modulate only one level (L1, L2, or L3) of PCD, resulting three variants of our TMNet called TMB-L1, TMB-L2, and TMB-L3. These variants are trained on the Adobe240fps train set <ref type="bibr" target="#b34">[35]</ref> and evaluate them on test set. As shown in <ref type="table" target="#tab_5">Table 2</ref>, the three variants perform in descending order, indicating that the 1-st level of PCD is more important for temporal modulation. By modulating all three levels of PCD, our TMNet outperforms the three variants on STVSR, by better exploiting the motion cues of videos. 3. How to design our TMB block? The goal of our TMB is to transform the hyper-parameter t into a modulation vector v t comfortable with the PCD module. A trivial design of our TMB is a linear convolutional layer. We call it TMB-Linear. We train our TMB and the TMB-Linear on Adobe240fps train set <ref type="bibr" target="#b34">[35]</ref> and evaluate them on test set. The PSNR results are listd in <ref type="table" target="#tab_6">Table 3</ref>, in which the TMB-Linear is 0.02dB lower than our TMB with three nonlinear convolutional layers. This shows that nonlinear transformation is only a little better than the linear one.  <ref type="table" target="#tab_7">Table 4</ref>. One can see that our TMNet (LFC?GFF) achieves the best results on all cases, and outperforms GFF by 0.07dB on Vid4, 0.17dB on Vimeo-Fast, 0.15dB on Vimeo-Medium, and 0.11dB on Vimeo-Slow. This indicates that our LFC module is essential to the success of our TMNet on STVSR, by exploiting short-term motion cues among adjacent frames. In our TMNet, we combine the high-quality features F H with the initial features F L before the Pixel-Shuffle layers for final STVSR. Since the initial features F L largely influence our LFC module, we remove them both from our TMNet and obtain a variant "Baseline". Then we add F L to the "Baseline", and obtain a variant model "+F L ". We train our TMNet and the two variants on the Vimeo-90K septuplet dataset <ref type="bibr" target="#b45">[46]</ref>, and evaluate them on Vimeo-90K test and Vid4 <ref type="bibr" target="#b22">[23]</ref> datasets. As shown in <ref type="table" target="#tab_8">Table 5</ref>, the variant "+F L " clearly exceeds the "Baseline". This validates that combining high-quality features F H with initial ones F L is helpful to our TMNet on STVSR. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we proposed a Temporal Modulation Network (TMNet) to flexibly interpolate intermediate frames for space-time video super-resolution (STVSR). Specifically, we introduced a Temporal Modulation Block to modulate the learning of the deformable convolution framework for controllable feature interpolation. To well exploit motion cues, we performed short-term and long-term temporal feature fusion consisting of our proposed Locally-temporal Feature Comparison (LFC) module and a Bi-directional Deformable ConvLSTM, respectively. Experiments on three benchmarks demonstrated the flexibility of our TMNet on interpolating intermediate frames, quantitative and qualitative advantages of our TMNet over previous methods, and effectiveness of our LFC module, for STVSR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author Contributions</head><p>J.X. conceived and managed the project, contributed major ideas, designed experimental settings, and rewrote the paper. G.X. contributed ideas, implemented networks, performed experiments, and wrote the first draft. Z.L. plotted some initial figures. L.W., X.S. and M.C. contributed discussions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices 1. Content</head><p>In this supplemental file, we provide more details of our Temporal Modulation Network (TMNet) for Space-Time Video Super-Resolution (STVSR). Specifically, we provide ? the detailed network structure of our TMNet in ?2;</p><p>? more details of our two-step training scheme in ?3;</p><p>? flexibility of our TMNet on interpolating arbitrary number of intermediate frames in ?4;</p><p>? more visual comparisons of our TMNet with previous STVSR methods in ?5;</p><p>? how the one-stage training (instead of two-stage) influences our TMNet with TMB on STVSR in ?6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Detailed Network Structure of Our TMNet</head><p>Here, we illustrate the detailed network architecture of our proposed TMNet in <ref type="figure">Figure 6</ref>.</p><p>We first extract the corresponding initial features F L = {F L 2i?1 } n i=1 via five residual blocks. Each residual block contains a sequence of "Conv-ReLU-Conv" operations with a skip connection. The Controllable Feature Interpolation (CFI) is performed by the Pyramid, Cascading and Deformable (PCD) module <ref type="bibr" target="#b40">[41]</ref> modulated by our proposed Temporal Modulation Block (TMB), which is illustrated in <ref type="figure">Figure 2</ref> of our main paper. The detailed structure of our TMB block is shown in 7 (right). The proposed Locallytemporal Feature Comparison (LFC) module is presented in <ref type="figure">Figure 7</ref> (left). The BDConvLSTM part is directly implemented by employing the Bi-directional Deformable Con-vLSTM network in <ref type="bibr" target="#b44">[45]</ref>. The Upsampling part contains operations of two "Convolutions (Conv), Pixel-Shuffle, and LeakyReLU", and one "Conv-LeakyReLU-Conv".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">More Details of Two-step Training Scheme</head><p>Here, we provide more details of the two-step training strategy for our TMNet.</p><p>In Step 1, we use the Vimeo-90K septuplet dataset <ref type="bibr" target="#b45">[46]</ref> as the training set, and the Vid4 <ref type="bibr" target="#b22">[23]</ref>, Vimeo-Fast, Vimeo-Medium, and Vimeo-Slow sets as the evaluation sets. The Vimeo-90K septuplet, Vimeo-Fast, Vimeo-Medium, and Vimeo-Slow datasets <ref type="bibr" target="#b45">[46]</ref> consist of 7-frame video sequences, and the Vid4 <ref type="bibr" target="#b22">[23]</ref> dataset contains 4 video clips, which contains 41, 34, 49 and 47 frames, respectively. All the frames in the Vid4 dataset <ref type="bibr" target="#b22">[23]</ref> are split into sequences containing 7 continuous frames. We downsample all the original HR frames to obtain the lowresolution (LR) input frames via Bicubic interpolation, by a factor of 4. When we train our TMNet, we initialize the parameters of our TMNet by Kaiming initialization <ref type="bibr" target="#b10">[11]</ref> without pre-trained weights. We set t = 0.5 to get rid of the TMB block and take the 1-st, 3-rd, 5-th, and 7th LR frames of every sequence as a low-frame-rate and low-resolution input video to train our TMNet. Thus, with the supervision of the corresponding 7-frame HR video sequences in the Vimeo-90K septuplet dataset <ref type="bibr" target="#b45">[46]</ref>, our TMNet can learn to generate the 7-frame high-resolution and high-frame-rate video sequence. It costs 8.71 days (209.04 hours) to train our TMNet for 600,000 iterations.</p><p>In Step 2, we fix the weights of our main network learned in Step 1 and only train our TMB block for temporal modulation. Here, we train our TMNet on the Adobe240fps dataset <ref type="bibr" target="#b34">[35]</ref>, which has 133 videos in 720P with high-frame-rate (240fps). At first, We randomly split the Adobe240fps dataset <ref type="bibr" target="#b34">[35]</ref> into the train, val, and test subsets with 100, 16, and 17 videos, respectively. Then we split the frames from Adobe240fps train, valid, and test sets into sequences of 7 continuous frames. We first downsample the original HR frames with the resolution of 1280?720 by a factor of 2 and take them as the ground truths (GTs). Then we downsample the GTs to create the corresponding LR input frames by a factor of 4. All the downsample operations are performed via Bicubic interpolation. The 1-st and 7-th LR frames of each video sequence are input to our TMNet. We set the temporal hyper-parameter t?{ <ref type="bibr">1 6</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Flexible STVSR with Arbitrary Number of Intermediate Frames</head><p>To show the flexibility of our TMNet for interpolating arbitrary number of intermediate frames on STVSR, we provide the results generated by our TMNet between the input two frames using multiple temporal hyper-parameter t. As the motions in Adobe240fps <ref type="bibr" target="#b34">[35]</ref> dataset are extremely slow, we validate the flexibility of our TMNet on the Vimeo-90K dataset <ref type="bibr" target="#b45">[46]</ref>. To this end, we set the temporal hyperparameter t?{0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9} to interpolate 9 intermediate frames between any two adjacent frames, though our TMNet is trained to interpolate 5 intermediate frames between Frame 1 and Frame 7. The results are shown in <ref type="figure">Figure 8</ref>. One can see that the interpolated frames vary continuously with the change of t from 0.1 to 0.9. This demonstrates that our TMNet is feasible to generate a number of intermediate frames, which is different from the training stage. That is, our TMNet is very flexible on interpolating arbitrary number of intermediate frames, according to the temporal hyper-parameter t ? (0, 1).</p><p>In <ref type="figure">Figure 9</ref>, we visualize the temporal consistency of our TMNet and Zooming SlowMo <ref type="bibr" target="#b44">[45]</ref>, on the Clip 0277 of "00006" from the Vimeo-Fast set <ref type="bibr" target="#b45">[46]</ref>. Our TMNet interpolates 9 frames, while Zooming Slow-Mo <ref type="bibr" target="#b44">[45]</ref> interpolates 1 frame, between Frames 1 and 3. To illustrate the temporal motion of the videos, we extract a 1D pixel vector over the whole frames from the red line shown in the left figure, and concatenate the 1D pixel vector into a 2D image. We observe that our TMNet <ref type="figure">(Figure 9</ref>, upper right) produces more consistent temporal motion trajectory than Zooming SlowMo <ref type="bibr" target="#b44">[45]</ref>  <ref type="figure">(Figure 9</ref>, lower right), which suffers from clear breaking variations. This demonstrates the superiority of our TMNet on flexible frame interpolation for STVSR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">More Visual Comparisons on STVSR</head><p>On the Vid4 <ref type="bibr" target="#b22">[23]</ref> and Vimeo-90K <ref type="bibr" target="#b34">[35]</ref> datasets, we compare our TMNet with previous one-stage and twostage STVSR methods. For one-stage STVSR methods, we compare our TMNet with Zooming SlowMo <ref type="bibr" target="#b44">[45]</ref> and STARnet <ref type="bibr" target="#b7">[8]</ref>. For the two-stage STVSR methods, we perform video frame interpolation (VFI) by SuperSloMo <ref type="bibr" target="#b14">[15]</ref>, DAIN <ref type="bibr" target="#b0">[1]</ref>, or SepConv <ref type="bibr" target="#b27">[28]</ref>, and perform video superresolution (VSR) by RCAN <ref type="bibr" target="#b46">[47]</ref>, RBPN <ref type="bibr" target="#b6">[7]</ref>, or EDVR <ref type="bibr" target="#b40">[41]</ref>. We set t = 0.5 in our TMNet to generate the frame at the middle moment of any two adjacent frames, which means that the 1-st, 3-rd, 5-th, and 7-th LR frames of each clip in Vimeo-90K are fed into our TMNet to reconstruct the 7 HR frames. All these methods are trained on the Vimeo-90K septuplet dataset <ref type="bibr" target="#b45">[46]</ref>, and evaluated on the Vimeo-90K test set <ref type="bibr" target="#b45">[46]</ref> and the Vid4 <ref type="bibr" target="#b34">[35]</ref> dataset. The visualization results of the comparison result are shown in Figures 10-13.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Training our TMNet in One-step</head><p>Although trained by a two-step scheme, our TMNet can be directly trained with the proposed TMB block, resulting in a one-step training scheme. That is, in this one-step scheme, all the parameters of our main TMNet and the TMB block are optimized simultaneously without pre-training. In our two-step scheme, the two sets of parameters in our main TMNet and the TMB block are optimized separately (first the main TMNet, and then the TMB block). Here, we compare the performance of our TMNet trained with our two-step and the one-step schemes, resulting in two variants called TMNet-two (the original TMNet) and TMNet-one, respectively. Both variants are trained on the Adobe240fps train set <ref type="bibr" target="#b34">[35]</ref> and evaluated on the Adobe240fps test set <ref type="bibr" target="#b34">[35]</ref>. As shown in <ref type="table" target="#tab_10">Table 6</ref>, comparing with our TMNettwo, the variant TMNet-one suffers from a performance drop of 1.84dB in terms of PSNR, on the Adobe240fps test set <ref type="bibr" target="#b34">[35]</ref>. This demonstrates that our TMNet trained in a one-step scheme fail to estimate the motion cues, and interpolate the intermediate frames at an arbitrary moment t ? (0, 1). The main reason is that, in initial training iterations, our TMNet with TMB trained from scratch could not extract useful motion cues from videos, and thus fails to optimize the parameters of our TMB block for meaningful features at an arbitrary moment t.  <ref type="figure">Figure 6</ref>: Main structure of our TMNet. The basic part of "Residual Block" and "Upsample" are illustrated on the right side. n is the number of input frames. H and W denote the height and width of the image or feature map. C in and C out denote the number of input and output channels, respectively.  (2-nd, 4-th, and 6-th columns) on three video clips from the Vimeo-Fast dataset <ref type="bibr" target="#b45">[46]</ref>. We show the intermediate frames between the adjacent two frames according to the temporal hyper-parameter t?{0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9}</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TMNet (Ours)</head><p>Zooming Slow-Mo Clip 0277 of "00006" in Vimeo-Fast <ref type="figure">Figure 9</ref>: Temporal consistency of our TMNet on STVSR. OUr TMNet interpolates 9 frames, while Zooming Slow-Mo <ref type="bibr" target="#b44">[45]</ref> interpolates 1 frame between Frames 1 and 3. We extract a 1D pixel vector over the whole frames from the red line shown in the left figure, and concatenate the 1D pixel vector into a 2D image, which is horizontally scaled to better visualize the temporal consistency of the videos. One can see that our TMNet (upper right) achieves clearly consistent temporal interpolation, while Zooming Slow-Mo <ref type="bibr" target="#b44">[45]</ref> (lower right) suffers from clear breaking variations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Flexible STVSR performance by our TMNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative and quantitative results of different methods on STVSR. The test video clips are from the Adobe240fps<ref type="bibr" target="#b34">[35]</ref> (1-st row), Vimeo-Fast<ref type="bibr" target="#b45">[46]</ref> (2-nd row, left) and Vimeo-Slow<ref type="bibr" target="#b45">[46]</ref> (2-nd row, right) datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Detailed structures of our Locally-temporal Feature Comparison (LFC) module (left) and Temporal Modulation Block (TMB) (right). 2i ? 1, 2i, and 2i + 1 are the indexes of frames. H and W denote the height and width of the image or feature map. C in and C out denote the number of input and output channels, respectively. "0004" in Vimeo-Fast Clip 0474 of "0035" in Vimeo-Fast Clip 0356 of "0006" in Vimeo-Comparison of flexibility on STVSR by our TMNet (1-st, 3-rd, and 5-th columns) and Zooming Slow-Mo<ref type="bibr" target="#b44">[45]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>PSNR results on Adobe240fps test set by different strategies that our TMB modulates the PCD.</figDesc><table><row><cell>Variant</cell><cell>TMB-L1</cell><cell>TMB-L2</cell><cell>TMB-L3</cell><cell>TMNet</cell></row><row><cell>PSNR (dB)</cell><cell>26.92</cell><cell>26.82</cell><cell>26.60</cell><cell>26.95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>PSNR results on Adobe240fps test set by our TMB with linear or nonlinear design. Our TMNet performs a two-stage temporal feature fusion: first local fusion via LFC and then global fusion via GFF. Thus, our TMNet can be called "LFC?GFF". Inverting the order, i.e., GFF?LFC, makes our TMNet collapse during the training. The main reason is that performing GFF before LFC brings noisy long-term information, confusing the learning of deformable convolution in LFC. Thus, we do not evaluate this variant. To study how our LFC contributes to the two-stage fusion in our TMNet, we remove LFC from our TMNet and call this variant "GFF". 90K septuplet dataset and evaluate them on the Vid4<ref type="bibr" target="#b22">[23]</ref>, Vimeo-Fast, Vimeo-Medium, and Vimeo-Slow datasets. The PSNR results are listed in</figDesc><table><row><cell>Variant</cell><cell>TMB-Linear</cell><cell>TMB</cell></row><row><cell>PSNR (dB)</cell><cell>26.93</cell><cell>26.95</cell></row><row><cell cols="3">4. How important is the proposed LFC module to our</cell></row><row><cell>TMNet?</cell><cell></cell><cell></cell></row></table><note>Besides, the features from LFC and GFF can be concate- nated and fused by a convolutional layer, resulting in a vari- ant "LFC+GFF". We train our TMNet and its variants on the Vimeo-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Comparison of PSNR (dB) results by different variants of our TMNet on STVSR datasets.</figDesc><table><row><cell>Variant</cell><cell cols="3">GFF LFC+GFF LFC?GFF</cell></row><row><cell>Vid4 [23]</cell><cell>26.36</cell><cell>26.35</cell><cell>26.43</cell></row><row><cell>Vimeo-Fast</cell><cell>36.87</cell><cell>36.90</cell><cell>37.04</cell></row><row><cell cols="2">Vimeo-Medium 35.45</cell><cell>35.47</cell><cell>35.60</cell></row><row><cell>Vimeo-Slow</cell><cell>33.40</cell><cell>33.43</cell><cell>33.51</cell></row></table><note>5. The benefits of combining the high-quality feature maps F H and the initial feature maps F L for STVSR.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Comparison of PSNR (dB) by our TMNet and its variants on different STVSR datasets.</figDesc><table><row><cell>Variant</cell><cell>Baseline</cell><cell>+F L</cell><cell>TMNet</cell></row><row><cell>Vid4 [23]</cell><cell>26.33</cell><cell>26.36</cell><cell>26.43</cell></row><row><cell>Vimeo-Fast</cell><cell>36.75</cell><cell>36.87</cell><cell>37.04</cell></row><row><cell>Vimeo-Medium</cell><cell>35.35</cell><cell>35.45</cell><cell>35.60</cell></row><row><cell>Vimeo-Slow</cell><cell>33.28</cell><cell>33.40</cell><cell>33.51</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>PSNR results of our TMNet trained in two-step or one-step schemes on Adobe240fps test set<ref type="bibr" target="#b34">[35]</ref>.</figDesc><table><row><cell>Variant</cell><cell>TMNet-one</cell><cell>TMNet-two</cell></row><row><cell>PSNR (dB)</cell><cell>25.11</cell><cell>26.95</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 10</ref><p>: Quantitative and qualitative results of our TMNet and other STVSR methods on Clip "city" in the Vid4 dataset <ref type="bibr" target="#b34">[35]</ref>. For two-stage STVSR methods, we employ SuperSloMo <ref type="bibr" target="#b14">[15]</ref>, SepConv <ref type="bibr" target="#b27">[28]</ref> or DAIN <ref type="bibr" target="#b0">[1]</ref> for VFI and RCAN <ref type="bibr" target="#b46">[47]</ref>, RBPN <ref type="bibr" target="#b6">[7]</ref> or EDVR <ref type="bibr" target="#b40">[41]</ref> for VSR. For one-stage STVSR methods, we compare our TMNet with STARnet <ref type="bibr" target="#b7">[8]</ref> and Zooming Slow-Mo <ref type="bibr" target="#b44">[45]</ref>). The best results on PSNR (dB) and SSIM <ref type="bibr" target="#b42">[43]</ref> are highlighted in bold.  <ref type="figure">Figure 11</ref>: Quantitative and qualitative results of our TMNet and other STVSR methods on Clip 0200 of "00026" in Vimeo-Fast <ref type="bibr" target="#b45">[46]</ref>. For two-stage STVSR methods, we employ SuperSloMo <ref type="bibr" target="#b14">[15]</ref>, SepConv <ref type="bibr" target="#b27">[28]</ref> or DAIN <ref type="bibr" target="#b0">[1]</ref> for VFI and RCAN <ref type="bibr" target="#b46">[47]</ref>, RBPN <ref type="bibr" target="#b6">[7]</ref> or EDVR <ref type="bibr" target="#b40">[41]</ref> for VSR. For one-stage STVSR methods, we compare our TMNet with STARnet <ref type="bibr" target="#b7">[8]</ref> and Zooming Slow-Mo <ref type="bibr" target="#b44">[45]</ref>). The best results on PSNR (dB) and SSIM <ref type="bibr" target="#b42">[43]</ref> are highlighted in bold.  <ref type="figure">Figure 12</ref>: Quantitative and qualitative results of our TMNet and other STVSR methods on Clip 0723 of "00085" in Vimeo-Medium <ref type="bibr" target="#b45">[46]</ref>. For two-stage STVSR methods, we employ SuperSloMo <ref type="bibr" target="#b14">[15]</ref>, SepConv <ref type="bibr" target="#b27">[28]</ref> or DAIN <ref type="bibr" target="#b0">[1]</ref> for VFI and RCAN <ref type="bibr" target="#b46">[47]</ref>, RBPN <ref type="bibr" target="#b6">[7]</ref> or EDVR <ref type="bibr" target="#b40">[41]</ref> for VSR. For one-stage STVSR methods, we compare our TMNet with STARnet <ref type="bibr" target="#b7">[8]</ref> and Zooming Slow-Mo <ref type="bibr" target="#b44">[45]</ref>). The best results on PSNR (dB) and SSIM <ref type="bibr" target="#b42">[43]</ref> are highlighted in bold.  <ref type="figure">Figure 13</ref>: Quantitative and qualitative results of our TMNet and other STVSR methods on Clip 0679 of "00084" in Vimeo-Slow <ref type="bibr" target="#b45">[46]</ref>. For two-stage STVSR methods, we employ SuperSloMo <ref type="bibr" target="#b14">[15]</ref>, SepConv <ref type="bibr" target="#b27">[28]</ref> or DAIN <ref type="bibr" target="#b0">[1]</ref> for VFI and RCAN <ref type="bibr" target="#b46">[47]</ref>, RBPN <ref type="bibr" target="#b6">[7]</ref> or EDVR <ref type="bibr" target="#b40">[41]</ref> for VSR. For one-stage STVSR methods, we compare our TMNet with STARnet <ref type="bibr" target="#b7">[8]</ref> and Zooming Slow-Mo <ref type="bibr" target="#b44">[45]</ref>). The best results on PSNR (dB) and SSIM <ref type="bibr" target="#b42">[43]</ref> are highlighted in bold.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Depth-aware video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Memc-net: Motion estimation and motion compensation driven neural network for video interpolation and enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Realtime video super-resolution with spatio-temporal networks and motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4778" to="4787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Stochastic relaxation, gibbs distributions, and the bayesian restoration of images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Geman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="721" to="741" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recurrent back-projection network for video superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norimichi</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Space-time-aware multi-resolution video enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norimichi</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Modulating image restoration with continual levels via adaptive feature modification layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwen</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11056" to="11064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Multi-dimension modulation for image restoration with dynamic controllable residual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwen</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.05293</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Jittor: a novel deep learning framework with meta-operators and unified graph execution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi-Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dun</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Ye</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science China Information Sciences</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Super slomo: High quality estimation of multiple intermediate frames for video interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep video super-resolution network using dynamic upsampling filters without explicit motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Younghyun</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeyeon</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3224" to="3232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fisr: Deep joint frame interpolation and super-resolution with a multiscale temporal loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihyong</forename><surname>Soo Ye Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munchurl</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="11278" to="11286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int. Conf. Learn. Represent</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="624" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adacof: Adaptive collaboration of flows for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeongmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeoh</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Young</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daehyun</forename><surname>Pak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuseok</forename><surname>Ban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangyoun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5316" to="5325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Space-time super-resolution with patch group cuts prior. Signal Processing: Image Communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohai</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhi</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="147" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog. Worksh</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A bayesian approach to adaptive video super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Space-time super-resolution using graph-cut optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uma</forename><surname>Mudenagudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashis</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prem</forename><surname>Kumar Kalra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="995" to="1008" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Context-aware synthesis for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1701" to="1710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Softmax splatting for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5437" to="5446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Video frame interpolation via adaptive separable convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<editor>Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala</editor>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Space-time super-resolution from a single video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oded</forename><surname>Shahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Faktor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Increasing space-time resolution in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="753" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Space-time super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="531" to="545" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhourong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai-Kin</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chun</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep video deblurring for hand-held cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuochen</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauricio</forename><surname>Delbracio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Neural dynamics of the attentional blink revealed by encoding orientation selectivity during rapid visual presentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Ford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arabzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Enns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Troy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">B</forename><surname>Visser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mattingley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Tdan: Temporally-deformable alignment network for video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3360" to="3369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multiframe image restoration and registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Computer Vision and Image Processing</title>
		<imprint>
			<date type="published" when="1984" />
			<biblScope unit="page" from="317" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Digital Video Broadcasting (DVB); Specification for the use of Video and Audio Coding in Broadcasting Applications based on the MPEG-2 Transport Stream. ETSI</title>
		<idno>ETSI TS 101 154 V2.3.1</idno>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Cfsnet: Toward a controllable feature space for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiming</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4140" to="4149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Edvr: Video restoration with enhanced deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Kelvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog. Worksh</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep network interpolation for continuous imagery effect transition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1692" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero P</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Jcs: An explainable covid-19 diagnosis system by joint classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Huan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shang-Hua</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong-Guo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3113" to="3126" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Zooming slow-mo: Fast and accurate one-stage space-time video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">P</forename><surname>Allebach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020-06-01" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Video enhancement with task-oriented flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
