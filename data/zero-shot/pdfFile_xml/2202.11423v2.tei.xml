<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Delving Deep into One-Shot Skeleton-based Action Recognition with Diverse Occlusions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunyu</forename><surname>Peng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Roitberg</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
						</author>
						<title level="a" type="main">Delving Deep into One-Shot Skeleton-based Action Recognition with Diverse Occlusions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Occlusions are universal disruptions constantly present in the real world. Especially for sparse representations, such as human skeletons, a few occluded points might destroy the geometrical and temporal continuity critically affecting the results. Yet, the research of data-scarce recognition from skeleton sequences, such as one-shot action recognition, does not explicitly consider occlusions despite their everyday pervasiveness.</p><p>In this work, we explicitly tackle body occlusions for Skeletonbased One-shot Action Recognition (SOAR). We mainly consider two occlusion variants: 1) random occlusions and 2) more realistic occlusions caused by diverse everyday objects, which we generate by projecting the existing IKEA 3D furniture models into the camera coordinate system of the 3D skeletons with different geometric parameters, (e.g., rotation and displacement). We leverage the proposed pipeline to blend out portions of skeleton sequences of the three popular action recognition datasets (NTU-120, NTU-60 and Toyota Smart Home) and formalize the first benchmark for SOAR from partially occluded body poses. This is the first benchmark which considers occlusions for datascarce action recognition. Another key property of our benchmark are the more realistic occlusions generated by everyday objects, as even in standard recognition from 3D skeletons, only randomly missing joints were considered. We re-evaluate existing state-of-the-art frameworks for SOAR in the light of this new task and further introduce Trans4SOAR -a new transformer-based model which leverages three data streams and mixed attention fusion mechanism to alleviate the adverse effects caused by occlusions. While our experiments demonstrate a clear decline in accuracy with missing skeleton portions, this effect is smaller with Trans4SOAR, which outperforms other architectures on all datasets. Although we specifically focus on occlusions, Trans4SOAR additionally yields state-of-the-art in the standard SOAR without occlusion, surpassing the best published approach by 2.85% on NTU-120.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>B ENEFITING from the rapid progress of deep learning, conventional architectures for skeleton-based action recognition achieved remarkable results on a variety of benchmarks for body-pose based classification, e.g., NTU-120 <ref type="bibr" target="#b6">[2]</ref> and Toyota Smart Home <ref type="bibr" target="#b7">[3]</ref>. However, most of the previous works regard relatively clean datasets as the default starting point for training models <ref type="bibr" target="#b6">[2]</ref>, <ref type="bibr" target="#b8">[4]</ref>, <ref type="bibr" target="#b9">[5]</ref>, <ref type="bibr" target="#b10">[6]</ref>. Such datasets often explicitly ensure good body visibility through a suitable camera placement, but this assumption is rather naive, as in real-life the input is often disrupted through occlusions. <ref type="figure">Fig. 1</ref>. An overview of the two proposed and reformulated occlusion scenarios, i.e., REalistic synthesized occlusion (RE) proposed by us as depicted in <ref type="bibr" target="#b5">(1)</ref> and RAndom occlusion (RA) reformulated by us depicted in <ref type="bibr" target="#b6">(2)</ref>. In order to generate realistic occlusion, we randomly choose a 3D furniture model from PIX3D dataset <ref type="bibr" target="#b5">[1]</ref> and insert it into the world coordinate of the 3D pose using random translation and rotation. The occluded region of the skeletons are determined through camera view point projection, where the whole procedure is demonstrated in <ref type="bibr" target="#b7">(3)</ref>. In this work we investigate the influence brought by these two dominant occlusions for SOAR, as <ref type="bibr" target="#b8">(4)</ref>.</p><p>Skeleton-based action recognition algorithms operate on the coordinates of the 3D body joints and have attracted a lot of attention <ref type="bibr" target="#b8">[4]</ref>, <ref type="bibr" target="#b9">[5]</ref>, <ref type="bibr" target="#b10">[6]</ref>, <ref type="bibr" target="#b11">[7]</ref>, <ref type="bibr" target="#b12">[8]</ref>, <ref type="bibr" target="#b13">[9]</ref>, <ref type="bibr" target="#b14">[10]</ref>, <ref type="bibr" target="#b15">[11]</ref> due to the increasing precision of depth sensors and their privacy-preserving characteristics, but occlusions are especially damaging for such sparse representations, where a few missing joints have a substantial impact on the geometric and temporal continuity.</p><p>Learning new concepts with only few labelled examples is often posed in the form of one-or few-shot recognition problem <ref type="bibr" target="#b16">[12]</ref>, <ref type="bibr" target="#b17">[13]</ref>, <ref type="bibr" target="#b18">[14]</ref>, <ref type="bibr" target="#b19">[15]</ref>, <ref type="bibr" target="#b20">[16]</ref> and still remains one of the key problems in human action recognition. Especially if only few training examples are available, occlusions are critical since no diverse data is available for new categories and the quality of the few provided samples dominate the final results.</p><p>In this paper, we are interested in categorizing sequences of unseen 3D body poses from only one reference sample, where portions of these sequences are missing due to occlusions. Since no past work on one-shot action recognition from skeleton data explicitly considers occlusions, we first introduce a new benchmark by blending out skeleton parts in three arXiv:2202.11423v2 [cs.CV] 13 Jul 2022 established action recognition datasets. Our idea is to use a library <ref type="bibr" target="#b5">[1]</ref> of 3D objects to generate REalistic occlusions (RE), which we project onto the original data with different geometric parameters, such as rotation and displacement. We believe, that by projecting everyday objects (as shown in <ref type="figure">Figure 1</ref>), the occlusions are far more realistic than random dropping of data points, which has been considered in standard <ref type="bibr" target="#b21">[17]</ref>, <ref type="bibr" target="#b22">[18]</ref>, (i.e., not data scarce) recognition from 3D skeletons in the past while preserving the dimension of skeleton joints compared with <ref type="bibr" target="#b23">[19]</ref>. Still, we also consider random blending of body joints while considering both spatial and temporal information as a second occlusion variant, i.e., RAndom occlusion (RA), in our benchmark.</p><p>We also introduce Trans4SOAR -a new transformer-based model which comprises three data streams and mixed fusion to overcome the challenges caused by occlusions. Until now, Skeleton-based One-shot Action Recognition (SOAR) has been mostly addressed with Convolutional Neural Networks (CNNs) coupled with metric learning <ref type="bibr" target="#b24">[20]</ref>, <ref type="bibr" target="#b25">[21]</ref>, <ref type="bibr" target="#b26">[22]</ref> or meta learning <ref type="bibr" target="#b27">[23]</ref>. While few recent works considered transformers networks in conventional video-based human activity classification <ref type="bibr" target="#b28">[24]</ref>, their potential as signal encoders of body movement, their transfer capabilities to new data-scarce activities classes and their ability to deal with body pose occlusions have been overlooked. For example, Skeleton-DML and SL-DML, the state-of-the-art approaches for the SOAR task <ref type="bibr" target="#b24">[20]</ref>, <ref type="bibr" target="#b25">[21]</ref>, both leverage a CNN-based encoder for signal-level skeleton representation and the deep metric learning paradigm. Our experimental strategy is therefore to first re-evaluate Skeleton-DML <ref type="bibr" target="#b24">[20]</ref> and SL-DML <ref type="bibr" target="#b25">[21]</ref> as the current state-of-theart approaches in the light of our new occlusion-centered task. Then, we revisit image-like modelling of skeleton dynamics with the rapidly emerging visual transformers within our Trans4SOAR model. Apart from being the first visual transformer-based architecture for encoding skeleton signals as image-like representations targeting at the SOAR task, we alleviate the adverse effects caused by occlusions through two novel design choices. First, we leverage complementary types of information (velocities, bones and joints) and propose the Mixed Attention Fusion Mechanism (MAFM) which learns how to link the information from diverse streams at the patch embedding level while considering the spatial and temporal neighborhood information. Secondly, we leverage the Latent Space Consistency (LSC) loss encouraging the model to output similar results with an additional auxiliary branch, if the embedding in the middle layer of the auxiliary branch has been altered by category agnostic prototypes, which suits naturally to the use-case of disturbances through occlusions. This paper explicitly explores occlusions for SOAR and makes the following contributions:</p><p>? We for the first time tackle occlusions for Skeletonbased One-Shot Action Recognition (SOAR) and build a benchmark for this task by augmenting three established datasets for action recognition through our occlusion pipeline. Our pipeline considers different occlusion settings, where RAndom occlusion (RA) and REalistically synthesised occlusion (RE) based on everyday objects are the most important ones. We view the latter case as a more practical scenario closer to real-life applications and achieve it by using a library of 3D objects obtained from the IKEA 3D furniture dataset <ref type="bibr" target="#b5">[1]</ref>, which are inserted into the world coordinate system of the body poses with different geometric parameters. ? We introduce TRANS4SOAR -an new three-stream transformer-based model specifically aimed at overcoming data occlusions by 1) leveraging diverse types of input (velocities, bones and joints) and enabling information exchange at the patch embedding level via the Mixed Attention Fusion Mechanism (MAFM) and 2) extensively augmenting the intermediate transformer representations through iteratively estimated category-specific prototypes and the Latent Space Consistency (LSC) loss. ? We conduct in-depth experiments in the SOAR task, evaluating two existing state-of-the art frameworks <ref type="bibr" target="#b24">[20]</ref>, <ref type="bibr" target="#b25">[21]</ref> as well as our TRANS4SOAR network and its individual building blocks under four different types of occlusions. Unsurprisingly, introducing occlusions adversely impacts the outcome, marking an important future work direction. Our TRANS4SOAR model yields state-of-the-art on all three datasets under the presence of occlusions. ? As a side-observation, we discover that TRANS4SOAR also outperforms state-of-the-art in the standard SOAR, (i.e., the not occluded SOAR task), surpassing the best previously published model on the challenging NTU-120 SOAR benchmark by &gt; 2.8%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. One-shot Action Recognition</head><p>One-shot recognition, belonged to data-scarce representation learning paradigm, aims at the recognition of unseen category with only one reference samples given as guidance. Compared with existing works in one-shot image classification, where meta learning-based approaches <ref type="bibr" target="#b27">[23]</ref>, <ref type="bibr" target="#b30">[26]</ref>, <ref type="bibr" target="#b31">[27]</ref>, <ref type="bibr" target="#b32">[28]</ref>, <ref type="bibr" target="#b33">[29]</ref>, <ref type="bibr" target="#b34">[30]</ref> dominate the important positions by reinitializing a new task set every epoch following the paradigm regarding learning to learn, Deep Metric Learning (DML) based approaches <ref type="bibr" target="#b24">[20]</ref>, <ref type="bibr" target="#b25">[21]</ref>, <ref type="bibr" target="#b26">[22]</ref>, which aims at achieving highly discriminative representation and closer representation distance for inter-and intra-category samples in the latent space, are well utilized for Skeleton-based One-shot Action Recognition (SOAR) benchmarked by NTU-120 <ref type="bibr" target="#b6">[2]</ref> with predefined reference frames. One-shot action recognition has been well studied for several down-stream tasks, e.g., semantic segmentation <ref type="bibr" target="#b35">[31]</ref> and video classification <ref type="bibr" target="#b16">[12]</ref>, <ref type="bibr" target="#b18">[14]</ref>, <ref type="bibr" target="#b19">[15]</ref>, <ref type="bibr" target="#b36">[32]</ref>, however the research of SOAR are much more sparse and mostly benchmarked on the NTU-120 dataset <ref type="bibr" target="#b6">[2]</ref>, <ref type="bibr" target="#b24">[20]</ref>, <ref type="bibr" target="#b37">[33]</ref>, <ref type="bibr" target="#b38">[34]</ref>, <ref type="bibr" target="#b39">[35]</ref>. State-of-the-art recognition results are currently reached by the approaches of Memmesheimer et al. <ref type="bibr" target="#b25">[21]</ref>, <ref type="bibr" target="#b24">[20]</ref>, which use a CNN-based encoding of 3D skeletons represented as images and optimizes the framework with deep metric learning using a mixture of cross entropy and triplet margin losses. In this work, we investigate the transformer architecture and propose a new model named as TRANS4SOAR for the SOAR task while considering different occlusion scenarios. We build our optimization paradigm based on SL-DML [20] <ref type="figure" target="#fig_2">Fig. 2</ref>. An overview of the proposed TRANS4SOAR architecture, which is a Transformer for Skeleton-based One-Shot Action Recognition. First the skeleton signals are encoded in three kinds of format, i.e., joints, bones and velocities. Image-like representations are formulated through the concatenation along the temporal axis of the skeleton data, which are further divided into several patches and fed into its corresponding patch embedding net. Then, Mixed Attention Fusion Mechanism (MAFM) fuses the embeddings from these three different streams by using Mixed Fusion (MF) to achieve the cross-stream aggregation on Key, Query and Value together with the proposed SCA operation. An Latent Space Consistency (LSC) loss is also leveraged in our TRANS4SOAR by integrating an prototype augmented auxiliary branch and trying to use cosine similarity loss to force the embeddings from the main branch and the embeddings from the auxiliary branch more similar. Three losses, i.e., triplet margin loss, cross entropy loss and LSC loss, are leveraged for discriminative representation learning. The basic transformer attention block is depicted on the left hand side proposed by LeViT <ref type="bibr" target="#b29">[25]</ref>, which builds up the transformer block in the later stage of our TRANS4SOAR architecture through stacking. while further proposing a novel patch embedding level fusion approach considering different skeleton encoding formats.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Visual Transformers</head><p>Transformer networks <ref type="bibr" target="#b40">[36]</ref> are rapidly gaining popularity in computer vision since their operationalization on image patched within the ViT <ref type="bibr" target="#b41">[37]</ref> and DeiT <ref type="bibr" target="#b42">[38]</ref> architectures. Recently, transformer models, known for capturing essential long-range context <ref type="bibr" target="#b40">[36]</ref>, become increasingly appealing in vision tasks since ViT <ref type="bibr" target="#b41">[37]</ref> and DeiT <ref type="bibr" target="#b42">[38]</ref> directly utilize a pure transformer on image patches. A large amount of transformerbased models are thereby put forward regarding this new trend, while some of them target at pursuing better accuracy on image classification task <ref type="bibr" target="#b43">[39]</ref>, <ref type="bibr" target="#b44">[40]</ref>, <ref type="bibr" target="#b45">[41]</ref>, resource-efficiency <ref type="bibr" target="#b29">[25]</ref>, <ref type="bibr" target="#b46">[42]</ref> and the others are designed for more specific tasks, e.g., semantic segmentation <ref type="bibr" target="#b35">[31]</ref>. In action recognition task, the transformer-based models could be arranged into two main groups, which are video-based transformer <ref type="bibr" target="#b28">[24]</ref>, <ref type="bibr" target="#b47">[43]</ref>, <ref type="bibr" target="#b48">[44]</ref>, <ref type="bibr" target="#b49">[45]</ref> and skeleton-based transformer <ref type="bibr" target="#b11">[7]</ref>, <ref type="bibr" target="#b13">[9]</ref>, <ref type="bibr" target="#b14">[10]</ref>, <ref type="bibr" target="#b15">[11]</ref>, <ref type="bibr" target="#b50">[46]</ref>, <ref type="bibr" target="#b8">[4]</ref> using standard sequential skeleton as input. We for the first time investigate visual transformer for skeleton data by encoding the skeleton as image-like representation which has the same encoding procedure with SL-DML <ref type="bibr" target="#b25">[21]</ref> while using an additional auxiliary branch and loss to achieve latent space consistency. Furthermore, in order to achieve high robustness against different occlusion scenarios, a novel feature extraction architecture TRANS4SOAR is proposed by integrating a Mixed Attention Fusion Mechanism (MAFM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Skeleton-based Action Recognition with Occlusion</head><p>Most of the skeleton extraction approaches, e.g., Alpha-Pose <ref type="bibr" target="#b51">[47]</ref>, <ref type="bibr" target="#b52">[48]</ref>, <ref type="bibr" target="#b53">[49]</ref>, tend to directly give zero output regarding the occluded human body joints. Thereby some researchers formulated the occluded action recognition task through randomly setting different body regions per frame as zeros to simulate spatial occlusion or setting the randomly selected frame as zeros to simulate temporal occlusion <ref type="bibr" target="#b21">[17]</ref>, <ref type="bibr" target="#b22">[18]</ref>, <ref type="bibr" target="#b54">[50]</ref>, <ref type="bibr" target="#b55">[51]</ref>, <ref type="bibr" target="#b56">[52]</ref>, <ref type="bibr" target="#b57">[53]</ref> while self-occlusion caused by human body movement is considered in <ref type="bibr" target="#b58">[54]</ref>. Notice that, all the aforementioned related works are for skeleton-based action recognition, which is not for SOAR, tackled by our work. In this work, we jointly consider both spatial occlusion and temporal occlusion at the same time to form random occlusion which is a more reasonable randomly generated occlusion setting since temporal and spatial occlusion should be considered together if the whole skeleton sequence is seen as a sample for SOAR. Besides, for realistic synthesized occlusion, OSD dataset <ref type="bibr" target="#b23">[19]</ref> for the first time projected 3D real world objects into image plane and then generated occluded skeleton in 2D image coordinate. The dimension of raw data is unfortunately not preserved which results in massive information loss by converting 3D data into 2D data. In order to tackle the occlusion issue in more realistic scenario while preserving dimension of the data, we thereby propose a dimension preserving realistic synthesized occlusion pipeline using an additional 3D model dataset PIX3D <ref type="bibr" target="#b5">[1]</ref>. Moreover, we for the first time investigate SOAR under diverse occlusions while all the existing works target not-occluded one-shot action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. BENCHMARK</head><p>To address the lack of related benchmarks, we collect and publicly release the first testbed for SOAR under presence of occlusions. Our benchmark augments three prominent datasets for SOAR with different occlusions, of which random spatiotemporal occlusions and the realistically synthesised occlusions derived from an existing 3D library of furniture objects being the most important ones. Next, we give a formal definition of the addressed task (Section III-A) and describe the data obstruction mechanism that we have developed to reach our design goal of realistic everyday occlusions (Section III-B) as well as the more conventional random occlusion pipeline (Section III-C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem Definition</head><p>The task we address is SOAR <ref type="bibr" target="#b24">[20]</ref> where a priori knowledge acquired from data-rich action classes is transferred to <ref type="figure">Fig. 3</ref>. An overview of Signal-to-Noise Ratio (SNR) distribution of the realistic synthesized occlusion dataset, where (a), (b) and (c) are for the NTU-120 <ref type="bibr" target="#b6">[2]</ref>, the NTU-60 <ref type="bibr" target="#b59">[55]</ref> and the Toyota Smart Home <ref type="bibr" target="#b7">[3]</ref> respectively. The legend indicates the corresponding SNR range.</p><p>categorize new data-scarce classes, while certain regions of the skeleton are not visible. Following the standard evaluation protocol for data-scarce action recognition, we build on the one-shot evaluation setting of NTU-120 <ref type="bibr" target="#b6">[2]</ref>, where new categories of the body pose sequences are categorized from a single reference sample. Formally, C base denotes the set of |C base | data-rich categories available during training through large amount of labelled data</p><formula xml:id="formula_0">D base = {(S i , l i )} U i=1 , l i ? C base while U indicates the number of samples in D base .</formula><p>Our goal is to distinguish the |C novel | new activity classes C novel , for which only ? = 1 reference training example is available for each class. These data-scarce examples are referred to as support</p><formula xml:id="formula_1">set D supp = {S i } O i=1</formula><p>while O indicates the number of samples in D supp and C base ?C novel = / 0. The final task is then to assign a category l n ? C novel to each sample from the test set D test containing examples from the data-scarce categories C novel .</p><p>Since our idea is to study and address occlusions for skeleton-based recognition with little training data, we augment both, the support set D supp and the test set D test with our pipeline described in the upcoming sections. Note, that in our experiments, we consider both: 1) obstructing the reference examples from the support set and the test set examples and 2) considering occlusions in the test set only, while using complete sequences as our reference samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Realistic Synthesized Occlusion</head><p>Past work on skeleton-based activity recognition (without the data scarcity constraint) considered occlusions as random temporal or spatial obstructions only <ref type="bibr" target="#b21">[17]</ref>, achieved by randomly setting a fixed number of frames or a fixed number of joints to zero respectively. Such occlusions have highly controllable characteristics but are rather unusual in the real world, where objects are a common cause for obstructions and missing skeleton points exhibit specific geometric consistencies. To tackle this issue, we build new occluded versions of three public datasets, i.e., NTU-120 <ref type="bibr" target="#b6">[2]</ref>, NTU-60 <ref type="bibr" target="#b59">[55]</ref> and Toyota Smart Home <ref type="bibr" target="#b7">[3]</ref>, by inserting the 3D IKEA furniture models obtained from the PIX3D dataset <ref type="bibr" target="#b5">[1]</ref> into the world coordinates system of the human body. Note, that while NTU-120 <ref type="bibr" target="#b6">[2]</ref> and NTU-60 <ref type="bibr" target="#b59">[55]</ref> contain 3D skeletons, while Toyota Smart Home <ref type="bibr" target="#b7">[3]</ref> covers 2D data, the process of augmenting the data with the realistic synthesized occlusions is different and will be explained in the following sections. The statistics of Signal-to-Noise Ratio (SNR) for each dataset with the proposed realistic synthesized occlusion is depicted in <ref type="figure">Figure 3</ref>. 3D realistic synthesized occlusion dataset generation. The NTU-120 <ref type="bibr" target="#b6">[2]</ref> and NTU-60 <ref type="bibr" target="#b59">[55]</ref> datasets cover different camera views. We therefore need to consider the cross-view consistency when obstructing the body poses with the furniture models. Unfortunately, the datasets do not provide the calibration data of the individual cameras, which would be the first essential piece of information when addressing this problem. Luckily, our goal is the skeleton data and each frame containing a skeleton provides sufficient world coordinate information about joints, which can be directly used to calibrate the relative position of the different cameras. A single skeleton sequence sample with T frames and J joints gives us a set of coordinates with number of T ? J. If N samples are provided in our dataset, the known number of the world coordinates thereby rises to N * T * J which is much higher than the rank of projection matrix between two different cameras. The calibration matrix F i j between two utilized cameras i and j can be estimated through the following equation:</p><formula xml:id="formula_2">F i j = (X T i X) ?1 i X T i X j<label>(1)</label></formula><p>where X denotes the collection of human body joints with homogeneous coordinate format captured simultaneously by two different cameras. Thereby, the projection matrix between every two cameras can be obtained as F i j ? F. A detailed description of the 3D occlusion generation procedure is formalized in Alg. 1. First, we randomly select 3D object model from the existing IKEA furniture library <ref type="bibr" target="#b5">[1]</ref> containing 395 different models from 9 categories and augment it via random rotation and translation regarding the vertical axis (bottom to up) and the horizontal plane respectively to simulate life-like occlusions while trying to ensure the bottom points of the skeleton and furniture are in the same vertical height level through vertical translation. To ensure the cross-view consistency, samples collected simultaneously by different cameras share the rotation and translation augmented furniture model by using the projection matrix from the calibration set F. Next, we need to determine which skeleton body joints are blended out by the current occluded object from the perspective of the camera. The skeleton body joints</p><formula xml:id="formula_3">x = [x 1 , x 2 , x 3 ] (where x 3</formula><p>indicates the axis which is parallel to the focus axis of the camera) and the points of the augmented furniture model z are first horizontally projected along the focus axis of camera into x and z , as</p><formula xml:id="formula_4">x = [x 1 /x 3 , x 2 /x 3 ] and z = [z 1 /z 3 , z 2 /z 3 ].</formula><p>Then, we build up a two-dimensional convex hull based on the projected points regarding z and use the following equation to determine if the human body joints fall into the occlusion convex hull from the camera point of view or not:</p><formula xml:id="formula_5">IsInHull = IsTrue(A ? (P T ) ? Tile(?b, (1, len(P))), 0), (2)</formula><p>where A denotes the construction equation of the 2D convex hull of z , b denotes the last convex hull equation, P denotes the point that needs to be determined, Tile indicates whether all array elements along a given axis are able to be evaluated as True or not and len(?) is the coordinate point dimension. Thereby the IsInHull is a binary indicator, while True marks the point being inside the convex hull and vice versa. Finally Algorithm 1 3D realistic synthesized occlusion generation.</p><p>Input: F -the set of projection matrix for each camera pair; Xthe set of skeleton data; Z -the collection of 3D furniture models from PIX3D dataset; R and T -random rotation and translation augmentations; X Occ -a empty set for occluded skeleton data; [a, b] -predefined occluded SNR range for the acceptance, where a is the lower limitation and b is the upper limitation. <ref type="bibr">1:</ref> for all x ? X: do 2:</p><p>Set Accept = False.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3:</head><p>while Accept! = True do 4:</p><p>Set Found = False.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Set N Occ = 0. <ref type="bibr">6:</ref> {N Occ is the occluded sample number for X d+1 .} <ref type="bibr">7:</ref> while Found! = True do 8:</p><p>Search X d collected simultaneously with x from different views <ref type="bibr">9:</ref> Extract the calibration set F d for X d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>10:</head><p>Randomly select z, where z ? Z.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11:</head><p>Obtain augmented z, i.e., z , by z = R(T (z)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>Get Z d by applying f d ? F d on z.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>13:</head><p>Define Z d+1 = Z d ? {z} and X d+1 = X d ? {x} <ref type="bibr">14:</ref> if Z d+1 has no intersection with X d+1 for each corresponding element: then </p><formula xml:id="formula_6">for (x d , z d ) ? zip(X d+1 , Z d+1 ) do 19:</formula><p>Horizontally project z d and x d along focus axis of camera d into 2D plane as z d and x d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>20:</head><p>Build up 2D convex hull ? of z d . <ref type="bibr" target="#b25">21</ref>:</p><formula xml:id="formula_7">Mask d = IsInHull(?, x d ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>22:</head><p>Calculate SNR d = Sum(Mask d )/len(Mask d ) for x d . <ref type="bibr" target="#b27">23</ref>: Set Accept = True.</p><formula xml:id="formula_8">Occlude x d by x d [Mask d ] = zeros_like(x d [Mask d ]) 24: Append x d into X Occ d 25: if SNR d in [a, b]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>33:</head><p>Del X d from X and append the X Occ d into X Occ . <ref type="bibr">34:</ref> end if <ref type="bibr">35:</ref> end while 36: end for we will get a Mask for each queried skeleton data (see Alg. 1), and the occluded 3D points are set to zero. 2D realistic synthesized occlusion dataset generation. Since the third dataset we leverage, i.e., Toyota Smart Home <ref type="bibr" target="#b7">[3]</ref>, only contains 2D skeletons in the image plane, the aforementioned pipeline for generating 3D realistic synthesized occlusion is modified to suit the 2D use-case. Instead of directly augmenting the 3D furniture model via rotation and translation, we use a randomly generated projection matrix, which transforms the points from the camera coordinates to the image coordinate system, to project the furniture model onto the image plane. Then, similar to the 3D realistic synthesized occlusion generation procedure, the convex hull of the occluded region is built up according to the projected points of the furniture model on the 2D image plane and an occlusion-aware mask is obtained through the ISInHull function. Finally, the corresponding 2D skeleton joints within the mask are changed to zeros.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Random Occlusion</head><p>The second occlusion variant we considered is random data obstruction, which is a combination of random temporal and spatial occlusions used in past work on standard, (i.e., without the data scarcity constraint) skeleton-based action recognition <ref type="bibr" target="#b21">[17]</ref>, <ref type="bibr" target="#b22">[18]</ref>. For random temporal occlusions, a fixed number of frames are blended out randomly for each skeleton sequence to simulate full occlusion for certain points in time. For random spatial occlusions, a fixed number of joints are randomly set to zero in every frame of the skeleton data stream. This is a very specialized type of occlusions, since the exact number of joints are not visible in all frames. However, mixing both, random temporal occlusions and random spatial occlusions, is a more reasonable scenario with less predefined controllable conditions and a higher chance to happen in real-life. With x ? R T ?J?B being the skeleton data, we first flatten it into R (T ?J)?B , after which a set of data points ? ? (T ? J) are randomly chosen with a predefined SNR ratio ? and blended out. Although we view the mixed spatial and temporal occlusion as a more reasonable option, we also conduct experiments with isolated random spatial and temporal occlusions for consistency. Overall, our experiments described in the later sections will indicate, that the realistic synthesized occlusions are the most challenging ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. METHODS: TRANS4SOAR</head><p>We introduce Trans4SOAR -a three-stream transformerbased model designed to overcome adverse effects of occlusions (an overview is provided in <ref type="figure" target="#fig_2">Fig. 2</ref>). The key ingredients of our model are 1) the Mixed Attention Fusion Mechanism (MAFM) which learns to aggregate three different types of skeleton information, (i.e., joints, velocities and bones) at the patch embedding level and 2) an extensive data augmentation technique at the feature-level, where an auxiliary branch is augmented by category agnostic prototypes. The motivation of the latter component is to encourage better robustness against imperfect data brought by occlusions through an additional consistency cost computed between the obtained body pose embedding and its prototype-augmented version.</p><p>Next, we describe the basic components regarding the input encoding, the patch embedding procedure and the basic transformer blocks of TRANS4SOAR (Sec. IV-A) and MAFM -the central building block model responsible for the threestream fusion at the patch embedding level (Sec. IV-B). Finally we introduce our auxiliary Latent Space Consistency (LSC) loss for encouraging invariance to transformations through consistency constraints and augmentations with previously learned action category prototypes (Sec. IV-C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Illustration of the Base Components</head><p>Input encoding. We follow the skeleton encoding for body joints proposed by SL-DML <ref type="bibr" target="#b25">[21]</ref> to cast the sequential skeleton data as image-like representations. Assuming that s?R T ?J?B denotes a sequential skeleton sample, where T indicates the temporal length, J indicates the total number of joints, and B indicates the dimension of the coordinates of the skeleton joints. The input of our approach is achieved by interpolation, which transfers s from T ?J?B to H?W ?B to match the image-wise input. Moreover, TRANS4SOAR is a threestream architecture, which does not only consider joints (as in <ref type="bibr" target="#b25">[21]</ref>) but also bones b and velocities v defined as v t =s t ?s t?1 , denoting the velocity for each joint during human body motion at time stamp t, and b i, j =s i ?s j , f or (i, j) ? ? bones , denoting the vector of each bone of the human skeleton, respectively. These vectors are subsequently mapped to image-like arrays using the described above procedure.</p><p>Finally, after the skeleton format encoding and image-wise transformation, we obtain three individually image-wise inputs including joints, velocities, and bones which have the same shape as H?W ?B. So at the end of input encoding we have three streams of input, i.e., joints, velocities and bones. Patch embedding and transformer blocks. Modern CNNs are excellent at preserving details, while transformers are known for capturing long-range dependencies, making the combination of CNN-and transformer blocks a potential double win. LeViT <ref type="bibr" target="#b29">[25]</ref> opened the door for this combination by using a CNN with four layers for patch embedding, before the stack of transformer blocks. Standing on the shoulders of giants, we adopt the basic transformer blocks and patch embedding layers proposed by LeViT <ref type="bibr" target="#b29">[25]</ref> in TRANS4SOAR. The basic transformer attention block of LeViT <ref type="bibr" target="#b29">[25]</ref> is depicted on the left hand side of <ref type="figure" target="#fig_2">Figure 2</ref>. After the acquisition of the Query Q, Key K and Value V through the projection layers Proj Q , Proj K , and Proj V respectively, the final attended output can be calculated as Proj Top (HardWish((So f tmax(Q?K))+ Bias att ) ? V), where each projection layer is composed of an 1x1 conv and a batch normalization layer, and Bias att denotes the attention bias. First, the leveraged three streams of inputs namely joints, velocities, and bones are separately divided into N patch =(H/P)?(W /P) patches with a predefined path size P and thereby three input sequences are produced, which are denoted as S j , S v , and S b for joints, velocities, and bones, respectively. Then, we build up patch embedding layers through a stack of convolutional neural networks, denoted as M ? j , M ? v , and M ? b to extract high-dimensional embeddings for the patch sequence of each stream, denoted separately as E patch, j , E patch,v , and E patch,b . The corresponding relationship is depicted in Eq. <ref type="formula">(3)</ref>.</p><formula xml:id="formula_9">E patch, j , E patch,v , E patch,b = M ? j (S j ), M ? v (S v ), M ? b (S b ). (3)</formula><p>The resulted three embedding streams are then fed into the key component of the proposed TRANS4SOAR architecture, i.e. Mixed Attention Fusion Mechanism (MAFM), for multistream fusion, which we now introduce in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multimodal Fusion at the Patch Embedding Level</head><p>Mixed Fusion (MF). The proposed Mixed Attention Fusion Mechanism (MAMF) builds upon the the Mixed Fusion (MF) strategy. The main purpose of MF is to transfer the important cues from the two auxiliary streams, i.e., velocities and bones, to the main stream, i.e., joints, to achieve multi-stream fusion of skeleton data on the patch embedding level. We propose to use a mixture of Value and Key for multi-stream fusion.</p><p>While such a concept regarding the mixture of Key and Value is studied in MixFormer <ref type="bibr" target="#b60">[56]</ref> for template matching, the design of our proposed MF mechanism is non-trivial. Unlike the mixture in template matching which aims to push the model to focus on similarity cues, our MF scheme is designed to harvest complementary cross-modality dependencies and enable a multi-stream agreement for discriminative embedding learning. Apart from using a concept of mixture of Key and Value, we design a unique three-stream patchembedding fusion architecture regarding MF and MAFM to suit the discriminative embedding learning the for SOAR. In the following, we introduce the proposed MF for multi-stream patch embedding fusion in detail. First, we encode the patch embeddings of the joints E patch, j through two different linear projection layers, i.e., Proj jv Q and Proj b j Q , as depicted in Eq. (4).</p><formula xml:id="formula_10">Q jv , Q b j = Proj jv Q (E patch, j ), Proj b j Q (E patch, j ).<label>(4)</label></formula><p>Then, for Keys and Values of the jv branch and the b j branch, the input embeddings are aggregated together through concatenation, which is indicated as Cat. After that, for each single term, a projection layer is used for encoding. For example, Proj jv V is the projection layer for Value of the jv branch. As a result, V iv , K jv , V b j , and K b j are yielded after the encoding:</p><formula xml:id="formula_11">V iv = Proj jv V (Cat(E patch, j , E patch,v )),<label>(5)</label></formula><formula xml:id="formula_12">K jv = Proj jv K (Cat(E patch, j , E patch,v )),<label>(6)</label></formula><formula xml:id="formula_13">V b j = Proj b j V (Cat(E patch, j , E patch,b )),<label>(7)</label></formula><formula xml:id="formula_14">K b j = Proj b j K (Cat(E patch, j , E patch,b )).<label>(8)</label></formula><p>After the aforementioned procedures, we have obtained Query, Key, and Value for the two branches, separately. Then the question lies in how to further aggregate these two branches. We introduce Softmax Concentrated Aggregation (SCA), which is realized through the following equations to achieve aggregation between V jv and V b j , K jv and K b j , and Q jv and Q b j :</p><formula xml:id="formula_15">V = (So f tmax(V jv ) T V b j + So f tmax(V b j ) T V jv )/2,<label>(9)</label></formula><formula xml:id="formula_16">K = (So f tmax(K jv ) T K b j + So f tmax(K b j ) T K jv )/2,<label>(10)</label></formula><formula xml:id="formula_17">Q = (So f tmax(Q jv ) T Q b j + So f tmax(Q b j ) T Q jv )/2.<label>(11)</label></formula><p>After the SCA operation, we obtain the aggregated Query Q, Key K, and Value V, which is merged together to formulate the desired attention by Att = So f tmax(QK T / ? s k )V, where the scale factor is used to avoid the negative influence brought by the dot product on the variance and Att denotes the calculated attention value. Mixed Attention Fusion Mechanism (MAFM). The MAFM is depicted on the upper right corner of <ref type="figure" target="#fig_2">Figure 2</ref>, which is designed for a further step of aggregation, while considering layer normalization (LN), averaged skip connection, and path drop out. First, the attended embedding E att is obtained through Eq. <ref type="bibr" target="#b16">(12)</ref>. </p><formula xml:id="formula_18">E att = MF(LN(E patch, j ), LN(E patch,v ), LN(E patch, j )). (12)</formula><formula xml:id="formula_19">E N?1 = f 1?N?1 ? (E patch ), E * N?1 = f 1?N?1 ? (E patch ) 12: if epoch &lt; N t then E * aug = WarmU pAug(E * N?1 ) 13: else E * aug = PrototypeAug(E * N?1 , E * P ) 14: end if 15: E = EMB( f N ? (E N?1 )), E * = EMB( f N ? (E * aug )) 16: L t pl = TripletMarginLoss(E, E n , E p ) 17: L LSC = ConsistencyLoss(E, E * ) ?LSC loss 18:</formula><p>L class = Classi f icationLoss(Head(E), label S )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>19:</head><p>BackPropagation(WeightedSum(L t pl , L class , L LSC )) <ref type="bibr">20:</ref> end for <ref type="bibr">21:</ref> if epoch &lt; N t ? 1 then 22:</p><formula xml:id="formula_20">CalculatePrototypes(D train ) ? Set(E N?1 ) ? PMB 23:</formula><p>end if 24: end for As shown in <ref type="figure" target="#fig_2">Figure 2</ref>, the original patch embeddings E patch, j , E patch,v , and E patch,b are firstly averaged and then added with the path-dropped attended embedding E att to have E asn , an embedding after averaging (AV G) and applying an skip connection, as depicted in Eq. <ref type="bibr" target="#b17">(13)</ref>.</p><formula xml:id="formula_21">E asn = AV G(E patch, j , E patch,v , E patch,b ) + DP(E att ),<label>(13)</label></formula><p>where, DP indicates the drop path operation. Then, the final mixed resulted embedding E mixed is obtained via Eq. <ref type="bibr" target="#b18">(14)</ref>.</p><formula xml:id="formula_22">E mixed = DP(MLP(LN(E asn ))) + E asn .<label>(14)</label></formula><p>Finally, the resulted mixed embedding is further fed into the stack of transformer blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Prototype-based Latent Space Consistency Loss</head><p>To learn data-efficient one-shot action recognition, we put forward a Latent Space Consistency (LSC) loss, which encourages the consistency of the embeddings learned through the main branch and an additional prototype-based feature augmentation branch by cosine similarity loss, as illustrated in Alg. 2. The intention behind the design of LSC loss is to increase the robustness of the model by forcing the model to learn consistent embeddings even under the disruption of the feature-level augmentation, whose capability against occlusion is verified through our experiments. We build on top of a recent feature augmentation approach from semisupervised learning <ref type="bibr" target="#b61">[57]</ref>, but additionally propose a warmup self-augmentation phase and certain architecture changes, which have proven to be effective in improving both the accuracy and the robustness of the model. Estimating action category prototypes. For the auxiliary branch augmentations at feature-level, we draw inspiration from FeatMatch <ref type="bibr" target="#b61">[57]</ref>, a recent method for semi-supervised image classification, where a learned weighted combined category-specific prototypes is used to enhance the intermediate features when referring to feature-level augmentations. Specifically, for each data-rich action category l i ? C base , we iteratively estimate its prototype in the latent space as the center of all training set embeddings of the specific action (we use the embeddings after the N?1 block if N is our number of transformer blocks). Note, that unlike FeatMatch, we use the centers of the data-rich base categories available during training (while clustering is used in semi-supervised learning due to absence of labels). Every epoch, these action category prototypes are iteratively updated and stored into a fixed-sized vector by category-wise mean average, which we refer to as the Prototype Memory Bank (PMB). These action category prototypes are then used for feature augmentations in order to estimate the consistency cost. Prototype-based feature enhancement with selfaugmentation warm-up. Leveraging prototype-based augmentation in context of one-shot learning requires further conceptual changes. First, since the prototypes indeed correspond to actual action categories from C base (i.e. only one of the current training categories is correct), we first apply Softmax normalization across the channel dimension for prototypes vector E * P and then refine it with the feature E * N?1 and project it into an embedding space as E * r,N?1 =g 2 ? (So f tmax(E * P ) ? E * N?1 ). At the same time, the feature E * N?1 is also projected as E * l,N?1 =g 1 ? (E * N?1 ). Then, the attention weight W is calculated as W=So f tmax(E * l,N?1 T E * r,N?1 ). After aggregating the information coming from prototypes vector E * P to the original feature E * N?1 as depicted in Eq. <ref type="formula" target="#formula_2">(15)</ref>,</p><formula xml:id="formula_23">E * agg,N?1 = g 3 ? ([WE * r,N?1 , E * l,N?1 ]),<label>(15)</label></formula><p>the final augmented feature E * aug is then obtained through a residual connection with the original feature E * N?1 by E * aug =ReLU(E * N?1 +E * agg,N?1 ), where g 1 ? and g 2 ? indicate two fully-connected (fc) layers (no weight sharing), and g 3 ? indicates a stack of two fc layers with ReLU in between.</p><p>[?] denotes concatenation.</p><p>As in our case the prototypes are linked to true action categories from C base (in contrast to unsupervised clustering necessary in self-supervised tasks), using centers of the assigned categories in the early training epochs would be unreliable. To alleviate this issue, we introduce an additional warm-up phase. The key idea is to leverage self-augmentation instead of prototype-based augmentation until certain level of convergence is reached. At earlier stages, we therefore replace the attended prototype representation with the embedding E * N?1 . <ref type="figure" target="#fig_3">Figure 4</ref> illustrates the difference between the selfaugmentation warm-up phase (top) and the prototype-based augmentation (bottom). Then, we switch to the phase at the bottom of <ref type="figure" target="#fig_3">Figure 4</ref>, while leveraging zero prototypes to achieve decenterization for a fixed number of epochs before using the class-agnostic prototype to do the feature-level prototype-based augmentation. After the acquisition of the embeddings E and E from the main branch and the auxiliary branch, the LSC loss is computed as L LSC = 1?cos(E, E ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset Introduction</head><p>We perform comprehensive studies for the SOAR on three challenging datasets: NTU-60 <ref type="bibr" target="#b59">[55]</ref>, NTU-120 <ref type="bibr" target="#b6">[2]</ref> and Toyota Smart Home <ref type="bibr" target="#b7">[3]</ref>. We follow the SOAR protocol in NTU-120 and formulate the evaluation protocols of Toyota Smart Home and NTU-60 to suit our data-scarce representation learning task. Additionally, we propose the occluded SOAR benchmarks for the first time building on top of these three datasets (see Sec. III). The NTU-120/NTU-60/Toyota Smart Home benchmarks feature 100/48/24 data-rich training categories and 20/12/7 data-scarce test categories respectively for one reference per unseen category. The protocols and occluded datasets will be publicly available in our benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>For TRANS4SOAR training we set the warm-up phase threshold N t = 20 while using another 10 epochs for decenterization. We train our model optimized by AdamW <ref type="bibr" target="#b62">[58]</ref> with Cosine Annealing Scheduler for 50 epochs and batch size of 32 using Nvidia A100 GPU with PyTorch 1.8.0 to reproduce the best performance. We use an initial learning rate of 3.5e ?5 with the weights of the three losses, i.e., Triplet Margin Loss, Cross Entropy Loss and LSC loss as 1.0, 0.4 and 0.1. Our TRANS4SOAR (Small) has D Key : 1, N head : [2, 2, 2], H dep : <ref type="bibr" target="#b6">[2,</ref><ref type="bibr" target="#b8">4,</ref><ref type="bibr" target="#b8">4]</ref> and C dim : [384, 512, 512] with 23M parameters while TRANS4SOAR has D Key : 32, N head : <ref type="bibr" target="#b10">[6,</ref><ref type="bibr" target="#b13">9,</ref><ref type="bibr" target="#b16">12]</ref>, H dep : <ref type="bibr" target="#b8">[4,</ref><ref type="bibr" target="#b8">4,</ref><ref type="bibr" target="#b8">4]</ref> and C dim : [384, 512, 768] with 43M parameters, where D Key , N head , H dep and C dim denote dimension of Key, number of the attention head, number of the basic transformer attention block inside each transformer block and the unified feature dimension inside each transformer block respectively. Both of our approaches have three main transformer blocks. The protocols and the occlusion benchmarks will be released.  <ref type="bibr" target="#b39">[35]</ref> 42.1 ---AP ? <ref type="bibr" target="#b39">[35]</ref> 42.9 ---APSR <ref type="bibr" target="#b39">[35]</ref> 45.3 ---TCN-OneShot <ref type="bibr" target="#b37">[33]</ref> 46.3 ---SL-DML <ref type="bibr" target="#b25">[21]</ref> 50.9 ---Skeleton-DML <ref type="bibr" target="#b24">[20]</ref> 54.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Analyses for SOAR Without Occlusion</head><p>Performance analyses regarding different components. As in <ref type="table" target="#tab_0">Table I</ref>, we firstly empirically evaluate the benefits brought by the LSC loss achieved through prototype-based feature augmentation and an additional auxiliary branch. The baseline we chose is SL-DML <ref type="bibr" target="#b25">[21]</ref>, which has the same data preprocessing technique with our approach. Specifically, we use the SL-DML pipeline and equip the selected transformer-based architecture, i.e., LeViT <ref type="bibr" target="#b29">[25]</ref>, with an additional auxiliary branch for attention-based augmentations via feature-level prototypes. The LSC loss is obtained through the calculation of cosine similarity loss between the embedding from the main branch and the embedding from the auxiliary branch.  <ref type="table" target="#tab_0">Table IV</ref> (a)), compared with SL-DML (LeViT), which has overall better performance compared with SL-DML <ref type="bibr" target="#b25">[21]</ref> and Skeleton-DML <ref type="bibr" target="#b24">[20]</ref> for the SOAR without occlusion. We observe the benefits of our LSC loss on NTU-120 <ref type="bibr" target="#b6">[2]</ref>, surpassing the previous two approaches, i.e., SL-DML <ref type="bibr" target="#b25">[21]</ref> by &gt;5% and Skeleton-DML [20] by 1.74% <ref type="table" target="#tab_0">(Table I)</ref>. Our ablation experi- ments regarding the main components of LSC loss are shown in <ref type="table" target="#tab_0">Table VII</ref> regarding the last three experiments, showing the importance of the warm-up stage and de-centerization stage which bring a performance improvement by 1.5% compared with LSC loss without both the aforementioned components. Then, the combination of the LSC loss and the MAFM, mixing three streams of input at patch embedding level, further contributes a remarkable performance gain regarding the SOAR without occlusion compared with the existing state-of-theart works <ref type="bibr" target="#b24">[20]</ref>, <ref type="bibr" target="#b24">[20]</ref>. On the NTU-120 <ref type="bibr" target="#b6">[2]</ref>, TRANS4SOAR (Base) surpasses Skeleton-DML <ref type="bibr" target="#b24">[20]</ref> and SL-DML [21] by 2.85% and 6.15% for accuracy while outperforming SL-DML (LeViT) + LSC by 1.11%, indicating an incremental performance enhancement considering the discriminative ability of the learned embedding by using MAFM and LSC loss. We also conduct experiments to showcase the individual performance gain brought by LSC and MAFM in <ref type="table" target="#tab_0">Table VII</ref> regarding the first three experiments. Furthermore, consistent improvements are achieved by TRANS4SOAR in the other two datasets, e.g., NTU-60 in <ref type="table" target="#tab_0">Table III</ref> (a) and Toyota Smart Home <ref type="bibr" target="#b7">[3]</ref> in <ref type="table" target="#tab_0">Table IV</ref> (a) for the SOAR without occlusion.</p><p>The NTU-60 <ref type="bibr" target="#b59">[55]</ref> has less training categories than the NTU-120 <ref type="bibr" target="#b6">[2]</ref>, thus, it is used to evaluate the generalizability of the leveraged models, which means realizing the SOAR with less a prior knowledge. In <ref type="table" target="#tab_0">Table III</ref> (a), our TRANS4SOAR (Base) surpasses SL-DML <ref type="bibr" target="#b25">[21]</ref> and Skeleton-DML [20] by 19.37% and 18.65% for accuracy, indicating that, given less a prior knowledge, TRANS4SOAR has better capability to harvest more discriminative representation. Furthermore, the Toyota Smart Home <ref type="bibr" target="#b7">[3]</ref> contains 2D skeleton data in image coordinate format, delivering a valuable data format to explore the SOAR task. In Table IV (a), our TRANS4SOAR (Base) undoubtedly shows the best performance over all the previous approaches with large margin. Observing the other three metrics, i.e., F1-score, precision and recall, since the first two datasets have balanced distributed samples for different categories, theses three terms do not have large difference compared with the accuracy. However, since the action categories on the Toyota Smart Home <ref type="bibr" target="#b7">[3]</ref> is not equal distributed, these three terms are able to showcase whether the true prediction is balanced distributed in the test set or not. Our TRANS4SOAR surpasses all the approaches in terms of all metrics on the investigated datasets. In order to ablate the effect of different model scales, we construct TRANS4SOAR (Small) with only 23M parameters which pursues both light model structure and high accuracy, and achieves second best performance, showcasing that the LSC loss and MAFM are helpful for learning discriminative features via different model variants.</p><p>We also conduct experiments in <ref type="table" target="#tab_0">Table XII</ref> to compare with graph convolutional approach <ref type="bibr" target="#b9">[5]</ref> and skeleton transformer approach <ref type="bibr" target="#b14">[10]</ref>, however the performance of these two encoder architectures for the SOAR task even without occlusion is not satisfied compared with TRANS4SOAR and SL-DML (LeViT).</p><p>Tolerance to noisy inputs. The quality of the skeleton data is influenced by a variety of factors, such as sensor noise or occlusions. First, a larger gap between the TRANS4SOAR and standard DML trained on the Toyota Smart Home <ref type="bibr" target="#b7">[3]</ref> (which is noisier than the more controlled NTU-datasets) hints towards its advantages specifically for imperfect input. To validate if this is the case, we evaluate the model for inputs corrupted by different magnitudes of Gaussian noise and discover a remarkable tolerance of TRANS4SOAR (in <ref type="table" target="#tab_6">Table V</ref>). While the prediction quality diminishes for basic DML-based models, the utilizing of LSC loss on the SL-DML (LeViT) is more robust when confronted with unreliable data, which showcases the superiority of the proposed LSC loss against Gaussian noise input. In particular, the performance for the SL-DML (LeViT) with the LSC loss falls from 55.94% on clean data to 51.91% for Gaussian noise with ? =0.05, while this decline is much higher (53.19% ? 21.97%) for the SL-DML (LeViT). We attribute this to the extensive learned augmentations at the featurelevel taking place in the auxiliary branch while formulating the LSC loss. The LSC loss encourages the model to output similar results if the embedding has been altered, which suits naturally to the use-case of noise disturbances. Furthermore, TRANS4SOAR (Base) surpasses all the other investigated approaches with no doubt by 54.74% and 53.09% in terms of accuracy for the Gaussian noise conditioned by ? =0.05 and ? =0.1 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Analyses for REalistic Synthesized Occlusion (RE)</head><p>We conduct experiments regarding RE for NTU-120 <ref type="bibr" target="#b6">[2]</ref>, NTU-60 <ref type="bibr" target="#b59">[55]</ref> and Toyota Smart Home <ref type="bibr" target="#b7">[3]</ref> in <ref type="table" target="#tab_0">Table II (a),  Table III (b), and Table IV</ref> (b) with SNR range 0.05 ? 0.2 and with occlusion on the reference set. First, the performance of all investigated approaches for the SOAR with RE benchmark is degraded compared to the SOAR without occlusion benchmark, indicating that the proposed RE is very challenging     MAFM on dealing with the disruption brought by the RE through aggregating three different skeleton encoding formats, which contains potential de-occlusion cues, and also show the superiority of our reformulated TRANS4SOAR regarding the robustness against the occlusion disruption from the real life compared with the LeViT, on which we build up our TRANS4SOAR based. Similar comparison and analyses could be found on the other two datasets, i.e., NTU-60 <ref type="bibr" target="#b59">[55]</ref> in <ref type="table" target="#tab_0">Table III</ref> (b) and Toyota Smart Home <ref type="bibr" target="#b7">[3]</ref> in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Analyses Regarding Random Occlusion (RA)</head><p>Random occlusion, considered as a combination between random temporal and spatial occlusions, is leveraged as the  second main occlusion in our work on NTU-120 <ref type="bibr" target="#b6">[2]</ref>, NTU-60 <ref type="bibr" target="#b59">[55]</ref> and Toyota Smart Home <ref type="bibr" target="#b7">[3]</ref>, depicted in <ref type="table" target="#tab_0">Table II</ref> (b), <ref type="table" target="#tab_0">Table III (c) and Table IV</ref> (c), with SNR = 0.1 and without occlusion on reference set respectively. The proposed TRANS4SOAR (Base) keeps surpassing all the existing approaches by large margins, e.g., SL-DML <ref type="bibr" target="#b25">[21]</ref> and Skeleton-DML [20] by 10.64% and 18.02% on NTU-120 <ref type="bibr" target="#b6">[2]</ref>. The performance of Skeleton-DML <ref type="bibr" target="#b24">[20]</ref> under RA is worse than that of SL-DML <ref type="bibr" target="#b25">[21]</ref>, while the case is reversed on RE, which means most of the existing approaches can not be robust against different occlusions. However, TRANS4SOAR overcomes this issue and demonstrates promising performances over different occlusions, especially for RE, which is an important ability for learning discriminative representation. The proposed MAFM is also proved to have strong capability while dealing with different occlusions, which is well addressed through taking the three stream of skeleton patch embedding as input. MAFM is further illustrated as the best fusion architecture among all the investigated fusion methods regarding the two main occlusions in <ref type="table" target="#tab_0">Table VI</ref> which will be introduced later. Considering the three streams encoding, first, since bone and velocity use temporal and spatial difference respectively, more cues regarding the neighbourhood of the occluded region could be encoded in different perspectives. Furthermore, while tackling with RA on the NTU-60 <ref type="bibr" target="#b59">[55]</ref> and Toyota Smart Home <ref type="bibr" target="#b7">[3]</ref>, TRANS4SOAR (Base) and TRANS4SOAR (Small) also demonstrate promising state-ofthe-art performance across all the leveraged metrics, which reflects the strengths of the proposed models in multiple point of views. Similar ablations regarding the SNR ratio, i.e., 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Analyses for Occlusion Disruption on Reference Samples</head><p>Experiments are conducted in <ref type="table" target="#tab_12">Table X</ref> to investigate different occlusions on the NTU-60 <ref type="bibr" target="#b59">[55]</ref> reference set with the occlusion state denotes by OCCVal, where T and F indicate with occlusion and without occlusion. We set SNR = 0.1 for RA and SNR range 0.05 ? 0.2 for RE, which is comparable regarding averaged SNR. SL-DML <ref type="bibr" target="#b25">[21]</ref> and Skeleton-DML <ref type="bibr" target="#b24">[20]</ref> have absolute performance fluctuation for accuracy by 3.46% and 11.13% for RA, and 2.61% and 1.63% for RE. What we desire is that the model should have small absolute fluctuation regarding different OCCVal setting. The experimental results of TRANS4SOAR suit this desire with absolute fluctuation 1.00% for RA and 0.80% for RE, illustrating the strong ability against the occlusion on the reference set. Considering RA and RE with OCCVal = F, both the performances of SL-DML <ref type="bibr" target="#b25">[21]</ref> and Skeleton-DML <ref type="bibr" target="#b24">[20]</ref> are worse with RE than that with RA, indicating that RE is more challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Analyses for Ablation of Fusion Mechanisms</head><p>To demonstrate the efficiency of MAFM, we conduct comparison experiments among several fusion approaches in <ref type="table" target="#tab_0">Table VI</ref>, where we set SNR = 0.1 for RA and SNR range 0.05 ? 0.2 for RE. The mostly leveraged fusion technique is late fusion which conducts fusion at the decision level. However the design of late fusion triplicates the model size as 113M while the others are at 40M level . Here, we consider to propose a efficient fusion mechanism at patch embedding level which takes both the model performance and size into consideration. The baselines for patch embedding level fusion includes the addition, multiplication and concatenation, which are directly executed after the acquisition of the patch embeddings for the three streams. Another method we compared with is late fusion, which conducts addition after obtaining the final embeddings of the three streams and has a three times larger model size. The experimental results indicate that MAFM has great performance compared with all the leveraged patch-embedding level fusiom baselines and the competitive late fusion on the NTU-60 <ref type="bibr" target="#b59">[55]</ref> under different occlusions, e.g., No occlusion (N), REalistic synthesized occlusion (RE) and RAndom occlusion (RA). Specifically, TRANS4SOAR with MAFM surpasses the late fusion by 2.35%, 1.43%, and 17.04% on the RE, RA, and N respectively, while having a smaller model size for both inference and training. Simultaneously. TRANS4SOAR with MAFM surpasses the investigated approach with the best performance among the basic patch embedding level fusion approach by 0.67%, 4.54% and 6.44% for RE, RA and N.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Analyses for Random Temporal and Spatial Occlusions</head><p>In order to show the performance of all the leveraged models with the two existing occlusions in the related work, i.e., random temporal and spatial occlusion, which might also be interesting to the community regarding the specific occlusion considering temporal and spatial components individually, we conducted experiments on three datasets while using the most effective approaches investigated in our work, as described in Table XI (a) and Table XI (b), where we choose the occluded frame number as 10 and the occluded joints number as 5 respectively. Compared with RE, these two leveraged occlusions which is specific controlled through predefined occluded frame and joint numbers are easier to be addressed as their randomness is not satisfied. But the important thing is that our proposed TRANS4SOAR (Base) and TRANS4SOAR (Small) still surpass all existing works by large margins on all datasets with these two occlusions which further illustrates the efficiency of our model against different occlusions, even the occlusion is specifically controlled by predefined concepts, e.g., the occluded frame number.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Qualitative Analyses</head><p>The qualitative results are given in <ref type="figure" target="#fig_4">Figure 5</ref> for SOAR with RE on the NTU-60 <ref type="bibr" target="#b59">[55]</ref>, where the occluded body joints are marked as red dots. TRANS4SOAR has overall great performance while comparing with Skeleton-DML <ref type="bibr" target="#b24">[20]</ref> with true prediction 3 : 2 among the selected 4 samples. The  occlusion of the joints which is dominant to the action has a large influence on the model, e.g., arm and hand joints for TakeOffGlasses action, where Skeleton-DML <ref type="bibr" target="#b24">[20]</ref> gives a false prediction while TRANS4SOAR pursues a true prediction. However, due to the high similarity between several actions, e.g., WipeFace and DrinkingWater, TRANS4SOAR still has false prediction but the predicted DrinkingWater action is more similar with WipeFace compared with TearUpPaper predicted by Skeleton-DML <ref type="bibr" target="#b24">[20]</ref>, showing that there is still research space for the future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this work, we look into the problem of data-scarce recognition of daily activities through the lens of one-shot recognition, while considering diverse occlusions. First, we propose realistic synthesized and random occlusion to better address the occlusion problem. Then, a novel architecture TRANS4SOAR is put forward to provide discriminative representations for skeleton input and enhance the robustness against different scenarios. We design a Mixed Attention Fusion Mechanism (MAFM), featuring a three-stream of skeleton encoding inputs to realize efficient fusion on the patchembedding level. Inspired by recent success of augmentationbased methods in semi-supervised learning, we further introduce the latent space consistency loss, which leverages an additional auxiliary branch encouraging the embedder to produce similar results despite extensive augmentations at the feature level. TRANS4SOAR sets the new state of the art on both normal and occluded SOAR benchmarks established on three datasets. In the future, occluded one-shot recognition based on video data is still attractive to be researched.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>if N Occ &lt; T Occ or N Rep &lt; T Rep then 30: Set Accept = False and N Rep + = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 2 if</head><label>2</label><figDesc>An overview of the training pipeline with latent space consistency (LSC) loss. Input: S -a batch in D train ; S p and S n -positive and negative anchor; f 1?N?1 ? and f 1?N?1 ? -first N-1 transformer layers of main and auxiliary branches; f N ? and f N ? -the N-th (last) transformer layer for main and auxiliary branches; EMB -Embedding layer; N e -maximum training epochs; N t -epoch threshold for the stage changing; E and E * -embedding for main and auxiliary branches; PMB -Prototypes memory bank; WarmUpAug and PrototypeAug -Warm-up stage and prototypebased feature augmentation stage 1: for all epoch ? Range(N e ) do 2: for all S ? D train do 3: if epoch &gt; N t then 4: for all l in label S do Append(PMB[l]) ? List p BaseModel is not None then S = BaseModel(S)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Overview of the self-augmentation at feature-level leveraged in the auxiliary branch of the proposed method. During the warm-up phase (top), the feature itself is used as the basis to compute attention masks used to self-augment the feature. At the later stage, we use action-specific prototypes Softmax-normalized along the channel dimension in order to augment the embedding (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Qualitative results on NTU-60<ref type="bibr" target="#b59">[55]</ref> with RE for SOAR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I EXPERIMENTS</head><label>I</label><figDesc>FOR SOAR WITHOUT OCCLUSION ON NTU-120<ref type="bibr" target="#b6">[2]</ref>.</figDesc><table><row><cell>Encoder</cell><cell cols="2">Accuracy F1</cell><cell>Precision</cell><cell>Recall</cell></row><row><cell cols="2">Previously Published Approaches</cell><cell></cell><cell></cell><cell></cell></row><row><cell>AN  ? [35]</cell><cell>41.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>FC  ?</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II</head><label>II</label><figDesc></figDesc><table><row><cell cols="9">EXPERIMENTS REGARDING REALISTIC SYNTHESIZED OCCLUSION (A)</cell></row><row><cell cols="8">AND RANDOM OCCLUSION (B) FOR SOAR ON NTU-120 [2].</cell></row><row><cell>Encoder</cell><cell>Acc.</cell><cell cols="2">(a) With RE F1 Prec.</cell><cell>Rec.</cell><cell>Acc.</cell><cell cols="2">(b) With RA F1 Prec.</cell><cell>Rec.</cell></row><row><cell>SL-DML [21]</cell><cell cols="6">39.82 37.85 39.32 39.86 42.53 42.24</cell><cell>44.79</cell><cell>42.56</cell></row><row><cell>Skeleton-DML [20]</cell><cell cols="6">49.21 46.82 48.10 49.18 35.15 32.59</cell><cell>34.29</cell><cell>35.22</cell></row><row><cell>SL-DML (LeViT [25])</cell><cell cols="6">44.22 42.29 44.20 44.31 35.00 33.24</cell><cell>41.45</cell><cell>35.10</cell></row><row><cell>SL-DML (Swin [40])</cell><cell cols="6">47.19 45.64 46.78 47.29 47.19 45.64</cell><cell>46.78</cell><cell>47.29</cell></row><row><cell cols="7">SL-DML (LeViT) + LSC 48.28 46.03 47.58 48.31 38.04 35.93</cell><cell>37.87</cell><cell>38.11</cell></row><row><cell>Trans4SOAR (Small)</cell><cell cols="2">51.64 50.47</cell><cell cols="2">52.36 51.70</cell><cell>53.27</cell><cell>51.33</cell><cell>53.80</cell><cell>53.35</cell></row><row><cell>Trans4SOAR (Base)</cell><cell>52.35</cell><cell>48.79</cell><cell cols="2">52.87 52.43</cell><cell>53.17</cell><cell>52.89</cell><cell>54.50</cell><cell>53.21</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table</head><label></label><figDesc></figDesc><table><row><cell>The aforemen-</cell></row><row><cell>tioned approach with LSC loss is denoted as SL-DML (LeViT)</cell></row><row><cell>+ LSC compared with SL-DML (LeViT), which replaces</cell></row><row><cell>CNN by the LeViT in the SL-DML pipeline. Although LSC</cell></row><row><cell>loss does not have any influence on the architecture at test-</cell></row><row><cell>time, it performs surprisingly well for the SOAR task without</cell></row><row><cell>occlusion. For instance, SL-DML (LeViT) + LSC Loss leads</cell></row><row><cell>to accuracy gains by 2.75% (NTU-120, Table I), 3.22% (NTU-</cell></row><row><cell>60,</cell></row></table><note>III (a)) and 2.24% (Toyota Smart Home,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III EXPERIMENTS</head><label>III</label><figDesc>ON THE NTU-60 [55] FOR SOAR CONSIDERING THE SCENARIOS (A) WITHOUT OCCLUSION, (B) WITH REALISTIC OCCLUSION (RE) AND (C) WITH RANDOM OCCLUSION (RA). SL-DML [21] 54.82 54.31 56.72 54.65 36.90 35.86 36.59 37.05 45.28 43.13 45.00 45.42 Skeleton-DML [20] 55.54 50.88 53.13 51.24 42.66 40.90 41.50 42.82 60.43 59.66 61.37 60.54</figDesc><table><row><cell>Encoder</cell><cell>Acc.</cell><cell>(a) Without Occlusion F1 Prec. Rec.</cell><cell>Acc.</cell><cell>(b) With RE F1 Prec.</cell><cell>Rec.</cell><cell>Acc.</cell><cell>(c) With RA F1 Prec.</cell><cell>Rec.</cell></row><row><cell cols="3">Previously Published Approaches</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Transformer-based Encoder Optimized by DML (Ours)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SL-DML (Swin [40])</cell><cell cols="2">56.99 56.24 58.67 56.99</cell><cell cols="6">51.71 50.60 52.54 51.82 64.65 63.74 66.57 64.77</cell></row><row><cell>SL-DML (LeViT [25])</cell><cell cols="2">64.45 64.17 66.35 64.47</cell><cell cols="6">52.72 52.19 54.90 52.86 56.73 55.89 57.57 56.85</cell></row><row><cell cols="3">Our Extended and Evaluated Approached (Ours)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">SL-DML (LeViT) + LSC 67.67 67.87 68.74 67.67</cell><cell cols="6">53.79 52.76 54.18 53.88 60.78 58.75 59.97 60.90</cell></row><row><cell>Trans4SOAR (Small)</cell><cell cols="2">69.74 70.52 72.45 69.82</cell><cell cols="6">56.84 55.84 58.27 56.98 67.90 67.32 68.94 68.01</cell></row><row><cell>Trans4SOAR (Base)</cell><cell cols="2">74.19 74.34 75.91 74.20</cell><cell cols="6">59.28 58.96 59.91 59.40 72.59 71.82 73.89 72.66</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV EXPERIMENTS</head><label>IV</label><figDesc>ON THE TOYOTA SMART HOME<ref type="bibr" target="#b7">[3]</ref> FOR SOAR (A)WITHOUT  OCCLUSION, (B) WITH REALISTIC OCCLUSION (RE) AND (C) WITH RANDOM OCCLUSION (RA). SL-DML [21] 58.98 27.15 27.64 35.00 38.93 25.16 32.93 28.48 53.79 26.28 27.24 29.67 Skeleton-DML [20] 47.31 18.45 18.58 23.80 47.67 24.86 27.93 27.35 48.91 21.60 25.00 21.75</figDesc><table><row><cell>Encoder</cell><cell>(a) Without Occlusion Acc. F1 Prec. Rec.</cell><cell>Acc.</cell><cell cols="2">(b) With RE F1 Prec.</cell><cell>Rec.</cell><cell>Acc.</cell><cell>(c) With RA F1. Prec.</cell><cell>Rec.</cell></row><row><cell cols="2">Previously Published Approaches</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Transformer-based Encoder Optimized by DML (Ours)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SL-DML (Swin [21])</cell><cell>58.76 28.83 29.17 32.34</cell><cell cols="4">35.43 18.48 23.24 23.80</cell><cell>65.50</cell><cell cols="2">29.20 30.78 29.69</cell></row><row><cell>SL-DML (LeViT [25])</cell><cell>62.22 31.98 37.56 35.16</cell><cell cols="4">38.48 22.58 27.66 24.62</cell><cell>61.96</cell><cell cols="2">26.42 28.52 29.20</cell></row><row><cell cols="2">Our Extended and Evaluated Approached (Ours)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">SL-DML (LeViT) + LSC 64.46 31.91 34.07 33.58</cell><cell cols="4">41.82 24.34 29.02 26.67</cell><cell>63.77</cell><cell cols="2">27.72 29.09 29.90</cell></row><row><cell>Trans4SOAR (Small)</cell><cell>66.87 28.08 31.47 34.63</cell><cell cols="2">55.12 26.90</cell><cell cols="2">29.41 30.69</cell><cell cols="2">68.47 28.86 29.56</cell><cell>32.25</cell></row><row><cell>Trans4SOAR (Base)</cell><cell>70.22 33.96 37.81 35.33</cell><cell>60.15</cell><cell>25.50</cell><cell cols="2">33.12 31.86</cell><cell cols="2">68.91 29.27 34.15</cell><cell>31.45</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V EXPERIMENTS</head><label>V</label><figDesc>FOR SOAR WITHOUT OCCLUSION ON NTU-120 [2] UNDER GAUSSIAN NOISE DISRUPTION.</figDesc><table><row><cell>Gaussian Noise</cell><cell cols="2">? = 0.1, ? = 0</cell><cell></cell><cell></cell><cell cols="2">? = 0.05, ? = 0</cell><cell></cell><cell></cell></row><row><cell>Encoder</cell><cell>Acc.</cell><cell>F1</cell><cell>Prec.</cell><cell>Rec.</cell><cell>Acc.</cell><cell>F1</cell><cell>Prec.</cell><cell>Rec.</cell></row><row><cell>SL-DML [21]</cell><cell cols="3">21.42 11.83 8.50</cell><cell cols="4">21.71 21.76 12.23 8.70</cell><cell>21.86</cell></row><row><cell>SL-DML (LeViT)</cell><cell cols="3">22.31 12.32 8.79</cell><cell cols="4">22.40 21.97 12.82 9.69</cell><cell>22.07</cell></row><row><cell cols="8">SL-DML (LeViT) + LSC 52.54 51.16 51.61 52.65 51.91 50.08 51.67</cell><cell>52.01</cell></row><row><cell>Trans4SOAR</cell><cell cols="7">53.09 51.89 53.05 53.15 54.74 54.65 56.33</cell><cell>54.83</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VI EXPERIMENTS</head><label>VI</label><figDesc>FOR DIFFERENT FUSION TECHNIQUES ON NTU60<ref type="bibr" target="#b59">[55]</ref> UNDER DIFFERENT OCCLUSION SCENARIOS.</figDesc><table><row><cell>Fusion Method</cell><cell cols="3">OCC Accuracy F1</cell><cell>Precision</cell><cell>Recall</cell></row><row><cell>Single (Joints)</cell><cell>RE</cell><cell>53.79</cell><cell cols="2">52.76 54.18</cell><cell>53.88</cell></row><row><cell>Single (Bones)</cell><cell>RE</cell><cell>54.22</cell><cell cols="2">53.73 54.86</cell><cell>54.33</cell></row><row><cell>Single (Velocities)</cell><cell>RE</cell><cell>56.93</cell><cell cols="2">56.10 57.97</cell><cell>57.03</cell></row><row><cell>Addition</cell><cell>RE</cell><cell>56.37</cell><cell cols="2">54.48 55.68</cell><cell>56.51</cell></row><row><cell>Multiplication</cell><cell>RE</cell><cell>53.35</cell><cell cols="2">51.91 53.69</cell><cell>53.50</cell></row><row><cell>Concatenation</cell><cell>RE</cell><cell>58.61</cell><cell cols="2">57.21 57.63</cell><cell>58.73</cell></row><row><cell>Late Fusion</cell><cell>RE</cell><cell>56.93</cell><cell cols="2">56.10 57.97</cell><cell>57.03</cell></row><row><cell>MAFM</cell><cell>RE</cell><cell>59.28</cell><cell cols="2">58.96 59.91</cell><cell>59.40</cell></row><row><cell>Single (Joints)</cell><cell>RA</cell><cell>60.78</cell><cell cols="2">58.75 59.97</cell><cell>60.90</cell></row><row><cell>Single (Bones)</cell><cell>RA</cell><cell>55.15</cell><cell cols="2">53.56 56.63</cell><cell>54.16</cell></row><row><cell>Single (Velocities)</cell><cell>RA</cell><cell>33.15</cell><cell cols="2">30.54 29.67</cell><cell>33.82</cell></row><row><cell>Addition</cell><cell>RA</cell><cell>65.09</cell><cell cols="2">65.03 66.36</cell><cell>65.18</cell></row><row><cell>Multiplication</cell><cell>RA</cell><cell>67.54</cell><cell cols="2">67.51 68.65</cell><cell>67.63</cell></row><row><cell>Concatenation</cell><cell>RA</cell><cell>68.05</cell><cell cols="2">68.54 70.90</cell><cell>68.13</cell></row><row><cell>Late Fusion</cell><cell>RA</cell><cell>71.16</cell><cell cols="2">71.58 73.16</cell><cell>71.22</cell></row><row><cell>MAFM</cell><cell>RA</cell><cell>72.59</cell><cell cols="2">71.82 73.89</cell><cell>72.66</cell></row><row><cell>Single (Joints)</cell><cell>N</cell><cell>67.67</cell><cell cols="2">67.87 68.74</cell><cell>67.67</cell></row><row><cell>Single (Bones)</cell><cell>N</cell><cell>61.45</cell><cell cols="2">61.44 63.50</cell><cell>61.57</cell></row><row><cell>Single (Velocities)</cell><cell>N</cell><cell>49.74</cell><cell cols="2">50.08 51.31</cell><cell>49.89</cell></row><row><cell>Addition</cell><cell>N</cell><cell>67.05</cell><cell cols="2">66.88 68.09</cell><cell>67.12</cell></row><row><cell>Multiplication</cell><cell>N</cell><cell>64.63</cell><cell cols="2">65.05 66.34</cell><cell>64.75</cell></row><row><cell>Concatenation</cell><cell>N</cell><cell>67.75</cell><cell cols="2">67.79 69.56</cell><cell>67.86</cell></row><row><cell>Late Fusion</cell><cell>N</cell><cell>57.15</cell><cell cols="2">56.52 57.57</cell><cell>57.26</cell></row><row><cell>MAFM</cell><cell>N</cell><cell>74.19</cell><cell cols="2">74.34 75.91</cell><cell>74.20</cell></row><row><cell cols="6">for discriminative representation learning. In Table II (a),</cell></row><row><cell cols="6">our TRANS4SOAR (Base) shows the best performance by</cell></row><row><cell cols="6">52.35%, 48.79%, 52.87% and 52.43% for accuracy, F1-score,</cell></row><row><cell cols="6">precision and recall, indicating that the performance is equally</cell></row><row><cell cols="6">distributed among the investigated classes on the NTU-120 [2].</cell></row><row><cell cols="6">The TRANS4SOAR (Small) achieves second best performance</cell></row><row><cell cols="6">among all the metrics on NTU-120 with RE by 51.64% for</cell></row><row><cell cols="6">accuracy. Note, that the SL-DML (LeViT) demonstrates worse</cell></row><row><cell cols="6">performances on all the conducted datasets with RE for SOAR.</cell></row><row><cell cols="6">On NTU-120 [2] with RE, the SL-DML (LeViT) approach</cell></row><row><cell cols="6">only has an accuracy of 44.22% which is lower than the</cell></row><row><cell cols="6">Skeleton-DML [20] with an accuracy of 49.21%. However,</cell></row><row><cell cols="6">compared with SL-DML (LeViT), SL-DML (LeViT) + LSC</cell></row><row><cell cols="6">loss still has a better accuracy of 48.28%, indicating that</cell></row><row><cell cols="6">LeViT architecture is not good at dealing with RE, while LSC</cell></row><row><cell cols="6">loss can alleviate the negative influence. After the using of</cell></row><row><cell cols="6">the MAFM to form our TRANS4SOAR (Base), a superior</cell></row><row><cell cols="6">performance of 52.35% in accuracy shows up, indicating that</cell></row><row><cell cols="6">the disruption issue caused by RE can be well addressed</cell></row></table><note>through the triplet stream encoding and MAFM. These ex- perimental results illustrate the importance of the proposed</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VII ABLATION</head><label>VII</label><figDesc>STUDY OF LSC AND MAFM USED IN THE TRANS4SOAR ON NTU-60 [55] WITHOUT OCCLUSION.</figDesc><table><row><cell cols="2">With LSC Self-aug. wp De-centerization MAFM Accuracy F1</cell><cell cols="2">Precision Recall</cell></row><row><cell>64.45</cell><cell cols="2">64.17 66.35</cell><cell>64.47</cell></row><row><cell>67.67</cell><cell cols="2">67.87 68.74</cell><cell>67.67</cell></row><row><cell>71.55</cell><cell cols="2">71.85 73.45</cell><cell>71.63</cell></row><row><cell>72.69</cell><cell cols="2">72.80 74.27</cell><cell>72.73</cell></row><row><cell>73.09</cell><cell cols="2">73.39 74.54</cell><cell>73.14</cell></row><row><cell>74.19</cell><cell cols="2">74.34 75.91</cell><cell>74.20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VIII EXPERIMENTS</head><label>VIII</label><figDesc>REGARDING DIFFERENT RANDOM OCCLUSION RATIO ON THE NTU-60 [55] FOR THE SOAR.</figDesc><table><row><cell>Model</cell><cell cols="3">RA_ratio Accuracy F1</cell><cell cols="2">Precision Recall</cell></row><row><cell>SL-MDL [21]</cell><cell></cell><cell>45.28</cell><cell cols="2">43.13 45.00</cell><cell>45.42</cell></row><row><cell>Skeleton-DML [20]</cell><cell></cell><cell>60.43</cell><cell cols="2">59.66 61.37</cell><cell>60.54</cell></row><row><cell>SL-DML (LeViT [25]) SL-DML (LeViT) + LSC</cell><cell>0.1</cell><cell>56.73 60.78</cell><cell cols="2">55.89 57.75 58.75 59.97</cell><cell>56.85 60.90</cell></row><row><cell>Trans4SOAR (Small)</cell><cell></cell><cell>69.74</cell><cell cols="2">70.52 72.45</cell><cell>69.82</cell></row><row><cell>Trans4SOAR (Base)</cell><cell></cell><cell>72.59</cell><cell cols="2">71.82 73.89</cell><cell>72.66</cell></row><row><cell>SL-DML [21]</cell><cell></cell><cell>46.39</cell><cell cols="2">42.82 46.69</cell><cell>46.54</cell></row><row><cell>Skeleton-DML [20]</cell><cell></cell><cell>58.93</cell><cell cols="2">56.07 58.45</cell><cell>59.05</cell></row><row><cell>SL-DML (LeViT [25]) SL-DML (LeViT) + LSC</cell><cell>0.3</cell><cell>46.32 47.82</cell><cell cols="2">43.78 43.94 45.02 48.41</cell><cell>46.40 47.91</cell></row><row><cell>Trans4SOAR (Small)</cell><cell></cell><cell>66.57</cell><cell cols="2">66.26 67.94</cell><cell>66.65</cell></row><row><cell>Trans4SOAR (Base)</cell><cell></cell><cell>72.39</cell><cell cols="2">72.81 74.68</cell><cell>72.43</cell></row><row><cell>SL-DML [21]</cell><cell></cell><cell>43.44</cell><cell cols="2">38.46 41.30</cell><cell>43.57</cell></row><row><cell>Skeleton-DML [20]</cell><cell></cell><cell>44.69</cell><cell cols="2">41.89 45.74</cell><cell>44.79</cell></row><row><cell>SL-DML (LeViT [25]) SL-DML (LeViT) + LSC</cell><cell>0.5</cell><cell>35.77 40.53</cell><cell cols="2">32.56 36.22 37.38 38.33</cell><cell>35.94 40.59</cell></row><row><cell>Trans4SOAR (Small)</cell><cell></cell><cell>52.92</cell><cell cols="2">50.78 55.13</cell><cell>53.02</cell></row><row><cell>Trans4SOAR (Base)</cell><cell></cell><cell>54.82</cell><cell cols="2">55.01 58.01</cell><cell>54.93</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table IV</head><label>IV</label><figDesc></figDesc><table><row><cell>(b), where</cell></row><row><cell>the TRANS4SOAR (Base) surpasses Skeleton-DML [20] and</cell></row><row><cell>SL-DML [21] by 16.62% and 22.38% on NTU-60 [55],</cell></row><row><cell>and 12.48% and 21.22% on Toyota Smart Home [3], while</cell></row><row><cell>TRANS4SOAR (Small) also shows competitive performances.</cell></row><row><cell>We also conduct experiments by using different Sigal-to-</cell></row><row><cell>Noise Ratio (SNR) range for the SOAR with RE as depicted</cell></row><row><cell>in Table IX, TRANS4SOAR shows promising and stable</cell></row><row><cell>performance &gt; 56% in terms of accuracy considering both</cell></row><row><cell>TRANS4SOAR (Base) and TRANS4SOAR (Small) for three</cell></row><row><cell>SNR ranges, i.e., 0.05 ? 0.2, 0.05 ? 0.35 and 0.05 ? 0.5 on</cell></row><row><cell>NTU-60 [55].</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE IX EXPERIMENTS</head><label>IX</label><figDesc>WITH DIFFERENT REALISTIC SYNTHESIZED OCCLUSION RATIO ON THE NTU-60 [55] FOR THE SOAR.</figDesc><table><row><cell>Model</cell><cell cols="3">RE_Range Accuracy F1</cell><cell cols="2">Precision Recall</cell></row><row><cell>SL-DML [21]</cell><cell></cell><cell>36.90</cell><cell cols="2">35.86 36.59</cell><cell>37.05</cell></row><row><cell>Skeleton-DML [20]</cell><cell></cell><cell>35.15</cell><cell cols="2">32.59 34.29</cell><cell>35.22</cell></row><row><cell>SL-DML (LeViT [25]) SL-DML (LeViT) + LSC</cell><cell>0.05-0.2</cell><cell>52.72 53.79</cell><cell cols="2">52.19 54.90 52.76 54.18</cell><cell>52.86 53.88</cell></row><row><cell>Trans4SOAR (Small)</cell><cell></cell><cell>56.84</cell><cell cols="2">55.84 58.27</cell><cell>56.98</cell></row><row><cell>Trans4SOAR (Base)</cell><cell></cell><cell>59.28</cell><cell cols="2">58.96 59.91</cell><cell>59.40</cell></row><row><cell>SL-DML [21]</cell><cell></cell><cell>39.26</cell><cell cols="2">38.71 39.59</cell><cell>39.43</cell></row><row><cell>Skeleton-DML [20]</cell><cell></cell><cell>38.52</cell><cell cols="2">38.74 39.23</cell><cell>38.64</cell></row><row><cell>SL-DML (LeViT [25]) SL-DML (LeViT) + LSC</cell><cell>0.05-0.35</cell><cell>53.17 53.58</cell><cell cols="2">52.52 54.16 52.75 54.07</cell><cell>53.34 53.77</cell></row><row><cell>Trans4SOAR (Small)</cell><cell></cell><cell>61.69</cell><cell cols="2">61.60 64.01</cell><cell>61.81</cell></row><row><cell>Trans4SOAR (Base)</cell><cell></cell><cell>58.27</cell><cell cols="2">56.63 58.81</cell><cell>58.40</cell></row><row><cell>SL-DML [21]</cell><cell></cell><cell>34.89</cell><cell cols="2">32.63 31.85</cell><cell>35.07</cell></row><row><cell>Skeleton-DML [20]</cell><cell></cell><cell>42.83</cell><cell cols="2">42.33 42.46</cell><cell>42.93</cell></row><row><cell>SL-DML (LeViT [25]) SL-DML (LeViT) + LSC</cell><cell>0.05-0.5</cell><cell>54.84 55.07</cell><cell cols="2">54.07 57.06 55.01 57.56</cell><cell>54.99 55.21</cell></row><row><cell>Trans4SOAR (Small)</cell><cell></cell><cell>59.59</cell><cell cols="2">59.21 59.49</cell><cell>59.70</cell></row><row><cell>Trans4SOAR (Base)</cell><cell></cell><cell>57.52</cell><cell cols="2">57.21 59.61</cell><cell>57.64</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE X</head><label>X</label><figDesc></figDesc><table><row><cell cols="7">EXPERIMENTS FOR REFERENCE W/ OR W/O OCCLUSIONS ON</cell></row><row><cell></cell><cell></cell><cell cols="2">NTU-60 [55].</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="4">OCC OCCVal Accuracy F1</cell><cell>Precision</cell><cell>Recall</cell></row><row><cell>SL-MDL [21]</cell><cell></cell><cell></cell><cell>48.74</cell><cell>46.46</cell><cell>47.45</cell><cell>48.88</cell></row><row><cell>Skeleton-DML [20]</cell><cell></cell><cell></cell><cell>49.30</cell><cell>48.57</cell><cell>49.62</cell><cell>49.45</cell></row><row><cell>SL-DML (LeViT [25]) SL-DML (LeViT) + LSC</cell><cell>RA</cell><cell>T</cell><cell>53.47 53.57</cell><cell>52.35 53.73</cell><cell>54.94 56.55</cell><cell>53.63 53.72</cell></row><row><cell>Trans4SOAR (Small)</cell><cell></cell><cell></cell><cell>72.16</cell><cell>72.42</cell><cell>73.67</cell><cell>72.23</cell></row><row><cell>Trans4SOAR (Base)</cell><cell></cell><cell></cell><cell>71.59</cell><cell>72.22</cell><cell>73.95</cell><cell>71.67</cell></row><row><cell>SL-DML [21]</cell><cell></cell><cell></cell><cell>45.28</cell><cell>43.13</cell><cell>45.00</cell><cell>45.42</cell></row><row><cell>Skeleton-DML [20]</cell><cell></cell><cell></cell><cell>60.43</cell><cell>59.66</cell><cell>61.37</cell><cell>60.54</cell></row><row><cell>SL-DML (LeViT [25]) SL-DML (LeViT) + LSC</cell><cell>RA</cell><cell>F</cell><cell>56.73 60.78</cell><cell>55.89 58.75</cell><cell>57.57 59.97</cell><cell>56.85 60.90</cell></row><row><cell>Trans4SOAR (Small)</cell><cell></cell><cell></cell><cell>67.90</cell><cell>67.32</cell><cell>68.94</cell><cell>68.01</cell></row><row><cell>Trans4SOAR (Base)</cell><cell></cell><cell></cell><cell>72.59</cell><cell>71.82</cell><cell>73.89</cell><cell>72.66</cell></row><row><cell>SL-DML [21]</cell><cell></cell><cell></cell><cell>36.90</cell><cell>35.86</cell><cell>36.59</cell><cell>37.05</cell></row><row><cell>Skeleton-DML [20]</cell><cell></cell><cell></cell><cell>42.66</cell><cell>40.90</cell><cell>41.50</cell><cell>42.82</cell></row><row><cell>SL-DML (LeViT [25]) SL-DML (LeViT) + LSC</cell><cell>RE</cell><cell>T</cell><cell>52.72 53.79</cell><cell>52.19 52.76</cell><cell>54.90 54.18</cell><cell>52.86 53.88</cell></row><row><cell>Trans4SOAR (Small)</cell><cell></cell><cell></cell><cell>56.84</cell><cell>55.84</cell><cell>58.27</cell><cell>56.98</cell></row><row><cell>Trans4SOAR (Base)</cell><cell></cell><cell></cell><cell>59.28</cell><cell>58.96</cell><cell>59.91</cell><cell>59.40</cell></row><row><cell>SL-DML [21]</cell><cell></cell><cell></cell><cell>39.51</cell><cell>39.64</cell><cell>40.82</cell><cell>39.64</cell></row><row><cell>Skeleton-DML [20] SL-DML (LeViT [25])</cell><cell>RE</cell><cell>F</cell><cell>44.29 55.12</cell><cell>43.10 55.22</cell><cell>44.26 57.51</cell><cell>44.46 55.26</cell></row><row><cell>SL-DML (LeViT) + LSC</cell><cell></cell><cell></cell><cell>55.07</cell><cell>55.01</cell><cell>57.56</cell><cell>55.21</cell></row><row><cell>Trans4SOAR (Small)</cell><cell></cell><cell></cell><cell>54.37</cell><cell>52.97</cell><cell>55.08</cell><cell>54.38</cell></row><row><cell>Trans4SOAR (Base)</cell><cell></cell><cell></cell><cell>58.48</cell><cell>57.10</cell><cell>57.75</cell><cell>58.61</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>1, 0.2 and 0.3, of the RA, are done inTable VIII, where the performances of TRANS4DARC (Base) and TRANS4DARC (Small) surpass all the leveraged approaches. The experiments are done with occlusion on reference set. Especially for SNR = 0.1 and SNR = 0.3, TRANS4DARC (Base) achieves 72.59% and 72.39% for accuracy while the Skeleton-DML<ref type="bibr" target="#b24">[20]</ref> only achieves 60.43% and 58.93%. Due to the large disruption by using SNR = 0.5 with RA, the performance of TRANS4SOAR (Base) only achieves 54.82% accuracy while still outperforming the state-of-the-art approach by 10.13%.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE XI EXPERIMENTS</head><label>XI</label><figDesc>FOR RANDOM TEMPORAL AND SPATIAL OCCLUSION.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE XII A</head><label>XII</label><figDesc>COMPARISON TO OTHER ENCODER ARCHITECTURES.</figDesc><table><row><cell>Methods</cell><cell cols="2">Accuracy F1</cell><cell cols="2">Recall Precision</cell></row><row><cell>SL-DML (CTR-GCN[5])</cell><cell>43.92</cell><cell cols="2">41.38 45.21</cell><cell>43.89</cell></row><row><cell>SL-DML (STTR[10])</cell><cell>39.56</cell><cell cols="2">39.45 41.92</cell><cell>39.58</cell></row><row><cell>SL-DML (LeViT) + LSC</cell><cell>55.94</cell><cell cols="2">54.29 55.80</cell><cell>56.04</cell></row><row><cell>Trans4SOAR (Small)</cell><cell>56.27</cell><cell cols="2">56.43 58.59</cell><cell>56.32</cell></row><row><cell>Trans4SOAR (Base)</cell><cell>57.05</cell><cell cols="2">55.90 57.26</cell><cell>57.12</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Model (a) Random temporal occlusion (b) Random spatial occlusion Acc. F1</title>
	</analytic>
	<monogr>
		<title level="m">Prec. Rec. Acc. F1. Prec. Rec. Experiments on NTU-120 with random temporal occlusion</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Trans4soar</surname></persName>
		</author>
		<idno>Small) 51.60 50.73 52.65 50.99 46.99 46.24 49.71 47.07</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Trans4soar</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Experiments on Toyota Smart Home with random temporal occlusion</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Trans4soar</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pix3D: Dataset and methods for single-image 3D shape modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">NTU RGB+D 120: A large-scale benchmark for 3D human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Toyota smarthome: Real-world activities of daily living</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">STST: Spatial-temporal specialized transformer for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Channelwise topology refinement graph convolution for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-second AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Hierarchical graph convolutional skeleton transformer for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.02860</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Motion-transformer: Self-supervised pre-training for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<editor>MMAsia</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Action transformer: A self-attention model for short-time human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mazzia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Angarano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Salvetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Angelini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chiaberge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition via spatial and temporal transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Plizzari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cannici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matteucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Image Understanding</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Spatial temporal transformer network for skeleton-based action recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Few-shot action recognition with implicit temporal alignment and pair similarity optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A pairwise attentive adversarial spatiotemporal network for cross-domain few-shot action recognition-R2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Video pose distillation for few-shot, fine-grained sports action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gharbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fatahalian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised fewshot action recognition via action-appearance aligned meta-adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Patravali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Temporal-relational CrossTransformers for few-shot action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Masullo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Burghardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirmehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Richly activated graph convolutional network for action recognition with incomplete skeletons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Richly activated graph convolutional network for robust skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">OSD: An occlusion skeleton dataset for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feng</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in Big Data, 2020</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Skeleton-DML: Deep metric learning for skeleton-based one-shot action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Memmesheimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>H?ring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Theisen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paulus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>WACV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">SL-DML: Signal level deep metric learning for multimodal one-shot action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Memmesheimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Theisen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paulus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hierarchical temporal memory enhanced one-shot distance learning for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICME</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adaptationoriented feature projection for one-shot action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">ViVit: A video vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">LeViT: a vision transformer in ConvNet&apos;s clothing for faster inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">One-shot image classification by learning to restore prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Meta-reinforced synthetic data for one-shot fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tsutsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Crandall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pushing the limits of simple pipelines for few-shot learning: External data and fine-tuning make a difference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>St?hmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Squeezing backbone feature distributions to the max for efficient few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pateux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gripon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithms</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">EASY: Ensemble augmented-shot y-shaped learning: State-of-the-art few-shot classification with simple ingredients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bendou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.09699</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Trans4Trans: Efficient transformer for transparent object segmentation to help visually impaired people navigate in the real world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Constantinescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCVW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Semantic-guided relation propagation network for fewshot action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">One-shot action recognition in challenging therapy scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sabater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Santos-Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bernardino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Montesano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Murillo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Skeletonbased action recognition using spatio-temporal LSTM network with trust gates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Global contextaware attention LSTM networks for 3D action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Twins: Revisiting the design of spatial attention in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Going deeper with image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">ResT: An efficient transformer for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">GroupFormer: Group activity recognition with clustered spatial-temporal transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">VidTr: Video transformer without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Trans-DARC: Transformer-based driver activity recognition with latent space feature calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roitberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">STAR: Sparse transformer-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.07089</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">RMPE: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">CrowdPose: Efficient crowded scenes pose estimation and a new benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Pose Flow: Efficient online pose tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Partially occluded skeleton action recognition based on multi-stream fusion graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CGI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Generalized graph convolutional networks for action recognition with occluded skeletons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<editor>ICCPR</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Quantification of occlusion handling capability of 3D human pose estimation framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghafoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahmood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">2D posebased real-time human action recognition with occlusion-handling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Angelini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Naqvi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Robust 3D action recognition through sampling local appearances and global distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">NTU RGB+D: A large scale dataset for 3D human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">MixFormer: End-to-end tracking with iterative mixed attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">FeatMatch: Featurebased augmentation for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">One weird trick for parallelizing convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.5997</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m">SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt;0.5MB model size</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
