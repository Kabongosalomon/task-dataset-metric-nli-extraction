<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FoldingNet: Point Cloud Auto-encoder via Deep Grid Deformation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoqing</forename><surname>Yang</surname></persName>
							<email>yyaoqing@andrew.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Feng</surname></persName>
							<email>cfeng@merl.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Mitsubishi Electric Research Laboratories (MERL)</orgName>
								<orgName type="institution">Clemson University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiru</forename><surname>Shen</surname></persName>
							<email>yirus@g.clemson.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Tian</surname></persName>
							<email>tian@merl.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Mitsubishi Electric Research Laboratories (MERL)</orgName>
								<orgName type="institution">Clemson University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FoldingNet: Point Cloud Auto-encoder via Deep Grid Deformation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent deep networks that directly handle points in a point set, e.g., PointNet, have been state-of-the-art for supervised learning tasks on point clouds such as classification and segmentation. In this work, a novel end-toend deep auto-encoder is proposed to address unsupervised learning challenges on point clouds. On the encoder side, a graph-based enhancement is enforced to promote local structures on top of PointNet. Then, a novel folding-based decoder deforms a canonical 2D grid onto the underlying 3D object surface of a point cloud, achieving low reconstruction errors even for objects with delicate structures. The proposed decoder only uses about 7% parameters of a decoder with fully-connected neural networks, yet leads to a more discriminative representation that achieves higher linear SVM classification accuracy than the benchmark. In addition, the proposed decoder structure is shown, in theory, to be a generic architecture that is able to reconstruct an arbitrary point cloud from a 2D grid. Our code is available at http://www.merl.com/research/ license#FoldingNet</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>3D point cloud processing and understanding are usually deemed more challenging than 2D images mainly due to a fact that point cloud samples live on an irregular structure while 2D image samples (pixels) rely on a 2D grid in the image plane with a regular spacing. Point cloud geometry is typically represented by a set of sparse 3D points. Such a data format makes it difficult to apply traditional deep learning framework. E.g. for each sample, traditional convolutional neural network (CNN) requires its neighboring samples to appear at some fixed spatial orientations and distances so as to facilitate the convolution. Unfortunately, point cloud samples typically do not follow such constraints. One way to alleviate the problem is to voxelize a point cloud to mimic the image representation and then to operate on voxels. The downside is that voxelization has to either sacrifice the representation accuracy or incurs huge redundancies, that may pose an unnecessary cost in the sub-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>2D grid 1st folding 2nd folding <ref type="table">Table 1</ref>. Illustration of the two-step-folding decoding. Column one contains the original point cloud samples from the ShapeNet dataset <ref type="bibr" target="#b56">[57]</ref>. Column two illustrates the 2D grid points to be folded during decoding. Column three contains the output after one folding operation. Column four contains the output after two folding operations. This output is also the reconstructed point cloud. We use a color gradient to illustrate the correspondence between the 2D grid in column two and the reconstructed point clouds after folding operations in the last two columns. Best viewed in color. sequent processing, either at a compromised performance or an rapidly increased processing complexity. Related priorarts will be reviewed in Section 1.1.</p><p>In this work, we focus on the emerging field of unsupervised learning for point clouds. We propose an autoencoder (AE) that is referenced as FoldingNet. The output from the bottleneck layer in the auto-encoder is called a codeword that can be used as a high-dimensional embedding of an input point cloud. We are going to show that a 2D grid structure is not only a sampling structure for imaging, but can indeed be used to construct a point cloud through arXiv:1712.07262v2 [cs.CV] 3 Apr 2018  <ref type="figure">Figure 1</ref>. FoldingNet Architecture. The graph-layers are the graph-based max-pooling layers mentioned in <ref type="bibr" target="#b1">(2)</ref> in Section 2.1. The 1st and the 2nd folding are both implemented by concatenating the codeword to the feature vectors followed by a 3-layer perceptron. Each perceptron independently applies to the feature vector of a single point as in <ref type="bibr" target="#b40">[41]</ref>, i.e., applies to the rows of the m-by-k matrix.</p><p>the proposed folding operation. This is based on the observation that the 3D point clouds of our interest are obtained from object surfaces: either discretized from boundary representations in CAD/computer graphics, or sampled from line-of-sight sensors like LIDAR. Intuitively, any 3D object surface could be transformed to a 2D plane through certain operations like cutting, squeezing, and stretching. The inverse procedure is to glue those 2D point samples back onto an object surface via certain folding operations, which are initialized as 2D grid samples. As illustrated in <ref type="table">Table 1</ref>, to reconstruct a point cloud, successive folding operations are joined to reproduce the surface structure. The points are colorized to show the correspondence between the initial 2D grid samples and the reconstructed 3D point samples. Using the folding-based method, the challenges from the irregular structure of point clouds are well addressed by directly introducing such an implicit 2D grid constraint in the decoder, which avoids the costly 3D voxelization in other works <ref type="bibr" target="#b55">[56]</ref>. It will be demonstrated later that the folding operations can build an arbitrary surface provided a proper codeword. Notice that when data are from volumetric format instead of 2D surfaces, a 3D grid may perform better.</p><p>Despite being strongly expressive in reconstructing point clouds, the folding operation is simple: it is started by augmenting the 2D grid points with the codeword obtained from the encoder, which is then processed through a 3layer perceptron. The proposed decoder is simply a concatenation of two folding operations. This design makes the proposed decoder much smaller in parameter size than the fully-connected decoder proposed recently in <ref type="bibr" target="#b0">[1]</ref>. In Section 4.6, we show that the number of parameters of our folding-based decoder is about 7% of the fully connected decoder in <ref type="bibr" target="#b0">[1]</ref>. Although the proposed decoder has a sim-ple structure, we theoretically show in Theorem 3.2 that this folding-based structure is universal in that one folding operation that uses only a 2-layer perceptron can already reproduce arbitrary point-cloud structure. Therefore, it is not surprising that our FoldingNet auto-encoder exploiting two consecutive folding operations can produce elaborate structures.</p><p>To show the efficiency of FoldingNet auto-encoder for unsupervised representation learning, we follow the experimental settings in <ref type="bibr" target="#b0">[1]</ref> and test the transfer classification accuracy from ShapeNet dataset <ref type="bibr" target="#b6">[7]</ref> to ModelNet dataset <ref type="bibr" target="#b56">[57]</ref>. The FoldingNet auto-encoder is trained using ShapeNet dataset, and tested out by extracting codewords from Mod-elNet dataset. Then, we train a linear SVM classifier to test the discrimination effectiveness of the extracted codewords. The transfer classification accuracy is 88.4% on the ModelNet dataset with 40 shape categories. This classification accuracy is even close to the state-of-the-art supervised training result <ref type="bibr" target="#b40">[41]</ref>. To achieve the best classification performance and least reconstruction loss, we use a graphbased encoder structure that is different from <ref type="bibr" target="#b40">[41]</ref>. This graph-based encoder is based on the idea of local feature pooling operations and is able to retrieve and propagate local structural information along the graph structure.</p><p>To intuitively interpret our network design: we want to impose a "virtual force" to deform/cut/stretch a 2D grid lattice onto a 3D object surface, and such a deformation force should be influenced or regulated by interconnections induced by the lattice neighborhood. Since the intermediate folding steps in the decoder and the training process can be illustrated by reconstructed points, the gradual change of the folding forces can be visualized. Now we summarize our contributions in this work:</p><p>? We train an end-to-end deep auto-encoder that consumes unordered point clouds directly. ? We propose a new decoding operation called folding and theoretically show it is universal in point cloud reconstruction, while providing orders to reconstructed points as a unique byproduct than other methods. ? We show by experiments on major datasets that folding can achieve higher classification accuracy than other unsupervised methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related works</head><p>Applications of learning on point clouds include shape completion and recognition <ref type="bibr" target="#b56">[57]</ref>, unmanned autonomous vehicles <ref type="bibr" target="#b35">[36]</ref>, 3D object detection, recognition and classification <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b52">53]</ref>, contour detection <ref type="bibr" target="#b20">[21]</ref>, layout inference <ref type="bibr" target="#b17">[18]</ref>, scene labeling <ref type="bibr" target="#b30">[31]</ref>, category discovery <ref type="bibr" target="#b59">[60]</ref>, point classification, dense labeling and segmentation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b57">58]</ref>, Most deep neural networks designed for 3D point clouds are based on the idea of partitioning the 3D space into regular voxels and extending 2D CNNs to voxels, such as <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b36">37]</ref>, including the the work on 3D generative adversarial network <ref type="bibr" target="#b55">[56]</ref>. The main problem of voxel-based networks is the fast growth of neural-network size with the increasing spatial resolution. Some other options include octree-based <ref type="bibr" target="#b43">[44]</ref> and kd-tree-based <ref type="bibr" target="#b28">[29]</ref> neural networks. Recently, it is shown that neural networks based on purely 3D point representations <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref> work quite efficiently for point clouds. The point-based neural networks can reduce the overhead of converting point clouds into other data formats (such as octrees and voxels), and in the meantime avoid the information loss due to the conversion.</p><p>The only work that we are aware of on end-to-end deep auto-encoder that directly handles point clouds is <ref type="bibr" target="#b0">[1]</ref>. The AE designed in <ref type="bibr" target="#b0">[1]</ref> is for the purpose of extracting features for generative networks. To encode, it sorts the 3D points using the lexicographic order and applies a 1D CNN on the point sequence. To decode, it applies a three-layer fully connected network. This simple structure turns out to outperform all existing unsupervised works on representation extraction of point clouds in terms of the transfer classification accuracy from the ShapeNet dataset to the ModelNet dataset <ref type="bibr" target="#b0">[1]</ref>. Our method, which has a graph-based encoder and a folding-based decoder, outperforms this method in transfer classification accuracy on the ModelNet40 dataset <ref type="bibr" target="#b0">[1]</ref>. Moreover, compared to <ref type="bibr" target="#b0">[1]</ref>, our AE design is more interpretable: the encoder learns the local shape information and combines information by max-pooling on a nearestneighbor graph, and the decoder learns a "force" to fold a two-dimensional grid twice in order to warp the grid into the shape of the point cloud, using the information obtained by the encoder. Another closely related work reconstructs a point set from a 2D image <ref type="bibr" target="#b16">[17]</ref>. Although the deconvolution network in <ref type="bibr" target="#b16">[17]</ref> requires a 2D image as side information, we find it useful as another implementation of our folding operation. We compare FoldingNet with the deconvolutionbased folding and show that FoldingNet performs slightly better in reconstruction error with fewer parameters (see Supplementary Section 9).</p><p>It is hard for purely point-based neural networks to extract local neighborhood structure around points, i.e., features of neighboring points instead of individual ones. Some attempts for this are made in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b41">42]</ref>. In this work, we exploit local neighborhood features using a graph-based framework. Deep learning on graph-structured data is not a new idea. There are tremendous amount of works on applying deep learning onto irregular data such as graphs and point sets <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b58">59]</ref>. Although using graphs as a processing framework for deep learning on point clouds is a natural idea, only several seminal works made attempts in this direction <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b46">47]</ref>. These works try to generalize the convolution operations from 2D images to graphs. However, since it is hard to define convolution operations on graphs, we use a simple graph-based neural network layer that is different from previous works: we construct the K-nearest neighbor graph (K-NNG) and repeatedly conduct the max-pooling operations in each node's neighborhood. It generalizes the global maxpooling operation proposed in <ref type="bibr" target="#b40">[41]</ref> in that the max-pooling is only applied to each local neighborhood to generate local data signatures. Compared to the above graph based convolution networks, our design is simpler and computationally efficient as in <ref type="bibr" target="#b40">[41]</ref>. K-NNGs are also used in other applications of point clouds without the deep learning framework such as surface detection, 3D object recognition, 3D object segmentation and compression <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51]</ref>.</p><p>The folding operation that reconstructs a surface from a 2D grid essentially establishes a mapping from a 2D regular domain to a 3D point cloud. A natural question to ask is whether we can parameterize 3D points with compatible meshes that are not necessarily regular grids, such as cross-parametrization <ref type="bibr" target="#b29">[30]</ref>. From <ref type="table">Table 2</ref>, it seems that FoldingNet can learn to generate "cuts" on the 2D grid and generate surfaces that are not even topologically equivalent to a 2D grid, and hence make the 2D grid representation universal to some extent. Nonetheless, the reconstructed points may still have genus-wise distortions when the original surface is too complex. For example, in <ref type="table">Table 2</ref>, see the missing winglets on the reconstructed plane and the missing holes on the back of the reconstructed chair. To recover those finer details might require more input point samples and more complex encoder/decoder networks. Another method to learn the surface embedding is to learn a metric alignment layer as in <ref type="bibr" target="#b15">[16]</ref>, which may require computationally intensive internal optimization during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Preliminaries and Notation</head><p>We will often denote the point set by S. We use bold lower-case letters to represent vectors, such as x, and use bold upper-case letters to represent matrices, such as A. The codeword is always represented by ?. We call a matrix m-by-n or m ? n if it has m rows and n columns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">FoldingNet Auto-encoder on Point Clouds</head><p>Now we propose the FoldingNet deep auto-encoder. The structure of the auto-encoder is shown in <ref type="figure">Figure 1</ref>. The input to the encoder is an n-by-3 matrix. Each row of the matrix is composed of the 3D position (x, y, z). The output is an m-by-3 matrix, representing the reconstructed point positions. The number of reconstructed points m is not necessarily the same as n. Suppose the input contains the point set S and the reconstructed point set is the set S. Then, the reconstruction error for S is computed using a layer defined as the (extended) Chamfer distance,</p><formula xml:id="formula_0">d CH (S, S) = max 1 |S| x?S min x? S x ? x 2 , 1 | S| x? S min x?S x ? x 2 ? ? ? .<label>(1)</label></formula><p>The term min x? S x ? x 2 enforces that any 3D point x in the original point cloud has a matching 3D point x in the reconstructed point cloud, and the term min x?S x ? x 2 enforces the matching vice versa. The max operation enforces that the distance from S to S and the distance vice versa have to be small simultaneously. The encoder computes a representation (codeword) of each input point cloud and the decoder reconstructs the point cloud using this codeword. In our experiments, the codeword length is set as 512 in accordance with [1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Graph-based Encoder Architecture</head><p>The graph-based encoder follows a similar design in <ref type="bibr" target="#b45">[46]</ref> which focuses on supervised learning using point cloud neighborhood graphs. The encoder is a concatenation of multi-layer perceptrons (MLP) and graph-based maxpooling layers. The graph is the K-NNG constructed from the 3D positions of the nodes in the input point set. In experiments, we choose K = 16. First, for every single point v, we compute its local covariance matrix of size 3-by-3 and vectorize it to size 1-by-9. The local covariance of v is computed using the 3D positions of the points that are one-hop neighbors of v (including v) in the K-NNG. We concatenate the matrix of point positions with size n-by-3 and the local covariances for all points of size n-by-9 into a matrix of size n-by-12 and input them to a 3-layer perceptron. The perceptron is applied in parallel to each row of the input matrix of size n-by-12. It can be viewed as a per-point function on each 3D point. The output of the perceptron is fed to two consecutive graph layers, where each layer applies max-pooling to the neighborhood of each node. More specifically, suppose the K-NN graph has adjacency matrix A and the input matrix to the graph layer is X. Then, the output matrix is</p><formula xml:id="formula_1">Y = A max (X)K,<label>(2)</label></formula><p>where K is a feature mapping matrix, and the (i,j)-th entry of the matrix A max (X) is</p><formula xml:id="formula_2">(A max (X)) ij = ReLU( max k?N (i) x kj ).<label>(3)</label></formula><p>The local max-pooling operation max k?N (i) in <ref type="formula" target="#formula_2">(3)</ref> essentially computes a local signature based on the graph structure. This signature can represent the (aggregated) topology information of the local neighborhood. Through concatenations of the graph-based max-pooling layers, the network propagates the topology information into larger areas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Folding-based Decoder Architecture</head><p>The proposed decoder uses two consecutive 3-layer perceptrons to warp a fixed 2D grid into the shape of the input point cloud. The input codeword is obtained from the graph-based encoder. Before we feed the codeword into the decoder, we replicate it m times and concatenate the m-by-512 matrix with an m-by-2 matrix that contains the m grid points on a square centered at the origin. The result of the concatenation is a matrix of size m-by-514. The matrix is processed row-wise by a 3-layer perceptron and the output is a matrix of size m-by-3. After that, we again concatenate the replicated codewords to the m-by-3 output and feed it into a 3-layer perceptron. This output is the reconstructed point cloud. The parameter n is set as per the input point cloud size, e.g. n = 2048 in our experiments, which is the same as <ref type="bibr" target="#b0">[1]</ref>.We choose m grid points in a square, so m is chosen as 2025 which is the closest square number to 2048. Definition 1. We call the concatenation of replicated codewords to low-dimensional grid points, followed by a pointwise MLP a folding operation.</p><p>The folding operation essentially forms a universal 2Dto-3D mapping. To intuitively see why this folding operation is a universal 2D-to-3D mapping, denote the input 2D grid points by the matrix U. Each row of U is a twodimensional grid point. Denote the i-th row of U by u i and the codeword output from the encoder by ?. Then, after concatenation, the i-th row of the input matrix to the MLP is [u i , ?]. Since the MLP is applied in parallel to each row of the input matrix, the i-th row of the output matrix can be written as f ([u i , ?]), where f indicates the function conducted by the MLP. This function can be viewed as a parameterized high-dimensional function with the codeword ? being a parameter to guide the structure of the function (the folding operation). Since MLPs are good at approximating non-linear functions, they can perform elaborate folding operations on the 2D grids. The high-dimensional codeword essentially stores the force that is needed to do the folding, which makes the folding operation more diverse.</p><p>The proposed decoder has two successive folding operations. The first one folds the 2D grid to 3D space, and the second one folds inside the 3D space. We show the outputs after these two folding operations in <ref type="table">Table 1</ref>. From column C and column D in <ref type="table">Table 1</ref>, we can see that each folding operation conducts a relatively simple operation, and the composition of the two folding operations can produce quite elaborate surface shapes. Although the first folding seems simpler than the second one, together they lead to substantial changes in the final output. More successive folding operations can be applied if more elaborate surface shapes are required. More variations of the decoder including changes of grid dimensions and the number of folding operations can be found in Supplementary Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Theoretical Analysis</head><p>Theorem 3.1. The proposed encoder structure is permutation invariant, i.e., if the rows of the input point cloud matrix are permuted, the codeword remains unchanged.</p><p>Proof. See Supplementary Section 6.</p><p>Then, we state a theorem about the universality of the proposed folding-based decoder. It shows the existence of a folding-based decoder such that by changing the codeword ?, the output can be an arbitrary point cloud.</p><p>Theorem 3.2. There exists a 2-layer perceptron that can reconstruct arbitrary point clouds from a 2-dimensional grid using the folding operation.</p><p>More specifically, suppose the input is a matrix U of size m-by-2 such that each row of U is the 2D position of a point on a 2-dimensional grid of size m. Then, there exists an explicit construction of a 2-layer perceptron (with handcrafted coefficients) such that for any arbitrary 3D point cloud matrix S of size m-by-3 (where each row of S is the (x, y, z) position of a point in the point cloud), there exists a codeword vector ? such that if we concatenate ? to each row of U and apply the 2-layer perceptron in parallel to each row of the matrix after concatenation, we obtain the point cloud matrix S from the output of the perceptron.</p><p>Proof in sketch. The full proof is in Supplementary Section 7. In the proof, we show the existence by explicitly constructing a 2-layer perceptron that satisfies the stated properties. The main idea is to show that in the worst case, the points in the 2D grid functions as a selective logic gate to map the 2D points in the 2D grid to the corresponding 3D points in the point cloud.</p><p>Notice that the above proof is just an existence-based one to show that our decoder structure is universal. It does not indicate what happens in reality inside the FoldingNet autoencoder. The theoretically constructed decoder requires 3m hidden units while in reality, the size of the decoder that we use is much smaller. Moreover, the construction in Theorem 3.2 leads to a lossless reconstruction of the point cloud, while the FoldingNet auto-encoder only achieves lossy reconstruction. However, the above theorem can indeed guarantee that the proposed decoding operation (i.e., concatenating the codewords to the 2-dimensional grid points and processing each row using a perceptron) is legitimate because in the worst case there exists a folding-based neural network with hand-crafted edge weights that can reconstruct arbitrary point clouds. In reality, a good parameterization of the proposed decoder with suitable training leads to better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Visualization of the Training Process</head><p>It might not be straightforward to see how the decoder folds the 2D grid into the surface of a 3D point cloud. Therefore, we include an illustration of the training process to show how a random 2D manifold obtained by the initial random folding gradually turns into a meaningful point cloud. The auto-encoder is a single FoldingNet trained using the ShapeNet part dataset <ref type="bibr" target="#b57">[58]</ref> which contains 16 categories of the ShapeNet dataset. We trained the FoldingNet using ADAM with an initial learning rate 0.0001, batch size 1, momentum 0.9, momentum2 0.999, and weight decay 1e?6, for 4 ? 10 6 iterations (i.e., 330 epochs). The reconstructed point clouds of several models after different numbers of training iterations are reported in <ref type="table">Table 2</ref>. From the training process, we see that an initial random 2D manifold can be warped/cut/squeezed/stretched/attached to form the point cloud surface in various ways.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Point Cloud Interpolation</head><p>A common method to demonstrate that the codewords have extracted the natural representations of the input is to see if the auto-encoder enables meaningful novel interpolations between two inputs in the dataset. In <ref type="table">Table 3</ref>, we show both inter-class and intra-class interpolations. Note that we used a single AE for all shape categories for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Illustration of Point Cloud Clustering</head><p>We also provide an illustration of clustering 3D point clouds using the codewords obtained from FoldingNet. We used the ShapeNet dataset to train the AE and obtain codewords for the ModelNet10 dataset, which we will explain in details in Section 4.4. Then, we used T-SNE <ref type="bibr" target="#b33">[34]</ref> to obtain an embedding of the high-dimensional codewords in Input 5K iters 10K iters 20K iters 40K iters 100K iters 500K iters 4M iters <ref type="table">Table 2</ref>. Illustration of the training process. Random 2D manifolds gradually transform into the surfaces of point clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source</head><p>Interpolations <ref type="table">Target   Table 3</ref>. Illustration of point cloud interpolation. The first 3 rows: intra-class interpolations. The last 3 rows: inter-class interpolations. We have visually checked these two pairs of classes, and found that many pairs cannot be easily distinguished even by a human. In <ref type="table" target="#tab_0">Table 4</ref>, we list the most common mistakes made in classifying the ModelNet10 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Transfer Classification Accuracy</head><p>In this section, we show the efficiency of FoldingNet in representation learning and feature extraction from 3D point clouds. In particular, we follow the routine from <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b55">56]</ref> to train a linear SVM classifier on the ModelNet dataset <ref type="bibr" target="#b56">[57]</ref> using the codewords (latent representations) obtained from the auto-encoder, while training the autoencoder from the ShapeNet dataset <ref type="bibr" target="#b6">[7]</ref>. The train/test splits  of the ModelNet dataset in our experiment is the same as in <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b55">56]</ref>. The point-cloud-format of the ShapeNet dataset is obtained by sampling random points on the triangles from the mesh models in the dataset. It contains 57447 models from 55 categories of man-made objects. The ModelNet datasets are the same one used in <ref type="bibr" target="#b40">[41]</ref>, and the MN40/MN10 datasets respectively contain 9843/3991 models for training and 2468/909 models for testing. Each point cloud in the selected datasets contains 2048 points with (x,y,z) positions normalized into a unit sphere as in <ref type="bibr" target="#b40">[41]</ref>. The codewords obtained from the FoldingNet autoencoder is of length 512, which is the same as in <ref type="bibr" target="#b0">[1]</ref> and smaller than 7168 in <ref type="bibr" target="#b56">[57]</ref>. When training the auto-encoder, we used ADAM with an initial learning rate of 0.0001 and batch size of 1. We trained the auto-encoder for 1.6 ? 10 7 iterations (i.e., 278 epochs) on the ShapeNet dataset. Similar to <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b40">41]</ref>, when training the AE, we applied random rotations to each point cloud. Unlike the random rotations in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b40">41]</ref>, we applied the rotation that is one of the 24 axisaligned rotations in the right-handed system. When training the linear SVM from the codewords obtained by the AE, we did not apply random rotations. We report our results in <ref type="table">Table 5</ref>. The results of <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b44">45]</ref> are according to the report in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b55">56]</ref>. Since the training of the AE and the training of the SVM are based on different datasets, the experiment shows the transfer robustness of the FoldingNet. We also include a figure (see <ref type="figure" target="#fig_3">Figure 3</ref>) to show how the reconstruction loss decreases and the linear SVM classification accuracy increases during training. From <ref type="table">Table 5</ref>, we can see that FoldingNet outperforms all other methods on the MN40 Method MN40 MN10 SPH <ref type="bibr" target="#b25">[26]</ref> 68.2% 79.8% LFD <ref type="bibr" target="#b7">[8]</ref> 75.5% 79.9% T-L Network <ref type="bibr" target="#b18">[19]</ref> 74.4% -VConv-DAE <ref type="bibr" target="#b44">[45]</ref> 75.5% 80.5% 3D-GAN <ref type="bibr" target="#b55">[56]</ref> 83.3% 91.0% Latent-GAN <ref type="bibr" target="#b0">[1]</ref> 85.7% 95.3% FoldingNet (ours) 88.4% 94.4% <ref type="table">Table 5</ref>. The comparison on classification accuracy between Fold-ingNet and other unsupervised methods. All the methods train a linear SVM on the high-dimensional representations obtained from unsupervised training. dataset. On the MN10 dataset, the auto-encoder proposed in <ref type="bibr" target="#b0">[1]</ref> performs slightly better. However, the point-cloud format of the ModelNet10 dataset used in <ref type="bibr" target="#b0">[1]</ref> is not public, so the point-cloud sampling protocol of ours may be different from the one in <ref type="bibr" target="#b0">[1]</ref>. So it is inconclusive whether <ref type="bibr" target="#b0">[1]</ref> is better than ours on MN10 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Semi-supervised Learning: What Happens when Labeled Data are Rare</head><p>One of the main motivations to study unsupervised classification problems is that the number of labeled data is usually much smaller compared to the number of unlabeled data. In Section 4.4, the experiment is very close to this setting: the number of data in the ShapeNet dataset is large, which is more than 5.74 ? 10 4 , while the number of data in the labeled ModelNet dataset is small, which is around 1.23 ? 10 4 . Since obtaining human-labeled data is usually hard, we would like to test how the performance of Fold-ingNet degrades when the number of labeled data is small. We still used the ShapeNet dataset to train the FoldingNet auto-encoder. Then, we trained the linear SVM using only a% of the overall training data in the ModelNet dataset, where a can be 1, 2, 5, 7.5, 10, 15, and 20. The test data for the linear SVM are always all the data in the test data partition of the ModelNet dataset. If the codewords obtained by the auto-encoder are already linearly separable, the re-  quired number of labeled data to train a linear SVM should be small. To demonstrate this intuitive statement, we report the experiment results in <ref type="figure" target="#fig_4">Figure 4</ref>. We can see that even if only 1% of the labeled training data are available (98 labeled training data, which is about 1?3 labeled data per class), the test accuracy is still more than 55%. When 20% of the training data are available, the test classification accuracy is already close to 85%, higher than most methods listed in <ref type="table">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Effectiveness of the Folding-Based Decoder</head><p>In this section, we show that the folding-based decoder performs better in extracting features than the fullyconnected decoder proposed in <ref type="bibr" target="#b0">[1]</ref> in terms of classification accuracy and reconstruction loss. We used the ModelNet40 dataset to train two deep auto-encoders. The first autoencoder uses the folding-based decoder that has the same structure as in Section 2.2, and the second auto-encoder uses a fully-connected three-layer perceptron as proposed in <ref type="bibr" target="#b0">[1]</ref>. For the fully-connected decoder, the number of inputs and number of outputs in the three layers are respectively {512,1024}, {1024,2048}, {2048,2048?3}, which are the same as in <ref type="bibr" target="#b0">[1]</ref>. The output is a 2048-by-3 ma-trix that contains the three-dimensional points in the output point cloud. The encoders of the two auto-encoders are both the graph-based encoder mentioned in Section 2.1. When training the AE, we used ADAM with an initial learning rate 0.0001, a batch size 1, for 4 ? 10 6 iterations (i.e., 406 epochs) on the ModelNet40 training dataset.</p><p>After training, we used the encoder to process all data in the ModelNet40 dataset to obtain a codeword for each point cloud. Then, similar to Section 4.4, we trained a linear SVM using these codewords and report the classification accuracy to see if the codewords are already linearly separable after encoding. The results are shown in <ref type="figure" target="#fig_5">Figure 5</ref>. During the training process, the reconstruction loss (measured in Chamfer distance) keeps decreasing, which means the reconstructed point cloud is more and more similar to the input point cloud. At the same time, the classification accuracy of the linear SVM trained on the codewords is increasing, which means the codeword representation becomes more linearly separable.</p><p>From the figure, we can see that the folding decoder almost always has a higher accuracy and lower reconstruction loss. Compared to the fully-connected decoder that relies on the unnatural "1D order" of the reconstructed 3D points in 3D space, the proposed decoder relies on the folding of an inherently 2D manifold corresponding to the point cloud inside the 3D space. As we mentioned earlier, this folding operation is more natural than the fully-connected decoder. Moreover, the number of parameters in the fully-connected decoder is 1.52 ? 10 7 , while the number of parameters in our folding decoder is 1.05 ? 10 6 , which is about 7% of the fully-connected decoder.</p><p>One may wonder if uniformly random sampled 2D points on a plane can perform better than the 2D grid points in reconstructing point clouds. From our experiments, 2D grid points indeed provide reduced reconstruction loss than random points ( <ref type="table">Table 6</ref> in Supplementary Section 8). Notice that our graph-based max-pooling encoder can be viewed as a generalized version of the maxpooling neural network PointNet <ref type="bibr" target="#b40">[41]</ref>. The main difference is that the pooling operation in our encoder is done in a local neighborhood instead of globally (see Section 2.1). In Supplementary Section 10, we show that the graph-based encoder architecture is better than an encoder architecture without the graph-pooling layers mentioned in Section 2.1 in terms of robustness towards random disturbance in point positions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Acknowledgment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Supplementary: Proof of Theorem 3.1</head><p>Denote the input n-by-12 matrix by L. Denote by ? the codeword obtained by the encoder. Now we prove if the input is PL where P is an n-by-n permutation matrix, the codeword obtained from the encoder is still ?.</p><p>The first part of the encoder is a per-point function, i.e., the 3-layer perceptron is applied to each row of the input matrix L. Denote the function by f 1 . Then, it is obvious that f 1 (PL) = Pf 1 (L). The second part computes <ref type="bibr" target="#b1">(2)</ref>. Now we prove that for <ref type="formula" target="#formula_1">(2)</ref>,</p><formula xml:id="formula_3">PY = A max (PX)K.<label>(4)</label></formula><p>Since Y = A max (X)K, we only need to prove</p><formula xml:id="formula_4">A max (PX) = PA max (X).<label>(5)</label></formula><p>Suppose the permutation operation P makes the i-th row of PX equal to x ?(i) , where ?(?) is a permutation function on the set of row indexes {1, 2, . . . , n}. Then, from <ref type="formula" target="#formula_2">(3)</ref>, the (i,j)-th entry of the matrix A max (PX) is</p><formula xml:id="formula_5">(A max (PX)) ij = ReLU( max k?N (?(i))</formula><p>x kj ).</p><p>In the meantime, the (?(i),j)-th entry of A max (PX) is</p><formula xml:id="formula_7">(A max (X)) ?(i)j = ReLU( max k?N (?(i))</formula><p>x kj ).</p><p>Since the right hand side of (6) and <ref type="bibr" target="#b6">(7)</ref> are the same, we know that the matrix A max (PX) can be obtained by changing the i-th row of A max (X) to the ?(i)-th row, which means A max (PX) = PA max (X). Thus, we have proved that for the second part of the encoder, permuting the input rows is equivalent to permuting the output rows, i.e., <ref type="bibr" target="#b3">(4)</ref> holds. Therefore, if we permute the input to the encoder, the output of the graph layers also permute. Then, we apply global max-pooling to the output of the graph layers. It is obvious that the result remains the same if the rows of the input to the global max-pooling layer (or the output of the graph layers) permute. The conclusion of Theorem 3.1 is hence proved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Supplementary: Proof of Theorem 3.2</head><p>We prove the existence-based Theorem 3.2 by explicitly constructing a 2-layer perceptron and a codeword vector ? that satisfy the stated properties.</p><p>The codeword is simply chosen as the vectorized form of the point cloud matrix S. where [x i , y i ] is the position of the i-th 2D grid point. Suppose the 2D grid points have an interval 2?, i.e., the distance between any two points in the 2D grid is at least 2?. Further assume these m grid points can all be written as [x i , y i ] = [(2? i + 1)?, (2? i + 1)?], where ? i and ? i are two integers whose absolute values are smaller than a positive constant M . One example of a set of 4-by-4 grid points is   </p><formula xml:id="formula_9">{[?3?, ?3?], [?3?, ?1?], [?3?, 1?], [?3?, 3?], [?1?, ?3?], [?1?, ?1?], [?1?,</formula><p>where u is a positive constant to be specified later. Suppose the linear combination output is y j,k . The linear combination is followed by a nonlinear activation function 1 that computes the following</p><formula xml:id="formula_11">z j,k = y j,k , if |y j,k | &lt; c, 0, if |y j,k | ? c,<label>(11)</label></formula><p>where c is a constant to be specified later. The outputs of the activation functions are linearly combined to produce the final output. There are three neurons in the output layer. The k-th neuron (k=1,2,3) computes</p><formula xml:id="formula_12">w k = m j=1 z j,k .<label>(12)</label></formula><p>We assume the parameters (?, u, c, M ) satisfy</p><formula xml:id="formula_13">u &gt; 0, c &gt; 0, ? &gt; 0, M &gt; 0,<label>(13)</label></formula><formula xml:id="formula_14">u? 2 &gt; c + 1,<label>(14)</label></formula><p>u &gt; 8M 2 + 4M + 1,</p><formula xml:id="formula_15">(15) c &gt; 1.<label>(16)</label></formula><p>Now we prove that for this perceptron, the final out-</p><formula xml:id="formula_16">put [w 1 , w 2 , w 3 ] is indeed [s i1 , s i2 , s i3 ] when the input to the perceptron is v i .</formula><p>For the i-th input v i = [x i , y i , s 11 , s 12 , s 13 , s 21 , s 22 , s 23 , . . . , s m1 , s m2 , s m3 ], the k-th neuron in the j-th group in the hidden layer computes the following linear combination</p><formula xml:id="formula_17">y j,k =? j1 x i + ? j2 y i + ? j3 s j,k + b =u 2 x j x i + uy j y i + s j,k ? u 2 x 2 j ? uy 2 j =u 2 x j (x i ? x j ) + uy j (y i ? y j ) + s j,k .<label>(17)</label></formula><p>Notice that we have assumed [x i , y i ] = [(2? i + 1)?, (2? i + 1)?], ?i. So we have</p><formula xml:id="formula_18">y j,k = u 2 x j (x i ? x j ) + uy j (y i ? y j ) + s j,k =2u 2 ? 2 (2? j + 1)(? i ? ? j ) + 2u? 2 (2? j + 1)(? i ? ? j ) + s j,k =u 2 ? 2 m 1 + u? 2 m 2 + s j,k ,<label>(18)</label></formula><p>where the two integer constants m 1 = 2(2? j + 1)(? i ? ? j ) and m 2 = 2(2? j + 1)(? i ? ? j ), and m 1 = 0 only if x i = x j and m 2 = 0 only if y i = y j . Since the absolute values of ? i , ? j , ? i and ? j are all smaller than M , we have</p><formula xml:id="formula_19">|m 1 | ? 2|2? j +1|?|? i ?? j | &lt; 2(2M +1)?2M = 8M 2 +4M.<label>(19)</label></formula><p>Similarly, we have</p><formula xml:id="formula_20">|m 2 | ? 2|2? j +1|?|? i ?? j | &lt; 2(2M +1)?2M = 8M 2 +4M.</formula><p>(20) Now we consider 3 possible cases:</p><p>? |m 1 | ? 1: In this case,</p><formula xml:id="formula_21">|y j,k | =|u 2 ? 2 m 1 + u? 2 m 2 + s j,k | &gt;u 2 ? 2 |m 1 | ? u? 2 |m 2 | ? |s j,k | &gt;u 2 ? 2 ? u? 2 (8M 2 + 4M ) ? 1 =u? 2 [u ? (8M 2 + 4M )] ? 1 (a) &gt; (c + 1) ? 1 ? 1 = c,<label>(21)</label></formula><p>where step (a) follows from the assumption <ref type="bibr" target="#b13">(14)</ref>.</p><p>? m 1 = 0 but |m 2 | ? 1: In this case,</p><formula xml:id="formula_22">|y j,k | =|u? 2 m 2 + s j,k | ?u? 2 |m 2 | ? |s j,k | ?u? 2 (a) ? c + 1 &gt; c,<label>(22)</label></formula><p>where step (a) follows from assumption <ref type="bibr" target="#b14">(15)</ref>.</p><p>? m 1 = m 2 = 0. In this case,</p><formula xml:id="formula_23">|y j,k | = |s j,k | ? 1 (a) &lt; c,<label>(23)</label></formula><p>where step (a) follows from assumption <ref type="bibr" target="#b15">(16)</ref>.</p><p>Notice that the first two cases are equivalent to i = j and the last case is equivalent to i = j. Thus, from <ref type="formula" target="#formula_0">(11)</ref>, we have</p><formula xml:id="formula_24">z j,k = s j,k , if j = i, 0, if j = i.<label>(24)</label></formula><p>Thus, from <ref type="formula" target="#formula_0">(12)</ref>, the final output is</p><formula xml:id="formula_25">w k = m j=1 z j,k = s i,k , k = 1, 2, 3,<label>(25)</label></formula><p>which means the output is indeed [s i,1 , s i,2 , s i,3 ] when the input is v i . This concludes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Supplementary: Decoder Variations</head><p>The current decoder design has two consecutive folding operations that apply on a 2D grid. Therefore, one may wonder if the performance of FoldingNet can be improved if we utilize (1) more folding operations or (2) the same number of folding operations on regular grids of different dimensions. In this section, we report the results for these different settings. The experimental settings are the same with Section 4.6. The experiment results are shown in Table 6. As one can see from line 1 and line 2, increasing the number of folding operations does not significantly increase the performance. Comparing line 1 and line 3, one can see that a 2D grid is better than a 1D grid for both classification and reconstruction. From line 1 and line 4, one can see that a 3D grid only brings a marginal improvement. As we discussed in the introduction, this is because the intrinsic dimensionality of data in the ShapeNet and ModelNet datasets is 2, as they are sampled from object surfaces. If point clouds are intrinsically volumetric, we believe using a 3D grid in the decoder is more suitable. In addition, we also tried to generate the fixed grid by uniformly random  <ref type="table">Table 6</ref>. Comparison between different FoldingNet decoders. "Uniform": the grid is uniformly random sampled. "Regular": the grid is regularly sampled with fixed spacings. sampling in the square. However, it leads to slightly worse results. We believe it is caused by the local density variation introduced by the random sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Supplementary: Folding by Deconvolution</head><p>The folding operation in Definition 1 is essentially a per-point 2D-to-3D function from a 2D grid to a 3D surface. A natural question to ask is whether introducing explicit correlations in the functions imposed on neighboring grid points can help improve the performance. We noted that there is a closely related work on reconstructing 3D point sets using side information from images <ref type="bibr" target="#b16">[17]</ref>. The point reconstruction network in <ref type="bibr" target="#b16">[17]</ref> uses deconvolution to fuse information on the regular grid structure imposed by the image, which is similar to the idea above. Here, we compare a deconvolution network with FoldingNet on the reconstruction performance. The feature sizes of the deconvolution network (C?H?W) are 512?1?1?256?3?3?128?5?5?64?15?15?3?45?45 with kernel sizes 3, 3, 5, 5. The comparison is shown in <ref type="table">Table 7</ref>. We conjecture that deconvolution goes beyond point-wise operations, thus imposes a stronger constraint on the smoothness of the reconstructed surface. Thus, its reconstruction is worse (although with comparable classification accuracy). On the other hand, the use of grids with point-wise MLP in FoldingNet only impose an implicit constraint, thus leading to better reconstructions.</p><p>Cl. Acc. Tst. Loss # Params. FoldingNet 88.41% 0.0296 1.0?10 6 Deconv 88.86% 0.0319 1.7?10 6 <ref type="table">Table 7</ref>. Comparison of two different implementations of the folding operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Supplementary: Robustness of the graphbased encoder</head><p>Here, we use one experiment to show that the graphpooling layers are useful in maintaining the good performance of the FoldingNet when the data is subject to random noise. The following experiment compares FoldingNet with a deep auto-encoder that has the same folding-based decoder architecture but a different encoder architecture in which the graph-based max-pooling layers are removed. The setting of the experiment is the same as in Section 4.6 except that 5 percents of the points in each point cloud in the ModelNet40 dataset are randomly shifted to other positions (but still within the bounding box of the original point cloud). We use this noisy data to see how the performances degrade for the graph-based encoder and the encoder without graph-based max-pooling layers. The results are reported in <ref type="figure">Figure 6</ref>. We can see that when the graph-based Reconstruction loss (chamfer distance)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparing Encoders with or without Graph Layers</head><p>Graph-based Encoder Encoder without Graph Layers <ref type="figure">Figure 6</ref>. Comparison between the graph-based encoder in Section 2.1 and the encoder from which the graph-based max-pooling layers are removed. The encoder with no graph-based layers is similar to the one proposed in <ref type="bibr" target="#b40">[41]</ref> which is for a different goal (supervised learning).</p><p>max-pooling layers are removed, the performance degrades by approximately 2 percents when noise is injected into the dataset. However, the classification accuracy of FoldingNet does not change much (when compared with <ref type="figure" target="#fig_5">Figure 5</ref> in Section 4.6). Thus, it can be seen that the graph-based encoder can make FoldingNet more robust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">Supplementary: More Details on the Linear SVM Experiment on ModelNet10</head><p>The classification accuracy obtained in Section 4.4 on MN10 dataset is 94.4%. We stated in Section 4.5 that many pairs which are wrongly classified are actually hard to distinguish even by a human. In the table on the next page, we list all the incorrectly classified models and their point cloud representations. A phrase like "table ? desk" means the point cloud has label "table" but it is wrongly classified as "desk" by the linear SVM. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>R 2 .</head><label>2</label><figDesc>The parameter "perplexity" in T-SNE was set as 50.We show the embedding result inFigure 2. From the figure, we see that most classes are easily separable except {dresser (violet) v.s. nightstand (pink)} and {desk (red) v.s. table (yellow)}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>The T-SNE clustering visualization of the codewords obtained from FoldingNet auto-encoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Linear SVM classification accuracy v.s. reconstruction loss on ModelNet40 dataset. The auto-encoder is trained using data from the ShapeNet dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Linear SVM classification accuracy v.s. percentage of available labeled training data in ModelNet40 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Comparison between the fully-connected (FC) decoder in<ref type="bibr" target="#b0">[1]</ref> and the folding decoder on ModelNet40.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>In particular, For a matrix S of size m-by-3, if S = [s jk ], j = 1, 2, . . . m and k = 1, 2, 3, the codeword vector ? is chosen to be ? = [s 11 , s 12 , s 13 , s 21 , s 22 , s 23 , . . . , s m1 , s m2 , s m3 ]. Then, the i-th row after concatenation is v i = [x i , y i , s 11 , s 12 , s 13 , s 21 , s 22 , s 23 , . . . , s m1 , s m2 , s m3 ],</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>( 8 )</head><label>8</label><figDesc>In this case, the choice of M is 4. Also assume that the output point cloud is bounded inside 3-dimensional box of length 2 centered at the origin, i.e., |s ij | ? 1.Now, we construct a 2-layer perceptron f that takes the rows v i as inputs and provides the outputs f(v i ) = [s i1 , s i2 , s i3 ], for i = 1, 2, . . . ,m. The input layer takes the vector intput v i which has 3m + 2 scalars. The hidden layer has 3m neurons. The output layer provides three scalar outputs [s i1 , s i2 , s i3 ]. The 3m neurons in the hidden layer are partitioned into m groups of 3 neurons. The k-th neuron (k = 1, 2, 3) in the j-th group (j = 1, 2, 3, . . . , m) is only connected to three inputs x i , y i and [s j,k ], and it computes a linear combination of x i , y i and s j,k with weights ? j1 = u 2 x j , ? j2 = uy j , ? j3 = 1,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>?u 2 x 2 j ? uy 2 j</head><label>22</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>Item 1</cell><cell>Item 2</cell><cell>Number of mistakes</cell></row><row><cell>dresser</cell><cell>night stand</cell><cell>19</cell></row><row><cell>table</cell><cell>desk</cell><cell>15</cell></row><row><cell>bed</cell><cell>bath tub</cell><cell>3</cell></row><row><cell>night stand</cell><cell>table</cell><cell>3</cell></row></table><note>. The first four types of mistakes made in the classification of ModelNet10 dataset. Their images are shown in the Supple- mentary Section 11.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">It is not hard to prove that this function can be obtained by concatenating ReLU functions with appropriate bias terms. We specifically avoid using the ReLU function in order not to hinder the main intuition. In all of our experiments, we use ReLU activation functions.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work is supported by MERL. The authors would like to thank the helpful comments and suggestions from the anonymous reviewers, Teng-Yok Lee, Ziming Zhang, Zhiding Yu, Siheng Chen, Yuichi Taguchi, Mike Jones and Alan Sullivan.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Representation learning and adversarial generation of 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Achlioptas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Diamanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.02392</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unstructured point cloud semantic labeling using deep segmentation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Audebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics Workshop on 3D Object Retrieval</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generative and discriminative voxel modeling with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems, Workshop on 3D learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<title level="m">Spectral networks and locally connected networks on graphs. International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">An information-rich 3d model repository</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On visual similarity based 3D model retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-P</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ouhyoung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="223" to="232" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">3D object proposals for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Object partitioning using local convexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Christoph</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schoeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Papon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Worgotter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="304" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3D reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning hierarchical semantic segmentations of LIDAR data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Matejek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="273" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Graph based convolutional neural network. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gwcnn: A metric alignment layer for deep shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ezuz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ben-Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="49" to="57" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A point set generation network for 3D object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Joint 3D object and layout inference from a single RGB-D image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="183" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning a predictable and generative vector representation for objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="484" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Shape-based recognition of 3d point clouds in urban environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Golovinskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2154" to="2161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Contour detection in unstructured 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1610" to="1618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fast semantic segmentation of 3D point clouds with strongly varying density. ISPRS Annals of Photogrammetry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<idno>2016. 3</idno>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing &amp; Spatial Information Sciences</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A generalization of convolutional neural networks to graph-structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hechtlinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chakravarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.08165</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep convolutional networks on graph-structured data</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Point cloud labeling using 3D convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">23rd International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2670" to="2675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rotation invariant spherical harmonic representation of 3d shape descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kazhdan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rusinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on geometry processing</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="156" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">3D scene understanding by voxel-CRF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1425" to="1432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="j">International Conference on Learning Representations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Escape from cells: Deep Kdnetworks for the recognition of 3D point cloud models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">ternational Conference on Computer Vision (ICCV</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cross-parameterization and compatible remeshing of 3D models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kraevoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sheffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="861" to="869" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning for 3D scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3050" to="3057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Cayleynets: Graph convolutional neural networks with complex rational spectral filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Levie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07664</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fpnn: Field probing neural networks for 3D data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="307" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Geodesic convolutional neural networks on riemannian manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International conference on computer vision workshops</title>
		<meeting>the IEEE International conference on computer vision workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="37" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">3D convolutional neural networks for landing zone detection from LIDAR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3471" to="3478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Voxnet: A 3D convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodol?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fast and robust multi-view 3D object recognition in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="171" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3D classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep learning with sets and point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR), workshop track</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Ulusoys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<title level="m">Octnet: Learning deep 3D representations at high resolutions. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Vconv-DAE: Deep volumetric shape learning without object labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2016 Workshops</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="236" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Mining point cloud local structures by kernel correlation and graph pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Dynamic edgeconditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Convolutional-recursive deep learning for 3D object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="656" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep sliding shapes for amodal 3D object detection in RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="808" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Graph-based segmentation for colored 3d laser point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Strom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Olson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Graph-based compression of dynamic 3d point cloud sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thanou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1765" to="1778" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Generalizing the convolution operator to extend cnns to irregular domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Vialatte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gripon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mercier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01166</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Surflet-pairrelation histograms: a statistical 3D-shape representation for rapid classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Hillenbrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hirzinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on 3-D Digital Imaging and Modeling</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="474" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">3d shape segmentation via shape fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Label propagation from Im-ageNet to 3D point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3135" to="3142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning a probabilistic latent space of object shapes via 3D generative-adversarial modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">3D Shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A scalable active framework for region annotation in 3d shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sheffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Unsupervised 3D category discovery and point labeling from a large urban environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shibasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2685" to="2692" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
