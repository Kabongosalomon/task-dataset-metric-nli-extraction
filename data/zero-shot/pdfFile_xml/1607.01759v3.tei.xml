<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bag of Tricks for Efficient Text Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-08-09">9 Aug 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
							<email>ajoulin@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
							<email>egrave@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
							<email>bojanowski@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
							<email>tmikolov@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Bag of Tricks for Efficient Text Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-08-09">9 Aug 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper explores a simple and efficient baseline for text classification.</p><p>Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore CPU, and classify half a million sentences among 312K classes in less than a minute.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text classification is an important task in Natural Language Processing with many applications, such as web search, information retrieval, ranking and document classification <ref type="bibr" target="#b0">(Deerwester et al., 1990;</ref><ref type="bibr" target="#b5">Pang and Lee, 2008)</ref>.</p><p>Recently, models based on neural networks have become increasingly popular <ref type="bibr" target="#b4">(Kim, 2014;</ref><ref type="bibr">Zhang and LeCun, 2015;</ref><ref type="bibr">Conneau et al., 2016)</ref>. While these models achieve very good performance in practice, they tend to be relatively slow both at train and test time, limiting their use on very large datasets.</p><p>Meanwhile, linear classifiers are often considered as strong baselines for text classification problems <ref type="bibr" target="#b3">(Joachims, 1998;</ref><ref type="bibr">McCallum and Nigam, 1998;</ref><ref type="bibr">Fan et al., 2008)</ref>. Despite their simplicity, they often obtain stateof-the-art performances if the right features are used <ref type="bibr" target="#b8">(Wang and Manning, 2012)</ref>.</p><p>They also have the potential to scale to very large corpus <ref type="bibr" target="#b0">(Agarwal et al., 2014)</ref>.</p><p>In this work, we explore ways to scale these baselines to very large corpus with a large output space, in the context of text classification. Inspired by the recent work in efficient word representation learning <ref type="bibr" target="#b4">(Mikolov et al., 2013;</ref><ref type="bibr" target="#b4">Levy et al., 2015)</ref>, we show that linear models with a rank constraint and a fast loss approximation can train on a billion words within ten minutes, while achieving performance on par with the state-of-the-art. We evaluate the quality of our approach fastText 1 on two different tasks, namely tag prediction and sentiment analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model architecture</head><p>A simple and efficient baseline for sentence classification is to represent sentences as bag of words (BoW) and train a linear classifier, e.g., a logistic regression or an SVM <ref type="bibr" target="#b3">(Joachims, 1998;</ref><ref type="bibr">Fan et al., 2008)</ref>. However, linear classifiers do not share parameters among features and classes. This possibly limits their generalization in the context of large output space where some classes have very few examples.</p><p>Common solutions to this problem are to factorize the linear classifier into low rank matrices <ref type="bibr" target="#b6">(Schutze, 1992;</ref><ref type="bibr" target="#b4">Mikolov et al., 2013)</ref> or to use multilayer neural networks <ref type="bibr">(Collobert and Weston, 2008;</ref><ref type="bibr">Zhang et al., 2015)</ref>. <ref type="figure" target="#fig_0">Figure 1</ref> shows a simple linear model with rank constraint. The first weight matrix A is a look-up table over the words. The word representations are then averaged into a text representation, which is in turn fed to a linear classifier. The text representa- tion is an hidden variable which can be potentially be reused. This architecture is similar to the cbow model of <ref type="bibr" target="#b4">Mikolov et al. (2013)</ref>, where the middle word is replaced by a label. We use the softmax function f to compute the probability distribution over the predefined classes. For a set of N documents, this leads to minimizing the negative loglikelihood over the classes:</p><formula xml:id="formula_0">? 1 N N n=1 y n log(f (BAx n )),</formula><p>where x n is the normalized bag of features of the nth document, y n the label, A and B the weight matrices. This model is trained asynchronously on multiple CPUs using stochastic gradient descent and a linearly decaying learning rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Hierarchical softmax</head><p>When the number of classes is large, computing the linear classifier is computationally expensive. More precisely, the computational complexity is O(kh) where k is the number of classes and h the dimension of the text representation. In order to improve our running time, we use a hierarchical softmax <ref type="bibr" target="#b2">(Goodman, 2001</ref>) based on the Huffman coding tree <ref type="bibr" target="#b4">(Mikolov et al., 2013)</ref>. During training, the computational complexity drops to O(h log 2 (k)).</p><p>The hierarchical softmax is also advantageous at test time when searching for the most likely class. Each node is associated with a probability that is the probability of the path from the root to that node. If the node is at depth l + 1 with parents n 1 , . . . , n l , its probability is</p><formula xml:id="formula_1">P (n l+1 ) = l i=1 P (n i ).</formula><p>This means that the probability of a node is always lower than the one of its parent. Exploring the tree with a depth first search and tracking the maximum probability among the leaves allows us to discard any branch associated with a small probability. In practice, we observe a reduction of the complexity to O(h log 2 (k)) at test time. This approach is further extended to compute the T -top targets at the cost of O(log(T )), using a binary heap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">N-gram features</head><p>Bag of words is invariant to word order but taking explicitly this order into account is often computationally very expensive. Instead, we use a bag of n-grams as additional features to capture some partial information about the local word order. This is very efficient in practice while achieving comparable results to methods that explicitly use the order <ref type="bibr" target="#b8">(Wang and Manning, 2012)</ref>.</p><p>We maintain a fast and memory efficient mapping of the n-grams by using the hashing trick <ref type="bibr" target="#b8">(Weinberger et al., 2009)</ref> with the same hashing function as in <ref type="bibr" target="#b4">Mikolov et al. (2011)</ref> and 10M bins if we only used bigrams, and 100M otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We evaluate fastText on two different tasks. First, we compare it to existing text classifers on the problem of sentiment analysis. Then, we evaluate its capacity to scale to large output space on a tag prediction dataset. Note that our model could be implemented with the Vowpal Wabbit library, 2 but we observe in practice, that our tailored implementation is at least 2-5? faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sentiment analysis</head><p>Datasets and baselines. We employ the same 8 datasets and evaluation protocol of <ref type="bibr">Zhang et al. (2015)</ref>. We report the n-grams and TFIDF baselines from <ref type="bibr">Zhang et al. (2015)</ref>, as well as the character level convolutional model (char-CNN) of Zhang and LeCun <ref type="formula">(2015)</ref>, the character based convolution recurrent network (char-CRNN) of (Xiao and Cho, 2016) and the very deep convolutional network (VDCNN) of <ref type="bibr">Conneau et al. (2016)</ref>.</p><p>We also compare  to <ref type="bibr" target="#b7">Tang et al. (2015)</ref> following their evaluation protocol.</p><p>We report their main baselines as well as their two approaches based on recurrent networks (Conv-GRNN and LSTM-GRNN).</p><p>Results. We present the results in <ref type="figure" target="#fig_0">Figure 1</ref>. We use 10 hidden units and run fastText for 5 epochs with a learning rate selected on a validation set from {0.05, 0.1, 0.25, 0.5}. On this task, adding bigram information improves the performance by 1-4%. Overall our accuracy is slightly better than char-CNN and char-CRNN and, a bit worse than VDCNN. Note that we can increase the accuracy slightly by using more n-grams, for example with trigrams, the performance on Sogou goes up to 97.1%. Finally, <ref type="figure">Figure 3</ref> shows that our method is competitive with the methods presented in <ref type="bibr" target="#b7">Tang et al. (2015)</ref>. We tune the hyperparameters on the validation set and observe that using n-grams up to 5 leads to the best performance. Unlike <ref type="bibr" target="#b7">Tang et al. (2015)</ref>, fastText does not use pre-trained word embeddings, which can be explained the 1% difference in accuracy.   We show a few correct and incorrect tag predictions.</p><p>up compared to neural network based methods increases with the size of the dataset, going up to at least a 15,000? speed-up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Tag prediction</head><p>Dataset and baselines. To test scalability of our approach, further evaluation is carried on the YFCC100M dataset <ref type="bibr">(Thomee et al., 2016)</ref> which consists of almost 100M images with captions, titles and tags. We focus on predicting the tags according to the title and caption (we do not use the images). We remove the words and tags occurring less than 100 times and split the data into a train, validation and test set. The train set contains 91,188,648 examples (1.5B tokens). The validation has 930,497 examples and the test set 543,424. The vocabulary size is 297,141 and there are 312,116 unique tags. We will release a script that recreates this dataset so that our numbers could be reproduced. We report precision at 1. We consider a frequency-based baseline which predicts the most frequent tag. We also compare with <ref type="bibr">Tagspace (Weston et al., 2014)</ref>, which is a tag prediction model similar to ours, but based on the Wsabie model of <ref type="bibr">Weston et al. (2011)</ref>. While the Tagspace model is described using convolutions, we consider the linear version, which achieves comparable performance but is much faster.  and 200. Both models achieve a similar performance with a small hidden layer, but adding bigrams gives us a significant boost in accuracy. At test time, Tagspace needs to compute the scores for all the classes which makes it relatively slow, while our fast inference gives a significant speed-up when the number of classes is large (more than 300K here). Overall, we are more than an order of magnitude faster to obtain model with a better quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and training time.</head><p>The speedup of the test phase is even more significant (a 600? speedup). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion and conclusion</head><p>In this work, we propose a simple baseline method for text classification. Unlike unsupervisedly trained word vectors from word2vec, our word features can be averaged together to form good sentence representations. In several tasks, fastText obtains performance on par with recently proposed methods inspired by deep learning, while being much faster. Although deep neural networks have in theory much higher representational power than shallow models, it is not clear if simple text classification problems such as sentiment analysis are the right ones to evaluate them. We will publish our code so that the research community can easily build on top of our work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Model architecture of fastText for a sentence with N ngram features x1, . . . , xN . The features are embedded and averaged to form the hidden variable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>ModelAG Sogou DBP Yelp P. Yelp F. Yah. A. Amz. F. Amz. P. Test accuracy [%] on sentiment datasets. FastText has been run with the same parameters for all the datasets. It has 10 hidden units and we evaluate it with and without bigrams. For char-CNN, we show the best reported numbers without data augmentation.</figDesc><table><row><cell cols="2">BoW (Zhang et al., 2015)</cell><cell>88.8</cell><cell>92.9</cell><cell>96.6</cell><cell>92.2</cell><cell>58.0</cell><cell>68.9</cell><cell>54.6</cell><cell>90.4</cell></row><row><cell cols="2">ngrams (Zhang et al., 2015)</cell><cell>92.0</cell><cell>97.1</cell><cell>98.6</cell><cell>95.6</cell><cell>56.3</cell><cell>68.5</cell><cell>54.3</cell><cell>92.0</cell></row><row><cell cols="2">ngrams TFIDF (Zhang et al., 2015)</cell><cell>92.4</cell><cell>97.2</cell><cell>98.7</cell><cell>95.4</cell><cell>54.8</cell><cell>68.5</cell><cell>52.4</cell><cell>91.5</cell></row><row><cell cols="3">char-CNN (Zhang and LeCun, 2015) 87.2</cell><cell>95.1</cell><cell>98.3</cell><cell>94.7</cell><cell>62.0</cell><cell>71.2</cell><cell>59.5</cell><cell>94.5</cell></row><row><cell cols="2">char-CRNN (Xiao and Cho, 2016)</cell><cell>91.4</cell><cell>95.2</cell><cell>98.6</cell><cell>94.5</cell><cell>61.8</cell><cell>71.7</cell><cell>59.2</cell><cell>94.1</cell></row><row><cell cols="2">VDCNN (Conneau et al., 2016)</cell><cell>91.3</cell><cell>96.8</cell><cell>98.7</cell><cell>95.7</cell><cell>64.7</cell><cell>73.4</cell><cell>63.0</cell><cell>95.7</cell></row><row><cell>fastText, h = 10</cell><cell></cell><cell>91.5</cell><cell>93.9</cell><cell>98.1</cell><cell>93.8</cell><cell>60.4</cell><cell>72.0</cell><cell>55.8</cell><cell>91.2</cell></row><row><cell cols="2">fastText, h = 10, bigram</cell><cell>92.5</cell><cell>96.8</cell><cell>98.6</cell><cell>95.7</cell><cell>63.9</cell><cell>72.3</cell><cell>60.2</cell><cell>94.6</cell></row><row><cell></cell><cell cols="2">Zhang and LeCun (2015)</cell><cell></cell><cell cols="3">Conneau et al. (2016)</cell><cell></cell><cell cols="2">fastText</cell></row><row><cell cols="4">small char-CNN big char-CNN</cell><cell cols="4">depth=9 depth=17 depth=29</cell><cell cols="2">h = 10, bigram</cell></row><row><cell>AG</cell><cell>1h</cell><cell>3h</cell><cell></cell><cell>24m</cell><cell>37m</cell><cell>51m</cell><cell></cell><cell>1s</cell></row><row><cell>Sogou</cell><cell>-</cell><cell>-</cell><cell></cell><cell>25m</cell><cell>41m</cell><cell>56m</cell><cell></cell><cell>7s</cell></row><row><cell>DBpedia</cell><cell>2h</cell><cell>5h</cell><cell></cell><cell>27m</cell><cell>44m</cell><cell>1h</cell><cell></cell><cell>2s</cell></row><row><cell>Yelp P.</cell><cell>-</cell><cell>-</cell><cell></cell><cell>28m</cell><cell>43m</cell><cell cols="2">1h09</cell><cell>3s</cell></row><row><cell>Yelp F.</cell><cell>-</cell><cell>-</cell><cell></cell><cell>29m</cell><cell>45m</cell><cell cols="2">1h12</cell><cell>4s</cell></row><row><cell>Yah. A.</cell><cell>8h</cell><cell>1d</cell><cell></cell><cell>1h</cell><cell>1h33</cell><cell>2h</cell><cell></cell><cell>5s</cell></row><row><cell>Amz. F.</cell><cell>2d</cell><cell>5d</cell><cell></cell><cell>2h45</cell><cell>4h20</cell><cell>7h</cell><cell></cell><cell>9s</cell></row><row><cell>Amz. P.</cell><cell>2d</cell><cell>5d</cell><cell></cell><cell>2h45</cell><cell>4h25</cell><cell>7h</cell><cell></cell><cell>10s</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Training time for a single epoch on sentiment analysis datasets compared to char-CNN and VDCNN.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparision with<ref type="bibr" target="#b7">Tang et al. (2015)</ref>. The hyperparameters are chosen on the validation set. We report the test accuracy.</figDesc><table><row><cell>Training time. Both char-CNN and VDCNN are</cell></row><row><cell>trained on a NVIDIA Tesla K40 GPU, while our</cell></row><row><cell>models are trained on a CPU using 20 threads. Ta-</cell></row><row><cell>ble 2 shows that methods using convolutions are sev-</cell></row><row><cell>eral orders of magnitude slower than fastText.</cell></row><row><cell>While it is possible to have a 10? speed up for</cell></row><row><cell>char-CNN by using more recent CUDA implemen-</cell></row><row><cell>tations of convolutions, fastText takes less than</cell></row><row><cell>a minute to train on these datasets. The GRNNs</cell></row><row><cell>method of Tang et al. (2015) takes around 12 hours</cell></row><row><cell>per epoch on CPU with a single thread. Our speed-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Examples from the validation set of YFCC100M dataset obtained with fastText with 200 hidden units and bigrams.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc>presents a comparison of fastText and the baselines. We run fastText for 5 epochs and compare it to Tagspace for two sizes of the hidden layer, i.e., 50</figDesc><table><row><cell>Model</cell><cell>prec@1</cell><cell cols="2">Running time</cell></row><row><cell></cell><cell></cell><cell>Train</cell><cell>Test</cell></row><row><cell>Freq. baseline</cell><cell>2.2</cell><cell>-</cell><cell>-</cell></row><row><cell>Tagspace, h = 50</cell><cell>30.1</cell><cell>3h8</cell><cell>6h</cell></row><row><cell>Tagspace, h = 200</cell><cell>35.6</cell><cell>5h32</cell><cell>15h</cell></row><row><cell>fastText, h = 50</cell><cell>31.2</cell><cell>6m40</cell><cell>48s</cell></row><row><cell cols="2">fastText, h = 50, bigram 36.7</cell><cell>7m47</cell><cell>50s</cell></row><row><cell>fastText, h = 200</cell><cell>41.1</cell><cell cols="2">10m34 1m29</cell></row><row><cell cols="2">fastText, h = 200, bigram 46.1</cell><cell cols="2">13m38 1m37</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Prec@1 on the test set for tag prediction on YFCC100M. We also report the training time and test time.</figDesc><table><row><cell>Test time is reported for a single thread, while training uses 20</cell></row><row><cell>threads for both models.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>shows some quali-</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Using the options --nn, --ngrams and --log multi</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. We thank Gabriel Synnaeve, Herv? G?gou, Jason Weston and L?on Bottou for their help and comments. We also thank Alexis Conneau, Duyu Tang and Zichao Zhang for providing us with information about their methods.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Agarwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01781</idno>
	</analytic>
	<monogr>
		<title level="m">Holger Schwenk, Lo?c Barrault, and Yann Lecun. 2016. Very deep convolutional networks for natural language processing</title>
		<editor>JMLR. [Collobert and Weston2008] Ronan Collobert and Jason Weston</editor>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Indexing by latent semantic analysis. Journal of the American society for information science</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Liblinear: A library for large linear classification</title>
		<editor>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin</editor>
		<imprint>
			<date type="published" when="2008" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Classes for fast maximum entropy training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Text categorization with support vector machines: Learning with many relevant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving distributional similarity with lessons learned from word embeddings. TACL. [McCallum and Nigam1998] Andrew McCallum and Kamal Nigam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
	</analytic>
	<monogr>
		<title level="m">Efficient estimation of word representations in vector space</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Workshop on Automatic Speech Recognition and Understanding</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Opinion mining and sentiment analysis. Foundations and trends in information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee2008] Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dimensions of meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schutze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Supercomputing</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Document modeling with gated recurrent neural network for sentiment classification</title>
	</analytic>
	<monogr>
		<title level="m">Yfcc100m: The new data in multimedia research</title>
		<editor>EMNLP. [Thomee et al.2016] Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li</editor>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="64" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient character-level document classification by combining convolution and recurrent layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manning2012] Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D Manning ;</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.00367</idno>
		<idno>arXiv:1502.01710</idno>
	</analytic>
	<monogr>
		<title level="m">Zhang and LeCun2015] Xiang Zhang and Yann LeCun. 2015. Text understanding from scratch</title>
		<editor>IJCAI. [Weston et al.2014] Jason Weston, Sumit Chopra, and Keith Adams</editor>
		<meeting><address><addrLine>Weston</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>NIPS</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
