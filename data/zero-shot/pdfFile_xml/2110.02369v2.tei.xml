<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ENTQA: ENTITY LINKING AS QUESTION ANSWERING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-07">7 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzheng</forename><surname>Zhang</surname></persName>
							<email>wenzheng.zhang@rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyue</forename><surname>Hua</surname></persName>
							<email>wenyue.hua@rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
							<email>karl.stratos@rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ENTQA: ENTITY LINKING AS QUESTION ANSWERING</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-07">7 Mar 2022</date>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2022</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A conventional approach to entity linking is to first find mentions in a given document and then infer their underlying entities in the knowledge base. A well-known limitation of this approach is that it requires finding mentions without knowing their entities, which is unnatural and difficult. We present a new model that does not suffer from this limitation called EntQA, which stands for Entity linking as Question Answering. EntQA first proposes candidate entities with a fast retrieval module, and then scrutinizes the document to find mentions of each candidate with a powerful reader module. Our approach combines progress in entity linking with that in open-domain question answering and capitalizes on pretrained models for dense entity retrieval and reading comprehension. Unlike in previous works, we do not rely on a mention-candidates dictionary or large-scale weak supervision. EntQA achieves strong results on the GERBIL benchmarking platform.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>We consider the most general form of entity linking (EL) in which a system, given a document, must both extract entity mentions and link the mentions to their corresponding entries in a knowledge base (KB). EL is a foundational building block in automatic text understanding with applications to question answering (QA) <ref type="bibr" target="#b9">(Ferrucci, 2012)</ref>, information retrieval <ref type="bibr" target="#b41">(Xiong et al., 2017;</ref><ref type="bibr" target="#b13">Hasibi et al., 2016;</ref><ref type="bibr" target="#b3">Balog et al., 2013;</ref><ref type="bibr" target="#b34">Reinanda et al., 2015)</ref>, and commercial recommendation systems <ref type="bibr" target="#b43">(Yang et al., 2018;</ref><ref type="bibr">Slawski, 2015)</ref>.</p><p>The output space in EL is intractably large. Any subset of all possible spans in the document linked to any KB entries (typically in the order of millions) can be a system output. To get around the intractability, existing methods decompose EL into mention detection (MD) and entity disambiguation (ED) and tackle them with varying degrees of independence. In all cases, however, the order of these two subproblems is MD followed by ED: first the system identifies potential entity mentions, and then the mentions are resolved to KB entries. Previous works either assume that mentions are given <ref type="bibr" target="#b12">(Gupta et al., 2017)</ref>, run an off-the-shelf named-entity recognition (NER) system to extract mentions and resolve them by ED (MD?ED pipeline) <ref type="bibr" target="#b15">(Hoffart et al., 2011;</ref><ref type="bibr" target="#b26">Ling et al., 2015;</ref><ref type="bibr" target="#b38">van Hulst et al., 2020)</ref>, or train an end-to-end model that jointly performs MD?ED by beam search <ref type="bibr" target="#b20">(Kolitsas et al., 2018;</ref>.</p><p>A limitation of performing MD before ED is that it requires finding mentions without knowing the corresponding entities. By definition, a mention needs an entity (i.e., a mention of what?). Existing methods suffer from the dilemma of having to predict mentions before what they refer to, which is unnatural and difficult. For example, the MD?ED pipeline heuristically extracts mentions from spans of named entities found by a third-party NER system, and the performance bottleneck is often errors in MD propagated to ED. End-to-end models alleviate the problem of error propagation, but the search is only approximate and the dilemma, albeit to a lesser degree, remains.</p><p>In this work, we propose flipping the order of the two subproblems and solving ED before MD. We first find candidate entities that might be mentioned in the given document, then for each candidate find its mentions if possible. Our key observation is that while finding mentions is difficult without the knowledge of relevant entities, finding relevant entities is easy without the knowledge of their specific mentions. This simple change fundamentally solves the dilemma above since identifying mentions of a particular entity is well defined.</p><p>We cast the problem as inverted open-domain QA. Specifically, given a document, we use a dual encoder retriever to efficiently retrieve top-K candidate entities from the KB as "questions". Then we apply a deep cross-attention reader on the document for each candidate to identify mentions of the candidate in the document as "answer spans". Unlike in standard QA, the model must predict an unknown number of questions and answers. We present a simple and effective solution based on thresholding. We call our model EntQA, standing for Entity linking as Question Answering.</p><p>Beyond conceptual novelty, EntQA also offers many practical advantages. First, EntQA allows us to piggyback on recent progress in dense entity retrieval and open-domain QA. For instance, we warm start EntQA with the BLINK entity retriever <ref type="bibr" target="#b39">(Wu et al., 2020a)</ref> and ELECTRA finetuned on a QA dataset <ref type="bibr" target="#b5">(Clark et al., 2019)</ref> to obtain an easy improvement. Second, EntQA has no dependence on a hardcoded mention-candidates dictionary which is used in previous works to reduce the search space and bias the model <ref type="bibr" target="#b10">(Ganea &amp; Hofmann, 2017;</ref><ref type="bibr" target="#b20">Kolitsas et al., 2018;</ref>. The dictionary is typically constructed using a large KB-specific labeled corpus (e.g., Wikipedia hyperlinks), thus having no dependence on it makes our approach more broadly applicable to KBs without such resources. Third, training EntQA is data efficient and can be done with an academic budget, in contrast with <ref type="bibr">GENRE (De Cao et al., 2021)</ref> which requires industry-scale pretraining by weak supervision.</p><p>EntQA achieves strong performance on the GERBIL benchmarking platform <ref type="bibr" target="#b36">(R?der et al., 2018)</ref>. The in-domain F 1 score on the test portion of the AIDA-CoNLL dataset is 85.8 (2.1 absolute improvement). The macro-averaged F 1 score across 8 evaluation datasets is 60.5 (2.3 absolute improvement). <ref type="bibr">1</ref> We analyze EntQA and find that its retrieval performance is extremely strong (over 98 top-100 recall on the validation set of AIDA), verifying our hypothesis that finding relevant entities without knowing their mentions is easy. We also find that the reader makes reasonable errors such as accurately predicting missing hyperlinks or linking a mention to a correct entity that is more specific than the gold label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MODEL</head><p>Let E denote the set of entities in a KB associated with a text title and description. Let V denote the vocabulary and X = x ? V T : 1 ? T ? T max the set of all documents up to length T max . EL is the task of mapping x ? X to y ? P(Y(x)) where Y(x) = {(s, t, e) : 1 ? s ? t ? |x| , e ? E} is the set of all possible linked spans in x and P is the power set. The size of the output space is O(2 T 2 max |E| ) where |E| is typically very large (e.g., around 6 million in Wikipedia) and T max can also be large (e.g., &gt; 3000 in AIDA), ruling out any naive exhaustive search as a feasible approach.</p><p>EntQA decomposes EL into two subproblems: entity retrieval and question answering. More specifically, given a document x ? X , 1. The retriever module retrieves top-K candidate entities that might be mentioned in x. 2. The reader module extracts mentions of each candidate entity in x (or rejects it), then returns a subset of globally reranked labeled mentions as the final prediction.</p><p>Our approach bears superficial similarities to a standard framework in open-domain QA that pipelines retrieval and span finding <ref type="bibr">(Karpukhin et al., 2020, inter alia)</ref>, but it has the following important differences. First, instead of retrieving passages given a question, it retrieves questions (i.e., candidate entities) given a passage. Second, even when considering a single question, there can be multiple answer spans (i.e., mentions) instead of one. Both the number of gold entities present in a document and the number of mentions of each gold entity are unknown, making this setting more challenging than standard QA in which we only need to find a single answer span for a single question on a passage.</p><p>Input representation. Both the retriever and the reader work with text representations of documents and entities, thus applicable to a zero-shot setting (e.g., linking to a new KB at test time by reading entity descriptions). We use the title ? title (e) ? V + and the description ? desc (e) ? V + to represent an entity e ? E. Since a document x ? X is generally too long to encode with a Transformer encoder which has a quadratic dependency on the input length, we break it down in m x ? N overlapping passages p 1 (x) . . . p mx (x) ? V L of length L with stride S (e.g., L = 32 and S = 16) and operate at the passage-level similarly as in QA <ref type="bibr" target="#b1">(Alberti et al., 2019)</ref>. When a document is long, individual passages may lose global information. For long documents, we find it beneficial to carry a document-level topical text ? topic (x) ? V + across passages in that document (e.g., first sentence). We emphasize that we do not use any extra information outside the document. In our experiments we simply set ? topic (x) = x 1 ? V (i.e., the first token in the document).</p><p>Notation. We write enc ? S : V T ? R d?T to denote a Transformer encoder that maps any token sequence to the same-length sequence of corresponding contextual embeddings; the symbol S is used to distinguish different encoders. We assume the usual special tokens in the input popularized by BERT <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref>: [CLS] to represent the whole input and [SEP] to indicate an input boundary. We write ? to denote the text concatenation; we insert an unused token type in the vocabulary in between two texts being concatenated. We write M i ? R d to denote the i-th column of matrix M ? R d?T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">RETRIEVER</head><p>Given a passage p ? V + in document x and an entity e ? E, the retriever computes</p><formula xml:id="formula_0">P = enc ? P ([CLS]p[SEP]? topic (x)) E e = enc ? E ([CLS]? title (e) ? ? desc (e)[SEP]) score ? retr (p, x, e) = P ? 1 E e 1</formula><p>At inference time, we precompute E e ? R d for each e ? E and use Faiss <ref type="bibr" target="#b17">(Johnson et al., 2019)</ref> for fast top-K retrieval.</p><p>Training. We train the retriever by a multi-label variant of noise contrastive estimation (NCE). Given a passage p in document x, we have a set of multiple gold entities E(p) ? E that are mentioned in the passage and optimize the per-example objective</p><formula xml:id="formula_1">max ? e?E(p) log exp score ? retr (p, x, e) exp score ? retr (p, x, e) + e ? ?N(E,p) exp score ? retr (p, x, e ? )<label>(1)</label></formula><p>where N(E, p) ? E\E(p) is a set of negative examples that excludes all gold entities E(p). The objective effectively constructs |E(p)| independent NCE instances, each of which treats a gold entity as the only correct answer while ensuring that other gold entities are not included in negative examples. We obtain 90% of N(E, p) by sampling entities uniformly at random from E\E(p) and 10% by hard negative mining (i.e., using highest-scoring incorrect entities under the model), which is well known to be beneficial in entity retrieval <ref type="bibr" target="#b11">(Gillick et al., 2019;</ref><ref type="bibr" target="#b39">Wu et al., 2020a;</ref><ref type="bibr" target="#b44">Zhang &amp; Stratos, 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">READER</head><p>Let e 1:K = (e 1 . . . e K ) ? E K denote K candidate entities for a passage p in document x. For each k ? {1 . . . K}, the reader computes a joint encoding of (p, x, e k ) by</p><formula xml:id="formula_2">H k = enc ? H ([CLS]p ? ? topic (x)[SEP]? title (e k ) ? ? desc (e k )[SEP]</formula><p>) then defines a conditional distribution over mention spans of e k in p by</p><formula xml:id="formula_3">p ? start (s|p, x, e k ) = exp w ? start H k s |p|+1 i=1 exp w ? start H k i ?s ? {1 . . . |p| + 1} p ? end (t|p, x, e k ) = exp w ? end H k t |p|+1 i=1 exp w ? end H k i ?t ? {1 . . . |p| + 1} p ? span (s, t|p, x, e k ) = p ? start (s|p, x, e k ) ? p ? end (t|p, x, e k ) ?s, t ? {1 . . . |p| + 1}</formula><p>where w start , w end ? R d are additional parameters. The reader also multitasks reranking: it uses w rerank ? R d to define a conditional distribution over candidate entities by Training. We obtain candidates e 1:K from a fully trained retrieval module to make training consistent with test time. During training, we always include all gold entities as candidates (i.e., E(p) ? e 1:K ). Let M(p, e) denote the set of gold mention spans of e ? E in p; if e is not present in p, we define M(p, e) = {(1, 1)}. We optimize the per-example objective</p><formula xml:id="formula_4">p ? rerank (e k |p, x, e 1:K ) = exp w ? rerank H k 1 K k ? =1 exp w ? rerank H k ? 1 ?k ? {1 . . .</formula><formula xml:id="formula_5">max ? K k=1 ?(e k ? E(p)) log p ? rerank (e k |p, x, e 1:K ) + (s,t)?M(p,e k ) log p ? span (s, t|p, x, e k ) (2)</formula><p>where ?(A) is the indicator function equal to one if A is true and zero otherwise. Note that the reader is trained to predict the [CLS] span for incorrect entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">INFERENCE</head><p>At test time, we process a new document x ? X in passages p ? V L independently as follows:</p><p>1. Retrieve top-K highest scoring entities e 1:K under score ? retr (p, x, e). 2. For each candidate k, extract top-P most likely mention spans (s k</p><formula xml:id="formula_6">1 , t k 1 ) . . . (s k P , t k P ) under p ? span (s, t|p,</formula><p>x, e k ) while discarding any span less probable than (1, 1). 3. Return a subset of the surviving labeled mentions (s, t, e k ) with p ? rerank (e k |p, x, e 1:K ) ? p ? span (s, t|p, x, e k ) &gt; ? as the final prediction.</p><p>We do not apply any further processing to combine passage-level predictions other than merging duplicate labeled spans (s, t, e) in the overlapping sections. This inference scheme is simple yet effective. For each candidate entity, the reader scrutinizes the passage with deep cross-attention to see if there are any mentions of the entity and has a chance to reject it by predicting (1, 1). The reader delays its final decision until it has processed all candidates to globally reconsider labeled mentions with ranking probabilities. <ref type="figure">Figure 1</ref> shows a successful prediction on a passage from the validation portion of AIDA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS</head><p>We evaluate EntQA on the GERBIL benchmarking platform <ref type="bibr" target="#b36">(R?der et al., 2018)</ref>, which offers reliable comparison with state-of-the-art EL methods on numerous public datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">SETTING</head><p>Datasets. We follow the established practice and report the InKB Micro F 1 score on the in-domain and out-of-domain datasets used in De Cao et al. <ref type="bibr">(2021)</ref>. Specifically, we use the AIDA-CoNLL dataset <ref type="bibr" target="#b15">(Hoffart et al., 2011)</ref> as the in-domain dataset: we train EntQA on the training portion of AIDA, use the validation portion (AIDA-A) for development, and reserve the test portion (AIDA-B) for in-domain test performance. We use seven out-of-domain test sets: MSNBC, Derczynski (Der) <ref type="bibr" target="#b7">(Derczynski et al., 2015)</ref>, KORE 50 (K50) <ref type="bibr" target="#b16">(Hoffart et al., 2012)</ref>, N3-Reuters-128 (R128), N3-RSS-500 (R500) <ref type="bibr" target="#b35">(R?der et al., 2014)</ref>, and OKE challenge 2015 and 2016 (OKE15 and OKE16) <ref type="bibr" target="#b30">(Nuzzolese et al., 2015)</ref>. We refer to <ref type="table">Table 6</ref> in <ref type="bibr" target="#b20">Kolitsas et al. (2018)</ref> for the datasets' statistics. For the KB, we use the 2019 Wikipedia dump provided in the KILT benchmark , which contains 5.9 million entities.</p><p>Model details. We initialize the passage encoder enc ? P and the entity encoder enc ? E in the retriever module with independent BLINK retrievers pretrained on Wikipedia hyperlinks <ref type="bibr" target="#b39">(Wu et al., 2020a)</ref> and optimize the NCE objective (1) with hard negative mining. We initialize the joint encoder enc ? H in the reader module with ELECTRA-large <ref type="bibr" target="#b5">(Clark et al., 2019)</ref> finetuned on SQuAD 2.0 <ref type="bibr" target="#b32">(Rajpurkar et al., 2018)</ref> and optimize the reader objective (2). We break up each document x ? X into overlapping passages of length L = 32 with stride S = 16 under WordPiece tokenization. For each passage in x, we concatenate the input with the first token of the document ? topic (x) = x 1 , which corresponds to the topic in AIDA but not in other datasets. We use 64 candidate entities in training for both the retriever and the reader; we use 100 candidates at test time. We predict up to P = 3 mention spans for each candidate entity. We use ? = 0.05 as the threshold in all experiments, chosen after trying values 0.01, 0.1, and 0.05 on the validation set. Additional experiments on automatically tuning ? are discussed in Appendix A. For optimization, we use Adam <ref type="bibr" target="#b19">(Kingma &amp; Ba, 2015)</ref> with learning rate 2e-6 for the retriever and 1e-5 for the reader; we use a linear learning rate decay schedule with warmup proportion 0.06 over 4 epochs for both modules. The batch size is 4 for the retriever and 2 for the reader. The retriever is trained on 4 GPUs (A100) for 9 hours; the reader is trained on 2 GPUs for 6 hours.</p><p>Baselines. We compare with state-of-the-art EL systems that represent a diverse array of approaches. <ref type="bibr" target="#b15">Hoffart et al. (2011)</ref> and van Hulst et al. (2020) use the MD?ED pipeline; despite the limitation of pipelining MD with ED, the latter achieve excellent performance by solving MD with a strong NER system <ref type="bibr" target="#b0">(Akbik et al., 2018)</ref>. <ref type="bibr" target="#b20">Kolitsas et al. (2018)</ref> use an end-to-end model that sequentially performs MD and ED; to make the problem tractable, they drastically prune the search space with a mention-candidates dictionary and the model score. De  propose GENRE, a sequence-to-sequence model for EL. The model conditions on the given document and autoregressively generates a labeled version of the document by at each position either copying a token, starting or ending a mention span, or, if the previous generation was the end of a mention m, generating the entity title associated with m token by token. At inference time, GENRE critically relies on a prefix tree (aka. trie) derived from Wikipedia to constrain the beam search so that it produces a valid entity title in the KB. Since each beam element must first predict a mention before predicting an entity, unless the beam size is unbounded so that every labeled span is considered, GENRE will suffer from MD errors propagating to ED. <ref type="table" target="#tab_1">Table 1</ref> shows the main results. EntQA achieves the best in-domain test F 1 score for AIDA (+2.1) and is also performant on out-of-domain datasets (+3.8 on KORE 50 and +7.4 on N3-Reuters-128, close second-best on Derczynski and N3-RSS-500). The performance is lower on OKE15 and OKE16 for the same reason pointed out by De : these datasets are annotated with coreference (i.e., they contain pronouns and common nouns linked to entities) which our model is not trained for, while many other systems have a component in their pipelines to handle these cases. We hypothesize that the performance on MSNBC is lagging because it has long documents (544 words per document on average) which are processed in relatively short passages under EntQA due to our computational constraints. Overall, EntQA achieves the best macro-averaged F 1 score across the 8 evaluation datasets (+2.3). The inference runtime of EntQA is clearly linear in the number of candidate entities K. To get a sense of speed, we compared the runtime of EntQA with that of GENRE on the AIDA validation set using 1 GPU on the same machine. GENRE took 1 hour and 10 minutes, excluding 31 minutes to first build a prefix tree. EntQA took 20 minutes with K = 100, 10 minutes with K = 50, and 4 minutes with K = 20, excluding 1 hour to first index entity embeddings, yielding F 1 scores 87.3, 87.4, and 87.0. Interestingly, we can obtain a significant speedup at a minor cost in performance by decreasing K. We believe this can be a useful feature of the model in controlling the speedperformance tradeoff.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">RESULTS</head><p>We note that there is an issue of using different editions of Wikipedia between the systems. . Thus there is a concern that differences in performance are due to different snapshots of Wikipedia. While we consider it out of scope in our work to fully address this concern, we find that using different editions of Wikipedia does not fundamentally change the performance of EntQA, which is consistent with GERBIL's intent of being KB-agnostic. For instance, we obtained the same validation F 1 on AIDA with our model trained on either the 2014 or 2019 dump. We use the KILT edition of Wikipedia mainly for convenience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">OTHER PRACTICAL HIGHLIGHTS</head><p>No dictionary. EntQA has no dependence on a mention-candidates dictionary. All previous works rely on a dictionary D : V + ? P(E) that maps a mention string m to a small set of candidate entities e ? E associated with empirical conditional probabilitiesp e|m &gt; 0 <ref type="bibr">(Hoffart et al., 2011, inter alia)</ref>. For instance, it is an essential component of the search procedure in the end-to-end model of <ref type="bibr" target="#b20">Kolitsas et al. (2018)</ref>. While not mentioned in the paper or on the GitHub repository, GENRE  also uses the dictionary from <ref type="bibr" target="#b20">Kolitsas et al. (2018)</ref> in their prefix tree to constrain the beam search (personal communication with one of the authors of the paper).</p><p>Constructing such a dictionary typically assumes the existence of a large KB-specific labeled corpus (e.g., internal links in Wikipedia). EntQA is thus more broadly applicable to KBs without such resources (e.g., for small domain-specific KBs).   88.4 -Omit the reranking probabilities p ? rerank (i.e., only use span probabilities) 87.9 -Omit the document-level information x1 in the input passage representation 87.5 Oracle experiment: use gold entities as the only candidate entities 94.9</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">ABLATION STUDIES</head><p>The final form of EntQA in Section 3.2 is the result of empirically exploring various modeling and optimization choices during development. We present an ablation study to illustrate the impact of these choices.</p><p>Retriever <ref type="table" target="#tab_3">Table 2</ref> shows an ablation study for the retriever module. We report top-100 recall (R@100) on the validation set of AIDA. The baseline retriever is initialized with BLINK <ref type="bibr" target="#b39">(Wu et al., 2020a)</ref>, uses the passage representation [CLS]p[SEP]x 1 , and is trained by optimizing the multilabel variant of NCE (1) that considers one gold entity at a time by excluding others in the normalization term. We see that the baseline retriever has an extremely high recall (98.2), confirming our hypothesis that it is possible to accurately infer relevant entities in a passage without knowing where they are mentioned. We also see that it is very important to use the proposed multi-label variant of NCE instead of naive NCE that normalizes over all gold entities, which results in a massive decrease in recall (82.7). We consider optimizing the marginal log-likelihood (i.e., the log of the sum of the probabilities of gold entities, rather than the sum of the log), but it yields much worse performance (83.8). It is helpful to initialize with BLINK rather than BERT-large, use hard negatives in NCE, and append x 1 to input passages. <ref type="table" target="#tab_3">Table 2</ref> additionally shows the BM25 recall, which is quite poor (36.6). Upon inspection, we find that BM25 fails to retrieve diverse entities. For instance, a passage on cricket may have diverse gold entities such as an organization (Leicestershire County Cricket Club), location (London), and person (Phil Simmons), but the top entities under BM25 are dominated by person entities (Alan Shipman, Dominique <ref type="bibr">Lewis, etc.)</ref>. This shows the necessity of explicitly training a retriever to prioritize diversity in our problem.</p><p>Reader <ref type="table" target="#tab_4">Table 3</ref> shows an ablation study for the reader module. We report F 1 on the validation set of AIDA. The baseline reader is initialized with ELECTRA-large <ref type="bibr" target="#b5">(Clark et al., 2019)</ref> finetuned on SQuAD 2.0, uses the joint passage-entity input representation [CLS]p ? x 1 [SEP]? title (e) ? ? desc (e) <ref type="bibr">[SEP]</ref>, and is trained by optimizing (2). Candidate entities are obtained from the baseline retriever in <ref type="table" target="#tab_3">Table 2</ref>. We see that BERT is less performant than ELECTRA for reader initialization, consistent with findings in the QA literature <ref type="bibr" target="#b42">(Yamada et al., 2021)</ref>. Training by optimizing the marginal log-likelihood is comparable to (2). Interestingly, we find that we can fit the reader just as well without using a SQuAD-finetuned ELECTRA, ranking probabilities, or x 1 in passages. However, in our preliminary investigation we found that these variants generalized slightly worse outside the training domain, thus we kept our original choice. We discuss other choices of documentlevel information in Appendix B. Lastly, we conduct an oracle experiment in which we provide only gold entities as candidates to the reader. In this scenario, the reader is very accurate (94.9 F 1 ), suggesting that the main performance bottleneck is correctly distinguishing gold vs non-gold entities from the candidates. We investigate this issue more in depth in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">ERROR ANALYSIS</head><p>To better understand the source of errors made by EntQA, we examine passages in the validation set for which the model's prediction is not completely correct. We partition them into three types:</p><p>(1) over-predicting (i.e., the gold mentions are a strict subset of the predicted mentions), (2) underpredicting (i.e., the predicted mentions are a strict subset of the gold mentions), and (3) neither overnor under-predicting. <ref type="table" target="#tab_5">Table 4</ref> shows examples of each error type. We find that over-predicting often happens because the model correctly "fills in" entity mentions missing in the gold annotation. Under-predicting happens most likely because the threshold value is too large to catch certain mentions. Finally, many errors that are neither over-nor under-predicting are largely due to annotation noise. For instance, the predicted entity Headingly Stadium is a correct and more specific entity for the span "headingley" than the gold entity Headingly (a suburb); the predicted span "the oval" is more suitable, or at least as correct as, the gold span "oval" for the entity The Oval.</p><p>We also consider distinguishing MD errors from ED errors on the validation set. EntQA obtains 87.5 overall F 1 . When we only measure the correctness of mention spans (equivalent to treating all entity predictions as correct), we obtain 92.3 F 1 . When we only measure the correctness of rejecting or accepting candidate entities, we obtain 64.5 F 1 at the passage level and 89.3 F 1 at the document level (i.e., consider the set of candidates from all passages). The reader's relatively low passage-level F 1 in rejecting or accepting candidates is consistent with the the oracle experiment in <ref type="table" target="#tab_4">Table 3</ref>. That is, the main performance bottleneck of EntQA is discriminating gold vs non-gold entities from the candidates, though this should be taken with a grain of salt given the noise in annotation illustrated in <ref type="table" target="#tab_5">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORK</head><p>Our work follows the recent trend of formulating language tasks as QA problems, but to our knowledge we are the first to propose reduction to inverted open-domain QA. Most previous works supply questions as input to the system, along with passages in which answer spans are found. They differ only in question formulation, for instance a predicate in semantic role labeling <ref type="bibr" target="#b14">(He et al., 2015)</ref>, a relation type along with its first argument in KB completion <ref type="bibr" target="#b22">(Levy et al., 2017;</ref><ref type="bibr" target="#b24">Li et al., 2019)</ref>, an entity category in (nested) NER <ref type="bibr" target="#b25">(Li et al., 2020)</ref>, an auxiliary verb or a wh-expression in ellipsis resolution <ref type="bibr" target="#b2">(Aralikatte et al., 2021)</ref>, and other task-specific questions <ref type="bibr" target="#b28">(McCann et al., 2018)</ref>. In contrast, we solve question formulation as part of the problem by exploiting recent advances in dense text retrieval.</p><p>A notable exception is CorefQA <ref type="bibr" target="#b40">(Wu et al., 2020b)</ref>, from which we take direct inspiration. In this approach, the authors formulate coreference resolution as QA in which questions are coreferring spans and answers are the spans' antecedents (i.e., earlier spans that belong to the same coreference cluster). Since coreferring spans are unknown, the authors rely on the end-to-end coreference resolution model of <ref type="bibr" target="#b21">Lee et al. (2017)</ref> that produces candidate spans by beam search. In contrast, EntQA handles varying numbers of questions in a simpler framework of text retrieval.</p><p>As in this work, some previous works propose methods to handle varying numbers of answer spans for a given question. But their methods are based on one-vs-all classification (i.e., each label is associated with a token-level binary classifier) or reduction to tagging (i.e., spans are expressed as a BIO-label sequence) <ref type="bibr" target="#b40">(Wu et al., 2020b;</ref><ref type="bibr" target="#b24">Li et al., 2019;</ref>. We found these methods to be ineffective in preliminary experiments, and instead develop a more effective inference scheme in which the model delays its final prediction to the end for global reranking (Section 2.3).</p><p>We discuss pros and cons of EntQA vs other models in practice. While EntQA outperforms GENRE without large-scale weakly supervised pretraining, it involves dense retrieval which incurs a large memory footprint to store and index dense embeddings as pointed out by De . But it can be done on a single machine with ample RAM (ours has 252G) which is cheap. Bypassing dense retrieval is a unique strength of the autoregressive approach of GENRE and orthogonal to ours; we leave combining their strengths as future work. Our model requires a threshold ? for inference, but we find that it is easy to pick a good threshold; we also argue that it can be a useful feature in a real-world setting in which the practitioner often needs a customized trade-off between precision and recall. The threshold-based inference implies another unique feature of EntQA not explored in this work: it can naturally handle nested entity mentions. We leave nested linking as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>Existing methods for entity linking suffer from the dilemma of having to predict mentions without knowing the corresponding entities. We have presented EntQA, a new model that solves this dilemma by predicting entities first and then finding their mentions. Our approach is based on a novel reduction to inverse open-domain QA in which we retrieve an unknown number of questions (candidate entities) and predict potentially multiple answer spans (mentions) for each question. Our solution is a simple pipeline that takes full advantage of progress in text retrieval and reading comprehension. EntQA achieves new state-of-the-art results on the GERBIL benchmarking platform without relying on a KB-specific mention-candidates dictionary or expensive model-specific pretraining. <ref type="table">Table 5</ref>: GERBIL test scores with and without using the first document token as document-level topical information.</p><p>The validation F1 score improved from 87.32 to 87.75, and the GERBIL test score improved from 60.46 to 60.55. Thus threshold optimization can yield a minor improvement, but overall we find that EntQA is robust to choices of threshold in a reasonable range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B DOCUMENT-LEVEL INFORMATION</head><p>We explored various ways of injecting document-level information in paragraphs. We tried the first token, the first sentence, and a continuous topic embedding (obtained by averaging all token embeddings in the document). We settled on the first-token version because it gave the best performance. For many of the GERBIL datasets, however, we obtain almost the same performance with or without the topic information. As it is somewhat dataset-specific (e.g., the first word in AIDA is always the topic word), we leave it as an option in our model for the user to decide. <ref type="table">Table 5</ref> shows the GERBIL performance without any topic information vs with the first token in the document.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>For instance, Hoffart et al. (2011) use the 2010 dump, van Hulst et al. (2020) and we use the 2019 dump, whereas Kolitsas et al. (2018) and De Cao et al. (2021) use the 2014 dump (even though the latter use the 2019 dump for pretraining)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Leicestershire County Cricket Club 2. Grace Road 3. Somerset County Cricket Club ? 4. Durham County Cricket Club ? 5. Nottinghamshire County Cricket Club ? 6. Derbyshire County Cricket Club ? 7. Warwickshire County Cricket Club ? 8. Leicestershire ? 9. Worcestershire County Cricket Club ? 10. Yorkshire County Cricket Club 11. England cricket team ? 12. Marylebone Cricket Club ? 13. Sussex County Cricket Club ? 14. Kent County Cricket Club ? 15. Leicester ? 16. Aylestone Road ? 17. County Cricket Ground, Derby . .</figDesc><table><row><cell>Top-K candidate entities</cell></row><row><cell>1.</cell></row><row><cell>Passage</cell></row><row><cell>After bowling [Somerset] 3 out for 83 on</cell></row><row><cell>the opening morning at [Grace Road] 2 ,</cell></row><row><cell>[Leicestershire] 1 extended their first in-</cell></row><row><cell>nings by 94 runs before being bowled</cell></row><row><cell>out for 296 with [England] 11</cell></row><row><cell>K}</cell></row></table><note>. Figure 1: Example prediction by EntQA taken from AIDA-A. Given a passage, the retriever mod- ule ranks K candidate entities, then the reader module finds mentions of each entity or rejects it (marked by ?). Both modules use entity descriptions (not shown). In this example, it predicts the span "England" for the 11th candidate England cricket team but rejects the 35th candidate England (the country).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>InKB Micro F 1 on the in-domain and out-of-domain test sets on the GERBIL benchmarking platform. For each dataset, bold indicates the best model and underline indicates the second best.</figDesc><table><row><cell></cell><cell>In-domain</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Out-of-domain</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>AIDA</cell><cell cols="8">MSNBC Der K50 R128 R500 OKE15 OKE16 Avg</cell></row><row><cell>Hoffart et al. (2011)</cell><cell>72.8</cell><cell>65.1</cell><cell cols="4">32.6 55.4 46.4 42.4</cell><cell>63.1</cell><cell>0.0</cell><cell>47.2</cell></row><row><cell>Steinmetz &amp; Sack (2013)</cell><cell>42.3</cell><cell>30.9</cell><cell cols="4">26.5 46.8 18.1 20.5</cell><cell>46.2</cell><cell>46.4</cell><cell>34.7</cell></row><row><cell>Moro et al. (2014)</cell><cell>48.5</cell><cell>39.7</cell><cell cols="4">29.8 55.9 23.0 29.1</cell><cell>41.9</cell><cell>37.7</cell><cell>38.2</cell></row><row><cell>Kolitsas et al. (2018)</cell><cell>82.4</cell><cell>72.4</cell><cell cols="4">34.1 35.2 50.3 38.2</cell><cell>61.9</cell><cell>52.7</cell><cell>53.4</cell></row><row><cell>Broscheit (2019)</cell><cell>79.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Martins et al. (2019)</cell><cell>81.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>van Hulst et al. (2020)</cell><cell>80.5</cell><cell>72.4</cell><cell cols="4">41.1 50.7 49.9 35.0</cell><cell>63.1</cell><cell>58.3</cell><cell>56.4</cell></row><row><cell>De Cao et al. (2021)</cell><cell>83.7</cell><cell>73.7</cell><cell cols="4">54.1 60.7 46.7 40.3</cell><cell>56.1</cell><cell>50.0</cell><cell>58.2</cell></row><row><cell>EntQA</cell><cell>85.8</cell><cell>72.1</cell><cell cols="4">52.9 64.5 54.1 41.9</cell><cell>61.1</cell><cell>51.3</cell><cell>60.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>No model-specific pretraining. EntQA does not require model-specific pretraining; it only uses standard pretrained Transformers for initialization and is directly finetuned on AIDA. This is in</figDesc><table /><note>contrast with GENRE which requires industry-scale pretraining by weak supervision. Specifically, GENRE is trained by finetuning BART (Lewis et al., 2020) on autoregressive EL training examples constructed from all Wikipedia abstract sections on 64 GPUs for 30 hours, followed by finetuning on AIDA. Thus training GENRE from scratch is beyond the means of most academic researchers, making it difficult to make substantial changes to the model. EntQA can be trained with academic resources and outperforms GENRE.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Ablation study for the retriever module. Each line makes a single change from the baseline retriever used inTable 1. We also compare with BM25.</figDesc><table><row><cell>Retriever</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation study for the reader module. Each line makes a single change from the baseline reader used inTable 1. Candidate entities are obtained from the baseline retriever inTable 2(except the oracle experiment).</figDesc><table><row><cell>Reader</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Categorizing errors on the validation set passages. The number of passages in each category is given in parentheses. G refers to the gold annotation; P refers to the predicted annotation.</figDesc><table><row><cell>Error</cell><cell>Examples (text snippets)</cell><cell></cell></row><row><cell>Over</cell><cell>G: england fast bowler [martin mccague]Martin McCague</cell><cell>(Fill in missing mentions)</cell></row><row><cell>(443)</cell><cell>P: [england]England cricket team fast bowler [martin mccague]Martin McCague</cell><cell></cell></row><row><cell></cell><cell>G: duran, 45, takes on little -known [mexican]Mexico</cell><cell></cell></row><row><cell></cell><cell>P: [duran]Roberto Dur?n, 45, takes on little -known [mexican]Mexico</cell><cell></cell></row><row><cell cols="2">Under G: second innings before [simmons]Phil Simmons stepped in</cell><cell>(Bad threshold)</cell></row><row><cell>(474)</cell><cell>P: second innings before simmons stepped in</cell><cell></cell></row><row><cell></cell><cell>G: [ato boldon]Ato Boldon -lpr -[trinidad]Trinidad -rpr -20.</cell><cell></cell></row><row><cell></cell><cell>P: [ato boldon]Ato Boldon -lpr -trinidad -rpr -20.</cell><cell></cell></row><row><cell cols="2">Neither G: match against yorkshire at [headingley]Headingly</cell><cell>(Ambiguous entity)</cell></row><row><cell>(378)</cell><cell>P: match against yorkshire at [headingley]Headingly Stadium</cell><cell></cell></row><row><cell></cell><cell>G: at the [oval]The Oval, surrey captain chris lewis</cell><cell>(Ambiguous span)</cell></row><row><cell></cell><cell>P: at [the oval]The Oval, surrey captain chris lewis</cell><cell></cell></row><row><cell></cell><cell>G: scores in [english]England county championship matches</cell><cell>(Others)</cell></row><row><cell></cell><cell>P: scores in [english county championship]County Championship matches</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code available at: https://github.com/WenzhengZhang/EntQA</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported by the Google Faculty Research Awards Program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Contextual string embeddings for sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duncan</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on computational linguistics</title>
		<meeting>the 27th international conference on computational linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1638" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08634</idno>
		<title level="m">A bert baseline for the natural questions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ellipsis resolution as question answering: An evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Aralikatte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Lamm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>S?gaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="810" to="817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-step classification approaches to cumulative citation recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krisztian</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heri</forename><surname>Ramampiaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naimdjon</forename><surname>Takhirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kjetil</forename><surname>N?rv?g</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th conference on open research areas in information retrieval</title>
		<meeting>the 10th conference on open research areas in information retrieval</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="121" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Investigating entity knowledge in bert with simple neural end-to-end entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Broscheit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)</title>
		<meeting>the 23rd Conference on Computational Natural Language Learning (CoNLL)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="677" to="685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Electra: Pre-training text encoders as discriminators rather than generators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Autoregressive entity retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>De Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=5k8F6UU39V" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Analysis of named entity recognition and linking for tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Derczynski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Maynard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Rizzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marieke</forename><surname>Van Erp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Genevieve</forename><surname>Gorrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rapha?l</forename><surname>Troncy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johann</forename><surname>Petrak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalina</forename><surname>Bontcheva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="32" to="49" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Introduction to &quot;this is watson</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ferrucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep joint entity disambiguation with local neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugen</forename><surname>Octavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2619" to="2629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning dense representations for entity retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sayali</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Lansing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Presta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Ie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Garcia-Olano</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K19-1049</idno>
		<ptr target="https://www.aclweb.org/anthology/K19-1049" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)</title>
		<meeting>the 23rd Conference on Computational Natural Language Learning (CoNLL)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11" />
			<biblScope unit="page" from="528" to="537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Entity linking via joint encoding of types, descriptions, and context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2681" to="2690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Exploiting entity linking in queries for entity retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faegheh</forename><surname>Hasibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krisztian</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svein</forename><forename type="middle">Erik</forename><surname>Bratsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 acm international conference on the theory of information retrieval</title>
		<meeting>the 2016 acm international conference on the theory of information retrieval</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="209" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Question-answer driven semantic role labeling: Using natural language to annotate natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 conference on empirical methods in natural language processing</title>
		<meeting>the 2015 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="643" to="653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Robust disambiguation of named entities in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><forename type="middle">Amir</forename><surname>Yosef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilaria</forename><surname>Bordino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hagen</forename><surname>F?rstenau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Spaniol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilyana</forename><surname>Taneva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="782" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Kore: keyphrase overlap relatedness for entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Seufert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dat</forename><surname>Ba Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Theobald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM international conference on Information and knowledge management</title>
		<meeting>the 21st ACM international conference on Information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="545" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Billion-scale similarity search with gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dense passage retrieval for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6769" to="6781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">End-to-end neural entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Kolitsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Octavian-Eugen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Conference on Computational Natural Language Learning</title>
		<meeting>the 22nd Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="519" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">End-to-end neural coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="188" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Zero-shot relation extraction via reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Conference on Computational Natural Language Learning</title>
		<meeting>the 21st Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="333" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Bart</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Entity-relation extraction as multi-turn question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiayu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duo</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1340" to="1350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A unified mrc framework for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingrong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5849" to="5859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Design challenges for entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="315" to="328" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Joint learning of named entity recognition and entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">Henrique</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zita</forename><surname>Marinho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr? Ft</forename><surname>Martins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="190" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">The natural language decathlon: Multitask learning as question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08730</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Entity linking meets word sense disambiguation: a unified approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Moro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Raganato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="231" to="244" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Open knowledge extraction challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Giovanni Nuzzolese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><forename type="middle">Lisa</forename><surname>Gentile</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentina</forename><surname>Presutti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aldo</forename><surname>Gangemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dar?o</forename><surname>Garigliotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Semantic Web Evaluation Challenges</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">KILT: a benchmark for knowledge intensive language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Majid</forename><surname>Yazdani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><forename type="middle">De</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Maillard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassilis</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.200</idno>
		<ptr target="https://aclanthology.org/2021.naacl-main.200" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="2523" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Know what you don&apos;t know: Unanswerable questions for squad</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="784" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<title level="m">Topic Info AIDA MSNBC Der K50 R128 R500 OKE15 OKE16 Avg None</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mining, ranking and recommending entity aspects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ridho</forename><surname>Reinanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Meij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">N 3 -a collection of datasets for named entity recognition and disambiguation in the nlp interchange format</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>R?der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Usbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Hellmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gerber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Both</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3529" to="3533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Bill Slawski. How google uses named entity disambiguation for entities with the same names</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>R?der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Usbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel-Cyrille Ngonga</forename><surname>Ngomo</surname></persName>
		</author>
		<ptr target="https://www.seobythesea.com/2015/09/disambiguate-entities-in-queries-and-pages/" />
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2021" to="2030" />
		</imprint>
	</monogr>
	<note>Gerbil-benchmarking named entity recognition and linking consistently</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Semantic multimedia information retrieval based on contextual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadine</forename><surname>Steinmetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Sack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Extended Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="382" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Rel: An entity linker standing on the shoulders of giants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faegheh</forename><surname>Johannes M Van Hulst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koen</forename><surname>Hasibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krisztian</forename><surname>Dercksen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjen P De</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vries</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2197" to="2200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Scalable zeroshot entity linking with dense entity retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Josifoski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6397" to="6407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Corefqa: Coreference resolution as query-based span prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6953" to="6963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Word-entity duet representations for document ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR conference on research and development in information retrieval</title>
		<meeting>the 40th International ACM SIGIR conference on research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="763" to="772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Efficient passage retrieval with hashing for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.00882</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Collective entity disambiguation with structured gradient tree boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozanirsoy</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazi Shefaet</forename><surname>Rahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="777" to="786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Understanding hard negatives in noise contrastive estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1090" to="1101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">To see if it is possible to improve over the static threshold value ? = 0.05, we tried automatically calibrating ? based on the AIDA validation performance by considering every effective threshold obtained from a sorted list of probabilities of labeled mentions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Threshold Optimization</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>The best threshold was ? = 0.03146</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
