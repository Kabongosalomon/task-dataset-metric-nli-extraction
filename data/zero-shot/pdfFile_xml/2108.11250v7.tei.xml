<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">YOLOP: You Only Look Once for Panoptic Driving Perception</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EIC</orgName>
								<orgName type="institution">Huazhong University of Science &amp; Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manwen</forename><surname>Liao</surname></persName>
							<email>mwliao@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of EIC</orgName>
								<orgName type="institution">Huazhong University of Science &amp; Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weitian</forename><surname>Zhang</surname></persName>
							<email>wtzhang@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of EIC</orgName>
								<orgName type="institution">Huazhong University of Science &amp; Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
							<email>xgwang@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of EIC</orgName>
								<orgName type="institution">Huazhong University of Science &amp; Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
							<email>xbai@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of EIC</orgName>
								<orgName type="institution">Huazhong University of Science &amp; Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqing</forename><surname>Cheng</surname></persName>
							<email>chengwq@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of EIC</orgName>
								<orgName type="institution">Huazhong University of Science &amp; Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EIC</orgName>
								<orgName type="institution">Huazhong University of Science &amp; Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">YOLOP: You Only Look Once for Panoptic Driving Perception</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A panoptic driving perception system is an essential part of autonomous driving. A high-precision and real-time perception system can assist the vehicle in making the reasonable decision while driving. We present a panoptic driving perception network (YOLOP) to perform traffic object detection, drivable area segmentation and lane detection simultaneously. It is composed of one encoder for feature extraction and three decoders to handle the specific tasks. Our model performs extremely well on the challenging BDD100K dataset, achieving state-of-the-art on all three tasks in terms of accuracy and speed. Besides, we verify the effectiveness of our multi-task learning model for joint training via ablative studies. To our best knowledge, this is the first work that can process these three visual perception tasks simultaneously in real-time on an embedded device Jetson TX2(23 FPS) and maintain excellent accuracy. To facilitate further research, the source codes and pre-trained models are released at https://github. com/hustvl/YOLOP</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently, extensive research on autonomous driving has revealed the importance of panoptic driving perception system. It plays a significant role in autonomous driving as it can extract visual information from the images taken by the camera and assist the decision system to control the actions of the vehicle. In order to restrict the maneuver of vehicles, the visual perception system should be able to understand the scene and then provide the decision system with information including: locations of the obstacles, judgements of whether the road is drivable, the position of the lanes etc. Object detection is usually involved in the panoptic driving perception system to help the vehicles avoid obstacles and follow traffic rules. Drivable area segmentation and lane detection are also needed as they are crucial for planning the driving route of the vehicle.</p><p>For such a panoptic driving perception system, high- precision and real-time are two most critical requirements, which are related to whether the autonomous vehicle can make accurate and timely decision to ensure safety. However, for practical autonomous driving system, especially the ADAS, the computational resources are often marginal and limited. Therefore, it is very challenging to take both requirements into account in real-world scenarios.</p><p>Many methods handle these tasks separately. For instance, Faster R-CNN <ref type="bibr" target="#b23">[24]</ref> and YOLOv4 <ref type="bibr" target="#b0">[1]</ref> deal with object detection; ENet <ref type="bibr" target="#b18">[19]</ref> and PSPNet <ref type="bibr" target="#b29">[30]</ref> are proposed to perform semantic segmentation. SCNN <ref type="bibr" target="#b17">[18]</ref> and SAD-ENet <ref type="bibr" target="#b8">[9]</ref> are used for detecting lanes. Despite the excellent performance these methods achieve, processing these tasks one after another takes longer time than tackling them all at once. When deploying the panoptic driving perception system on embedded devices commonly used in the selfdriving car, limited computational resources and latency should be taken into account. In addition, different tasks in traffic scenes understanding often have much related information. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, lanes are often the boundary of drivable area, and drivable area usually closely surrounds the traffic objects. A multi-task network is more suitable in this situation as <ref type="bibr" target="#b0">(1)</ref> it can accelerate the image analysis process by handling multiple tasks simultaneously rather than sequentially. (2) it can share information among multiple tasks as multi-task network often shares the same feature extraction backbone. Therefore, it is of essence to explore multi-task approaches in autonomous driving.</p><p>In order to solve the multi-task problem for panoptic driving perception, i.e., traffic object detection, drivable area segmentation and lane detection, while obtaining high precision and fast speed, we design a simple and efficient network architecture. We use a lightweight CNN <ref type="bibr" target="#b25">[26]</ref> as the encoder to extract features from the image. Then these feature maps are fed to three decoders to complete their respective tasks. Our detection decoder is based on the current best-performing single-stage detection network <ref type="bibr" target="#b0">[1]</ref> for two main reasons: (1) The single-stage detection network is faster than the two-stage detection network. <ref type="bibr">(</ref>2) The gridbased prediction mechanism of the single-stage detector is more related to the other two semantic segmentation tasks, while instance segmentation is usually combined with the region-based detector <ref type="bibr" target="#b6">[7]</ref>. And we verify the two viewpoints in the experiments section. The feature map output by the encoder incorporates semantic features of different levels and scales, and our segmentation branch can use these feature maps to complete pixel-wise semantic prediction excellently.</p><p>In addition to the end-to-end training strategy, we attempt some alternating optimization paradigms which train our model step-by-step. On the one hand, we can put unrelated tasks in different training steps to prevent interlimitation. On the other hand, the task trained first can guide other tasks. So this kind of paradigm sometimes works well though cumbersome. However, experiments show that it is unnecessary for our model as the one trained end to end can perform well enough. Our panoptic driving perception system reaches 41 FPS on a single NVIDIA TITAN XP and 23 FPS on Jetson TX2; meanwhile, it achieves state-of-the-art on the three tasks of the BDD100K dataset <ref type="bibr" target="#b27">[28]</ref>.</p><p>In summary, our main contributions are: (1) We put forward an efficient multi-task network that can jointly handle three crucial tasks in autonomous driving: object detection, drivable area segmentation and lane detection to save computational costs and reduce inference time. Our work is the first to reach real-time on embedded devices while maintaining state-of-the-art level performance on the BDD100K dataset. <ref type="bibr" target="#b1">(2)</ref> We design the ablative experiments to verify the effectiveness of our multi-tasking scheme. It is proved that the three tasks can be learned jointly without tedious alternating optimization. (3) We design the ablative experiments to prove that the grid-based prediction mechanism of detection task is more related to that of semantic segmentation task, which is believed to provide reference for other relevant multi-task learning research works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we review solutions to the above three tasks respectively, and then introduce some related multitask learning work. We only concentrate on solutions based on deep learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Traffic Object Detection</head><p>In recent years, with the rapid development of deep learning, many prominent object detection algorithms have emerged. Current mainstream object detection algorithms can be divided into two-stage methods and one-stage methods.</p><p>Two-stage methods complete the detection task in two steps. First, regional proposals are obtained, and then features in the regional proposals are used to locate and classify the objects. The generation of regional proposals has gone through several stages of development <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>The SSD-series <ref type="bibr" target="#b13">[14]</ref> and YOLO-series algorithms are milestones among one-stage methods. This kind of algorithm performs bounding box regression and object classification simultaneously. YOLO <ref type="bibr" target="#b20">[21]</ref> divides the picture into S?S grids instead of extracting regional proposals with the RPN network, which significantly accelerates the detection speed. YOLO9000 <ref type="bibr" target="#b21">[22]</ref> introduces the anchor mechanism to improve the recall of detection. YOLOv3 <ref type="bibr" target="#b22">[23]</ref> uses the feature pyramid network structure to achieve multi-scale detection. YOLOv4 <ref type="bibr" target="#b0">[1]</ref> further improves the detection performance by refining the network structure, activation function, loss function and applying abundant data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Drivable Area Segmentation</head><p>Due to the rapid development of deep learning, a number of CNN-based methods have made great success in semantic segmentation area, and they can be applied in drivable area segmentation task to provide pixel-level results. FCN <ref type="bibr" target="#b14">[15]</ref> firstly introduces fully convolutional network to semantic segmentation. Despite the skip-connection refinement, its performance is still limited by low-resolution output. PSPNet <ref type="bibr" target="#b29">[30]</ref> comes up with the pyramid pooling module to extract features in various scales to enhance its performance. Besides accuracy, speed is also a key element in evaluating this task. In order to achieve real-time inference speed, ENet <ref type="bibr" target="#b18">[19]</ref> reduces size of the feature maps. Recently, multitask learning is introduced to deal with this task, Ed-geNet <ref type="bibr" target="#b5">[6]</ref> combine edge detection with drivable area segmentation task to obtain more accurate segmentation results without compromising the inference speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Lane Detection</head><p>In lane detection, there are lots of innovative researches based on deep learning. <ref type="bibr" target="#b16">[17]</ref> constructs a dual-branch network to perform semantic segmentation and pixel embedding on images. It further clusters the dual-branch features to achieve lane instance segmentation. SCNN <ref type="bibr" target="#b17">[18]</ref> proposes slice-by-slice convolution, which enables the message to pass between pixels across rows and columns in a layer, but this convolution is very time-consuming. Enet-SAD <ref type="bibr" target="#b8">[9]</ref> uses self attention distillation method, which enables lowlevel feature maps to learn from high-level feature maps. This method improves the performance of the model while keeping the model lightweight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Multi-task Approaches</head><p>The goal of multi-task learning is to learn better representations through shared information among multiple tasks. Especially, a CNN-based multitask learning method can also achieve convolutional sharing of the network structure. Mask R-CNN <ref type="bibr" target="#b6">[7]</ref> extends Faster R-CNN by adding a branch for predicting object mask, which combines instance segmentation and object detection tasks effectively, and these two tasks can promote each other's performance. LSNet <ref type="bibr" target="#b2">[3]</ref> summarizes object detection, instance segmentation and pose estimation as location-sensitive visual recognition and uses a unified solution to handle these tasks. With a shared encoder and three independent decoders, MultiNet <ref type="bibr" target="#b24">[25]</ref> completes the three scene perception tasks of scene classification, object detection and segmentation of the driving area simultaneously. DLT-Net <ref type="bibr" target="#b19">[20]</ref> inherits the encoder-decoder structure, and contributively constructs context tensors between sub-task decoders to share designate information among tasks. <ref type="bibr" target="#b28">[29]</ref> puts forward mutually interlinked sub-structures between lane area segmentation and lane boundary detection. Meanwhile, it proposes a novel loss function to constrain the lane line to the outer contour of the lane area so that they're going to overlap geometrically. However, this prior assumption also limits its application as it only works well on scenarios where the lane line tightly wraps the lane area. What's more, the training paradigm of multitask model is also worth think-ing about. <ref type="bibr" target="#b9">[10]</ref> states that the joint training is appropriate and beneficial only when all those tasks are indeed related; otherwise, it is necessary to adopt alternating optimization. So Faster R-CNN <ref type="bibr" target="#b23">[24]</ref> adopts a pragmatic 4-step training algorithm to learn shared features. This paradigm sometimes may be helpful, but mostly it is tedious.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>We put forward a simple and efficient feed-forward network that can accomplish traffic object detection, drivable area segmentation and lane detection tasks altogether. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, our panoptic driving perception singleshot network, termed as YOLOP, contains one shared encoder and three subsequent decoders to solve specific tasks. There are no complex and redundant shared blocks between different decoders, which reduces computational consumption and allows our network to be easily trained end-to-end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Encoder</head><p>Our network shares one encoder, which is composed of a backbone network and a neck network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Backbone</head><p>The backbone network is used to extract the features of the input image. Usually, some classic image classification networks serve as the backbone. Due to the excellent performance of YOLOv4 <ref type="bibr" target="#b0">[1]</ref> on object detection, we choose CSP-Darknet <ref type="bibr" target="#b25">[26]</ref> as the backbone, which solves the problem of gradient duplication during optimization <ref type="bibr" target="#b26">[27]</ref>. It supports feature propagation and feature reuse which reduces the amount of parameters and calculations. Therefore, it is conducive to ensuring the real-time performance of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Neck</head><p>The neck is used to fuse the features generated by the backbone. Our neck is mainly composed of Spatial Pyramid Pooling (SPP) module <ref type="bibr" target="#b7">[8]</ref> and Feature Pyramid Network (FPN) module <ref type="bibr" target="#b10">[11]</ref>. SPP generates and fuses features of different scales, and FPN fuses features at different semantic levels, making the generated features contain multiple scales and multiple semantic level information. We adopt the method of concatenation to fuse features in our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Decoders</head><p>The three heads in our network are specific decoders for the three tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Detect Head</head><p>Similar to YOLOv4, we adopt an anchor-based multi-scale detection scheme. Firstly, we use a structure called Path Aggregation Network (PAN), a bottom-up feature pyramid network <ref type="bibr" target="#b12">[13]</ref>. FPN transfers semantic features top-down, and PAN transfers positioning features bottom-up. We combine them to obtain a better feature fusion effect, and then directly use the multi-scale fusion feature maps in the PAN for detection. Then, each grid of the multi-scale feature map will be assigned three prior anchors with different aspect ratios, and the detection head will predict the offset of position and the scaling of the height and width, as well as the corresponding probability of each category and the confidence of the prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Drivable Area Segment Head &amp; Lane Line Segment Head</head><p>Drivable area segment head and Lane line Segment head adopt the same network structure. We feed the bottom layer of FPN to the segmentation branch, with the size of (W/8, H/8, 256). Our segmentation branch is very simple. After three upsampling processes, we restore the output feature map to the size of (W, H, 2), which represents the probability of each pixel in the input image for the drivable area/lane line and the background. Because of the shared SPP in the neck network, we do not add an extra SPP module to segment branches like others usually do <ref type="bibr" target="#b29">[30]</ref>, which brings no improvement to the performance of our network.</p><p>Additionally, we use the Nearest Interpolation method in our upsampling layer to reduce computation cost instead of deconvolution. As a result, not only do our segment decoders gain high precision output, but also be very fast during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Loss Function</head><p>Since there are three decoders in our network, our multitask loss contains three parts. As for the detection loss L det , it is a weighted sum of classification loss, object loss and bounding box loss as in equation 1.</p><formula xml:id="formula_0">L det = ? 1 L class + ? 2 L obj + ? 3 L box ,<label>(1)</label></formula><p>where L class and L obj are focal loss <ref type="bibr" target="#b11">[12]</ref>, which is utilized to reduce the loss of well-classified examples, thus forces the network to focus on the hard ones. L class is used for penalizing classification and L obj for the confidence of one prediction. L box is L CIoU <ref type="bibr" target="#b30">[31]</ref>, which takes distance, overlap rate, the similarity of scale and aspect ratio between the predicted box and ground truth into consideration. Both of the loss of drivable area segmentation L da?seg and lane line segmentation L ll?seg contain Cross Entropy Loss with Logits L ce , which aims to minimize the classification errors between pixels of network outputs and the targets. It is worth mentioning that IoU loss: L IoU = 1 ? T P T P +F P +F N is added to L ll?seg as it is especially efficient for the prediction of the sparse category of lane lines. L da and L ll?seg are defined as equation <ref type="formula" target="#formula_1">(2)</ref>, (3) respectively.</p><formula xml:id="formula_1">L da?seg = L ce ,<label>(2)</label></formula><formula xml:id="formula_2">L ll?seg = L ce + L IoU .<label>(3)</label></formula><p>In conclusion, our final loss is a weighted sum of the three parts all together as in equation <ref type="formula" target="#formula_3">(4)</ref>.</p><formula xml:id="formula_3">L all = ? 1 L det + ? 2 L da?seg + ? 3 L ll?seg ,<label>(4)</label></formula><p>where ? 1 , ? 2 , ? 3 , ? 1 , ? 2 , ? 3 can be tuned to balance all parts of the total loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training Paradigm</head><p>We attempt different paradigms to train our model. The simplest one is training end to end, and then three tasks can be learned jointly. This training paradigm is useful when all tasks are indeed related. In addition, some alternating optimization algorithms also have been tried, which train our model step by step. In each step, the model can focus on one or multiple related tasks regardless of those unrelated. Even if not all tasks are related, our model can still learn adequately on each task with this paradigm. And Algorithm 1 illustrates the process of one step-by-step training method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Setting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Dataset Setting</head><p>The BDD100K dataset <ref type="bibr" target="#b27">[28]</ref> supports the research of multitask learning in the field of autonomous driving. With 100k frames of pictures and annotations of 10 tasks, it is the largest driving video dataset. As the dataset has the diversity of geography, environment, and weather, the algorithm trained on the BDD100k dataset is robust enough to migrate to a new environment. Therefore, we choose the BDD100k dataset to train and evaluate our network. The BDD100K dataset has three parts, training set with 70K images, validation set with 10K images, and test set with 20K images. Since the label of the test set is not public, we evaluate our network on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Implementation Details</head><p>In order to enhance the performance of our model, we empirically adopt some practical techniques and methods of data augmentation.</p><p>With the purpose of enabling our detector to get more prior knowledge of the objects in the traffic scene, we use the k-means clustering algorithm to obtain prior anchors from all detection frames of the dataset. We use Adam as  the optimizer to train our model and the initial learning rate, ? 1 , and ? 2 are set to be 0.001, 0.937, and 0.999 respectively. Warm-up and cosine annealing are used to adjust the learning rate during the training, which aim at leading the model to converge faster and better <ref type="bibr" target="#b15">[16]</ref>.</p><p>We use data augmentation to increase the variability of images so as to make our model robust in different environments. Photometric distortions and geometric distortions are taken into consideration in our training scheme. For photometric distortions, we adjust the hue, saturation and value of images. We use random rotating, scaling, translating, shearing, and left-right flipping to process images to handle geometric distortions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Experimental Setting</head><p>We select some excellent multi-task networks and networks that focus on a single task to compare with our network. Both MultiNet and DLT-Net handle multiple panoptic driving perception tasks, and they have achieved great performance in object detection and drivable area segmentation tasks on the BDD100k dataset. Faster-RCNN is an outstanding representative of the two-stage object detection network. YOLOv5 is the single-stage network that achieves state-of-the-art performance on the COCO dataset. PSP-Net achieves splendid performance on semantic segmentation task with its superior ability to aggregate global information. We retrain the above networks on the BDD100k dataset and compare them with our network on object detection and drivable area segmentation tasks. Since there is no suitable existing multi-task network that processes lane detection task on the BDD100K dataset, we compare our network with Enet <ref type="bibr" target="#b18">[19]</ref>, SCNN and Enet-SAD, three advanced lane detection networks. Besides, the performance of the joint training paradigm is compared with alternating training paradigms of many kinds. Moreover, we compare the accuracy and speed of our multi-task model trained to handle multiple tasks with the one trained to perform a specific task. Furthermore, we compare the performance of semantic segmentation task combined with single-stage detection task and two-stage detection task. Following <ref type="bibr" target="#b8">[9]</ref>, we resize images in BDD100k dataset from 1280?720?3 to 640?384?3. All control experiments follow the same experimental settings and evaluation metrics, and all experiments are run on NVIDIA GTX TITAN XP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Result</head><p>In this section, we just simply train our model end to end and then compare it with other representative models on all three tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Traffic Object Detection Result</head><p>Visualization of the traffic objects detection is shown in <ref type="figure" target="#fig_3">Figure 3</ref>. Since the Multinet and DLT-Net can only detect vehicles, we only consider the vehicle detection results of five models on the BDD100K dataset. As shown in <ref type="table" target="#tab_0">Table 1</ref>, we use Recall and mAP50 as the evaluation metric of detection accuracy. Our model exceeds Faster R-CNN, Multi-Net, and DLT-Net in detection accuracy, and is comparable to YOLOv5s that actually uses more tricks than ours. Moreover, our model can infer in real time. YOLOv5s is faster than ours because it does not have the lane line segment head and drivable area segment head. <ref type="figure" target="#fig_4">Figure 4</ref> shows the qualitative comparison between Faster R-CNN and YOLOP. Due to the information share of multi-task, the prediction results of YOLOP are more reasonable. For example, YOLOP will not misidentify the objects far from the road as vehicle. Moreover, the examples of false negative are much less and the bounding boxes are more accurate. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Drivable Area Segmentation Result</head><p>Visualization results of the drivable area segmentation can be seen in <ref type="figure">Figure 5</ref>. In this paper, both "area/drivable" and "area/alternative" classes in BDD100K dataset are categorized as "Drivable area" without distinction. Our model only needs to distinguish the drivable area and the background in the image. mIoU is used to evaluate the segmentation performance of different models. The results are shown in <ref type="table">Table 2</ref>. It can be seen that our model outperforms MultiNet, DLT-Net and PSPNet by 19.9%, 20.2%, and 1.9%, respectively. Furthermore, our inference speed is 4 to 5 times faster than theirs. The comparison between results of PSPNet and YOLOP is showed in <ref type="figure">Figure 6</ref>. Both PSPNet and YOLOP have perfomed well in this task. But YOLOP is significantly better at segmenting the edge areas that next to vehicles or lane lines. We think it's mainly because that both two other tasks provide the edge information for this task. Meanwhile, YOLOP makes fewer stupid mistakes, such as misjudging the opposite lane area as drivable area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Lane Detection Result</head><p>The visualization results of lane detection can be seen in <ref type="figure">Figure 7</ref>. The lane lines in BDD100K dataset are labeled with two lines, so it is very tricky to directly use the annotation. The experimental settings follow the <ref type="bibr" target="#b8">[9]</ref> in order to compare expediently. First of all, we calculate the center lines based on the two-line annotations. Then we draw  the lane line of the training with width set to 8 pixels while keeping the lane line width of the test set as 2 pixels. We use pixel accuracy and IoU of lanes as evaluation metrics. As shown in the <ref type="table" target="#tab_2">Table 3</ref>, the performance of our model dramatically exceeds the other three models. <ref type="figure">Figure 8</ref> shows the comparison of Lane line detection results of ENet-SAD and YOLOP. The segmentation results of YOLOP is more accurate and continuous than ENet-SAD obviously. With the imformation shared by the other two tasks, YOLOP will not mistake some areas where some ve-hicles are located or driveable as lane lines, but Enet-SAD always does.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>We designed the following three ablation experiments to further illustrate the effectiveness of our scheme. All the evaluation metrics in this section are consistent with above. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">End-to-end v.s. Step-by-step</head><p>In <ref type="table">Table 4</ref>, we compare the performance of joint training paradigm with alternating training paradigms of many kinds <ref type="bibr" target="#b0">1</ref> . Obviously, our model has performed very well enough through end-to-end training, so there is no need to perform alternating optimization. However, it is interesting that the paradigm training detection task firstly seems to perform better. We think it is mainly because our model is closer to a complete detection model and the model is harder to converge when performing detection tasks. What's more, the paradigm consist of three steps slightly outperforms that with two steps. Similar alternating training can be run for more steps, but we have observed negligible improvements. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Multi-task v.s. Single task</head><p>To verify the effectiveness of our multi-task learning scheme, we compare the performance of the multi-task scheme and single task scheme. On the one hand, we train our model to perform 3 tasks simultaneously. On the other hand, we train our model to perform traffic object detection, drivable area segmentation, and lane line segmentation tasks separately. <ref type="table">Table 5</ref> shows the comparison of the performance of these two schemes on each specific task. It can be seen that our model adopts the multi-task scheme to achieve performance is close to that of focusing on a single task. More importantly, the multitask model can save a lot of time compared to executing each task individually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Region-based v.s. Grid-based</head><p>To verify the viewpoint that the grid-based prediction mechanism is more related to the two semantic segmentation tasks than the region-based prediction mechanism. We extend Faster R-CNN by adding two semantic segment heads to perform three tasks in parallel as our model did, and we call such a new model R-CNNP. We train both YOLOP and R-CNNP to (i) perform detection task and two segmentation tasks separately and (ii) three tasks simultaneously. In both two experiments above, the two segmentation tasks are trained jointly as there is no need to consider the interaction between them. All the experimental settings are the same, and the results are shown in <ref type="table">Table 6</ref>. In the R-CNNP framework, the performance of multi-task training is much worse compared with training the detection task and semantic segmentation tasks separately. Obviously, the combination of two kinds of tasks conflicts in R-CNNP framework. But there is no such problem in our YOLOP framework, the performance of multi-task training is equal to that of focusing only on detection or semantic segmentation task. Thus we hold the opinion that this is due to the detection head of YOLOP, like two other semantic segmentation heads, directly perform global classification or regression tasks on the whole feature map output by Encoder, so they are similar and related in terms of prediction mechanism. Nevertheless, the detection head of R-CNNP needs to select region proposals first, and then perform prediction on the feature maps of each individual proposals, which is quite different from the global prediction mechanism of semantic segmentation. In addition, R-CNNP is far behind YOLOP in terms of inference speed. Therefore, our framework is a better choice for joint training detection and segmentation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we put forward a brand-new, simple and efficient network, which can simultaneously handle three driving perception tasks of object detection, drivable area segmentation and lane detection and can be trained end-toend. Our model performs exceptionally well on the challenging BDD100k dataset, achieving or greatly exceeding state-of-the-art level on all three tasks. And it is the first to realize real-time reasoning on embedded device Jetson TX2, which ensures that our network can be used in realworld scenarios. Moreover, we have verified that the gridbased prediction mechanism is more related to that of semantic segmentation task. which may be of certain reference significance to similar multi-task learning research works.</p><p>Currently, although our multi-task network can be trained end-to-end without compromising the performance of each other, we hope to improve the performance of those tasks with more appropriate paradigm for multitask learning. Furthermore, our model is limited in three tasks, more tasks related with autonomous driving perception system such as depth estimation can be added in our future frame work to make the whole system more complete and practical. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The input and output of our model. The purpose of our model is to process traffic objects detection, drivable area segmentation and lane detection simultaneously in one input image. In (b), the brown bounding boxes indicate traffic objects, the green areas are the drivable areas, and the blue lines represent the lane line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The architecture of YOLOP. YOLOP shares one encoder and combines three decoders to solve different tasks. The encoder consists of a backbone and a neck.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>repeat 3 : 4 :</head><label>34</label><figDesc>Sample a mini-batch (x s , y s ) from training set T . ? L all (F(x s ; ?), y s ) 5: ? ? arg min ? 6: until &lt; thr 7: end procedure 8: ? ? ? \ {? seg } // Freeze parameters of two Segmentation heads. 9: TRAIN(F, T ) 10: ? ? ? ? {? seg } \ {? det , ? enc } // Freeze parameters of Encoder and Detect head and activate parameters of two Segmentation heads. 11: TRAIN(F, T ) 12: ? ? ? ? {? det , ? enc } // Activate all parameters of the neural network. 13: TRAIN(F, T ) 14: return Trained network F(x; ?)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Visualization of the traffic objects detection results of YOLOP. Top Row: Traffic objects detection results in day-time scenes. Bottom row: Traffic objects detection results in night scenes. (a) Results of Faster R-CNN (b) Results of YOLOP</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Comparison between the traffic objects detection results of Faster R-CNN and YOLOP. Top Row: Traffic objects detection results of Faster R-CNN. Bottom row: Traffic objects detection results of YOLOP. The green bounding boxes are the detected correct vehicles. The yellow dotted bounding boxes are the false negative. The red bounding boxes indicate the false positive.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>Visualization of the drivable area segmentation results of YOLOP. Top Row: Drivable area segmentation results in day-time scenes. Bottom row: Drivable area segmentation results in night scenes. (a) Results of PSPNet (b) Results of YOLOP Comparison between the drivable area segmentation results of PSPNet and YOLOP. Top Row: Drivable area segmentation results of PSPNet. Bottom row: Drivable area segmentation results of YOLOP. The yellow ellipses are the false negative. The red ellipses indicate the false positive.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .Figure 8 .</head><label>78</label><figDesc>Visualization of the lane detection results of YOLOP. Top Row: Lane detection results in day-time scenes. Bottom row: Lane detection results in night scenes. (a) Results of ENet-SAD (b) Results of YOLOP Comparison between the lane detection results of ENet-SAD and YOLOP. Top Row: Lane detection results of ENet-SAD. Bottom row: Lane detection results of YOLOP. The yellow ellipses are the false negative. The red ellipses indicate the false positive.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Algorithm 1</head><label>1</label><figDesc>One step-by-step Training Method. First, we only train Encoder and Detect head. Then we freeze the Encoder and Detect head as well as train two Segmentation heads. Finally, the entire network is trained jointly for all three tasks. Input: Target neural network F with parameter group: ? = {? enc , ? det , ? seg };</figDesc><table><row><cell>Training set: T ;</cell></row><row><cell>Threshold for convergence: thr;</cell></row><row><cell>Loss function: L all</cell></row><row><cell>Output: Well-trained network: F(x; ?)</cell></row><row><cell>1: procedure TRAIN(F, T )</cell></row><row><cell>2:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Lane Detection Results: comparing the proposed YOLOP with state-of-the-art lane detection methods.</figDesc><table><row><cell>Network</cell><cell cols="3">Accuracy(%) IoU(%) Speed(fps)</cell></row><row><cell>ENet</cell><cell>34.12</cell><cell>14.64</cell><cell>100</cell></row><row><cell>SCNN</cell><cell>35.79</cell><cell>15.84</cell><cell>19.8</cell></row><row><cell>ENet-SAD</cell><cell>36.56</cell><cell>16.02</cell><cell>50.6</cell></row><row><cell>YOLOP (ours)</cell><cell>70.50</cell><cell>26.20</cell><cell>41</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .Table 5 .Table 6 .</head><label>456</label><figDesc>Panoptic driving perception results: the end-to-end scheme v.s. different step-by-step schemes. Panoptic driving perception results: multi-task learning v.s. single task learning. Panoptic driving perception results: Grid-based v.s. Region-based.</figDesc><table><row><cell></cell><cell>Training method</cell><cell cols="2">Recall(%)</cell><cell>AP(%)</cell><cell>mIoU(%)</cell><cell>Accuracy(%)</cell><cell>IoU(%)</cell></row><row><cell></cell><cell>ES-W</cell><cell>87.0</cell><cell></cell><cell>75.3</cell><cell>90.4</cell><cell>66.8</cell><cell>26.2</cell></row><row><cell></cell><cell>ED-W</cell><cell>87.3</cell><cell></cell><cell>76.0</cell><cell>91.6</cell><cell>71,2</cell><cell>26.1</cell></row><row><cell></cell><cell>ES-D-W</cell><cell>87.0</cell><cell></cell><cell>75.1</cell><cell>91.7</cell><cell>68.6</cell><cell>27.0</cell></row><row><cell></cell><cell>ED-S-W</cell><cell>87.5</cell><cell></cell><cell>76.1</cell><cell>91.6</cell><cell>68.0</cell><cell>26.8</cell></row><row><cell></cell><cell>End-to-end</cell><cell>89.2</cell><cell></cell><cell>76.5</cell><cell>91.5</cell><cell>70.5</cell><cell>26.2</cell></row><row><cell cols="2">Training method</cell><cell>Recall(%)</cell><cell>AP(%)</cell><cell>mIoU(%)</cell><cell>Accuracy(%)</cell><cell>IoU(%)</cell><cell>Speed(ms/frame)</cell></row><row><cell></cell><cell>Det(only)</cell><cell>88.2</cell><cell>76.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>15.7</cell></row><row><cell cols="2">Da-Seg(only)</cell><cell>-</cell><cell>-</cell><cell>92.0</cell><cell>-</cell><cell>-</cell><cell>14.8</cell></row><row><cell cols="2">Ll-Seg(only)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>79.6</cell><cell>27.9</cell><cell>14.8</cell></row><row><cell></cell><cell>Multitask</cell><cell>89.2</cell><cell>76.5</cell><cell>91.5</cell><cell>70.5</cell><cell>26.2</cell><cell>24.4</cell></row><row><cell cols="2">Training method</cell><cell>Recall(%)</cell><cell>AP(%)</cell><cell>mIoU(%)</cell><cell>Accuracy(%)</cell><cell>IoU(%)</cell><cell>Speed(ms/frame)</cell></row><row><cell>R-CNNP</cell><cell>Det(only) Seg(only) Multitask</cell><cell>79.0 -77.2(-1.8)</cell><cell>67.3 -62.6(-4.7)</cell><cell>-90.2 86.8(-3.4)</cell><cell>-59.5 49.8(-9.7)</cell><cell>-24.0 21.5(-2.5)</cell><cell>--103.3</cell></row><row><cell>YOLOP</cell><cell>Det(only) Seg(only) Multitask</cell><cell>88.2 -89.2(+1.0)</cell><cell>76.9 -76.5(-0.4)</cell><cell>-91.6 91.5(-0.1)</cell><cell>-69.9 70.5(+0.6)</cell><cell>-26.5 26.2(-0.3)</cell><cell>--24.4</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">(a) Day-time result (b) Night-time result</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">E, D, S and W refer to Encoder, Detect head, two Segment heads and whole network. So the Algorithm 1 can be marked as ED-S-W, and the same for others.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Yolov4: Optimal speed and accuracy of object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Yuan Mark</forename><surname>Liao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10934</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun. R-Fcn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.06409</idno>
		<title level="m">Object detection via region-based fully convolutional networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.04899</idno>
		<title level="m">Qingming Huang, and Qi Tian. Location-sensitive visual recognition with cross-iou loss</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Using channel-wise attention for deep cnn based real-time semantic segmentation with class-aware edge information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiang-Yu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Yung</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Chen</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1041" to="1051" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning lightweight lane detection cnns by self attention distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuenan</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning with whom to share in multi-task feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Towards end-to-end lane detection: an instance segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davy</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="286" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spatial as deep: Spatial cnn for traffic scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Enet: A deep neural network architecture for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangpil</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugenio</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02147</idno>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dlt-net: Joint detection of drivable areas, lane lines, and traffic objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeqiang</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4670" to="4679" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.01497</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Multinet: Real-time joint semantic reasoning for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Teichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Zoellner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.07695</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Yuan Mark</forename><surname>Liao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.08036</idno>
		<title level="m">Scaled-yolov4: Scaling cross stage partial network</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cspnet: A new backbone that can enhance learning capability of cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Yuan Mark</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueh-Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping-Yang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Wei</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I-Hau</forename><surname>Yeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="390" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Bdd100k: A diverse driving video database with scalable annotation tooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vashisht</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04687</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Geometric constrained joint lane segmentation and lane boundary detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="486" to="502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Distance-iou loss: Faster and better learning for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohui</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinze</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongguang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongwei</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12993" to="13000" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
