<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GMAN: A Graph Multi-Attention Network for Traffic Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanpan</forename><surname>Zheng</surname></persName>
							<email>zhengchuanpan@stu.xmu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Fujian Key Laboratory of Sensing and Computing for Smart Cities</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Fujian Institute of Urban Traffic Big Data Research</orgName>
								<orgName type="institution" key="instit2">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Fan</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Fujian Key Laboratory of Sensing and Computing for Smart Cities</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Fujian Institute of Urban Traffic Big Data Research</orgName>
								<orgName type="institution" key="instit2">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Wang</surname></persName>
							<email>cwang@xmu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Fujian Key Laboratory of Sensing and Computing for Smart Cities</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Fujian Institute of Urban Traffic Big Data Research</orgName>
								<orgName type="institution" key="instit2">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhong</forename><surname>Qi</surname></persName>
							<email>jianzhong.qi@unimelb.edu.au</email>
							<affiliation key="aff3">
								<orgName type="department">School of Computing and Information Systems</orgName>
								<orgName type="institution">University of Melbourne</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GMAN: A Graph Multi-Attention Network for Traffic Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Long-term traffic prediction is highly challenging due to the complexity of traffic systems and the constantly changing nature of many impacting factors. In this paper, we focus on the spatio-temporal factors, and propose a graph multi-attention network (GMAN) to predict traffic conditions for time steps ahead at different locations on a road network graph. GMAN adapts an encoder-decoder architecture, where both the encoder and the decoder consist of multiple spatio-temporal attention blocks to model the impact of the spatio-temporal factors on traffic conditions. The encoder encodes the input traffic features and the decoder predicts the output sequence. Between the encoder and the decoder, a transform attention layer is applied to convert the encoded traffic features to generate the sequence representations of future time steps as the input of the decoder. The transform attention mechanism models the direct relationships between historical and future time steps that helps to alleviate the error propagation problem among prediction time steps. Experimental results on two real-world traffic prediction tasks (i.e., traffic volume prediction and traffic speed prediction) demonstrate the superiority of GMAN. In particular, in the 1 hour ahead prediction, GMAN outperforms state-of-the-art methods by up to 4% improvement in MAE measure. The source code is available at https://github.com/zhengchuanpan/GMAN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Traffic prediction aims to predict the future traffic conditions (e.g., traffic volume or speed) in road networks based on historical observations (e.g., recorded via sensors). It plays a significant role in many real-world applications. For example, the accurate traffic prediction can help transportation agencies better control the traffic to reduce traffic congestion <ref type="bibr">(Lv et al. 2018;</ref><ref type="bibr" target="#b34">Zheng et al. 2019)</ref>.</p><p>The traffic conditions at nearby locations are expected to impact each other. To capture such spatial correlations, Convolutional neural networks (CNN) are widely used <ref type="bibr" target="#b32">(Zhang, Zheng, and Qi 2017;</ref><ref type="bibr" target="#b29">Yao et al. 2018;</ref>. Meanwhile, The traffic condition at a location is also correlated with its historical observations. Recurrent neural networks (RNN) are the traffic condition of sensor 3 at time step t + l + 1 may be more correlated to that of distant time steps (e.g., t ? 1) rather than recent time steps (e.g., t + l). widely applied to model such temporal correlations <ref type="bibr" target="#b17">(Ma et al. 2015;</ref><ref type="bibr" target="#b21">Song, Kanasugi, and Shibasaki 2016)</ref>. Recent studies formulate the traffic prediction as a graph modeling problem, since the traffic conditions are restricted on road network graphs <ref type="bibr" target="#b14">(Li et al. 2018b;</ref><ref type="bibr" target="#b31">Yu, Yin, and Zhu 2018;</ref><ref type="bibr" target="#b27">Wu et al. 2019b</ref>). Using graph convolutional networks (GCN) <ref type="bibr" target="#b5">(Defferrard, Bresson, and Vandergheynst 2016)</ref>, these studies achieve promising results for short-term (5 ? 15 minutes ahead) traffic prediction. However, the longterm (up to a few hours ahead <ref type="bibr" target="#b10">(Hou and Li 2016)</ref>) traffic prediction still lacks a satisfactory progress in the literature, mainly due to the following challenges. 1) Complex spatio-temporal correlations.</p><p>? Dynamic spatial correlations. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the correlations of traffic conditions among sensors in a road network change significantly over time (e.g., before and during peak hours). How to dynamically select relevant sensors' data to predict a target sensor's traffic conditions in long-term horizon is a challenging issue. ? Non-linear temporal correlations. Also in <ref type="figure" target="#fig_0">Figure 1</ref>, the traffic condition at a sensor may fluctuate tremendously and suddenly (e.g., because of an accident), affecting the correlations between different time steps. How to adaptively model the non-linear temporal correlations when the time goes further into the future remains a challenge.</p><p>2) Sensitivity to error propagation. In the long-term horizon, small errors in each time step may amplify when predictions are made further into the future. Such error propagations make predictions into far future highly challenging. To address the aforementioned challenges, we propose a Graph Multi-Attention Network (GMAN) to predict traffic conditions on a road network graph over time steps ahead. Here, the traffic conditions refer to observations over a traffic system that can be reported in numeric values. For illustration purpose, we focus on traffic volume and traffic speed predictions, although our model could be applied to predictions of other numerical traffic data.</p><p>GMAN follows the encoder-decoder architecture, where the encoder encodes the input traffic features and the decoder predicts the output sequence. A transform attention layer is added between the encoder and the decoder to convert the encoded historical traffic features to generate future representations. Both the encoder and the decoder are composed of a stack of ST-Attention blocks. Each ST-Attention block is formed by a spatial attention mechanism to model the dynamic spatial correlations, a temporal attention mechanism to model the non-linear temporal correlations, and a gated fusion mechanism to adaptively fuse the spatial and temporal representations. The transform attention mechanism models direct relationships between historical and future time steps to alleviate the effect of error propagation. Experiments on two real-world datasets confirm that GMAN achieves state-of-the-art performances.</p><p>The contributions of this work are summarized as follow: ? We propose spatial and temporal attention mechanisms to model the dynamic spatial and non-linear temporal correlations, respectively. Moreover, we design a gated fusion to adaptively fuse the information extracted by spatial and temporal attention mechanisms. ? We propose a transform attention mechanism to transform the historical traffic features to future representations. This attention mechanism models direct relationships between historical and future time steps to alleviate the problem of error propagation. ? We evaluate our graph multi-attention network (GMAN) on two real-world traffic datasets, and observe 4% improvement and superior fault-tolerance ability over stateof-the-art baseline methods in 1 hour ahead prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Traffic Prediction Traffic prediction has been extensively studied in past decades. Deep learning approaches (e.g., long short-term memory (LSTM) <ref type="bibr" target="#b17">(Ma et al. 2015)</ref>) show more superior performance in capturing temporal correlations in traffic conditions, compared with traditional timeseries methods (e.g., auto-regressive integrated moving average (ARIMA) <ref type="bibr" target="#b18">(Makridakis and Hibon 1997)</ref>) and machine learning models (e.g., support vector regression (SVR) <ref type="bibr" target="#b28">(Wu, Ho, and Lee 2004)</ref>, k-nearest neighbor (KNN) <ref type="bibr" target="#b33">(Zheng and Su 2014)</ref>). To model spatial correlations, researchers apply convolutional neural networks (CNN) to capture the dependencies in Euclidean space <ref type="bibr" target="#b32">(Zhang, Zheng, and Qi 2017;</ref><ref type="bibr" target="#b29">Yao et al. 2018;</ref>. Recent studies formulate the traffic prediction on graphs and employ graph convolutional networks (GCN) to model the non-Euclidean correlations in the road network <ref type="bibr" target="#b14">(Li et al. 2018b;</ref><ref type="bibr">Lv et al. 2018)</ref>. These graphbased models generate multiple steps ahead predictions via a step-by-step approach and may suffer from error propagation between different prediction steps.</p><p>Deep Learning on Graphs Generalizing neural networks to graph-structured data is an emerging topic <ref type="bibr" target="#b1">(Bronstein et al. 2017;</ref><ref type="bibr" target="#b26">Wu et al. 2019a)</ref>. A line of studies generalize CNN to model arbitrary graphs on spectral <ref type="bibr" target="#b5">(Defferrard, Bresson, and Vandergheynst 2016;</ref><ref type="bibr" target="#b12">Kipf and Welling 2017;</ref><ref type="bibr" target="#b13">Li et al. 2018a)</ref> or spatial <ref type="bibr" target="#b0">(Atwood and Towsley 2016;</ref><ref type="bibr" target="#b8">Hamilton, Ying, and Leskovec 2017;</ref><ref type="bibr" target="#b2">Chen, Ma, and Xiao 2018)</ref> perspective. Another line of studies focus on graph embedding, which learns low-dimensional representations for vertices that preserve the graph structure information <ref type="bibr" target="#b7">(Grover and Leskovec 2016;</ref><ref type="bibr" target="#b4">Cui et al. 2019)</ref>. <ref type="bibr" target="#b27">(Wu et al. 2019b)</ref> integrates WaveNet (van den Oord et al. 2016) into GCN for spatiotemporal modeling. As it learns static adjacency matrices, this method faces difficulties in capturing dynamic spatial correlations.</p><p>Attention Mechanism Attention mechanisms have been widely applied to various domains due to their high efficiency and flexibility in modeling dependencies <ref type="bibr" target="#b23">(Vaswani et al. 2017;</ref><ref type="bibr" target="#b20">Shen et al. 2018;</ref><ref type="bibr" target="#b6">Du et al. 2018)</ref>. The core idea of attention mechanisms is to adaptively focus on the most relevant features according to the input data <ref type="bibr" target="#b3">(Cheng et al. 2018)</ref>. Recently, researchers apply attention mechanisms to graph-structured data <ref type="bibr" target="#b24">(Velikovi et al. 2018)</ref> to model spatial correlations for graph classification. We extend the attention mechanism to graph spatio-temporal data prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preliminaries</head><p>We denote a road network as a weighted directed graph G = (V, E, A). Here, V is a set of N = |V| vertices representing points (e.g., traffic sensors) on the road network; E is a set of edges representing the connectivity among vertices; and A ? R N ?N is the weighted adjacency matrix, where A vi,vj represents the proximity (measured by the road network distance) between vertex v i and v j . The traffic condition at time step t is represented as a graph signal X t ? R N ?C on graph G, where C is the number of traffic conditions of interest (e.g., traffic volume, traffic speed, etc.).</p><p>Problem Studied Given the observations at N vertices of historical P time steps X = (X t1 , X t2 , ..., X t P ) ? R P ?N ?C , we aim to predict the traffic conditions of the next Q time steps for all vertices, denoted as? = (X t P +1 ,X t P +2 , ...,X t P +Q ) ? R Q?N ?C . L ST-Attention blocks (STAtt Block) with residual connections <ref type="bibr" target="#b9">(He et al. 2016)</ref>. Each ST-Attention block is composed of spatial and temporal attention mechanisms with gated fusion. Between the encoder and the decoder, a transform attention layer is added to the network to convert the encoded traffic features to the decoder. We also incorporate the graph structure and time information into multi-attention mechanisms through a spatio-temporal embedding (STE). In addition, to facilitate the residual connection, all layers produce outputs of D dimensions. The modules are detailed next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Multi-Attention Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatio-Temporal Embedding</head><p>Since the evolution of traffic conditions is restricted by the underlying road network <ref type="bibr">(Lv et al. 2018)</ref>, it is crucial to incorporate the road network information into prediction models. To this end, we propose a spatial embedding to encode vertices into vectors that preserve the graph structure information. Specifically, we leverage the node2vec approach <ref type="bibr" target="#b7">(Grover and Leskovec 2016)</ref> to learn the vertex representations. In addition, to co-train the pre-learned vectors with the whole model, these vectors are fed into a two-layer fully-connected neural network. Then, we obtain the spatial embedding, represented as e S vi ? R D , where v i ? V. The spatial embedding only provides static representations, which could not represent the dynamic correlations among traffic sensors in the road network. We thus further propose a temporal embedding to encode each time step into a vector. Specifically, let a day be with T time steps. We encode the day-of-week and time-of-day of each time step into R 7 and R T using one-hot coding, and concatenate them into a vector R T +7 . Next, we apply a two-layer fully-connected neural network to transform the time feature to a vector R D . In our model, we embed time features for both historical P and future Q time steps, represented as e T tj ? R D , where t j = t 1 , ..., t P , ..., t P +Q .</p><p>To obtain the time-variant vertex representations, we fuse the aforementioned spatial embedding and temporal embedding as spatio-temporal embedding (STE), as shown in <ref type="figure">Figure 2</ref>(b). Specifically, for vertex v i at time step t j , the STE is defined as e vi,tj = e S vi + e T tj . Therefore, the STE of N vertices in P + Q time steps is represented as E ? R (P +Q)?N ?D . The STE contains both graph structure and time information, and it will be used in spatial, temporal and transform attention mechanisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ST-Attention Block</head><p>As shown in <ref type="figure">Figure 2</ref>(c), the ST-Attention block includes a spatial attention, a temporal attention and a gated fusion. We denote the input of the l th block as H (l?1) , where the hidden state of vertex v i at time step t j is represented as h vi,tj , respectively. After the gated fusion, we obtain the output of the l th block, represented as H <ref type="bibr">(l)</ref> .</p><p>For illustration purpose, we denote a non-linear transformation as:</p><formula xml:id="formula_0">f (x) = ReLU(xW + b),<label>(1)</label></formula><p>where W, b are learnable parameters, and ReLU (Nair and Hinton 2010) is the activation function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatial Attention</head><p>The traffic condition of a road is affected by other roads with different impacts. Such impact is highly dynamic, changing over time. To model these properties, we design a spatial attention mechanism to adaptively capture the correlations between sensors in the road network. The key idea is to dynamically assign different weights to different vertices (e.g., sensors) at different time steps, as shown in <ref type="figure">Figure 3</ref>. For vertex v i at time step t j , we compute a weighted sum from all vertices:</p><formula xml:id="formula_1">hs (l) vi,tj = v?V ? vi,v ? h (l?1) v,tj ,<label>(2)</label></formula><p>where V denotes a set of all vertices, ? vi,v is the attention score indicating the importance of vertex v to v i , and the summation of attention scores equals to 1: v?V ? vi,v = 1. At a certain time step, both the current traffic conditions and the road network structure could affect the correlations between sensors. For example, a congestion on a road may significantly affect the traffic conditions of its adjacent roads. Motivated by this intuition, we consider both traffic features and the graph structure to learn the attention score. Specifically, we concatenate the hidden state with the spatiotemporal embedding, and adopt the scaled dot-product approach <ref type="bibr" target="#b23">(Vaswani et al. 2017)</ref> to compute the relevance between vertex v i and v:  where represents the concatenation operation, ?, ? denotes the inner product operator, and 2D is the dimension of h (l?1) vi,tj e vi,tj . Then, s vi,v is normalized via softmax as:</p><formula xml:id="formula_2">s vi,v = h (l?1) vi,tj e vi,tj , h (l?1) v,tj e v,tj ? 2D ,<label>(3)</label></formula><formula xml:id="formula_3">? vi,v = exp(s vi,v ) vr?V exp(s vi,vr ) .<label>(4)</label></formula><p>After the attention score ? vi,v is obtained, the hidden state can be updated through Equation 2.</p><p>To stabilize the learning process, we extend the spatial attention mechanism to be multi-head ones <ref type="bibr" target="#b23">(Vaswani et al. 2017)</ref>. Specifically, we concatenate K parallel attention mechanisms with different learnable projections:</p><formula xml:id="formula_4">s (k) vi,v = f (k) s,1 (h (l?1) vi,tj e vi,tj ), f (k) s,2 (h (l?1) v,tj e v,tj ) ? d ,<label>(5)</label></formula><formula xml:id="formula_5">? (k) vi,v = exp(s (k) vi,v ) vr?V exp(s (k) vi,vr ) ,<label>(6)</label></formula><formula xml:id="formula_6">hs (l) vi,tj = K k=1 v?V ? (k) vi,v ? f (k) s,3 (h (l?1) v,tj ) ,<label>(7)</label></formula><p>where f When the number of vertices N is large, the time and memory consumption is heavy as we need to compute N 2 attention scores. To address this limitation, we further propose a group spatial attention, which contains intra-group spatial attention and inter-group spatial attention, as shown in <ref type="figure">Figure 4</ref>.  <ref type="figure">Figure 5</ref>: The temporal attention mechanism models the non-linear correlations between different time steps.</p><p>We randomly partition N vertices into G groups, where each group contains M = N/G vertices (padding can be applied if necessary). In each group, we compute the intragroup attention to model the local spatial correlations among vertices through Equations 5, 6 and 7, where the learnable parameters are shared across groups. Then, we apply the max-pooling approach in each group to obtain a single representation for each group. Next, we compute the inter-group spatial attention to model the correlations between different groups, producing a global feature for each group. The local feature is added to the corresponding global feature as the final output.</p><p>In the group spatial attention, we need to compute GM 2 + G 2 = N M + (N/M ) 2 attention scores at each time step. By letting the gradient to zero, we know when M = 3 ? 2N , the number of attention scores reaches its minimum 2 ?1/3 N 4/3 N 2 .</p><p>Temporal Attention The traffic condition at a location is correlated with its previous observations, and the correlations vary over time steps non-linearly. To model these properties, we design a temporal attention mechanism to adaptively model the non-linear correlations between different time steps, as illustrated in <ref type="figure">Figure 5</ref>. Note that the temporal correlation is influenced by both the traffic conditions and the corresponding time context. For example, a congestion occurring in morning peak hours may affect the traffic for a few hours. Thus, we consider both traffic features and time to measure the relevance between different time steps. Specifically, we concatenate the hidden state with the spatiotemporal embedding, and adopt the multi-head approach to compute the attention score. Formally, considering vertex v i , the correlation between time step t j and t is defined as:</p><formula xml:id="formula_7">u (k) tj ,t = f (k) t,1 (h (l?1) vi,tj e vi,tj ), f (k) t,2 (h (l?1) vi,t e vi,t ) ? d , (8) ? (k) tj ,t = exp(u (k) tj ,t ) tr?Nt j exp(u (k) tj ,tr ) ,<label>(9)</label></formula><p>where u (k) tj ,t denotes the relevance between time step t j and t, ?  <ref type="figure">Figure 6</ref>: The transform attention mechanism models direct relationships between historical and future time steps.</p><p>two different learnable transforms, N tj denotes a set of time steps before t j , i.e., only considers information from time steps earlier than the target step to enable causality. Once the attention score is obtained, the hidden state of vertex v i at time step t j is updated as follows:</p><formula xml:id="formula_8">ht (l) vi,tj = K k=1 t?Nt j ? (k) tj ,t ? f (k) t,3 (h (l?1) vi,t ) , (10) where f (k)</formula><p>t,3 (?) represents a non-linear projection. The learnable parameters in Equations 8, 9 and 10 are shared across all vertices and time steps with paralleled computing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gated Fusion</head><p>The traffic condition of a road at a certain time step is correlated with both its previous values and other roads' traffic conditions. As shown in <ref type="figure">Figure 2(c)</ref>, we design a gated fusion to adaptively fuse the spatial and temporal representations. In the l th block, the outputs of the spatial and temporal attention mechanisms are represented as H T are fused as:</p><formula xml:id="formula_9">H (l) = z H (l) S + (1 ? z) H (l) T ,<label>(11)</label></formula><formula xml:id="formula_10">with z = ?(H (l) S W z,1 + H (l) T W z,2 + b z ),<label>(12)</label></formula><p>where W z,1 ? R D?D , W z,2 ? R D?D and b z ? R D are learnable parameters, represents the element-wise product, ?(?) denotes the sigmoid activation, z is the gate. The gated fusion mechanism adaptively controls the flow of spatial and temporal dependencies at each vertex and time step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transform Attention</head><p>To ease the error propagation effect between different prediction time steps in the long time horizon, we add a transform attention layer between the encoder and the decoder. It models the direct relationship between each future time step and every historical time step to convert the encoded traffic features to generate future representations as the input of the decoder. As shown in <ref type="figure">Figure 6</ref>, for vertex v i , the relevance between the prediction time step t j (t j = t P +1 , ..., t P +Q ) and the historical time step t (t = t 1 , ..., t P ) is measured via the spatio-temporal embedding: </p><formula xml:id="formula_11">? (k) tj ,t = f (k) tr,1 (e vi,tj ), f (k) tr,2 (e vi,t ) ? d ,<label>(13)</label></formula><formula xml:id="formula_12">? (k) tj ,t = exp(? (k) tj ,t ) t P tr=t1 exp(? (k) tj ,tr ) .<label>(14)</label></formula><p>With the attention score ? tj ,t , the encoded traffic feature is transformed to the decoder by adaptively selecting relevant features across all historical P time steps:</p><formula xml:id="formula_13">h (l) vi,tj = K k=1 t P t=t1 ? (k) tj ,t ? f (k) tr,3 (h (l?1) vi,t ) .<label>(15)</label></formula><p>Equations 13, 14, and 15 can be computed in parallel across all vertices and time steps, sharing the learnable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder-Decoder</head><p>As shown in <ref type="figure">Figure 2(a)</ref>, GMAN is an encoder-decoder architecture. Before entering into the encoder, the historical observation X ? R P ?N ?C is transformed to H (0) ? R P ?N ?D using fully-connected layers. Then, H (0) is fed into the encoder with L ST-Attention blocks, and produces an output H (L) ? R P ?N ?D . Following the encoder, a transform attention layer is added to convert the encoded feature H (L) to generate the future sequence representation H (L+1) ? R Q?N ?D . Next, the decoder stacks L ST-Attention blocks upon H (L+1) , and produces the output as H (2L+1) ? R Q?N ?D . Finally, the fully-connected layers produce the Q time steps ahead prediction? ? R Q?N ?C . GMAN can be trained end-to-end via back-propagation by minimizing the mean absolute error (MAE) between predicted values and ground truths:</p><formula xml:id="formula_14">L(?) = 1 Q t P +Q t=t P +1 Y t ?? t ,<label>(16)</label></formula><p>where ? denotes all learnable parameters in GMAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Datasets</head><p>We evaluate the performance of GMAN on two traffic prediction tasks with different road network scales: (1) traffic volume prediction on the Xiamen dataset <ref type="bibr" target="#b25">(Wang et al. 2017)</ref>, which contains 5 months of data recorded by 95 traffic sensors ranging from August 1st, 2015 to December 31st, 2015 in Xiamen, China;</p><p>(2) traffic speed prediction on the PeMS dataset <ref type="bibr" target="#b14">(Li et al. 2018b</ref>)), which contains 6 months of data recorded by 325 traffic sensors ranging from January 1st, 2017 to June 30th, 2017 in the Bay Area. The distributions of sensors in two datasets are visualized in <ref type="figure" target="#fig_9">Figure 7</ref>. Data Preprocessing We adopt the same data preprocessing procedures as in <ref type="bibr" target="#b14">(Li et al. 2018b</ref>). In both datasets, a time step denotes 5 minutes and the data is normalized via the Z-Score method. We use 70% of the data for training, 10% for validation, and 20% for testing. To construct the road network graph, each traffic sensor is considered as a vertex and we compute the pairwise road network distances between sensors. Then, the adjacency matrix is defined as:</p><formula xml:id="formula_15">A vi,vj = ? ? ? exp(? d 2 vi,vj ? 2 ), if exp(? d 2 vi,vj ? 2 ) ? 0, otherwise ,<label>(17)</label></formula><p>where d vi,vj is the road network distance from sensor v i to v j , ? is the standard deviation, and (assigned to 0.1) is the threshold to control the sparsity of the adjacency matrix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Settings</head><p>Metrics We apply three widely used metrics to evaluate the performance of our model, i.e., Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and Mean Absolute Percentage Error (MAPE).</p><p>Hyperparameters Following the previous works <ref type="bibr" target="#b14">(Li et al. 2018b;</ref><ref type="bibr" target="#b27">Wu et al. 2019b</ref>), we use P = 12 historical time steps (1 hour) to predict the traffic conditions of the next Q = 12 steps (1 hour). We train our model using Adam optimizer <ref type="bibr" target="#b11">(Kingma and Ba 2015)</ref> with an initial learning rate of 0.001. In the group spatial attention, we partition the vertices into G = 19 groups in the Xiamen dataset and G = 37 groups in the PeMS dataset, respectively. The number of traffic conditions on both datasets is C = 1. Totally, there are 3 hyperparameters in our model, i.e., the number of ST-Attention blocks L, the number of attention heads K, and the dimensionality d of each attention head (the channel of each layer D = K ?d). We tune these parameters on the validation set, and observe the best performance on the setting L = 3, K = 8, and d = 8 (D = 64).</p><p>Baselines We compare GMAN with the following baseline methods: (1) Auto-regressive integrated moving average (ARIMA) <ref type="bibr" target="#b18">(Makridakis and Hibon 1997)</ref>; (2) Support vector regression (SVR) <ref type="bibr" target="#b28">(Wu, Ho, and Lee 2004)</ref>; (3) Feedforward neural network (FNN); (4) FC-LSTM <ref type="bibr" target="#b22">(Sutskever, Vinyals, and Le 2014)</ref>, which is a sequence-to-sequence model with fully-connected LSTM layers in both encoder and decoder; (5) Spatio-temporal graph convolutional network (STGCN) <ref type="bibr" target="#b31">(Yu, Yin, and Zhu 2018)</ref> that combines graph convolutional layers and convolutional sequence learning layers; (6) Diffusion convolutional recurrent neural network (DCRNN) <ref type="bibr" target="#b14">(Li et al. 2018b</ref>) that integrates diffusion convolution with sequence-to-sequence architecture; (7) Graph WaveNet <ref type="bibr" target="#b27">(Wu et al. 2019b</ref>) that combines graph convolution with dilated casual convolution.</p><p>For models ARIMA, SVR, FNN, and FC-LSTM, we use the settings suggested by <ref type="bibr" target="#b14">(Li et al. 2018b</ref>). For models STGCN, DCRNN, and Graph WaveNet, we use the default settings from their original proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results</head><p>Forecasting Performance Comparison <ref type="table">Table 1</ref> shows the comparison of different methods for 15 minutes (3 steps), 30 minutes (6 steps), and 1 hour (12 steps) ahead predictions on two datasets. We observe that: (1) deep learning approaches outperform traditional time series methods and machine learning models, demonstrating the ability of deep neural networks in modeling non-linear traffic data; (2) among deep learning methods, graph-based models including STGCN, DCRNN, Graph WaveNet, and GMAN generally perform better than FC-LSTM, indicating the road net- work information is essential for traffic prediction; and (3) GMAN achieves state-of-the-art prediction performances and the advantages are more evident in the long-term horizon (e.g., 1 hour ahead). We argue that the long-term traffic prediction is more beneficial to practical applications, e.g., it allows transportation agencies to have more time to take actions to optimize the traffic according to the prediction.</p><p>We also use the T-Test to test the significance of GMAN in 1 hour ahead prediction compared to Graph WaveNet. The p-value is less than 0.01, which demonstrates that GMAN statistically outperforms Graph WaveNet.</p><p>Fault Tolerance Comparison The real-time values of traffic conditions may be missing partially, due to sensor malfunction, packet losses during data transmission, etc. To evaluate the fault-tolerance ability, we randomly drop a fraction ? (fault-ratio, ranging from 10% to 90%) of historical observations (i.e., randomly replace ??N ?P ?C input values with zeros) to make 1 hour ahead predictions. As shown in <ref type="figure">Figure 8</ref>, GMAN is more fault tolerant than state-of-theart methods. This shows that GMAN can capture the complex spatio-temporal correlations from the "contaminated" traffic data and adjust the dependencies from observations to future time steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Each Component</head><p>To investigate the effect of each component in our model, we evaluate four variants by removing spatial attention, temporal attention, gated fusion, and transform attention from GMAN separately, which are named as GMAN-NS, GMAN-NT, GMAN-NG, and GMAN-NTr respectively. <ref type="figure" target="#fig_12">Figure 9</ref> presents the MAE in each prediction step of GMAN and the four variants. We observe that GMAN consistently outperforms GMAN-NS, GMAN-NT, and GMAN-NG, indicating the effectiveness of spatial attention, temporal attention, and gated fusion in modeling the complex spatio-temporal correlations. Moreover, GMAN performs better than GMAN-NTr, especially in the long-term horizon, demonstrating that the transform attention mechanism effectively eases the effect of error propagation.</p><p>Computation Time We present the training time and inference time of STGCN, DCRNN, Graph WaveNet, and GMAN on the PeMS dataset in <ref type="table" target="#tab_2">Table 2</ref>. During the training phase, GMAN has a similar speed with Graph WaveNet.   DCRNN runs much slower than other methods due to the time-consuming sequence learning in recurrent networks. STGCN is the most efficient but shows poor prediction performance <ref type="table">(Table 1</ref>). In the inference phase, we report the total time cost on the validation data. STGCN and DCRNN is less efficient as they need iterative computation to generate the 12 prediction results. GMAN and Graph WaveNet could produce 12 steps ahead predictions in one run and thus take less time for inference. In respect of the second best model Graph WaveNet as suggested in <ref type="table">Table 1</ref>, GMAN compares favorably to Graph WaveNet in the long-term (e.g., 1 hour ahead) traffic predictions <ref type="table">(Table 1)</ref> with similar computation costs for both training and inference <ref type="table" target="#tab_2">(Table 2)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We proposed a graph multi-attention network (GMAN) to predict traffic conditions for time steps ahead on a road network graph. Specifically, we proposed spatial and temporal attention mechanisms with gated fusion to model the complex spatio-temporal correlations. We further designed a transform attention mechanism to ease the effect of error propagation to improve the long-term prediction performance. Experiments on two real-world datasets show that GMAN achieves state-of-the-art results, and the advantages are more evident as the predictions are made into far future. In the future, we will apply GMAN to other spatio-temporal prediction tasks, such as water consumption prediction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Complex spatio-temporal correlations. (a) Sensors in a road network. (b) Dynamic spatial correlations: sensors 1 and 2 are not always highly correlated, although they are close in the road network; non-linear temporal correlations:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 Figure 2 :</head><label>22</label><figDesc>illustrates the framework of our proposed graph multi-attention network (GMAN), which has an encoderdecoder structure. Both the encoder and the decoder contain The framework of Graph multi-attention network (GMAN). (a) GMAN consists of a spatio-temporal embedding (STE), an encoder and a decoder both with L ST-Attention blocks (STAtt Block), a transform attention layer (TransAtt), and two fully-connected layers (FCs). (b) The spatio-temporal embedding contains a spatial embedding and a temporal embedding. (c) The ST-Attention block combines spatial and temporal attention mechanisms via gated fusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(l?1) vi,tj . The outputs of spatial and temporal attention mechanisms in the l th block are represented as H(l) S and H (l) T , where the hidden states of vertex v i at time step t j are denoted as hs (l) vi,tj and ht (l)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>The spatial attention mechanism captures timevariant pair-wise correlations between vertices. Group spatial attention computes both intra-group and inter-group attention to model spatial correlations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>represent three different nonlinear projections (Equation 1) in the k th head attention, producing d = D/K dimensional outputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(k)tj ,t is the attention score in k th head indicating the importance of time step t to t j , f (k) t,1 (?) and f</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>T</head><label></label><figDesc>, both have the shapes of R P ?N ?D in the encoder or R Q?N ?D in the decoder. H</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Sensor distribution of Xiamen and PeMS datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Figure 8: Fault-tolerance comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 :</head><label>9</label><figDesc>MAE of each prediction step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The computation time on the PeMS dataset.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by grants from Natural Science Foundation of China (61872306 and U1605254), and Xiamen Science and Technology Bureau (3502Z20193017).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fastgcn: Fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A neural attention model for urban air quality inference: learning the weights of monitoring stations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2151" to="2158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A survey on network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="833" to="852" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A hybrid method for traffic flow forecasting using multimodal deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-J</forename><surname>Horng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02099</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Node2vec: scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Repeatability and similarity of freeway traffic flow and long-term prediction under big data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1786" to="1796" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adam: a method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adaptive graph convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Diffusion convolutional recurrent neural network: Data-driven traffic forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shahabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Lc-rnn: a deep learning model for traffic speed prediction</title>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="3470" to="3476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long short-term memory neural network for traffic speed prediction using remote microwave sensor data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part C: Emerging Technologies</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="187" to="197" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Arma models and the boxjenkins methodology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Makridakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hibon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="147" to="163" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Disan: Directional self-attention network for rnn/cnnfree language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5446" to="5455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deeptransport: Prediction and simulation of human mobility and transportation mode at a citywide level</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kanasugi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shibasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2618" to="2624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
	</analytic>
	<monogr>
		<title level="m">Wavenet: A generative model for raw audio</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Velikovi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unlicensed taxis detection service based on large-scale vehicles mobility data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICWS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="857" to="861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00596</idno>
		<title level="m">A comprehensive survey on graph neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Graph wavenet for deep spatial-temporal graph modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Travel-time prediction with support vector regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="276" to="281" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep multi-view spatial-temporal network for taxi demand prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2588" to="2595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Revisiting spatial-temporal similarity: A deep learning framework for traffic prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Spatio-temporal graph convolutional networks: A deep learning framework for traffic forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3634" to="3640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep spatio-temporal residual networks for citywide crowd flows prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1655" to="1661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Short-term traffic volume forecasting: A k-nearest neighbor approach enhanced by constrained linearly sewing principle component algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part C: Emerging Technologies</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="143" to="157" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deepstd: Mining spatio-temporal disturbances of multiple context factors for citywide traffic flow prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>to be published</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
