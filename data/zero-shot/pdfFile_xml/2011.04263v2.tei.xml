<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unified Quality Assessment of In-the-Wild Videos with Mixed Datasets Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingquan</forename><surname>Li</surname></persName>
							<email>dingquanli@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">National Engineering Laboratory for Video Technol-ogy</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Advanced Institute of Information Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">School of Mathematical Sciences</orgName>
								<orgName type="laboratory">Laboratory of Mathematics and Its Applications</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Beijing International Center for Mathematical Research</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>Jiang</surname></persName>
							<email>ttjiang@pku.edu.cncorrespondingauthor</email>
							<affiliation key="aff0">
								<orgName type="laboratory">National Engineering Laboratory for Video Technol-ogy</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Advanced Innovation Center for Future Visual Entertain-ment</orgName>
								<orgName type="institution">Beijing Film Academy</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Jiang</surname></persName>
							<email>ming-jiang@pku.edu.cn</email>
							<affiliation key="aff3">
								<orgName type="department">School of Mathematical Sciences</orgName>
								<orgName type="laboratory">Laboratory of Mathematics and Its Applications</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingquan</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Jiang</surname></persName>
						</author>
						<title level="a" type="main">Unified Quality Assessment of In-the-Wild Videos with Mixed Datasets Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Received: 20 December 2019 / Accepted: 4 November 2020</note>
					<note>International Journal of Computer Vision manuscript No. (will be inserted by the editor)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video quality assessment (VQA) is an important problem in computer vision. The videos in computer vision applications are usually captured in the wild. We focus on automatically assessing the quality of in-the-wild videos, which is a challenging problem due to the absence of reference videos, the complexity of distortions, and the diversity of video contents. Moreover, the video contents and distortions among existing datasets are quite different, which leads to poor performance of data-driven methods in the cross-dataset evaluation setting. To improve the performance of quality assessment models, we borrow intuitions from human perception, specifically, content dependency and temporal-memory effects of human visual system. To face the cross-dataset evaluation challenge, we explore a mixed datasets training strategy for training a single VQA model with multiple datasets. The proposed unified framework explicitly includes three stages: rel-ative quality assessor, nonlinear mapping, and datasetspecific perceptual scale alignment, to jointly predict relative quality, perceptual quality, and subjective quality. Experiments are conducted on four publicly available datasets for VQA in the wild, i.e., LIVE-VQC, LIVE-Qualcomm, KoNViD-1k, and CVD2014. The experimental results verify the effectiveness of the mixed datasets training strategy and prove the superior performance of the unified model in comparison with the state-of-the-art models. For reproducible research, we make the PyTorch implementation of our method available at https://github.com/lidq92/MDTVSFA.</p><p>Keywords Content dependency ? In-the-wild videos ? Mixed datasets training ? Temporal-memory effect ? Video quality assessment arXiv:2011.04263v2 [cs.CV] 15 Nov 2020</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Image/Video quality assessment (I/VQA) is a fundamental and longstanding problem in the image processing and computer vision community. It is involved in benchmarking and optimizing many vision applications, such as image classification <ref type="bibr" target="#b5">(Dodge and Karam, 2016)</ref>, object tracking <ref type="bibr" target="#b40">(Nieto et al., 2019)</ref>, video compression <ref type="bibr" target="#b44">(Rippel et al., 2019)</ref>, image inpainting <ref type="bibr" target="#b12">(Isogawa et al., 2019)</ref>, and super resolution <ref type="bibr" target="#b71">(Zhang et al., 2019a)</ref>. Because of its importance, I/VQA has attracted significant attention in the past two decades <ref type="bibr" target="#b60">(Wang et al., 2004a;</ref><ref type="bibr" target="#b36">Mittal et al., 2012;</ref><ref type="bibr" target="#b68">Zhang et al., 2014;</ref><ref type="bibr" target="#b14">Kang et al., 2014;</ref><ref type="bibr" target="#b31">Ma et al., 2016;</ref><ref type="bibr" target="#b28">Liu et al., 2017;</ref><ref type="bibr" target="#b15">Kim et al., 2018;</ref><ref type="bibr" target="#b26">Lin and Wang, 2018)</ref>. Videos obtained in the wild are often in low-quality because of many factors, such as out of focus, object motion, camera shakiness, under-/over-exposure, and adverse weather, etc.</p><p>(a) Three representative frames of the video on CVD2014 <ref type="bibr" target="#b41">(Nuutinen et al., 2016)</ref> with the worst quality (b) Three representative frames of the video on LIVE-VQC <ref type="bibr" target="#b51">(Sinno and Bovik, 2019a)</ref> with the worst quality <ref type="figure">Fig. 1</ref> An illustration of the videos with the worst quality on CVD2014 and LIVE-VQC, respectively (Full videos are provided in https://bit.ly/3csmHYk). The upper video has a better quality in comparison with the lower video. However, linear rescaling leads to the same quality labels for them. Such "inconformity" will disturb the training process, and lead to a poor performance.</p><p>With the guidance of VQA in the wild, one can automatically identify, cull, repair or enhance low-quality videos before sending them to the subsequent vision applications, so that the applications can work in the real scenario. Thus, VQA in the wild is necessary for computer vision in the wild, but few attention is paid to this task.</p><p>VQA in the wild is a challenging task for the reason that the pristine videos are not available, the distortions are complex, and the contents are diverse. Compared to synthetically-distorted videos, in-the-wild videos contain huge amount of contents and may be infected with mixed real-world distortions, especially some of which are temporally heterogeneous (e.g., temporary auto-focus blurs and exposure adjustments). Consequently, modern advanced I/VQA methods, e.g., BRISQUE <ref type="bibr" target="#b36">(Mittal et al., 2012)</ref> and VBLIINDS <ref type="bibr" target="#b45">(Saad et al., 2014)</ref>, validated on synthetically-distorted video datasets <ref type="bibr" target="#b39">Moorthy et al., 2012)</ref>, do a poor job in predicting the quality of inthe-wild videos <ref type="bibr" target="#b8">Ghadiyaram et al., 2018;</ref><ref type="bibr" target="#b41">Nuutinen et al., 2016;</ref><ref type="bibr" target="#b51">Sinno and Bovik, 2019a)</ref> (See <ref type="table">Table 5</ref> and <ref type="table" target="#tab_3">Table 6</ref>).</p><p>Some efforts have been made to generate a better feature for VQA in the wild <ref type="bibr" target="#b66">(You and Korhonen, 2019;</ref><ref type="bibr" target="#b17">Korhonen, 2019;</ref><ref type="bibr" target="#b20">Li et al., 2019a)</ref>. <ref type="bibr" target="#b17">Korhonen (2019)</ref> obtains well-behaved low-complexity features for all frames and high-complexity features for representative frames, so that good quality predictions can be achieved by the support vector regression or the random forest regression. <ref type="bibr" target="#b66">You and Korhonen (2019)</ref> learn effective spatio-temporal features with 3D con-volutional neural network (3D-CNN) and predict the video quality by a long-short term memory (LSTM) network. Our previous work <ref type="bibr" target="#b20">(Li et al., 2019a)</ref> borrows intuitions from human visual system (HVS), which extracts content-aware and distortion-sensitive features. Although the above mentioned methods achieve superior performances on the benchmark VQA datasets individually, their performances are poor in cross-dataset evaluation setting (See <ref type="table" target="#tab_5">Table 7</ref>). For example, when the model is trained on KoNViD-1k <ref type="bibr" target="#b11">(Hosu et al., 2017)</ref>, the test performance on LIVE-Qualcomm <ref type="bibr" target="#b8">(Ghadiyaram et al., 2018)</ref> or CVD2014 <ref type="bibr" target="#b41">(Nuutinen et al., 2016)</ref> drops sharply <ref type="bibr" target="#b17">(Korhonen, 2019)</ref>. This may be caused by the over-fitting problem in the training process and the discrepancy of data distribution among the datasets.</p><p>To face this cross-dataset evaluation challenge, one possible solution is to mix multiple datasets during the training phase, so that the data-driven model can learn the characteristics of video contents and distortions from all these datasets. Mixed datasets training provides two advantages. First, it provides a single unified model for all datasets/applications instead of multiple models for different datasets. Second, it makes the utmost of existing relevant data for VQA model training, since the largest size of current in-the-wild VQA datasets is only 1200 and acquiring new annotations is time-consuming. However, mixed datasets training is not trivial, since the ranges of subjective quality scores among different datasets are inconsistent. A na?ve strategy is the "linear re-scaling", which maps all subjective score ranges of different datasets to the same range. Nevertheless, the ranges of the inherent video quality An overview of the proposed unified framework. It consists of three stages: relative quality assessor, nonlinear mapping, and dataset-specific perceptual scale alignment for predicting relative quality, perceptual quality, and subjective quality, respectively. The supervisions for mixed datasets training at the three stages are monotonicity-induced loss, linearity-induced loss, and error-induced loss, respectively. D is the number of datasets.</p><p>among these datasets are not equal in most circumstances. For instance, in <ref type="figure">Fig. 1</ref>, both the two videos are the worst in their corresponding datasets. The video in <ref type="figure">Fig. 1</ref>(a) has better quality in comparison with the video in <ref type="figure">Fig. 1(b)</ref>, since the latter one contains more complicated distortions, including motion blur, under-/over-exposure, and grainy noise. However, linear rescaling leads to the same quality labels for them. Such "inconformity" will disturb the training process, thus a good performance is hard to achieve (See <ref type="figure">Fig. 6</ref> and <ref type="table">Table 5</ref>). To tackle the above inconformity problem, we should align subjective quality scores for different datasets. One way is conducting an additional subjective study to re-align the subjective quality scores. The other way is to learn the alignment of subjective quality scores for these datasets. As the first way is time-consuming and impracticable when more and more datasets are considered, we choose the second one. Before introducing our method, we first introduce three important quality concepts: perceptual quality, subjective quality, and relative quality.</p><p>Perceptual quality: Perceptual quality is an ideal concept that is related to human perception of video quality, and only if we gather all the videos in the wild and conduct the largest scale subjective study can we get the ground-truth of perceptual quality. Perceptual quality can be used for benchmarking and optimizing video processing systems/algorithms, but its ground-truth is impossible to obtain since we cannot conduct such a large-scale subjective study on all videos in the wild.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subjective quality:</head><p>As an "approximation" of perceptual quality, subjective quality is considered, whose ground-truth can be accessed by conducting a subjective study on a video dataset of limited size. Although subjective quality is designed to reflect perceptual quality, it may have different ranges for different datasets. In terms of this fact, we can assume the subjective quality to be linearly correlated with the perceptual quality for a single dataset, but the linear transformations between subjective quality and perceptual quality are not necessarily the same for different datasets. Subjective quality can be used as a supervised signal for the prediction of perceptual quality.</p><p>Relative quality: Compared to directly rating the quality of a video in the subjective study, it is easier for humans to choose a video with better quality from two videos. In terms of this fact, we define the concept of relative quality, which can be accessed by ranking the quality of videos. Relative quality can be used for benchmarking video processing algorithms. However, due to its nonlinearity to perceptual quality, it might not be directly used for optimization. For example, the optimization might be early stopped when the relative quality is approaching the perfect value while the perceptual quality is far from the perfect one.</p><p>With the above three concepts, we show our solution. We decompose the VQA problem into three subproblems, i.e., predicting relative quality, perceptual quality, and subjective quality in turn (See <ref type="figure">Fig. 2</ref>). For details, our proposed model contains three stages to solve these three sub-problems. First, to predict the relative quality, we use our previous HVS-inspired VQA model <ref type="bibr" target="#b20">(Li et al., 2019a)</ref> as the backbone. The relative quality assessor takes the video as input and outputs a relative quality score. This stage focuses on prediction monotonicity, which describes the ability to provide the quality ranking for any list of videos that is consistent with subjective quality. Correspondingly, we propose a monotonicity-induced loss for this stage. Second, to predict the perceptual quality, we adopt the well-known 4-parameter logistic function for characterizing the nonlinearity of human perception on video quality <ref type="bibr" target="#b57">(VQEG, 2000)</ref>. We reformulate this function and design it as a network module. The nonlinear mapping module maps the relative quality of a video to the perceptual quality of a video. The predicted perceptual quality is expected to be linearly correlated with the subjective quality, and we propose a linearity-induced loss as the supervision for this stage. Third, we learn a dataset-specific perceptual scale alignment for each dataset, which tries to map the perceptual quality of a video to the subjective quality of the video on its belonging dataset. With this dataset-specific alignment, an error-induced loss can be used as the supervision without disturbing the training. Under this model, we can use the above three losses for mixed datasets training to solve the "inconformity" problem.</p><p>To verify the effectiveness of the proposed unified model with the mixed datasets training strategy, we conduct comparative experiments on four publicly available datasets for VQA in the wild, i.e., KoNViD-1k <ref type="bibr" target="#b11">(Hosu et al., 2017)</ref>, CVD2014 <ref type="bibr" target="#b41">(Nuutinen et al., 2016)</ref>, LIVE-Qualcomm <ref type="bibr" target="#b8">(Ghadiyaram et al., 2018)</ref>, and LIVE-VQC <ref type="bibr" target="#b51">(Sinno and Bovik, 2019a)</ref>. Our method is compared with several modern advanced methods. In terms of prediction monotonicity and prediction accuracy, the superior performances of our method across datasets are verified by the experimental results.</p><p>Lastly, we highlight the relationship and difference between our previous work <ref type="bibr" target="#b20">(Li et al., 2019a)</ref> and this work. The model design in this work is build upon the model in our previous work. However, there are two major differences between our previous work and this work. First, this work focuses on model optimization with mixed datasets training while our previous work does not consider mixed datasets training. Second, in this work, it is the first time to decompose the VQA problem into three sub-problems: predicting relative quality, perceptual quality, and subjective quality, and we propose a unified VQA framework that explicitly designs three stages to tackle these three sub-problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>This section reviews some related work. Sec. 2.1 overviews several representative VQA methods, especially the VQA methods for in-the-wild videos. Sec. 2.2 introduces mixed datasets training in computer vision, especially in the tasks of quality assessment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Video Quality Assessment</head><p>Classical VQA methods are grounded on different cues, such as structures <ref type="bibr" target="#b61">(Wang et al., 2004b</ref><ref type="bibr" target="#b59">(Wang et al., , 2012</ref>, motion <ref type="bibr" target="#b33">Manasa and Channappayya, 2016)</ref>, energy <ref type="bibr" target="#b23">(Li et al., 2016a)</ref>, saliency <ref type="bibr" target="#b70">(Zhang and Liu, 2017;</ref><ref type="bibr" target="#b67">You et al., 2014)</ref>, gradients <ref type="bibr" target="#b29">(Lu et al., 2019)</ref>, or natural video statistics (NVS) <ref type="bibr" target="#b38">(Mittal et al., 2016;</ref><ref type="bibr" target="#b45">Saad et al., 2014;</ref><ref type="bibr" target="#b52">Sinno and Bovik, 2019b)</ref>. Besides, some VQA methods focus on the fusion of primary features <ref type="bibr" target="#b6">(Freitas et al., 2018;</ref><ref type="bibr" target="#b24">Li et al., 2016b)</ref>. Recently, several VQA methods exploit the use of deep learning in this task <ref type="bibr" target="#b73">(Zhang et al., 2019c;</ref><ref type="bibr" target="#b27">Liu et al., 2018;</ref><ref type="bibr" target="#b15">Kim et al., 2018;</ref><ref type="bibr" target="#b74">Zhang et al., 2020)</ref>. <ref type="bibr" target="#b15">Kim et al. (2018)</ref> obtain the spatio-temporal sensitivity maps by a CNN model. <ref type="bibr" target="#b27">Liu et al. (2018)</ref> exploit the 3D-CNN model for multi-task learning of codec classification and quality assessment for compressed videos. <ref type="bibr" target="#b73">Zhang et al. (2019c</ref><ref type="bibr" target="#b74">Zhang et al. ( , 2020</ref>) make use of both video and image data with transfer learning. However, all these methods are proposed for quality assessment of synthetically-distorted videos, and they are not applicable to in-the-wild videos or their performances are poor on in-the-wild datasets. Note that the relevant concept "streaming video quality-of-experience (QoE)" is beyond the scope of this work, and the interested reader can refer to these two good surveys <ref type="bibr" target="#b49">(Seufert et al., 2014;</ref><ref type="bibr" target="#b13">Juluri et al., 2015)</ref>.</p><p>Quality assessment of in-the-wild videos has been attracting significant attention in recent years. Four relevant datasets have been constructed with corresponding subjective studies, i.e., CVD2014 (Nuutinen et al., 2016), KoNViD-1k <ref type="bibr" target="#b11">(Hosu et al., 2017)</ref>, LIVE-Qualcomm <ref type="bibr" target="#b8">(Ghadiyaram et al., 2018)</ref>, and LIVE-VQC <ref type="bibr" target="#b51">(Sinno and Bovik, 2019a</ref>). Since we cannot access the pristine reference videos in this situation, only no-reference VQA (NR-VQA) methods are applicable. The deep learning-based VQA models described in the last paragraph are unfeasible in this problem since they either need the reference information <ref type="bibr" target="#b73">(Zhang et al., 2019c;</ref><ref type="bibr" target="#b15">Kim et al., 2018;</ref><ref type="bibr" target="#b74">Zhang et al., 2020)</ref> or only suit for compression artifacts <ref type="bibr" target="#b27">(Liu et al., 2018)</ref>. Thus, in our previous work <ref type="bibr" target="#b20">(Li et al., 2019a)</ref>, we propose a deep learning-based model for predicting the quality of in-the-wild videos. The model extracts content-aware distortion-sensitive features from CNN models trained for image classification tasks, and uses a gated recurrent unit (GRU) followed by a subjectively-inspired temporal pooling layer for modeling the temporalmemory effect. Concurrent works to our previous work are <ref type="bibr" target="#b66">You and Korhonen (2019)</ref>; <ref type="bibr" target="#b55">Varga (2019)</ref>; <ref type="bibr" target="#b56">Varga and Szir?nyi (2019)</ref>. Although all of these methods achieve a good performance, they do not enable mixing multiple datasets during the training phase. As a result, their performances are poor in the cross-dataset evaluation setting. The main purpose of this paper is to propose an elegant mixed datasets training strategy. With this strategy, we can train a unified model that learns the characteristics of videos from all datasets and thus further improve the overall performance over the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Mixed Datasets Training</head><p>Mixed datasets training has two advantages. One is to provide a unified model for all datasets. The other is to take full advantage of existing relevant datasets for improving the model learning. Therefore, many computer vision tasks consider mixed datasets training, such as person re-identification <ref type="bibr" target="#b30">(Lv et al., 2018;</ref><ref type="bibr" target="#b25">Li et al., 2019c)</ref>, monocular depth estimation <ref type="bibr" target="#b19">(Lasinger et al., 2019)</ref>, and human parsing .</p><p>There are some relevant works in quality assessment tasks that consider mixed datasets training. The challenge is that ranges of subjective quality scores are inconsistent across datasets. <ref type="bibr" target="#b17">Korhonen (2019)</ref> uses a na?ve method to handle this challenge, i.e., linearly rescaling the subjective quality scores of different datasets to the same range. Pair-wise learning considers only the relative quality score instead of the absolute subjective quality scores, and thus can bypass the "inconformity" problem. Therefore, several I/VQA works consider pairwise learning for mixed datasets training, while they use different loss functions for training <ref type="bibr" target="#b72">Zhang et al., 2019b;</ref><ref type="bibr" target="#b18">Krasula et al., 2020)</ref>. <ref type="bibr" target="#b64">Yang et al. (2019)</ref> use the margin ranking loss and the Euclidean loss. <ref type="bibr" target="#b72">Zhang et al. (2019b)</ref> consider the cross entropy loss and the fidelity loss. <ref type="bibr" target="#b18">Krasula et al. (2020)</ref> determine different and similar pairs based on statistical analysis on the mean and standard deviation of subjective ratings, and then define the training objective as the correct classification rate of these pairs. However, pair-wise learning will increase the training time. In the next section, we show that our proposed monotonicity-induced loss can be regarded as an extension of the losses in <ref type="bibr" target="#b64">Yang et al. (2019)</ref> and <ref type="bibr" target="#b72">Zhang et al. (2019b)</ref> with a more efficient implementation. Besides the monotonicity-induced loss, we also propose a linearity-induced loss and assign a dataset-specific perceptual scale alignment to enable mixing multiple datasets during the training phase.</p><p>3 Proposed Method 3.1 Overview <ref type="figure">Fig. 2</ref> shows the overview of the proposed unified VQA framework for quality assessment of in-the-wild videos. Our VQA model consists of three stages: relative quality assessor, nonlinear mapping, and dataset-specific perceptual scale alignment for predicting relative quality, perceptual quality, and subjective quality, respectively.</p><p>The flow of our proposed unified framework is as follows. At the first stage, to predict the relative quality, we learn a relative quality assessor with the supervision of a monotonicity-inspired loss, where the monotonicity-induced loss is derived from the monotonicity condition and it is the sum of all pair-wise losses. To account for the content dependency and temporal-memory effects of human perception, we design our relative quality assessor as a deep neural network that includes two key modules: content-aware feature extraction and modeling of temporal-memory effect. At the second stage, to predict the perceptual quality, a nonlinear mapping module is added after the relative quality assessor, to explicitly account for the nonlinearity of human perception. The parameters in this module are learned with the supervision of a linearity-induced loss based on Pearson's linear correlation. At the third stage, to predict the subjective quality, a dataset-specific perceptual scale alignment layer is added to map the predicted perceptual quality to the subjective quality of a video on each dataset. After the alignment, the widely-used error-induced loss is used as the supervision.</p><p>Thus, in our mixed datasets training strategy, three kinds of losses are involved. For each dataset, the overall loss is the sum of these three kinds of losses on the dataset. To emphasize the datasets with larger loss values, our final training loss is a softmax-weighted loss over all training datasets. With this strategy, we can learn a single unified VQA model for multiple datasets by mixing them all during the training phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Relative Quality Assessor</head><p>This subsection describes the design of the relative quality assessor. The framework of our relative quality assessor is shown in <ref type="figure">Fig. 3</ref>. We adopt the model in our previous work <ref type="bibr" target="#b20">(Li et al., 2019a)</ref> as the backbone </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modeling of Temporal-Memory Effect Content-Aware Feature Extraction</head><p>Frame Quality C 2C Relative Quality <ref type="figure">Fig. 3</ref> Relative Quality Assessor. It mainly consists of two modules. Module I includes a pre-trained CNN with effective global pooling (GP) serving as a content-aware feature extractor. Module II models temporal-memory effect and it includes two sub-modules: a GRU network and a subjectively-inspired temporal pooling layer. Note that the GRU network is the unrolled version of one GRU and the parallel CNNs/FCs share weights.</p><p>of the relative quality assessor. It integrates the two eminent effects of human perception into the assessor. One is the content dependency effect, which guides us introducing the content-aware feature extraction module. The other is the temporal-memory effect, which is modeled in the feature level and the quality score level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Content-Aware Feature Extraction</head><p>In the visual quality assessment task, human perception is content dependent <ref type="bibr" target="#b50">(Siahaan et al., 2018;</ref><ref type="bibr" target="#b53">Triantaphillidou et al., 2007;</ref><ref type="bibr" target="#b58">Wang et al., 2017;</ref><ref type="bibr" target="#b0">Bampis et al., 2017;</ref><ref type="bibr" target="#b69">Zhang et al., 2018;</ref><ref type="bibr">Li et al., 2019a,b)</ref>. This can be attributed to the fact that, the complexity of distortions, the human tolerance thresholds for distortions, and the human preferences could vary a lot in different contents/scenes. Since there are diverse contents in the in-the-wild scenario, a relative quality assessor which mimics human perception, should take this effect into accounts. So we need to extract features that are not only distortion-sensitive but also content-aware. The image classification models pretrained on ImageNet <ref type="bibr" target="#b4">(Deng et al., 2009</ref>) using CNN possess the discriminatory power of different content information. Thus, the deep features extracted from these models, e.g., ResNet <ref type="bibr" target="#b10">(He et al., 2016)</ref>, are expected to be content-aware. Meanwhile, <ref type="bibr" target="#b5">Dodge and Karam (2016)</ref> point out that the deep features are distortionsensitive. So it is reasonable to extract content-aware and distortion-sensitive features from pre-trained image classification models.</p><p>Assuming the video is a stack of T frames I t (t = 1, 2, . . . , T ), we feed each video frame into a pre-trained CNN model and get the corresponding deep feature maps M t from its top convolutional layer:</p><formula xml:id="formula_0">M t = CNN(I t ).</formula><p>(1)</p><p>M t contains a total of C feature maps. Then, we apply spatial global pooling (GP) for each feature map of M t . Applying only the spatial global average pooling operation (GP mean ) to M t discards much information of M t . We further consider the spatial global standard deviation pooling operation (GP std ) to preserve the variation information in M t . The output feature vectors of GP mean , GP std are f mean t , f std t respectively.</p><formula xml:id="formula_1">f mean t = GP mean (M t ), f std t = GP std (M t ).</formula><p>(2)</p><p>After that, f mean t and f std t are concatenated to serve as content-aware and distortion-sensitive features f t :</p><formula xml:id="formula_2">f t = f mean t ? f std t ,<label>(3)</label></formula><p>where ? is the concatenation operator and the length of f t is 2C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Modeling of Temporal-Memory Effect</head><p>Temporal-memory effect is another important clue for designing objective VQA models <ref type="bibr" target="#b42">(Park et al., 2013;</ref><ref type="bibr" target="#b47">Seshadrinathan and Bovik, 2011;</ref><ref type="bibr" target="#b62">Xu et al., 2014;</ref><ref type="bibr" target="#b3">Choi and Bovik, 2018;</ref><ref type="bibr" target="#b15">Kim et al., 2018)</ref>. It induces that video quality rating is influenced by historic memory. We model the temporal-memory effect in two aspects.</p><p>In the feature integration aspect, we adopt a GRU network for modeling the long-term dependencies in our method. In the quality pooling aspect, we propose a subjectively-inspired temporal pooling model and embed it into the network.</p><p>Long-term dependencies modeling. Existing NR-VQA methods cannot well model the long-term dependencies in the VQA task. To handle this issue, we resort to GRU <ref type="bibr" target="#b2">(Cho et al., 2014)</ref>, a recurrent neural network model with gates control.</p><p>The dimension of the extracted content-aware features is very high, which is not suitable for GRU training. Therefore, we perform dimension reduction using a single fully-connected (FC) layer before feeding them into GRU, that is:</p><formula xml:id="formula_3">x t = W f x f t + b f x ,<label>(4)</label></formula><p>where W f x and b f x are the parameters in the single FC layer. Without the bias term, it acts as a linear dimension reduction model. After dimension reduction, the reduced features x t (t = 1, 2, ? ? ? , T ) are sent to GRU. We consider the hidden states of GRU as the integrated features h t , whose initial values are h 0 . h t is calculated as follow.</p><formula xml:id="formula_4">h t = GRU(x t , h t?1 ).<label>(5)</label></formula><p>With the integrated features h t , we predict the frame quality score q t by adding a single FC layer:</p><formula xml:id="formula_5">q t = W hq h t + b hq ,<label>(6)</label></formula><p>where W hq and b hq are the weight and bias parameters. Subjectively-inspired temporal pooling. In subjective experiments, subjects are intolerant of poor quality video events <ref type="bibr" target="#b42">(Park et al., 2013)</ref>. More specifically, temporal hysteresis effect is found in the subjective experiments <ref type="bibr" target="#b47">(Seshadrinathan and Bovik, 2011)</ref>. That is, subjects react sharply to drops in video quality and provide poor quality for such time interval, but react dully to improvements in video quality thereon. Inspired by these observations, to connect the predicted frame-level quality to the video-level quality, we put forward a new differentiable temporal pooling model. Details are as follows.</p><p>To mimic the human's intolerance to poor quality events, we define a memory quality element l t at the t-th frame as the minimum of quality scores over the previous several frames:</p><formula xml:id="formula_6">l t = q t for t = 1, min k?Vprev q k for t &gt; 1,<label>(7)</label></formula><p>where</p><formula xml:id="formula_7">V prev = {max (1, t ? ? ), ? ? ? , t ? 2, t ? 1}</formula><p>is the index set of the considered frames, and ? is a hyperparameter relating to the temporal duration. Accounting for the fact that subjects react sharply to the drops in quality but react dully to the improvements in quality, we construct a current quality element m t at the t-th frame, using the weighted quality scores over the next several frames, where larger weights are assigned for worse quality frames. Specifically, we define the weights w k t by a differentiable softmin function, i.e., a composition of the negative linear function and the softmax function.</p><formula xml:id="formula_8">m t = k?Vnext q k w k t , w k t = e ?q k j?Vnext e ?qj , k ? V next , (8) where V next = {t, t + 1, ? ? ? , min (t + ?, T )} is the index set of the related frames.</formula><p>In the end, we approximate the subjective frame quality scores by linearly combining the memory quality and current quality elements. The relative quality score Q r is then calculated by temporal global average pooling (GAP) of the approximate scores and bounded by a sigmoid function:</p><formula xml:id="formula_9">q t = ?l t + (1 ? ?)m t ,<label>(9)</label></formula><formula xml:id="formula_10">Q r = ? 1 T T t=1 q t ,<label>(10)</label></formula><p>where ? is a hyper-parameter to balance the contributions of memory and current elements to the approximate score, and ?(?) is the sigmoid function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Nonlinear Mapping</head><p>For predicting the perceptual quality, we add a nonlinear mapping module after the relative quality assessor to explicitly account for the nonlinearity of human perception on video quality <ref type="bibr" target="#b57">(VQEG, 2000)</ref>. The nonlinear mapping module can be a complex neural network with many parameters, or just a simple nonlinear function with few parameters. Following the suggestion by Video Quality Experts Group <ref type="bibr" target="#b57">(VQEG, 2000)</ref>, we can use a 4-parameter logistic function for nonlinear mapping.</p><formula xml:id="formula_11">Q p = f (Q r ) = ? 1 ? ? 2 1 + e ? Qr ?? 3 |? 4 | + ? 2 ,<label>(11)</label></formula><p>where ? 1 to ? 4 are fitting parameters, Q r is the relative quality score, and Q p is the perceptual quality score. We can reformulate Eqn. (11) as the following.</p><formula xml:id="formula_12">Q p = ? 1 ?(? 4 Q r + ? 3 ) + ? 2 ,<label>(12)</label></formula><p>where</p><formula xml:id="formula_13">? 1 ? ? 1 ? ? 2 , ? 2 ? ? 2 , ? 3 ? ? ?3 |?4| , and ? 4 ? 1 |?4|</formula><p>. And ? 1 , ? 2 are parameters to control the range of Q p . ? 3 , ? 4 are parameters to control the normalization of Q r . Therefore, it is equivalent to "Linear (i.e., Multiply Weights and Add Bias)+Sigmoid+Linear", as shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. With the reformulation, we can design the 4parameter nonlinear mapping as a network module. Since we will handle the scale problem in the next stage, the nonlinear mapping just handles the nonlinearity and does not change the scale, i.e., the ranges of Q r and Q p are both [0, 1]. We need to initialize the 4 parameters in this module at the start of the training. Random initialization is not a good choice since we have priors of Q r and Q p . Therefore, we can have a better initialization as follows.</p><formula xml:id="formula_14">? 1 ? sup (Q p ) ? inf (Q p ) = 1, ? 2 ? inf (Q p ) = 0, ? 3 ? ?c * mean(Q r )/std(Q r ), c = 1, ? 4 ? c/std(Q r ),<label>(13)</label></formula><p>where mean(?), std(?), inf (?), sup (?) indicate the mean, standard deviation, infimum, and supremum functions, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Dataset-Specific Perceptual Scale Alignment</head><p>Since the subjective study is designed to reflect human perception on video quality, based on the concepts of subjective quality and perceptual quality, we can assume that the subjective quality is linearly correlated with the perceptual quality. Thus, the perceptual scale alignment can be simply set as a specific FC layer.</p><formula xml:id="formula_15">Q s = ? 1 Q p + ? 2 ,<label>(14)</label></formula><p>where Q s is the predicted subjective quality score, and ? 1 , ? 2 are the scale and shift parameters. Since different datasets have different ranges of subjective quality scores, we need a dataset-specific alignment of perceptual scale on each dataset. Eqn. <ref type="formula" target="#formula_3">(14)</ref> is then modified as follows.</p><formula xml:id="formula_16">Q (d) s = ? (d) 1 Q p + ? (d) 2 (d = 1, ? ? ? , D),<label>(15)</label></formula><p>where Q is the predicted subjective quality score on the d-th dataset, ? </p><p>2 are the scale and shift parameters for the d-th dataset, and D is the number of considered datasets. These parameters can be determined by least square regression (LSR) or just jointly learned with other parameters by iterative stochastic gradient decent (SGD) algorithm. The latter way can provide supervision for end-to-end network training and it is adopted in our mixed datasets training strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Mixed Datasets Training Strategy</head><p>We have introduced the unified VQA model in the above. In this subsection, we show how we can enable mixed datasets training when the ranges of subjective quality scores are not consistent among the VQA datasets. For the first and second stages, the relative quality and perceptual quality are not involved with the ranges of subjective quality. We bypass the inconformity problem by designing two losses to supervise the training process of predicting relative quality and perceptual quality. For the third stage, to predict subjective quality of videos on each dataset, we learn a dataset-specific perceptual scale alignment for each dataset to avoid the inconformity caused by the na?ve linear re-scaling. Such dataset-specific alignment enables mixing multiple datasets during the training without disturbing the process. Specifically, monotonicityinduced loss is proposed for Stage 1 "relative quality assessor", and linearity-induced loss is adopted for Stage 2 "nonlinear mapping". As for Stage 3 "dataset-specific perceptual scale alignment", we can just use the widelyused error-induced (i.e., normalized L 1 ) loss as the supervision.</p><p>Assume we have D datasets of VQA, and the d-th dataset contains N d videos (d = 1, ? ? ? , D). For the ith video of the d-th dataset, we denote its predicted relative quality score as Q d,i r , the predicted perceptual quality score as Q d,i p , the predicted subjective quality score as Q d,i s , and ground-truth subjective quality score as Q d,i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">Monotonicity-Induced Loss</head><p>The goal of relative quality assessor is to achieve the best prediction monotonicity. That is, it aims to give a quality ranking for any list/pair of videos from the same dataset, that is consistent with subjective quality. A natural objective is to maximize the Spearman's rank-order correlation coefficient (SROCC) or Kendall's rank-order correlation coefficient (KROCC). However, they are not applicable to back-propagation based neural network optimization due to their nondifferentiable property.</p><p>Let us take a close look at the monotonicity condition. For all i, j = 1, ? ? ? , N d , d = 1, ? ? ? , D,</p><formula xml:id="formula_18">(Q d,i r ? Q d,j r )(Q d,i ? Q d,j ) ? 0.<label>(16)</label></formula><p>So we can consider the sum of the pair-wise losses as a surrogate. We call this monotonicity-induced loss, which is defined as follows.</p><formula xml:id="formula_19">L (d) rel = 2 N d (N d ?1) i&lt;j E d,(i,j) r , E d,(i,j) r = max{(Q d,i r ? Q d,j r ) * sign(Q d,j ? Q d,i ), 0},<label>(17)</label></formula><p>where E d,(i,j) r is the error term induced by the monotonicity condition, i.e., Eqn. (16). Here, we choose the error term as the margin ranking loss used in <ref type="bibr" target="#b64">Yang et al. (2019)</ref>. It can also be in the form of the fidelity loss or the cross entropy loss as described in <ref type="bibr" target="#b72">Zhang et al. (2019b)</ref>. Note that compared to pair-wise learning, the number of forward operations is reduced from C 2 N d to N d in our list-wise learning setting. Together with the vectorization form, we provide a much more efficient implementation and save more training time than the pair-wise learning used in image quality assessment <ref type="bibr" target="#b72">Zhang et al., 2019b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2">Linearity-Induced Loss</head><p>The goal of the nonlinear mapping module is to achieve the best prediction linearity between the predicted perceptual quality scores and the subjective quality scores. Pearson's linear correlation coefficient (PLCC) is a good objective for characterizing linearity. And it is differentiable, so we can define our linearity-induced loss for nonlinear mapping module as follow.</p><formula xml:id="formula_20">L (d) lin = (1 ? PLCC d )/2, PLCC d = i (Q d,i p ?Q (d) p )(Q d,i ?Q (d) ) i (Q d,i p ?Q (d) p ) 2 i (Q d,i ?Q (d) ) 2 ,<label>(18)</label></formula><formula xml:id="formula_21">whereQ (d) p = 1 N d i Q d,i p andQ (d) = 1 N d i Q d,i .</formula><p>Note that PLCC-induced loss is also considered in <ref type="bibr" target="#b32">(Ma et al., 2018;</ref><ref type="bibr" target="#b27">Liu et al., 2018;</ref><ref type="bibr" target="#b22">Li et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.3">Error-Induced Loss</head><p>After dataset-specific perceptual scale alignment, our goal is to minimize the absolute prediction error. In this stage, any regression error can be used as the loss function. We simply choose the widely-used error-induced (i.e., normalized L 1 ) loss in this work. More general and robust regression losses may be explored to further improve the optimization performance <ref type="bibr" target="#b1">(Barron, 2019)</ref>. To balance the losses among different datasets, we consider the inverse scale on each dataset as a normalization factor.</p><formula xml:id="formula_22">L (d) err = i 1 N d Q d,i s ? Q d,i S d ,<label>(19)</label></formula><p>where S d = max (Q d,i ) ? min (Q d,i ) is the range of the subjective quality scores on the d-th dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.4">Final Loss for The Whole Model</head><p>We obtain the loss for the d-th dataset (d = 1, ? ? ? , D) from the above three losses L </p><p>err .</p><formula xml:id="formula_24">L (d) = L (d) rel + L (d) lin + L (d) err ,<label>(20)</label></formula><p>and the overall final loss for training a single unified model from multiple datasets is defined as a softmaxweighted average of the losses over all datasets.</p><formula xml:id="formula_25">L = d w (d) L (d) , w (d) = e L (d) / d e L (d) ,<label>(21)</label></formula><p>where w (d) is the weight of L (d) (d = 1, ? ? ? , D).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Implementation Details</head><p>We choose ResNet-50 <ref type="bibr" target="#b10">(He et al., 2016)</ref> pre-trained on ImageNet <ref type="bibr" target="#b4">(Deng et al., 2009</ref>) for the content-aware feature extraction, and the feature maps are extracted from its top convolutional layer "res5c". In this instance, the dimension of f t is 2C = 4096. The feature dimension is then reduced from 4096 to 128, followed by a single-layer GRU network with hidden size 32. ? and ? in the temporal pooling layer are set as 12 and 0.5, respectively. We choose the 4-parameter nonlinear mapping, and the parameters in the module are initialized based on Eqn. (13). We freeze the parameters in the pre-trained ResNet-50 to ensure that the content-aware property is not altered, and we train the other part of the network in an end-to-end manner. We train our model using Adam optimizer <ref type="bibr" target="#b16">(Kingma and Ba, 2014)</ref> for 40 epochs with an initial learning rate 1e-4, a training batch size 32 for each dataset. The proposed model is implemented with PyTorch <ref type="bibr" target="#b43">(Paszke et al., 2019)</ref>. To support reproducible scientific research, we release the code at https://github.com/lidq92/MDTVSFA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>This section reports and analyzes the experimental results. We first describe the experimental setup, including the benchmark datasets, compared methods and basic evaluation criteria. Next, we study the effectiveness of our mixed datasets training strategy. After that, the performance comparison is carried out between our method and the state-of-the-art methods. Finally, the computational efficiency is briefly discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Benchmark datasets. Currently, there are four datasets for quality assessment of in-the-wild videos, including CVD2014 <ref type="bibr" target="#b41">(Nuutinen et al., 2016)</ref>, KoNViD-1k <ref type="bibr" target="#b11">(Hosu et al., 2017)</ref>, LIVE-Qualcomm <ref type="bibr" target="#b8">(Ghadiyaram et al., 2018)</ref>, and LIVE-VQC <ref type="bibr" target="#b51">(Sinno and Bovik, 2019a)</ref>. We summarize their brief information in <ref type="table" target="#tab_0">Table 1</ref>. We can see that the four datasets have different characteristics and the ranges of mean opinion score (MOS) are different among these datasets. In the default setting, each dataset is split into 80%, and 20% for training and testing, respectively. No overlap is among training and testing data. And 25% of the training data are used for validation. We repeat this procedure 10 times to avoid performance bias.</p><p>Compared methods. Only NR methods are applicable for quality assessment of in-the-wild videos. We select five state-of-the-art NR methods for comparison, whose original codes are released by the authors, icluding VBLIINDS <ref type="bibr" target="#b45">(Saad et al., 2014)</ref>, VI-IDEO <ref type="bibr" target="#b38">(Mittal et al., 2016)</ref>, BRISQUE <ref type="bibr" target="#b36">(Mittal et al., 2012)</ref> 1 , NIQE <ref type="bibr" target="#b37">(Mittal et al., 2013)</ref>, and CORNIA <ref type="bibr" target="#b65">(Ye et al., 2012)</ref>. Besides, we also show some relevant results reported from previous arts, e.g., TLVQM <ref type="bibr" target="#b17">(Korhonen, 2019)</ref>. Note that the method in <ref type="bibr" target="#b73">Zhang et al. (2019c)</ref> needs scores of full-reference methods, methods in <ref type="bibr" target="#b15">Kim et al. (2018)</ref> and <ref type="bibr" target="#b74">Zhang et al. (2020)</ref> are full-reference methods, and thus they are unfeasible for this problem.</p><p>Basic evaluation criteria. We follow the suggestion from Video Quality Experts Group <ref type="bibr" target="#b57">(VQEG, 2000)</ref>, and report SROCC and PLCC as the criteria of prediction monotonicity and prediction accuracy, respectively. Better VQA methods should have larger values of SROCC and PLCC. When the predicted quality scores are not the same scale as the subjective scores, PLCC is calculated after nonlinear mapping with a 4-parameter logistic function as suggested by VQEG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Effectiveness of Mixed Datasets Training Strategy</head><p>In this subsection, we conduct experiments to verify the effectiveness of our mixed datasets training strategy in the following four aspects. We first consider different loss combinations in our strategy. Then, we compare our strategy with the na?ve linear re-scaling strategy. In the third and fourth aspects, we exploit whether our strategy helps further improving the performance with more training data available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Different loss combinations.</head><p>To verify the effectiveness of the proposed losses, we compare different combinations of monotonicity-induced loss L rel , linearity-induced loss L lin , and error-induced loss L err . We consider mixing all the four datasets (CVD2014, KoNViD-1k, LIVE-Qualcomm, and LIVE-VQC) in this experiment. <ref type="figure">Fig. 5</ref> shows the dataset-size weighted average of median SROCC results over the four datasets. It can be seen that the combination of three losses is better than that of two losses, and the combination of two losses is better than one of the two losses only. The three losses all contribute to the performance gain, but the contribution of linearity-induced loss is the largest.</p><p>Comparison with linear re-scaling. To verify the effectiveness of our dataset-specific perceptual scale alignment, we compare it with the na?ve linear rescaling. Similar to the last experiment, all the four datasets (CVD2014, KoNViD-1k, LIVE-Qualcomm, and LIVE-VQC) are considered. And both our strategy and the linear re-scaling strategy use all three losses. They are compared with the models trained on one of the datasets, i.e., "Trained only on CVD2014/KonViD-1k/LIVE-Qualcomm/LIVE-VQC". <ref type="figure">Fig. 6</ref> shows the dataset-size weighted average of median SROCC re- <ref type="table">Table 2</ref> Performance gain in terms of median SROCC when one more dataset is added into the training data. D + is the added dataset for training, D B indicates the base datasets for training before adding D + , and D T indicates the dataset for testing. "Overall Performance" is indicated by the dataset-size weighted average of median SROCC. Positive gain is shown in blue, while negative gain is shown in red. The performance values in the scenario D T ? D B are marked in a light gray background, and the performance values in the scenario D T ? (D B ? D + ) = ? are marked in a dark gray background.  <ref type="figure">Fig. 6</ref> Median SROCC results for models trained with our strategy and the linear re-scaling strategy in comparison with the models trained only on one of the datasets Mixing more datasets. In this experiment, we explore the effect of mixing more datasets into the training data. <ref type="table">Table 2</ref> shows the median SROCC results in <ref type="table">Table 4</ref> Cross dataset performance gain in terms of median SROCC when KoNViD-1k is added into the training data. Note that the testing is conducted on the full dataset, including its train and test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Train data</head><p>Test dataset CVD2014 (full) LIVE-Qualcomm <ref type="formula">(</ref> 10 runs for mixing different datasets. Each cell shows the base performance of the model trained on the train sets of D B and tested on a test set of D T , and the value in the brackets indicates the performance gain when the train set of D + is added into the training data. The "Overall Performance" is the dataset-size weighted average of median SROCC over these datasets. In general, the overall performance over the four datasets is improved in most cases. As for the performance on a single test set, there are mainly three scenarios.</p><p>-D T = D + : The performance values in this scenario, shown in the diagonal blocks of <ref type="table">Table 2,</ref>  This phenomenon is the consequence of the following two factors: (1) over-fitting problem during the training, (2) the discrepancy of data distribution between the train set and the test set. <ref type="table" target="#tab_2">Table 3</ref> shows the performance of the model trained on a single train set and tested on a single test set, which can somehow reflects how well the trained dataset can represent the test set. In <ref type="table" target="#tab_2">Table 3</ref>, the diagonal values are always the largest one in its column, i.e., the most similar data set to a test set is its corresponding train set. Thus, adding the train set of D + to the train sets of D B leads to a significant performance improvement on the test set of D + , but a minor performance drop on the test sets of D B . However, we can notice that adding one more train set to the LIVE-Qualcomm train set provides a performance gain on the LIVE-Qualcomm test set. This might be attributed to the fact that LIVE-Qualcomm is the smallest dataset among these four dataset and overfitting is most likely to happen during model training on LIVE-Qualcomm. Besides, the performance on the test set of an unseen dataset D T depends on whether the train set of D + or D B is more similar to the test set of D T . In this regard, to improve the model performance on unseen datasets, it is critical to collect similar data for training. When datasets with similar data distribution to the test set are added into training data, it is more likely to learn the characteristics that are needed for assessing the quality of the test video in the wild. For example, in <ref type="table">Table 4</ref>, when KoNViD-1k is added into the training data, the crossdataset evaluation performance on the unseen dataset is improved. Different training proportions. In this experiment, we utilize different proportions of training data from the four datasets (LIVE-VQC, LIVE-Qualcomm, KoNViD-1k, and CVD2014) to train our VQA model with the proposed strategy. <ref type="figure" target="#fig_8">Fig. 7</ref> shows the test perfor- <ref type="table">Table 5</ref> Overall performance comparison on CVD2014, KoNViD-1k, and LIVE-Qualcomm. Mean and standard deviation (std) of the dataset-size weighted performance values in 10 runs are reported, i.e., mean (? std). The p-value is also reported, where p &lt; 0.001 indicates our method MDTVSFA is significantly better than the method in that row.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>SROCC? p-value PLCC? p-value mean (? std) based on SROCC mean (? std) based on PLCC BRISQUE <ref type="bibr" target="#b36">(Mittal et al., 2012)</ref> 0.6610 (? 0.0218) 9.6754E-09 0.6032 (? 0.0144) 4.7276E-10 NIQE <ref type="bibr" target="#b37">(Mittal et al., 2013)</ref> 0.5255 (? 0.0479) 2.3066E-09 0.5396 (? 0.0430) 6.4720E-10 CORNIA <ref type="bibr" target="#b65">(Ye et al., 2012)</ref> 0.5913 (? 0.0253) 5.4983E-10 0.5954 (? 0.0240) 5.0748E-10 VIIDEO <ref type="bibr" target="#b38">(Mittal et al., 2016)</ref> 0.2368 (? 0.0595) 7.4623E-11 0.2351 (? 0.0574) 4.4222E-11 VBLIINDS <ref type="bibr" target="#b45">(Saad et al., 2014)</ref> 0.6628 (? 0.0321) 7.7577E-08 0.6127 (? 0.0833) 5.1515E-05 TLVQM <ref type="bibr" target="#b17">(Korhonen, 2019)</ref> 0.77 (? 0.02) * * 0.77 (? 0.02) * * LS-VSFA 0.7603 (? 0.0219) 4.0044E-07 0.7662 (? 0.0238) 1.9500E-06 MDTVSFA 0.7860 (? 0.0202) -0.7923 (? 0.0207) - * The results are cited from <ref type="table">Table VIII</ref> of the original paper <ref type="bibr" target="#b17">(Korhonen, 2019)</ref>. We can not calculate the p-value due to the lack of raw SROCC/PLCC values of TLVQM. mance on the four datasets under different training proportions of the training data. The performance on each dataset increases as the training proportion increases. Our method can still achieve a good performance even when the training proportion is 1/2, which means only half of the training data are used for training. And the increasing trend indicates that the performance can still be improved when more training data are available.</p><p>Based on the above study, we have learned that our mixed datasets training strategy is effective. To sum up, it is helpful for learning characteristics from all datasets and thus improving the overall performance. It also has the potential benefits for cross-dataset evaluation since the characteristics of the test videos are more likely to be learned, if more datasets with similar data distribution to the testing set are added into the training data. Besides, the performance can be further improved with more training data available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Performance Comparison</head><p>In this section, we compare our method with the stateof-the-art NR methods. For VBLIINDS, BRISQUE and our method, we choose the models with the highest SROCC values on the validation set during the training phase. NIQE, CORNIA, and VIIDEO are tested on the same 20% testing data after fitting the four-parameter logistic function with the training data.</p><p>Overall performance. In this part, all the methods are trained using mixed datasets. Similar to <ref type="bibr" target="#b17">Korhonen (2019)</ref>, the other compared methods use the na?ve linear re-scaling strategy. Our model trained with the na?ve linear re-scaling strategy, denoted as LS-VSFA, does not learn the dataset-specific perceptual scale alignment and uses all three losses after linear rescaling the subjective quality scores to the same range. We denote our VQA model trained with the proposed mixed datasets training strategy as MDTVSFA. Table 5 reports the overall performance over CVD2014, <ref type="bibr"></ref> where the overall performance is measured by the dataset-size weighted performance values over the three datasets. We can see that our VQA model achieves the best performance in terms of prediction monotonicity (SROCC) and prediction accuracy (PLCC). The last two rows show that our proposed mixed datasets training strategy can achieve better performance than the na?ve linear re-scaling strategy. We further carry out the statistical significance test to see whether these comparison results are statistical significant or not. On each dataset, the paired t-test is conducted at 1? significance level using the performance values (in 10 runs) of our method MDTVSFA and that of the compared one. The p-values are shown in <ref type="table">Table 5</ref>. All p-values are far smaller than 0.001 and it proves that our method is significantly better than all the other methods.</p><p>Scatter plot and qualitative examples. To have an intuitive feeling, in <ref type="figure" target="#fig_9">Fig. 8</ref>, we visualize the scatter plots between the subjective scores and predicted scores for the five best-performed methods (excluding TLVQM, since we do not have its raw predictions) in the 10-th run. Each row shows the scatter plots for a method. From top to down, the methods are BRISQUE, CORNIA, VBLIINDS, LS-VSFA, and MDTVSFA. The first, second, and third column show the scatter plots on CVD2014, KoNViD-1k, and LIVE-Qualcomm, respectively. In each sub-figure, the x-axis indicates the predicted score by the method while y-axis indicates the MOS. The scatter points are expected to be located at the diagonal line. We can see that the scatter plots for BRISQUE and CORNIA are more dispersive than the ones for VBLIINDS and our method. The scatter points for our method are more densely clustered around and centered at the diagonal line than the others. In <ref type="figure" target="#fig_10">Fig. 9</ref>, 10 and 11, we show several success and failure cases of our method. <ref type="figure" target="#fig_10">Fig. 9 and Fig. 10</ref> show the success cases of MDTVSFA, which means the predictions of MDTVSFA model is consistent with MOS. LS-VSFA has more failure cases than MDTVSFA since the linear re-scaling strategy disturbs the training process. We also show two failure cases of LS-VSFA in <ref type="figure" target="#fig_10">Fig. 9 and Fig. 10</ref>. Besides, there is still a large room for improving the performance of MDTVSFA, and we show a failure case of both MDTVSFA and LS-VSFA in <ref type="figure">Fig. 11</ref>. Such failure may be due to the fact that our models extract frame-level features and not fully exploit the motion and spatial-temporal information. For example, our methods do not account for the discomfort caused by suddenly and fast scene change. Performance on individual datasets. Besides the overall performance reported in last part, we report performance on individual datasets in this part. Our method is trained by mixing the four datasets while other methods are trained on individual datasets. <ref type="table" target="#tab_3">Table 6</ref> summarizes the performance values on the four datasets individually. The results provided by our method are based only on a single unified model while the results provided by other methods are based on dif-  ferent models trained for different datasets. The natural scene statistics (NSS)-based NR-IQA methods (such as BRISQUE) outperform VIIDEO. This may be owing to the fact that VIIDEO is based only on temporal scene statistics and cannot model the complex distortions. VBLIINDS and TLVQM rely on a lot of carefully-designed handcrafted features that capture the spatial and temporal distortions, and thus they achieve a better performance than the NR-IQA methods and VIIDEO. Our method achieves the best performance in terms of prediction monotonicity (SROCC) and prediction accuracy (PLCC) on the three datasets (LIVE-VQC, LIVE-Qualcomm, and KoNViD-1k). On CVD2014, MDTVSFA slightly outperforms TLVQM in terms of SROCC, while it slightly underperforms TLVQM in terms PLCC. However, we should note that the results of our method is based only on one single model, which indicates our unified model performs well across datasets.</p><p>We further prove the above statement by conducting experiments to compare the models trained by mixing  <ref type="bibr" target="#b37">(Mittal et al., 2013)</ref> 0.5892 (? 0.0538) 0.6112 (? 0.0554) 0.4628 (? 0.1052) 0.4638 (? 0.1362) CORNIA <ref type="bibr" target="#b65">(Ye et al., 2012)</ref> 0.5953 (? 0.0170) 0.5926 (? 0.0230) 0.4598 (? 0.1299) 0.4941 (? 0.1327) VIIDEO <ref type="bibr" target="#b38">(Mittal et al., 2016)</ref> 0.1498 (? 0.0995) 0.2454 (? 0.0740) 0.1267 (? 0.1368) -0.0012 (? 0.1062) VBLIINDS <ref type="bibr" target="#b45">(Saad et al., 2014)</ref> 0.7015 (? 0.0483) 0.7120 (? 0.0501) 0.5659 (? 0.0780) 0.5676 (? 0.0885) ST-Naturalness <ref type="bibr" target="#b52">(Sinno and Bovik, 2019b)</ref> 0.5994 * 0.6069 * --3D-CNN+LSTM <ref type="bibr" target="#b66">(You and Korhonen, 2019</ref>  <ref type="bibr" target="#b37">(Mittal et al., 2013)</ref> 0.5435 (? 0.0396) 0.5456 (? 0.0376) 0.4890 (? 0.0908) 0.5931 (? 0.0650) CORNIA <ref type="bibr" target="#b65">(Ye et al., 2012)</ref> 0.6096 (? 0.0343) 0.6075 (? 0.0318) 0.6140 (? 0.0754) 0.6178 (? 0.0792) VIIDEO <ref type="bibr" target="#b38">(Mittal et al., 2016)</ref> 0.2976 (? 0.0522) 0.3026 (? 0.0486) 0.0228 (? 0.1216) -0.0249 (? 0.1439) VBLIINDS <ref type="bibr" target="#b45">(Saad et al., 2014)</ref> 0.6947 (? 0.0239) 0.6576 (? 0.0254) 0.7458 (? 0.0564) 0.7525 (? 0.0528) FC Model  0.572 * 0.565 * --STFC Model <ref type="bibr" target="#b35">(Men et al., 2018)</ref> 0.606 * 0.639 * --STS-CNN200 <ref type="bibr" target="#b63">(Yan and Mou, 2019)</ref> 0.735 * ---TLVQM <ref type="bibr" target="#b17">(Korhonen, 2019)</ref> 0.78 (? 0.02) * 0.77 (? 0.02) * 0.83 (? 0.04) * 0.85 (? 0.04) * MDTVSFA 0.7812 (? 0.0278) 0.7856 (? 0.0240) 0.8314 (? 0.0416) 0.8407 (? 0.0296) * The reported results in their original papers are shown here for reference.  <ref type="bibr" target="#b17">(Korhonen, 2019)</ref> are shown here for reference. The "&lt;" relation is inferred from the <ref type="table">Table VII</ref> of <ref type="bibr" target="#b17">Korhonen (2019)</ref>. "-" indicates that the results are not reported.</p><p>CVD2014, KoNViD-1k, and LIVE-Qualcomm datasets with the models trained on one of the datasets. Table 7 shows the median SROCC of different models on the three datasets. We can see that, no matter which model it is, the unified model trained by mixing all datasets achieves better overall performance than the model trained on one of the datasets. And our model trained with our proposed strategy achieves better over-all performance across the datasets than the other models (VBLIINDS, BRISQUE, and TLVQM) trained with the linear re-scaling strategy. Among these datasets, the size of LIVE-Qualcomm dataset is the smallest one. And our model trained only on LIVE-Qualcomm dataset suffered from over-fitting problem. In such situation, mixed datasets training helps alleviating the problem to some extent. So a performance improvement of the proposed model with mixed dataset training is found on LIVE-Qualcomm dataset. This verifies the necessity of mixed datasets training and the effectiveness of our mixed datasets training strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Computational efficiency</head><p>Besides the performance, computational efficiency is also crucial for NR-VQA methods. To provide a fair comparison for the computational efficiency of different methods, all tests are carried out on the same desktop computer with Intel Core i7-6700K CPU@4.00 GHz, 12G NVIDIA TITAN Xp GPU, and 64 GB RAM. The operating system is Ubuntu 14.04. The compared methods are implemented with MATLAB R2016b while our method is implemented with Python 3.6. We use the default settings of the original codes without any modification. We select two videos with different lengths and different resolutions for testing. The tests are run in a separate environment and repeated ten times to avoid any influence. The logarithm (with base 10) of the average computation time (seconds) for each method is shown in <ref type="figure" target="#fig_13">Fig. 12</ref>. The point near the left is the fast one, and the point near the top is the good-performed one. Our method (CPU version) is faster than VBLIINDSthe method with the third-best performance. TLVQM, the second-best performed method, considers two-level features, i.e., low-complexity features for all frames and high-level features for only selected representative frames. It achieves a good trade-off between the performance and computational efficiency. It is worth mentioning that our method can be accelerated to 30x faster or more (The larger resolution and length the video has, the faster acceleration is) by simply switching the CPU mode to the GPU mode. With the GPU available, our method (GPU version) is at the upperleft, and thus it is the fastest one as well as the bestperformed one. To further improve the computational efficiency, we may resort to the light-weight networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this work, we propose a novel unified NR-VQA framework with a mixed datasets training strategy for  <ref type="table">Table 5</ref>) and the logarithm of average computation time (seconds) on videos with different resolutions and different lengths in-the-wild videos. The backbone model is a deep neural network designed for characterizing the two eminent effects of HVS, i.e., content-dependency and temporalmemory effects. We enable mixed datasets training by designing two losses (monotonicity-induced loss, linearity-induced loss) for predicting relative quality and perceptual quality, and assigning dataset-specific perceptual scale alignment layers for predicting subjective quality. Our proposed method is compared with the state-of-the-art methods on four publicly available inthe-wild VQA datasets (CVD2014, KoNViD-1k, LIVE-Qualcomm, and LIVE-VQC). Experiments show the superior performance of our method and also verify the effectiveness of our unified VQA model with the mixed datasets training strategy. However, our mixed datasets training strategy needs to re-train the unified VQA model every time when a new dataset is added to the training data. This will increase the burden of training. In the further study, we will explore lifelong learning for this task. Also, besides video capture, we intend to provide a unified and efficient VQA framework that can handle the whole chainflow of video production. Moreover, some meta information that is crucial for the video quality, like video resolution, can be used as extra features for improv-ing the model performance. Finally, we intend to apply our unified VQA model for practical computer vision applications such as video enhancement.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Fig. 2 An overview of the proposed unified framework. It consists of three stages: relative quality assessor, nonlinear mapping, and dataset-specific perceptual scale alignment for predicting relative quality, perceptual quality, and subjective quality, respectively. The supervisions for mixed datasets training at the three stages are monotonicity-induced loss, linearity-induced loss, and error-induced loss, respectively. D is the number of datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4</head><label>4</label><figDesc>Illustration of the nonlinear mapping module</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>all increase a lot. For example, when the train set of CVD2014 (D + ) is added into any train sets of D B , the performance on the test set of CVD2014 (D T ) is improved (0.1479+ gain). -D T ? D B : The performance values in this scenario, marked in a light gray background, mostly decrease a little. For example, when the train set of CVD2014 (D + ) is added into the train set of KoNViD-1k (D B ), the performance on the test set of KoNViD-1k (D T ) drops 0.0067. -D T ? (D B ? D + ) = ?: The performance values in this scenario, marked in a dark gray background, may increase or decrease. For example, when the train set of LIVE-Qualcomm (D + ) is added into the train set of LIVE-VQC (D B ), the performance on the test set of CVD2014 (D T ) is improved while that on the test set of KoNViD-1k (D T ) drops.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7</head><label>7</label><figDesc>Mean SROCC results under different training proportions when the model is trained by mixing all datasets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8</head><label>8</label><figDesc>Scatter plots for BRISQUE, CORNIA, VBLIINDS, LS-VSFA, and MDTVSFA on CVD2014, KoNViD-1k, and LIVE-Qualcomm datasets. The predictions of MDTVSFA shows the best correlation with the mean opinion scores (MOSs) across the datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9</head><label>9</label><figDesc>(a) Three representative frames of video A on KoNViD-1k (b) Three representative frames of video B on KoNViD-1k (c) Three representative frames of video C on KoNViD-1k (d) Three representative frames of video D on KoNViD-1k Qualitative example on KoNViD-1k test set. The quality rankings provided by MOS and MDTVSFA are both A&lt;B&lt;C&lt;D, but LS-VSFA gives a quality ranking of A&lt;C&lt;B&lt;D. Full-resolution videos are provided in https: //bit.ly/3csmHYk.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>(a) Three representative frames of video E on LIVE-VQC (b) Three representative frames of video F on LIVE-VQC Fig. 10 Qualitative example on LIVE-VQC. The quality rankings provided by MOS and MDTVSFA are both E&gt;F, but LS-VSFA gives a quality ranking of E&lt;F. Full-resolution videos are provided in https://bit.ly/3csmHYk.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>(a) Three representative frames of video G on LIVE-VQC (b) Three representative frames of video H on LIVE-VQC Fig. 11 Another qualitative example on LIVE-VQC. The quality rankings provided by LS-VSFA and MDTVSFA are both G&lt;H, but MOS gives a quality ranking of G&gt;H. Note that the scenes change fast in video H, where full-resolution videos are provided in https://bit.ly/3csmHYk.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 12</head><label>12</label><figDesc>Bubble charts with the overall performance (mean SROCC values in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Brief information of the four benchmark datasets, including the information of the videos and the information of the corresponding subjective study.<ref type="bibr" target="#b41">Nuutinen et al., 2016)</ref> <ref type="bibr" target="#b11">(Hosu et al., 2017)</ref> <ref type="bibr" target="#b8">(Ghadiyaram et al., 2018)</ref> <ref type="bibr" target="#b51">(Sinno and Bovik, 2019a)</ref> </figDesc><table><row><cell cols="2">Dataset (Number of Videos</cell><cell>CVD2014 234</cell><cell>KoNViD-1k 1200</cell><cell>LIVE-Qualcomm 208</cell><cell>LIVE-VQC 585</cell></row><row><cell>Number of Scenes</cell><cell></cell><cell>5</cell><cell>?1200</cell><cell>54</cell><cell>?585</cell></row><row><cell>Number of Devices</cell><cell></cell><cell>78</cell><cell>-</cell><cell>8</cell><cell>101</cell></row><row><cell>Number of Users</cell><cell></cell><cell>-</cell><cell>480</cell><cell>-</cell><cell>80</cell></row><row><cell>Video Orientations</cell><cell></cell><cell>Landscape</cell><cell>Landscape</cell><cell>Landscape</cell><cell>Portrait, Landscape</cell></row><row><cell>Video Resolutions</cell><cell cols="2">1280?720 or 640?480</cell><cell>960?540</cell><cell>1920?1080</cell><cell>1920?1080 to 320?240</cell></row><row><cell>Number of Resolutions</cell><cell></cell><cell>2</cell><cell>1</cell><cell>1</cell><cell>18</cell></row><row><cell>Frames Per Second</cell><cell></cell><cell>11 to 31</cell><cell>24, 25 or 30</cell><cell>30</cell><cell>19-30 (one 120)</cell></row><row><cell>Time Span</cell><cell></cell><cell>10-25s</cell><cell>8s</cell><cell>15s</cell><cell>10s</cell></row><row><cell>Max Video Length</cell><cell></cell><cell>830 frames</cell><cell>240 frames</cell><cell>526 frames</cell><cell>1202 frames</cell></row><row><cell>Test Methodology</cell><cell></cell><cell>Single stimulus</cell><cell>Single stimulus</cell><cell>Single stimulus</cell><cell>Single stimulus</cell></row><row><cell>Lab or Crowdsourcing</cell><cell></cell><cell>Lab</cell><cell>Crowdsourcing</cell><cell>Lab</cell><cell>Crowdsourcing</cell></row><row><cell>Number of Participants</cell><cell></cell><cell>210</cell><cell>642</cell><cell>39</cell><cell>4776</cell></row><row><cell>Number of Ratings</cell><cell></cell><cell>28-33</cell><cell>&gt;50, average 114</cell><cell>18</cell><cell>&gt;200, average 240</cell></row><row><cell>Raw Ratings Provided</cell><cell></cell><cell>Yes</cell><cell>Yes</cell><cell>No</cell><cell>No</cell></row><row><cell>Mean Opinion Score</cell><cell></cell><cell>[-6.50, 93.38]</cell><cell>[1.22, 4.64]</cell><cell>[16.5621, 73.6428]</cell><cell>[6.2237, 94.2865]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>The test performance of a model trained only on a single train set</figDesc><table><row><cell>SROCC</cell><cell>Test</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">CVD2014 KoNViD-1k LIVE-Qualcomm LIVE-VQC</cell></row><row><cell>Train</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CVD2014</cell><cell></cell><cell>0.8747</cell><cell>0.6051</cell><cell>0.3919</cell><cell>0.4950</cell></row><row><cell cols="2">KoNViD-1k</cell><cell>0.6474</cell><cell>0.7809</cell><cell>0.6732</cell><cell>0.7160</cell></row><row><cell cols="2">LIVE-Qualcomm</cell><cell>0.5879</cell><cell>0.6128</cell><cell>0.7538</cell><cell>0.6214</cell></row><row><cell>LIVE-VQC</cell><cell></cell><cell>0.4819</cell><cell>0.7059</cell><cell>0.6550</cell><cell>0.7470</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6</head><label>6</label><figDesc>Performance comparison on the four VQA datasets individually. Mean and standard deviation (std) of the performance values in 10 runs are reported, i.e., mean (? std). In each column, the best mean SROCC and PLCC values are marked in boldface, and the second-best performance values are underlined.</figDesc><table><row><cell>Method</cell><cell cols="2">LIVE-VQC (Sinno and Bovik, 2019a) SROCC? PLCC?</cell><cell cols="2">LIVE-Qualcomm (Ghadiyaram et al., 2018) SROCC? PLCC?</cell></row><row><cell>BRISQUE (Mittal et al., 2012)</cell><cell>0.5687 (? 0.0729)</cell><cell>0.5868 (? 0.0642)</cell><cell>0.5036 (? 0.1470)</cell><cell>0.5158 (? 0.1274)</cell></row><row><cell>NIQE</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7</head><label>7</label><figDesc>Performance comparison in terms of median SROCC between the single models trained by mixing all three datasets (CVD2014, KoNViD-1k, and LIVE-Qualcomm) and the models trained on one of the datasets. Overall performance indicates the dataset-size weighted median SROCC values in 10 runs. For each column, the largest value is marked in boldface.</figDesc><table><row><cell>Model</cell><cell>Train data</cell><cell>Mixed datasets training</cell><cell cols="4">Test dataset CVD2014 KoNViD-1k LIVE-Qualcomm Performance Overall</cell></row><row><cell></cell><cell>CVD2014</cell><cell>No</cell><cell>0.7582</cell><cell>0.5574</cell><cell>0.4632</cell><cell>0.5794</cell></row><row><cell>BRISQUE</cell><cell>KoNViD-1k LIVE-Qualcomm</cell><cell>No No</cell><cell>0.5388 0.3930</cell><cell>0.6191 0.2341</cell><cell>0.3019 0.5023</cell><cell>0.5621 0.2973</cell></row><row><cell></cell><cell>All three datasets</cell><cell>Linear re-scaling</cell><cell>0.7356</cell><cell>0.6300</cell><cell>0.3809</cell><cell>0.6107</cell></row><row><cell></cell><cell>CVD2014</cell><cell>No</cell><cell>0.7892</cell><cell>0.5787</cell><cell>0.4170</cell><cell>0.5864</cell></row><row><cell>VBLIINDS</cell><cell>KoNViD-1k LIVE-Qualcomm</cell><cell>No No</cell><cell>0.5681 0.5027</cell><cell>0.7078 0.5432</cell><cell>0.4583 0.6018</cell><cell>0.6544 0.5544</cell></row><row><cell></cell><cell>All three datasets</cell><cell>Linear re-scaling</cell><cell>0.6749</cell><cell>0.6890</cell><cell>0.4684</cell><cell>0.6640</cell></row><row><cell></cell><cell>CVD2014</cell><cell>No</cell><cell>0.83  *</cell><cell>0.54  *</cell><cell>0.38  *</cell><cell>-</cell></row><row><cell>TLVQM</cell><cell>KoNViD-1k LIVE-Qualcomm</cell><cell>No No</cell><cell>&lt;0.62  *  &lt;0.36  *</cell><cell>0.78  *  &lt;0.38  *</cell><cell>&lt;0.49  *  0.788  *</cell><cell>--</cell></row><row><cell></cell><cell>All three datasets</cell><cell>Linear re-scaling</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.77  *</cell></row><row><cell></cell><cell>CVD2014</cell><cell>No</cell><cell>0.8747</cell><cell>0.6051</cell><cell>0.3919</cell><cell>0.6165</cell></row><row><cell>Our model</cell><cell>KoNViD-1k LIVE-Qualcomm</cell><cell>No No</cell><cell>0.6474 0.5879</cell><cell>0.7809 0.6128</cell><cell>0.6732 0.7538</cell><cell>0.7483 0.6271</cell></row><row><cell></cell><cell>All three datasets</cell><cell>Our strategy</cell><cell>0.8412</cell><cell>0.7659</cell><cell>0.8157</cell><cell>0.7829</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* The reported SROCC results in the original paper</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Video-level features of BRISQUE are the average pooling of its frame-level features.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work was partially supported by the Natural Science Foundation of China under contracts 61572042, 61520106004, and 61527804. This work was also supported in part by National Key R&amp;D Program of China (2018YFB1403900). We acknowledge the High-Performance Computing Platform of Peking University for providing computational resources.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict of interest</head><p>The authors declare no conflict of interest.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Study of temporal effects on subjective video quality of experience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Bampis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katsavounidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5217" to="5231" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A general and adaptive robust loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4331" to="4339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>arXiv:14061078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Video quality assessment accounting for temporal visual masking of local flicker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="182" to="198" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Understanding how image quality affects deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Quality of Multimedia Experience (QoMEX)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Using multiple spatio-temporal features to estimate video quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">G</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Akamine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Farias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Perceptual quality prediction on authentically distorted images using a bag of features approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="32" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">In-capture mobile video distortions: A study of subjective behavior and objective algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2061" to="2077" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Grapy-ML: Graph pyramid mutual learning for cross-dataset human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno>arXiv:191112053</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The Konstanz natural video database (KoNViD-1k)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hosu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jenadeleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Szir?nyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Saupe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Quality of Multimedia Experience (QoMEX)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Which is the better inpainted image? training data generation without any manual operations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mikami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Iwai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kimata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="page" from="1751" to="1766" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Measurement of quality of experience of video-on-demand services: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Juluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tamarapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Medhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Surveys and Tutorials</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="401" to="418" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for no-reference image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1733" to="1740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep video quality assessor: From spatio-temporal visual sensitivity to a convolutional neural aggregation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="219" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>arXiv:14126980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Two-level approach for no-reference consumer video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5923" to="5938" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Training objective image and video quality estimators using multiple databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Krasula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yoann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Callet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="961" to="969" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lasinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno>arXiv:190701341</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Quality assessment of in-the-wild videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia (MM)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2351" to="2359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Which has better visual quality: The clear blue sky or a blurry animal?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1221" to="1234" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Norm-in-norm loss with faster convergence and better performance for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia (MM)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="789" to="797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Spatiotemporal statistics for video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3329" to="3342" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">No-reference video quality assessment with 3D shearlet transform and convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Po</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1044" to="1057" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Crossdataset person re-identification via unsupervised pose disentanglement and adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">B</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ycf</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7919" to="7929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hallucinated-IQA: Noreference image quality assessment via adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="732" to="741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">End-to-end blind quality assessment of compressed videos using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Duanmu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia (MM)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="546" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">RankIQA: Learning from rankings for no-reference image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1040" to="1049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A spatiotemporal model of video quality assessment via 3D gradient differencing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">478</biblScope>
			<biblScope unit="page" from="141" to="151" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised crossdataset person re-identification by transfer learning of spatial-temporal patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7948" to="7956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Group MAD competition -a new methodology to compare objective image quality models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Duanmu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1664" to="1673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Geometric transformation invariant image quality assessment using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Duanmu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6732" to="6736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An optical flowbased no-reference video quality assessment algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Manasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Channappayya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2400" to="2404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Empirical evaluation of no-reference VQA methods on a natural video quality database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Saupe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Quality of Multimedia Experience (QoMEX)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="3" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Spatiotemporal feature combination model for no-reference video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Saupe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Quality of Multimedia Experience (QoMEX)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="3" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">No-reference image quality assessment in the spatial domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4695" to="4708" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Making a &quot;completely blind&quot; image quality analyzer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="212" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A completely blind video integrity oracle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="289" to="300" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Video quality assessment on mobile devices: Subjective, behavioral and objective studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Veciana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="652" to="671" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">How video object tracking is affected by in-capture distortions?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hdb</forename><surname>Restrepo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Cabezas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2227" to="2231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">CVD2014-a database for evaluating no-reference video quality assessment algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nuutinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vaahteranoksa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vuori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Oittinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>H?kkinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3073" to="3086" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Video quality pooling adaptive to perceptual distortion severity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Seshadrinathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="610" to="620" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">PyTorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learned video compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3454" to="3463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Blind prediction of natural video quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charrier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1352" to="1365" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Motion tuned spatio-temporal quality assessment of natural videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Seshadrinathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="335" to="350" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Temporal hysteresis model of time varying subjective video quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Seshadrinathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1153" to="1156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Study of subjective and objective quality assessment of video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Seshadrinathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Cormack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1427" to="1441" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A survey on quality of experience of HTTP adaptive streaming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seufert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Egger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Slanina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zinner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ho?feld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tran-Gia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Surveys and Tutorials</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="469" to="492" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Semantic-aware blind image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Siahaan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hanjalic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Redi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="237" to="252" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Large scale study of perceptual video quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sinno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="612" to="627" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Spatio-temporal measures of naturalness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sinno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1750" to="1754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Triantaphillidou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allen</forename><forename type="middle">E</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<title level="m">Image quality comparison between JPEG and JPEG2000</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Scene dependency, scene analysis, and classification</title>
	</analytic>
	<monogr>
		<title level="j">Journal of Imaging Science and Technology</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="259" to="270" />
		</imprint>
	</monogr>
	<note>II</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">No-reference video quality assessment based on the temporal pooling of deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Varga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Processing Letters</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="2595" to="2608" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">No-reference video quality assessment via pretrained CNN and LSTM networks. Signal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Varga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Szir?nyi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Video Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1569" to="1576" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Final report from the Video Quality Experts Group on the validation of objective models of video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vqeg</surname></persName>
		</author>
		<ptr target="https://www.its.bldrdoc.gov/media/8212/frtv_phase1_final_report.doc" />
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">VideoSet: A large-scale compressed video quality dataset based on JND measurement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katsavounidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">O</forename><surname>Pun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kwong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ccj</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="292" to="302" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Novel spatiotemporal structural information based video quality metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technology</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="989" to="998" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Video quality assessment based on structural distortion measurement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="121" to="132" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">No-reference video quality assessment via feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="491" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">No-reference video quality assessment based on spatiotemporal slice images and deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE 11187, Optoelectronic Imaging and Multimedia Technology VI</title>
		<meeting>SPIE 11187, Optoelectronic Imaging and Multimedia Technology VI</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="74" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">CNNbased cross-dataset no-reference image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">T</forename><surname>Peltoketo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Kamarainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshop (ICCVW)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3913" to="3921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning framework for no-reference image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1098" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Deep neural networks for no-reference video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2349" to="2353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Attention driven foveated video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perkis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="200" to="213" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">VSI: A visual saliencyinduced index for perceptual image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4270" to="4281" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Study of saliency in objective video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1275" to="1288" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">RankSR-GAN: Generative adversarial networks with ranker for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3096" to="3105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<idno>arXiv:190700516</idno>
		<title level="m">Learning to blindly assess image quality in the laboratory and wild</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Blind video quality assessment with weakly supervised learning and resampling strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2244" to="2255" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Objective video quality assessment combining transfer learning with CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2716" to="2730" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
