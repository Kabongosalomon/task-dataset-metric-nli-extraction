<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Explainable Probabilistic Classifier for Categorical Data Inspired to Quantum Physics</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-05-26">26 May 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Guidotti</surname></persName>
							<email>emanuele.guidotti@unine.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Financial Analysis</orgName>
								<orgName type="institution">Universit? de Neuch?tel</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfio</forename><surname>Ferrara</surname></persName>
							<email>alfio.ferrara@unimi.it</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Data Science Research Center Universit? degli Studi di Milano</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Explainable Probabilistic Classifier for Categorical Data Inspired to Quantum Physics</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-05-26">26 May 2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents Sparse Tensor Classifier (STC), a supervised classification algorithm for categorical data inspired by the notion of superposition of states in quantum physics. By regarding an observation as a superposition of features, we introduce the concept of wave-particle duality in machine learning and propose a generalized framework that unifies the classical and the quantum probability. We show that STC possesses a wide range of desirable properties not available in most other machine learning methods but it is at the same time exceptionally easy to comprehend and use. Empirical evaluation of STC on structured data and text classification demonstrates that our methodology achieves state-of-theart performances compared to both standard classifiers and deep learning, at the additional benefit of requiring minimal data pre-processing and hyper-parameter tuning. Moreover, STC provides a native explanation of its predictions both for single instances and for each target label globally. All the code is released at https://sparsetensorclassifier.org</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The application of supervised learning algorithms for data classification in an increasing number of different contexts requires novel machine learning methods capable of improving generalisability and delivering performances robust to data pre-processing, parameter and hyper-parameter tuning, and small samples <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b18">19]</ref>. In particular, feature extraction and data pre-processing are crucial steps for categorical data, which are often highly heterogeneous in terms of size, structural differences, and noise. Representing this kind of data in the feature space requires a task-specific feature engineering work that is non-trivial and time consuming. Another limitation, that can potentially hinder the adoption of machine learning methodologies, is the growing demand for explainable and interpretable classification models, both for what concerns the classification of single instances (local explanation) and the rationale of a model prediction in terms of the most relevant features for a target class label (global explanation). In this context, we present Sparse Tensor Classifier (STC), an explainable classification algorithm for categorical data inspired by the notion of superposition of states in quantum physics. The empirical evaluation against consolidated algorithms and deep learning alternatives demonstrates the native capability of STC to achieve state-of-the-art performances without relying on data pre-processing and (hyper-)parameter tuning, at the additional benefit of providing a meaningful explanation of the classification results.</p><p>Related work. Several machine learning algorithms have long been studied in the context of categorical data in general and text classification in particular (see <ref type="bibr" target="#b6">[7]</ref> for a survey). This extensive literature includes simple but effective methods for text categorization with multi-label documents and many categories (see e.g., <ref type="bibr" target="#b12">[13]</ref>) and, more recently, deep learning alternatives based on Knowledge Base descriptors <ref type="bibr" target="#b20">[21]</ref>, cooperative Neural Networks <ref type="bibr" target="#b17">[18]</ref>, Neural Attentive Bag-of-Entities Models <ref type="bibr" target="#b19">[20]</ref>, ensemble Neural Networks <ref type="bibr" target="#b16">[17]</ref>. In all these approaches, feature engineering and data pre-processing are crucial steps given the unstructured and noisy nature of text <ref type="bibr" target="#b7">[8]</ref>. The need of dealing with many and potentially noisy categorical features goes beyond textual applications and it is shared across different disciplines. As an example, Extended Connectivity Circular Fingerprints <ref type="bibr" target="#b14">[15]</ref> are used to compute a bag-of-words style representation of a molecule for machine learning applications in cheminformatics and drug discovery <ref type="bibr" target="#b9">[10]</ref>. Another field related to our work is machine learning explanation and interpretation. Although there is still no agreed-upon formal definition of interpretability (see <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b2">3]</ref>), a taxonomy of explanation methods is given in <ref type="bibr" target="#b1">[2]</ref>. With respect to this framework, STC provides a native explanation of its result that is both 'local' and 'global', in that we compute a score to determine what features are important for a certain class both for single predictions (i.e., local explanation) and for the entire model (i.e., global explanation). In this paper, we also rely on LIME <ref type="bibr" target="#b13">[14]</ref> to compute explanations from a variety of other algorithms in order to compare them with STC. LIME learns an interpretable model, such as a linear regression model, in the proximity of each prediction and computes importance values based on the coefficients in the interpretable model. We refer the reader to the survey in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b10">11]</ref> for more explanation methods.</p><p>Contribution. By regarding an observation as a superposition of features, we introduce the concept of wave-particle duality in machine learning and propose a generalized framework that unifies the classical and the quantum probability. Embracing the new heuristic can be crucially beneficial for machine learning applications to better describe reality, similarly to what Einstein stated about the dual nature of light back in 1938 <ref type="bibr" target="#b4">[5]</ref>:</p><p>It seems as though we must use sometimes the one theory and sometimes the other, while at times we may use either. We are faced with a new kind of difficulty. We have two contradictory pictures of reality; separately neither of them fully explains the phenomena of light, but together they do.</p><p>On the practical side, we develop a new supervised classification algorithm that implements the wave-particle duality for categorical data, together with an entropy-based mechanism for noise filtering. We show that our methodology achieves state-of-the-art performances compared to both standard classifiers and deep learning, at the additional benefit of requiring minimal data pre-processing and hyper-parameter tuning. Moreover, the proposed algorithm supports multiclass and multilabel classification, online learning, prior knowledge, automatic dataset balancing, and provides a native explanation of its predictions both for single instances and for each target class label globally.</p><p>The paper is organized as follows: Section 2 presents some preliminary notions required to develop our methodology described in Section 3. Section 4 presents the STC algorithm whose standard configuration is empirically evaluated in Section 5. Finally, Section 6 gives our concluding remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>We use the multi-index notation i = (i 0 , i 1 , . . . , i n ) where the index i is an ordered tuple of indices (i 0 , i 1 , . . . , i n ). In this notation, T ij does not represent a matrix but rather a tensor of arbitrary dimensions T i0...inj0...jm . We use the index i to identify the dimensions associated with the target variable(s), and the index j for the features. We represent an observation with the joint probability distribution X ij ? P X (i, j) of the features and the target variables. The probability distribution can be arbitrarily chosen if prior knowledge is available or computed from the data by counting the number of co-occurrences. Then, given a set of observations {X (n) ij } n=1,...,N we compute the corpus represented by the tensor:</p><formula xml:id="formula_0">C ij = n X (n) ij<label>(1)</label></formula><p>Classical probability. The probability distribution of i can be recovered from the distribution of j by Bayes' theorem:</p><formula xml:id="formula_1">X i = P X (i) = j P X (i | j)P X (j)<label>(2)</label></formula><p>Quantum probability. In quantum physics a system is regarded as a superposition of states s and, using Dirac's notation, is represented by the wave function |? :</p><formula xml:id="formula_2">|? = s ? s |s<label>(3)</label></formula><p>In the Copenhagen interpretation, the modulus squared of the inner product is interpreted as the probability of the wave function ? collapsing to a new wave function ? upon measure of an observable. This is known as the Born rule, and is one of the fundamental postulates of quantum mechanics.</p><formula xml:id="formula_3">| ?|? | 2 = s ? * s ? s 2 = P (? ? ?)<label>(4)</label></formula><p>From equations <ref type="formula" target="#formula_2">(3)</ref> and <ref type="formula" target="#formula_3">(4)</ref>, we notice that 1 the coefficients |? s | 2 represent the probability of observation |? collapsing to state |s , that we denote with X s .</p><formula xml:id="formula_4">P (? ? s) = | s|? | 2 = s ? ? s ? s|s ? 2 = |? s | 2 ? X s<label>(5)</label></formula><p>Therefore the coefficients ? s are given by the complex numbers:</p><formula xml:id="formula_5">? s = e i?s X s<label>(6)</label></formula><p>where ? s represents the phase of the coefficient ? s .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>We now develop our methodology under the classical and the quantum probability settings, that are generalized and unified by the STC algorithm in Section 4.</p><p>Classical probability. In the classical probability setting, we estimate the conditional probability P X (i | j) in equation <ref type="formula" target="#formula_1">(2)</ref> with the conditional probability C i|j computed from the corpus.</p><formula xml:id="formula_6">C i|j = C ij i ? C i ? j<label>(7)</label></formula><p>Given a new observation X j for which only the features j are observed, we are now able to compute the probability distribution of the targets i by Bayes' theorem:</p><formula xml:id="formula_7">X i = j C i|j X j<label>(8)</label></formula><p>Quantum probability. In the quantum probability setting, we regard an observation as a superposition of features:</p><formula xml:id="formula_8">|? = j ? j |j<label>(9)</label></formula><p>Consider a new observation X j for which only the features j are observed. The probability distribution of the targets i can be obtained by computing the probability of the wave function |? to collapse on the targets |i :</p><formula xml:id="formula_9">X i = | i|? | 2 = j ? j i|j 2 = j ? j i * j 2<label>(10)</label></formula><p>1 As long as we are using an orthonormal basis s|s ? = ? ss ? where ? j = e i?j X j as given in <ref type="bibr" target="#b5">(6)</ref> and i * j are the complex conjugates of the coefficients of the wave function |i that represents the targets.</p><formula xml:id="formula_10">|i = j i j |j<label>(11)</label></formula><p>We obtain the coefficients i j by identifying the transition probability from |i to |j with the conditional probability of j given i computed from the corpus C ij .</p><formula xml:id="formula_11">C j|i = C ij j ? C ij ? = P (i ? j) = | j|i | 2 = |i j | 2<label>(12)</label></formula><p>From equation <ref type="formula" target="#formula_0">(12)</ref> we get the representation:</p><formula xml:id="formula_12">i j = e i?ij C j|i<label>(13)</label></formula><p>where ? ij represents the phase of the coefficient i j . We are now able to compute the probability distribution of the targets i by rewriting equation <ref type="formula" target="#formula_0">(10)</ref>.</p><formula xml:id="formula_13">X i = j e i(?j??ij ) X j C j|i 2<label>(14)</label></formula><p>The phase factor of the observation ? j is assumed to be observed together with the probability distribution X j . Instead, the phase factors ? ij are regarded as model parameters that can be learnt in the training process. The differences in the phase factors ? j ? ? ij control the interference between the interacting quantum states.</p><p>Entropy. Consider for simplicity the classical probability in equation <ref type="formula" target="#formula_7">(8)</ref> where we split the summation in the set S containing good predictors and the complementary set S c containing noise.</p><formula xml:id="formula_14">X i = j?S C i|j X j + j?S c C i|j X j<label>(15)</label></formula><p>With a sufficiently large and balanced sample, the noise in S c would be equally distributed among the targets i, so that the summation over S c would not alter the ranking of the probabilities X i . In practice, the finite sample size and the uncertainty affecting the entries of C ij may lead to a poor estimation of X i when the noisy features in S c are much more than the good predictors in S. Entropy can be used to distinguish between signal/noise, and weight them accordingly. For each feature j we compute the corresponding entropy with respect to the targets i.</p><formula xml:id="formula_15">H j = ? i P i|j ln(P i|j )<label>(16)</label></formula><p>The entropy H j is maximized for those features j uniformly distributed across the targets i. However, these coincide with the less informative features only if the dataset is balanced. For this reason we introduce the joint probability P ij of the artificially balanced dataset that coincides with the conditional probability C j|i of the original dataset, up to a normalization constant. Then, P i|j is the probability of i given j after balancing the dataset.</p><formula xml:id="formula_16">P ij = C ij j ? C ij ? , P i|j = P ij i ? P i ? j<label>(17)</label></formula><p>In this setting, the configuration of zero entropy corresponds to perfect signal, while the configuration of maximum entropy corresponds to random noise. We introduce the following weights, giving weight 1 to perfect signal and weight 0 to random noise:</p><formula xml:id="formula_17">H j ? 1 ? H j H max = 1 + i P i|j ln(P i|j ) ln( i 1)<label>(18)</label></formula><p>A robust estimator replaces the weights C i|j withH j C i|j . Introducing the robust weights in equation <ref type="bibr" target="#b7">(8)</ref> and splitting again the summation in the sets S and S c , we get:</p><formula xml:id="formula_18">X i = j?SH j C i|j X j + j?S cH j C i|j X j ? j?SH j C i|j X j<label>(19)</label></formula><p>asH j is small for j ? S c . In other words, the estimator is able to distinguish signal from noise and perform feature selection in a natural way. The same holds for the quantum probability in equation <ref type="bibr" target="#b13">(14)</ref> by replacing C j|i withH 2 j C j|i .</p><p>Sparsity. In real applications, both C and X in <ref type="bibr" target="#b7">(8)</ref> or <ref type="formula" target="#formula_0">(14)</ref> contain a large number of zero values. This is an issue both for the dimension of the tensors and because the product between C and X in <ref type="bibr" target="#b7">(8)</ref> or <ref type="formula" target="#formula_0">(14)</ref> is zero when all the non-zero entries of X j are associated with features j that are never seen in the training set. In this case we obtain the degenerate probability estimate X i = 0. To solve this issue we define a fallback mechanism by applying a tensor contraction, i.e. we drop one dimension from the observation X j0...j k ...jm ? j k X j0...j k ...jm and compute the corresponding marginal distribution from the corpus</p><formula xml:id="formula_19">C i0...inj0...j k ...jm ? j k C i0...inj0...j k ...jm .</formula><p>We iterate until X i = 0, eventually dropping all dimensions from X j ? X = 1 and from the corpus C ij ? C i . In other words, in this particular case we estimate X i with the distribution through which the targets i appeared in the corpus. We call policy the order in which to drop the dimensions. In STC, the policy can be arbitrarily specified if prior knowledge is available, or learnt via reinforcement learning to identify the optimal order in which to drop the dimensions j = (j 0 , . . . , j m ) when the degenerate distribution X i = 0 is estimated. In this case, we want to identify the optimal sets {j x } x?S k , ..., {j x } x?S1 , {} to apply subsequently until estimating X i = 0. We tackle the problem by implementing a myopic agent. In particular, we start from the state containing only the empty set [{}] and we estimate X i = 0 by construction for all the observations in the validation set. Then, given a loss function L, 2 , we compute the reward ?L(X,X) and the value of the state as the average reward received in all the iterations. Subsequently, we add one index to the empty set and explore all the states</p><formula xml:id="formula_20">[{}, {j 0 }], ..., [{}, {j m }].</formula><p>In other words, we estimate X i by using only one of the indexes j 0 , j 1 , . . . j m . We use X i obtained in the previous step for those observations resulting in X i = 0 and we compute the rewards for each state and the values of the states. Finally, we move to the state of maximum value [{}, {j * }]. This procedure is repeated by adding one index to the current set of indices and exploring all the corresponding states until reaching the set {j 0 , ..., j m }, containing all the indices, or when reaching a state where all the values of the next states are less than the value of the current state. The myopic agent learns the policy {}, {j x } x?S1 ..., {j x } x?S k that is sequentially improving the loss function L. We can now invert the order of the sets to understand which dimensions to drop first in case X i = 0.</p><p>Tensorization. By representing the input data as a tensor, STC is natively suitable for working with multi-valued attributes and multi-labeled data (see e.g. <ref type="bibr" target="#b21">[22]</ref>). In such cases, it may be convenient to represent the different attributes as separate dimensions of the tensor, in order to avoid mixing different aspects of the data items into a unique dimension of the feature space. When this is not required, for example when dealing with a single multi-valued attribute, such as text, or when dealing with multiple single-valued attributes, such as tabular data, STC can easily scale down to a matrix-based data representation, which uses a single dimension for all the features j = (j 0 ). In this case the learning of the policy is not needed, as it reduces to {}, {j 0 } by construction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Algorithm</head><p>In the training phase, we compute the corpus C ij given the set of observations in <ref type="bibr" target="#b0">(1)</ref>. Then, we set the phase factors ? ij in equation <ref type="bibr" target="#b19">(20)</ref>. These are regarded as model parameters that can be learnt from the data using gradient descent optimization algorithms <ref type="bibr" target="#b15">[16]</ref>, or arbitrary specified if prior knowledge is available. Also the policy can be arbitrary chosen or learnt from the data as described in Section 3. In the prediction phase, given a new observation X j for which only the features j are observed together with their phase factors ? j , we compute the normalized probability of the targets</p><formula xml:id="formula_21">X i = X i / i X i with: X i = j e i(?j ??ij )H h jC p ij X p j 1 p ,C ij = C ij ( i ? C i ? j ) 1?b ( j ? C ij ? ) b<label>(20)</label></formula><p>whereH j is given in <ref type="bibr" target="#b17">(18)</ref> and where the entropy h ? 0, the balance b ? 0, and the power p &gt; 0 are the model hyper-parameters. If the estimated probability is degenerated (i.e., X i = 0), we apply repeatedly the policy chosen in the training phase. <ref type="bibr" target="#b1">2</ref> Such as for instance the p-norm: 1 2 i |Xi ?Xi| p 1 p</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Properties</head><p>The classical probability setting in <ref type="bibr" target="#b7">(8)</ref> corresponds to the choice of the hyper-parameters h = 0, b = 0, p = 1, when setting ? ij = ? j = 0 in <ref type="bibr" target="#b19">(20)</ref>. The quantum probability setting in <ref type="bibr" target="#b13">(14)</ref> can be recovered with the particular choice h = 0, b = 1, and p = 1 2 . Choices different from the previous ones can be interpreted as a generalization of the probability measures where the power p controls the probability amplitude. Smaller values of p give similar weight to all the features regardless of their distribution. The entropic weights in <ref type="bibr" target="#b17">(18)</ref> are set with h = 1, dropped with h = 0, and their intensity can be tuned more in general with h ? 0. The summation in <ref type="bibr" target="#b19">(20)</ref> is dominated by fewer features for higher values of the entropy h, that lead to predictions based on less but more relevant features, thus more robust to noise. Missing values can be either ignored or treated like any other categorical value with their own semantics. The second approach allows to uncover patterns among the missing data. STC deals with imbalanced data by artificially balancing the sample when setting b = 1 in equation <ref type="bibr" target="#b19">(20)</ref>. Here b = 0 uses the conditional probability of i given j that is affected by imbalanced data, while b = 1 uses the conditional probability of j given i that artificially balances the sample. For 0 &lt; b &lt; 1 the sample is semi-balanced, increasing the weight of the less frequent classes but not enough to have a balanced sample. For b &gt; 1 the sample is super-balanced, where the weight of the less frequent classes is greater than the weight of the most frequent classes. The STC algorithm can be turned into a native multilabel classifier by selecting the indices with highest probability i * 0 ...i * n = arg max X i0...in , thus predicting several targets at the same time. The algorithm is ready to use in an online learning context. As new data X ij become available, they can be added to the corpus with minimal computational effort, that is C ij + X ij . STC supports a purely data-driven approach by estimating the optimal policy via reinforcement learning, learning the parameters ? ij with optimization algorithms, and tuning the hyper-parameters via cross-validation. However, it is also possible to include prior knowledge in the model by arbitrarily specifying the policy, arbitrarily setting the phase factors ? ij , and arbitrarily choosing the hyper-parameters h, b, p to recover the classical or quantum probability. A mixed approach is also possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Explainability</head><p>Local explanation. For what concerns the explanation of predictions on single instances (local explanation), the contribution of each feature j to the total probability X i is given by the addend e i(?j ??ij )H h jC p ij X p j in <ref type="bibr" target="#b19">(20)</ref>. Its modulus can be used to measure the importance of the features and assess the degree in which they contribute to the classification. In other words, the most influential feature for the classification i * is given by:</p><formula xml:id="formula_22">j * = arg max jH h jC p i * j X p j<label>(21)</label></formula><p>H h jC p i * j X p j is used in general to rank the features j by importance. Moreover, the phase factor e i(?j ?? i * j ) gives the direction of the contributions in the unit circle and it is able to uncover constructive or destructive interference between the features and the classification i * .</p><p>Global explanation. The explanation at the class level (global explanation) is obtained by investigating the tensorH h jC p ij to detect the global most influential feature for each target i according to:</p><formula xml:id="formula_23">j * i = arg max jH h jC p ij<label>(22)</label></formula><p>H h jC p ij is used in general to rank the features j by their global importance with respect to target i. Moreover, when using h = 0, the features with best discriminatory power among all the targets are given by the largest entries ofH h j . Finally, the phase factor e i?ij is used to uncover the interference pattern underlying the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Empirical evaluation</head><p>The goal of our empirical evaluation of STC is to assess the classification performance and the interpretability of the explanation, without relying on data pre-processing, (hyper-)parameter tuning, or learning of the policy. The capability of STC to deal with an arbitrary number of dimensions is not the main focus of this paper, as it may require additional efforts to learn the optimal shape of the tensor and the corresponding policy. Instead, we want to stress-test the quality and robustness of the standard configuration of STC. To this end, we perform experiments that use a simple matrix-based data representation as discussed in Section 3 and we execute STC with entropic weights by relying both on classical probability (STC Classic (STC-C)) and on quantum probability (STC Quantum (STC-Q)) as described in Section 4.1. <ref type="bibr" target="#b2">3</ref> Finally, we limit STC to real-valued wave functions by ignoring all the phase factors in <ref type="bibr" target="#b19">(20)</ref> </p><formula xml:id="formula_24">? ij = ? j = 0.</formula><p>In all the experiments, our approach is to compare STC against six consolidated algorithms, namely Decision Tree (DT), Random Forest (RF), K-Nearest Neighbors (KNN), Support Vector Machine (SVM), Multinomial Naive Bayes (MNB), and Logistic Regression (LR). The choice is motivated by the purpose of covering a spectrum of very diverse solutions for supervised classification and to exploit a homogeneous and stable third-part implementation using scikit-learn <ref type="bibr" target="#b11">[12]</ref>. In the second experiment, we also include evaluation performances of recent deep learning approaches as reported in literature.</p><p>All the experiments have been executed on a HP Z420 Workstation with Intel(R) Xeon(R) CPU 3.60GHz and 16GB System Memory equipped with Ubuntu 14.04. We also run all the experiment code on MacOS Big Sur and Windows 10. All the code is written in Python 3.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Structured data</head><p>In our first experiment, we rely on the Zoo dataset <ref type="bibr" target="#b3">[4]</ref>. Zoo provides a structured representation of 101 animals featured by 16 attributes. All the attributes, except for the number of legs, describe animal characteristics as boolean values denoting the fact that the animal has the characteristic at hand. For the number of legs we have 6 possible values. In the target there are 7 animal classes (e.g., Mammal, Bird, etc.). The dataset is quite imbalanced with the larger class (Mammal) containing 41 animals and the smaller (Amphibian) only 4 animals. The choice of Zoo is motivated by the fact that it is an interesting case where the number of instances is relatively small with respect to the number of classes and where classes are imbalanced. In the experiment, we execute STC and its competing algorithms in 100 independent runs. For each run the composition of the training and the test sets is randomly selected, with the test set size equal to the 30% of the whole dataset. For the competing algorithms, in each run we set the hyper-parameters through model selection performed by the grid search strategy on the training set targeting the weighted average f1-score. The model selection procedure includes the option of scaling data with respect to the minimum and maximum values. Instead, for STC we keep the standard configuration in order to stress-test our methodology.</p><p>The average precision, recall, and f1-score weighted by the class support over the 100 runs are reported in <ref type="table" target="#tab_0">Table 1</ref>, together with macro-averages. In this experiment, we are also interested in studying the stability of the algorithms with respect to the variation of the training set. To this end, we execute a pairwise comparison of the algorithms by counting, for each pair of algorithms, in how many experiment runs one of the two over-performed the other in terms of weighted f1-score. These results are also reported in <ref type="table" target="#tab_0">Table 1</ref>, where each entry [i, j] of the table contains the fraction of runs in which the i-th algorithm achieved a better f1-score than the j-th competitor (excluding ties). Results. We note how the quantum approach of STC-Q outperforms all the competing algorithms in <ref type="table" target="#tab_0">Table 1</ref> but also demonstrates to be the most stable with respect to the variation of the training set.</p><p>STC-Q is always the best choice against all the competitors and outperforms all the other algorithms in more than 50% of the runs. The comparison between STC-Q and STC-C highlights the benefits of switching from the classical to the quantum probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Text classification</head><p>In our second experiment, we want to evaluate the capability of STC to deal with unstructured and noisy data. To this end, we execute a task of text classification on the well-known 20 Newsgroups dataset. <ref type="bibr" target="#b3">4</ref> 20 Newsgroups provides about 19 000 news divided in a training and a test set and organized in 20 thematic classes that are almost balanced in terms of number of instances. In order to keep the text as noisy as possible and stress-test the algorithms, we perform only a simple word tokenization by the function nltk.word_tokenize. <ref type="bibr" target="#b4">5</ref> No other text transformation and cleaning procedure is performed in order to keep all the morphological and syntactical variety of text, including plurals, capital letters, stop-words and punctuation. The final dataset is composed by 185 733 features (i.e., document terms) and 11 314 documents in the training set and 7 532 documents in the test set. One of the main ideas of this experiment is to test what happens when a user takes a supervised classification algorithm off the shelf and applies it to a text dataset without spending any effort in tuning, feature engineering, or data pre-processing. To simulate this situation, we do not perform model selection and we use the default hyper-parameter values for STC competitors as they are defined in scikit-learn. Also for text vectorization and term weighting we rely on Tf-Idf as one of the most popular and standard approaches for representing text data in text classification.</p><p>For what concerns STC, we exploit the standard configuration of STC-Q, which works directly on terms without needing any vectorization or weighting scheme. In order to include an example of generalized probability without performing any kind of model selection, we also explore the results obtained by modifying the quantum probability amplitude of STC-Q from the square to the cubic root, thus setting the hyper-parameter p = 1/3 instead of p = 1/2. We extend the comparison to the performances reported in literature of recent deep learning approaches to text classification on 20 Newsgroups, including TextEnt <ref type="bibr" target="#b20">[21]</ref>, the Neural Attentive Bag-of-Entities Model (NABoE) <ref type="bibr" target="#b19">[20]</ref>, Cooperative Neural Networks (CoNN) <ref type="bibr" target="#b17">[18]</ref>, and the Diversified Ensemble Neural Network (DEns) <ref type="bibr" target="#b16">[17]</ref>. The results are shown in <ref type="table" target="#tab_1">Table 2</ref>, where we report the macro average of the evaluation metrics in order to be compliant with the results reported in literature. The computational time required by STC is comparable with Random Forest for training (about 50s) and with Support Vector Machine for prediction (about 80s).</p><p>Results. The results reported in <ref type="table" target="#tab_1">Table 2</ref> show that the standard configuration of STC-Q significantly outperforms the default configurations of the competing algorithms. Remarkably, STC-Q achieves results comparable, if not superior, to the deep learning approaches and improves the stateof-the-art. The result is especially surprising when considering that the deep learning approaches are specifically tuned for the given task, involve non-trivial data pre-processing, exploit semantics, embeddings and external knowledge bases, while our simple methodology avoids any text transformation and tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Explanation</head><p>In our third experiment, we want STC to explain the state-of-the-art results obtained on 20 Newsgroups (p = 1/3). In particular, we want to evaluate how much the terms in the native explanation of STC are interpretable, that is how much they appear to be meaningful for a human evaluator. We focus the experiment on global explanation by involving a small group of 45 anonymous evaluators in a survey. Each evaluator receives the same set of document terms extracted from 20 Newsgroups and is asked to decide whether each term is Relevant, Neutral or Not Relevant for a given class in 20 Newsgroups. The terms submitted to evaluation have been chosen according to the following procedure. First, for what concerns the competing algorithms, we exploit LIME <ref type="bibr" target="#b13">[14]</ref> in that it provides a uniform and model agnostic method for producing local relevance scores of terms for each instance in the test set with respect to the predicted class. Then, we create the global scores of terms for each algorithm by taking the sum of their local relevance scores for each class. To allow for a natural comparison, for STC we compute the relevance scores by summing the local weights described in Section 4.2 of all the instances in the test set. For each class in the survey, the final list has been composed by merging the top 6 relevant terms in each algorithm explanation (hiding the provenance) into a unique list. From this list, we removed English stopwords, punctuation, unclear abbreviations and other terms that are clearly unrelated to the target class in order to submit a list of about 25 terms per class to the human evaluators. The result is evaluated as follows. Given a class c and a term t, we denote r c (t) the fraction of the evaluators who evaluated t being relevant for c and n c (t) the fraction who evaluated t as not relevant. The score of t for c is then ? c (t) = r c (t) ? n c (t) with ? c (t) = ?1 for terms removed from the survey list. The final evaluation score for the algorithm i is the mean of the human evaluations over all the terms produced by i for all the classes in the survey.</p><p>Results. We find that STC achieves a score of 0.598, corresponding to a better interpretability than RF (0.376), SVM (0.270), LR (0.256), DT (0.146), KNN (?0.200). In this sense, STC is similar to one of the most well-known and interpretable models: MNB (0.696). We also notice that the results achieved by all the competing algorithms are due to the exploitation of LIME, which is remarkably inefficient in terms of computation time (about 30s for each document, resulting in more than 60 hours for each algorithm). Next, we extend the comparison with MNB by exploring the probability P (t | c) of a term t given the class c as internally computed by the model. We compare this explanation provided by MNB natively (MNB-G) with the STC global explanation (STC-G) computed as described in Section 4.2. A short example of the 6 top terms for the class rec.sport.baseball is shown in <ref type="table">Table 3</ref>, where we also report the top terms obtained by aggregating the native local STC weights and those obtained with MNB through LIME. It is interesting to note that the two global explanations derived from local explanations (i.e., STC and MNB) are similar, while STC-G provides very specific and class-related terms by selecting baseball teams and players names, such as in an entity recognition system. We note this behavior also for the other classes. These specific terms are less interpretable, in that the survey shows that many of them are not known to evaluators, but potentially more useful to explore a class content. The native explanation of MNB-G is instead dominated by noise, that STC has been able to filter out by exploiting the entropic weights described in Section 3. <ref type="table">Table 3</ref>: Top-6 terms in the explanations of STC and MNB for the class rec.sport.baseball obtained by aggregating the local weights (STC &amp; MNB) or computed natively (STC-G &amp; MNB-G).</p><p>STC "game", "team", "baseball", "games", "he", "pitcher" MNB "pitcher", "baseball, "game, "Sox", "games", "Braves" STC-G "Phillies", "pitching", "Braves", "Alomar", "Mets", "Players" MNB-G "&gt;", ":", "," , "the", ".", "-"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Concluding remarks</head><p>By regarding an observation as a superposition of features, we introduce the concept of wave-particle duality in machine learning and present Sparse Tensor Classifier (STC). The empirical evaluation against consolidated algorithms and deep learning alternatives demonstrates the native capability of STC to achieve state-of-the-art performances without relying on data pre-processing and hyperparameter tuning, at the additional benefit of providing a meaningful explanation of the classification results. Our future work will be devoted to further investigating STC from complementary perspectives. In particular, we aim at developing a learning procedure for the phase factors, investigating their implications in terms of explainability and classification performance, assessing the benefits of a tensor representation of the data items, and exploring the extension of STC to work with ordinal and continuous variables. We hope our novel methodology illustrates how the analogy with the theory of superposition of states in quantum physics can be successfully exploited in machine learning and open the door to future research in this direction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Average precision, recall, f1-score weighted by the class support, f1-macro, and pairwise competition over 100 runs of the Zoo experiment.</figDesc><table><row><cell></cell><cell>Prec.</cell><cell>Rec.</cell><cell>F1</cell><cell>F1-macro</cell><cell>DT</cell><cell>RF</cell><cell>KNN</cell><cell>SVM</cell><cell>MNB</cell><cell>LR</cell><cell>STC-C</cell><cell>STC-Q</cell><cell>Mean</cell></row><row><cell>DT</cell><cell>0.944</cell><cell>0.935</cell><cell>0.931</cell><cell>0.836</cell><cell>0.00</cell><cell>0.61</cell><cell>0.73</cell><cell>0.50</cell><cell>0.80</cell><cell>0.54</cell><cell>0.97</cell><cell>0.38</cell><cell>0.57</cell></row><row><cell>RF</cell><cell>0.936</cell><cell>0.931</cell><cell>0.923</cell><cell>0.827</cell><cell>0.39</cell><cell>0.00</cell><cell>0.70</cell><cell>0.32</cell><cell>0.72</cell><cell>0.38</cell><cell>0.98</cell><cell>0.34</cell><cell>0.48</cell></row><row><cell>KNN</cell><cell>0.916</cell><cell>0.915</cell><cell>0.905</cell><cell>0.792</cell><cell>0.27</cell><cell>0.30</cell><cell>0.00</cell><cell>0.29</cell><cell>0.63</cell><cell>0.28</cell><cell>0.96</cell><cell>0.25</cell><cell>0.37</cell></row><row><cell>SVM</cell><cell>0.946</cell><cell>0.941</cell><cell>0.934</cell><cell>0.847</cell><cell>0.50</cell><cell>0.68</cell><cell>0.71</cell><cell>0.00</cell><cell>0.79</cell><cell>0.52</cell><cell>0.96</cell><cell>0.39</cell><cell>0.57</cell></row><row><cell>MNB</cell><cell>0.916</cell><cell>0.903</cell><cell>0.897</cell><cell>0.764</cell><cell>0.20</cell><cell>0.28</cell><cell>0.37</cell><cell>0.21</cell><cell>0.00</cell><cell>0.22</cell><cell>0.86</cell><cell>0.13</cell><cell>0.28</cell></row><row><cell>LR</cell><cell>0.946</cell><cell>0.938</cell><cell>0.932</cell><cell>0.843</cell><cell>0.46</cell><cell>0.62</cell><cell>0.72</cell><cell>0.48</cell><cell>0.78</cell><cell>0.00</cell><cell>0.96</cell><cell>0.39</cell><cell>0.55</cell></row><row><cell>STC-C</cell><cell>0.790</cell><cell>0.850</cell><cell>0.804</cell><cell>0.659</cell><cell>0.03</cell><cell>0.02</cell><cell>0.04</cell><cell>0.04</cell><cell>0.14</cell><cell>0.04</cell><cell>0.00</cell><cell>0.00</cell><cell>0.04</cell></row><row><cell>STC-Q</cell><cell>0.953</cell><cell>0.953</cell><cell>0.945</cell><cell>0.871</cell><cell>0.62</cell><cell>0.66</cell><cell>0.75</cell><cell>0.61</cell><cell>0.87</cell><cell>0.61</cell><cell>1.00</cell><cell>0.00</cell><cell>0.64</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell cols="12">Comparison between STC and other text classification algorithms on average macro preci-</cell></row><row><cell cols="9">sion (Prec.), recall (Rec.), f1-score (F1), and accuracy (Acc.)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>DT</cell><cell>RF</cell><cell>KNN</cell><cell>SVM</cell><cell>MNB</cell><cell>LR</cell><cell>STC-Q</cell><cell>NABoE</cell><cell>TextEnt</cell><cell>CoNN</cell><cell>DEns</cell><cell>STC-Q  *</cell></row><row><cell>Prec.</cell><cell>0.546</cell><cell>0.748</cell><cell>0.599</cell><cell>0.799</cell><cell>0.822</cell><cell>0.808</cell><cell>0.863</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>0.871</cell></row><row><cell>Rec.</cell><cell>0.543</cell><cell>0.727</cell><cell>0.528</cell><cell>0.779</cell><cell>0.725</cell><cell>0.795</cell><cell>0.856</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>0.866</cell></row><row><cell>F1</cell><cell>0.543</cell><cell>0.725</cell><cell>0.539</cell><cell>0.784</cell><cell>0.724</cell><cell>0.797</cell><cell>0.856</cell><cell>0.862</cell><cell>0.839</cell><cell>N/A</cell><cell>N/A</cell><cell>0.866</cell></row><row><cell>Acc.</cell><cell>0.549</cell><cell>0.739</cell><cell>0.529</cell><cell>0.788</cell><cell>0.744</cell><cell>0.805</cell><cell>0.864</cell><cell>0.868</cell><cell>0.845</cell><cell>0.837</cell><cell>0.871</cell><cell>0.873</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* Executed by setting p = 1/3</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Preprint. Under review.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">STC-C: h = 1, b = 0, p = 1. STC-Q: h = 1, b = 1, p = 1/2.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We use the by-date version of the dataset obtained from http://qwone.com/~jason/20Newsgroups/ 5 See https://www.nltk.org/book/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Explaining deep neural networks with a polynomial time algorithm for shapley value approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Ancona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cengiz</forename><surname>?ztireli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><forename type="middle">H</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
		<editor>Kamalika Chaudhuri and Ruslan Salakhutdinov</editor>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019-06-15" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="272" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Arya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pin-Yu</forename><surname>Bellamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Dhurandhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vera</forename><surname>Houde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronny</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandra</forename><surname>Luss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mojsilovi?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03012</idno>
		<title level="m">One Explanation does not Fit All: a Toolkit and Taxonomy of AI Explainability Techniques</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to explain: An information-theoretic perspective on model interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>Jennifer G. Dy and Andreas Krause</editor>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsm?ssan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018-07-10" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="882" to="891" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheeru</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casey</forename><surname>Graff</surname></persName>
		</author>
		<ptr target="https://archive.ics.uci.edu/ml/datasets/Zoo" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The evolution of Physics. the growth of ideas from early concepts to relativity and quanta</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Einstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leopold</forename><surname>Infeld</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1938" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Key challenges for delivering clinical impact with artificial intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Karthikesalingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominic</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC medicine</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Text classification and classifiers: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vandana</forename><surname>Korde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Namrata</forename><surname>Mahender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Artificial Intelligence &amp; Applications</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">85</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Text classification algorithms: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamran</forename><surname>Kowsari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiana</forename><forename type="middle">Jafari</forename><surname>Meimandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mojtaba</forename><surname>Heidarysafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjana</forename><surname>Mendu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">150</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The Mythos of Model Interpretability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zachary C Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Queue</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="31" to="57" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Machine learning in chemoinformatics and drug discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><forename type="middle">E</forename><surname>Rensi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Torng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russ B</forename><surname>Altman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Drug discovery today</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1538" to="1546" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Explainable kmeans and k-medians clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Moshkovitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjoy</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrus</forename><surname>Rashtchian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nave</forename><surname>Frost</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<editor>Hal Daum? III and Aarti Singh</editor>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Text categorization for multi-label documents and many categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karine</forename><surname>I Sandu Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georges</forename><surname>Zeitouni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didier</forename><surname>Gardarin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeth</forename><surname>Nakache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>M?tais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twentieth IEEE International Symposium on Computer-Based Medical Systems (CBMS&apos;07)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="421" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Explaining the Predictions of Any Classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Marco Tulio Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 22nd ACM SIGKDD Int. Conference on Knowledge Discovery and Data Mining</title>
		<meeting>of the 22nd ACM SIGKDD Int. Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1135" to="1144" />
		</imprint>
	</monogr>
	<note>Why Should I Trust you?</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Extended-connectivity fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathew</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="742" to="754" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">An overview of gradient descent optimization algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04747</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The diversified ensemble neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi Yan Shaofeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems, NIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cooperative neural networks (conn): Exploiting prior independence structure for improved classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsh</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Bart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivas</forename><surname>Aluru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems, NIPS</title>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="4126" to="4136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Machine learning algorithm validation with a limited sample size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrius</forename><surname>Vabalas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Gowen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Poliakoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander J</forename><surname>Casson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">224365</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural attentive bag-of-entities model for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)</title>
		<meeting>the 23rd Conference on Computational Natural Language Learning (CoNLL)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11" />
			<biblScope unit="page" from="563" to="573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Representation learning of entities and documents from knowledge base descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshiyasu</forename><surname>Takefuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics (COLING)</title>
		<meeting>the 27th International Conference on Computational Linguistics (COLING)<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-08" />
			<biblScope unit="page" from="190" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-valued attribute and multi-labeled data decision tree algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiguo</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Machine Learning and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="67" to="74" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
