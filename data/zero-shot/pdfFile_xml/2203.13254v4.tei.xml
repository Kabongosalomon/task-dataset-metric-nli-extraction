<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EPro-PnP: Generalized End-to-End Probabilistic Perspective-n-Points for Monocular Object Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hansheng</forename><surname>Chen</surname></persName>
							<email>hanshengchen97@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Automotive Studies</orgName>
								<orgName type="institution">Tongji University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichao</forename><surname>Wang</surname></persName>
							<email>pichao.wang@alibaba-inc.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Wang</surname></persName>
							<email>fan.w@alibaba-inc.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tian</surname></persName>
							<email>tianwei@tongji.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Automotive Studies</orgName>
								<orgName type="institution">Tongji University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Xiong</surname></persName>
							<email>xionglu@tongji.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Automotive Studies</orgName>
								<orgName type="institution">Tongji University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">EPro-PnP: Generalized End-to-End Probabilistic Perspective-n-Points for Monocular Object Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Locating 3D objects from a single RGB image via Perspective-n-Points (PnP) is a long-standing problem in computer vision. Driven by end-to-end deep learning, recent studies suggest interpreting PnP as a differentiable layer, so that 2D-3D point correspondences can be partly learned by backpropagating the gradient w.r.t. object pose. Yet, learning the entire set of unrestricted 2D-3D points from scratch fails to converge with existing approaches, since the deterministic pose is inherently non-differentiable. In this paper, we propose the EPro-PnP, a probabilistic PnP layer for general end-to-end pose estimation, which outputs a distribution of pose on the SE(3) manifold, essentially bringing categorical Softmax to the continuous domain. The 2D-3D coordinates and corresponding weights are treated as intermediate variables learned by minimizing the KL divergence between the predicted and target pose distribution. The underlying principle unifies the existing approaches and resembles the attention mechanism. EPro-PnP significantly outperforms competitive baselines, closing the gap between PnP-based method and the taskspecific leaders on the LineMOD 6DoF pose estimation and nuScenes 3D object detection benchmarks. 3</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Estimating the pose (i.e., position and orientation) of 3D objects from a single RGB image is an important task in computer vision. This field is often subdivided into specific tasks, e.g., 6DoF pose estimation for robot manipulation and 3D object detection for autonomous driving. Although they share the same fundamentals of pose estimation, the different nature of the data leads to biased choice of methods. Top performers <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b49">50]</ref> on the 3D object <ref type="bibr">Figure 1</ref>. EPro-PnP is a general solution to end-to-end 2D-3D correspondence learning. In this paper, we present two distinct networks trained with EPro-PnP: (a) an off-the-shelf dense correspondence network whose potential is unleashed by end-to-end training, (b) a novel deformable correspondence network that explores new possibilities of fully learnable 2D-3D points. detection benchmarks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17]</ref> fall into the category of direct 4DoF pose prediction, leveraging the advances in end-toend deep learning. On the other hand, the 6DoF pose estimation benchmark <ref type="bibr" target="#b22">[23]</ref> is largely dominated by geometrybased methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b51">52]</ref>, which exploit the provided 3D object models and achieve a stable generalization performance. However, it is quite challenging to bring together the best of both worlds, i.e., training a geometric model to learn the object pose in an end-to-end manner.</p><p>There has been recent proposals for an end-to-end framework based on the Perspective-n-Points (PnP) approach <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12]</ref>. The PnP algorithm itself solves the pose from a set of 3D points in object space and their corresponding 2D projections in image space, leaving the problem of constructing these correspondences. Vanilla correspondence learning <ref type="bibr">[11, 28, 29, 35, 35-37, 40, 46, 52]</ref> leverages the geometric prior to build surrogate loss functions, forcing the network to learn a set of pre-defined correspondences. Endto-end correspondence learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12]</ref> interprets the PnP as a differentiable layer and employs pose-driven loss function, so that gradient of the pose error can be backpropagated to the 2D-3D correspondences.</p><p>However, existing work on differentiable PnP learns only a portion of the correspondences (either 2D coordinates <ref type="bibr" target="#b11">[12]</ref>, 3D coordinates <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6]</ref> or corresponding weights <ref type="bibr" target="#b8">[9]</ref>), assuming other components are given a priori. This raises an important question: why not learn the entire set of points and weights altogether in an end-to-end manner? The simple answer is: the solution of the PnP problem is inherently non-differentiable at some points, causing training difficulties and convergence issues. For example, a PnP problem can have ambiguous solutions <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b37">38]</ref>.</p><p>To overcome the above limitations, we propose a generalized end-to-end probabilistic PnP (EPro-PnP) approach that enables learning the weighted 2D-3D point correspondences entirely from scratch ( <ref type="figure">Figure 1</ref>). The main idea is straightforward: deterministic pose is non-differentiable, but the probability density of pose is apparently differentiable, just like categorical classification scores. Therefore, we interpret the output of PnP as a probabilistic distribution parameterized by the learnable 2D-3D correspondences. During training, the Kullback-Leibler (KL) divergence between the predicted and target pose distributions is minimized as the loss function, which can be efficiently implemented by the Adaptive Multiple Importance Sampling <ref type="bibr" target="#b13">[14]</ref> algorithm.</p><p>As a general approach, EPro-PnP inherently unifies existing correspondence learning techniques (Section 3.1). Moreover, just like the attention mechanism <ref type="bibr" target="#b43">[44]</ref>, the corresponding weights can be trained to automatically focus on important point pairs, allowing the networks to be designed with inspiration from attention-related work <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b53">54]</ref>.</p><p>To summarize, our main contributions are as follows:</p><p>? We propose the EPro-PnP, a probabilistic PnP layer for general end-to-end pose estimation via learnable 2D-3D correspondences. ? We demonstrate that EPro-PnP can easily reach toptier performance for 6DoF pose estimation by simply inserting it into the CDPN <ref type="bibr" target="#b28">[29]</ref> framework. ? We demonstrate the flexibility of EPro-PnP by proposing deformable correspondence learning for accurate 3D object detection, where the entire 2D-3D correspondences are learned from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Geometry-Based Object Pose Estimation In general, geometry-based methods exploit the points, edges or other types of representation that are subject to the projection constraints under the perspective camera. Then, the pose can be solved by optimization. A large body of work utilizes point representation, which can be categorized into sparse keypoints and dense correspondences. BB8 <ref type="bibr" target="#b36">[37]</ref> and RTM3D <ref type="bibr" target="#b27">[28]</ref> locate the corners of the 3D bounding box as keypoints, while PVNet <ref type="bibr" target="#b35">[36]</ref> defines the keypoints by farthest point sampling and Deep MANTA <ref type="bibr" target="#b10">[11]</ref> by handcrafted templates. On the other hand, dense correspondence methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b51">52]</ref> predict pixel-wise 3D coordinates within a cropped 2D region. Most existing geometry-based methods follow a two-stage strategy, where the intermediate representations (i.e., 2D-3D correspondences) are learned with a surrogate loss function, which is sub-optimal compared to end-to-end learning.</p><p>End-to-End Correspondence Learning To mitigate the limitation of surrogate correspondence learning, end-to-end approaches have been proposed to backpropagate the gradient from pose to intermediate representation. By differentiating the PnP operation, Brachmann and Rother <ref type="bibr" target="#b5">[6]</ref> propose a dense correspondence network where 3D points are learnable, BPnP <ref type="bibr" target="#b11">[12]</ref> predicts 2D keypoint locations, and BlindPnP <ref type="bibr" target="#b8">[9]</ref> learns the corresponding weight matrix given a set of unordered 2D/3D points. Beyond point correspondence, RePOSE <ref type="bibr" target="#b23">[24]</ref> proposes a feature-metric correspondence network trained in a similar end-to-end fashion. The above methods are all coupled with surrogate regularization loss, otherwise convergence is not guaranteed due to the non-differentiable nature of deterministic pose. Under the probabilistic framework, these methods can be regarded as a Laplace approximation approach (Section 3.1) or a local regularization technique (Section 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Probabilistic Deep Learning</head><p>Probabilistic methods account for uncertainty in the model and the data, known respectively as epistemic and aleatoric uncertainty <ref type="bibr" target="#b24">[25]</ref>. The latter involves interpreting the prediction as learnable probabilistic distributions. Discrete categorical distribution via Softmax has been widely adopted as a smooth approximation of one-hot arg max for end-to-end classification. This inspired works such as DSAC <ref type="bibr" target="#b3">[4]</ref>, a smooth RANSAC with a finite hypothesis pool. Meanwhile, tractable parametric distributions (e.g., normal distribution) are often used in predicting continuous variables <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b50">51]</ref>, and mixture distributions can be employed to further capture ambiguity <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b30">31]</ref>, e.g., ambiguous 6DoF pose <ref type="bibr" target="#b6">[7]</ref>. In this paper, we propose yet a unique contribution: backpropagating a complicated continuous distribution derived from a nested optimization layer (the PnP layer), essentially making it a continuous counterpart of Softmax.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Generalized End-to-End Probabilistic PnP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>Given an object proposal, our goal is to predict a set</p><formula xml:id="formula_0">X = x 3D i , x 2D i , w 2D i i = 1 ? ? ? N of N corresponding points, with 3D object coordinates x 3D i ? R 3 , 2D image coordinates x 2D i ? R 2 , and 2D weights w 2D i ? R 2 + , from</formula><p>which a weighted PnP problem can be formulated to estimate the object pose relative to the camera. The essence of a PnP layer is searching for an optimal pose y (expanded as rotation matrix R and translation vector t) that minimizes the cumulative squared weighted reprojection error:</p><formula xml:id="formula_1">arg min y 1 2 N i=1 w 2D i ? ?(Rx 3D i + t) ? x 2D i fi(y)?R 2 2 ,<label>(1)</label></formula><p>where ?(?) is the projection function with camera intrinsics involved, ? stands for element-wise product, and f i (y) compactly denotes the weighted reprojection error. Eq. (1) formulates a non-linear least squares problem that may have non-unique solutions, i.e., pose ambiguity <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b37">38]</ref>. Previous work <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12]</ref> only backpropagates through a local solution y * , which is inherently unstable and non-differentiable. To construct a differentiable alternative for end-to-end learning, we model the PnP output as a distribution of pose, which guarantees differentiable probability density. Consider the cumulative error to be the negative logarithm of the likelihood function p(X|y) defined as:</p><formula xml:id="formula_2">p(X|y) = exp ? 1 2 N i=1 f i (y) 2 .<label>(2)</label></formula><p>With an additional prior pose distribution p(y), we can derive the posterior pose p(y|X) via the Bayes theorem. Using an uninformative prior, the posterior density is simplified to the normalized likelihood:</p><formula xml:id="formula_3">p(y|X) = exp ? 1 2 N i=1 f i (y) 2 exp ? 1 2 N i=1 f i (y) 2 dy .<label>(3)</label></formula><p>Eq. (3) can be interpreted as a continuous counterpart of categorical Softmax. KL Loss Function During training, given a target pose distribution with probability density t(y), the KL divergence D KL (t(y) p(y|X)) is minimized as training loss. Intuitively, pose ambiguity can be captured by the multiple modes of p(y|X), and convergence is ensured such that wrong modes are suppressed by the loss function. Dropping the constant, the KL divergence loss can be written as:</p><formula xml:id="formula_4">L KL = ? t(y) log p(X|y) dy + log p(X|y) dy. (4)</formula><p>We empirically found it effective to set a narrow (Diraclike) target distribution centered at the ground truth y gt , yielding the simplified loss (after substituting Eq. (2)):</p><formula xml:id="formula_5">L KL = 1 2 N i=1 f i (y gt ) 2</formula><p>Ltgt (reproj. at target pose)</p><formula xml:id="formula_6">+ log exp ? 1 2 N i=1 f i (y) 2 dy</formula><p>Lpred (reproj. at predicted pose)</p><p>.</p><p>(5) The only remaining problem is the integration in the second term, which is elaborated in Section 3.2.</p><p>Discrete Classifica?on Unnormalized Prob.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Con?nuous Pose</head><formula xml:id="formula_7">g.t. g.t.</formula><p>Con?nuous Pose g.t. Propoer Loss Impropoer Loss <ref type="figure">Figure 2</ref>. Learning a discrete classifier vs. Learning the continuous pose distribution. A discriminative loss function (left) shall encourage the unnormalized probability for the correct prediction as well as penalize for the incorrect. A one-sided loss (right) will degrade the distribution if the model is not well-regularized.</p><p>Comparison to Reprojection-Based Method The two terms in Eq. (5) are concerned with the reprojection errors at target and predicted pose respectively. The former is often used as a surrogate loss in previous work <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. However, the first term alone cannot handle learning all 2D-3D points without imposing strict regularization, as the minimization could simply drive all the points to a concentrated location without pose discrimination. The second term originates from the normalization factor in Eq. (3), and is crucial to a discriminative loss function, as shown in <ref type="figure">Figure 2</ref>.</p><p>Comparison to Implicit Differentiation Method Existing work on end-to-end PnP <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12]</ref> derives a single solution of a particular solver y * = PnP (X) via implicit function theorem <ref type="bibr" target="#b18">[19]</ref>. In the probabilistic framework, this is essentially the Laplace method that approximates the posterior by N (y * , ? y * ), where both y * and ? y * can be estimated by the PnP solver with analytical derivatives <ref type="bibr" target="#b12">[13]</ref>. A special case is that, with ? y * simplified to be isotropic, the approximated KL divergence can be simplified to the L2 loss y * ? y gt 2 used in <ref type="bibr" target="#b8">[9]</ref>. However, the Laplace approximation is inaccurate for non-normal posteriors with ambiguity, therefore does not guarantee global convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Monte Carlo Pose Loss</head><p>In this section, we introduce a GPU-friendly efficient Monte Carlo approach to the integration in the proposed loss function, based on the Adaptive Multiple Importance Sampling (AMIS) algorithm <ref type="bibr" target="#b13">[14]</ref>.</p><p>Considering q(y) to be the probability density function of a proposal distribution that approximates the shape of the integrand exp ? 1 2 N i=1 f i (y) 2 , and y j to be one of the K samples drawn from q(y), the estimation of the second term L pred in Eq. (5) is thus:</p><formula xml:id="formula_8">L pred ? log 1 K K j=1 exp ? 1 2 N i=1 f i (y j ) 2 q(y j ) vj (importance weight) ,<label>(6)</label></formula><p>where v j compactly denotes the importance weight at y j . Eq. (6) gives the vanilla importance sampling, where the choice of proposal q(y) strongly affects the numerical stability. The AMIS algorithm is a better alternative as it iteratively adapts the proposal to the integrand. In brief, AMIS utilizes the sampled importance weights from past iterations to estimate the new proposal. Then, all previous samples are re-weighted as being homogeneously sampled from a mixture of the overall sum of proposals. Initial proposal can be determined by the mode and covariance of the predicted pose distribution (see supplementary for details). A pseudo-code is given in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Choice of Proposal Distribution</head><p>The proposal distributions for position and orientation have to be chosen separately in a decoupled manner, since the orientation space is non-Euclidean. For position, we adopt the 3DoF multivariate t-distribution. For 1D yaw-only orientation, we use a mixture of von Mises and uniform distribution. For 3D orientation represented by unit quaternion, the angular central Gaussian distribution <ref type="bibr" target="#b42">[43]</ref> is adopted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Backpropagation</head><p>In general, the partial derivatives of the loss function defined in Eq. (5) is:</p><formula xml:id="formula_9">?L KL ?(?) = ? ?(?) 1 2 N i=1 f i (y gt ) 2 ? E y?p(y|X) ? ?(?) 1 2 N i=1 f i (y) 2 ,<label>(7)</label></formula><p>where the first term is the gradient of reprojection errors at target pose, and the second term is the expected gradient of reprojection errors over predicted pose distribution, which is approximated by backpropagating each weighted sample in the Monte Carlo pose loss.</p><p>Balancing Uncertainty and Discrimination Consider the negative gradient w.r.t. the corresponding weights w 2D i :</p><formula xml:id="formula_10">? ?L KL ?w 2D i = w 2D i ? ?r ?2 i (y gt ) + E y?p(y|X) r ?2 i (y) ,<label>(8)</label></formula><p>where r i (y) = ?(Rx 3D i + t) ? x 2D i (unweighted reprojection error), and (?) ?2 stands for element-wise square. The first bracketed term ?r ?2 i (y gt ) with negative sign indicates that correspondences with large reprojection error (hence high uncertainty) shall be weighted less. The second term Ey?p(y|X) r ?2 i (y) is relevant to the variance of reprojection error over the predicted pose. The positive sign indicates that sensitive correspondences should be weighted more, because they provide stronger pose discrimination. The final gradient is thus a balance between the uncertainty and discrimination, as shown in <ref type="figure" target="#fig_1">Figure 3</ref>. Existing work <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b35">36]</ref> on learning uncertainty-aware correspondences only considers the former, hence lacking the discriminative ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Local Regularization of Derivatives</head><p>While the KL divergence is a good metric for the probabilistic distribution, for inference it is still required to es-Algorithm 1: AMIS-based Monte Carlo pose loss</p><formula xml:id="formula_11">Input : X = {x 3D i , x 2D i , w 2D i } Output: L pred 1 y * , ? y * ? PnP (X) // Laplace approximation 2 Fit q 1 (y) to y * , ? y * // initial proposal 3 for 1 ? t ? T do 4 Generate K samples y t j=1???K from q t (y) 5 for 1 ? j ? K do 6 P t j ? p(X|y t j ) // evaluate integrand 7 for 1 ? ? ? t and 1 ? j ? K do 8 Q ? j ? 1 t t m=1 q m (y ? j ) // eval proposal mix 9 v ? j ? P ? j /Q ? j // importance weight 10 if t &lt; T then 11</formula><p>Estimate q t+1 (y) from all weighted samples  timate the exact pose y * by solving the PnP problem in Eq. (1). The common choice of high precision is to utilize the iterative PnP solver based on the Levenberg-Marquardt (LM) algorithm -a robust variant of the Gauss-Newton (GN) algorithm, which solves the non-linear least squares by the first and approximated second order derivatives. To aid derivative-based optimization, we regularize the derivatives of the log density log p(y|X) w.r.t. the pose y, by encouraging the LM step ?y to find the true pose y gt .</p><formula xml:id="formula_12">{y ? j , v ? j | 1 ? ? ? t, 1 ? j ? K } 12 L pred ? log 1 T K T t=1 K j=1 v t</formula><p>To employ the regularization during training, a detached solution y * is obtained first. Then, at y * , another iteration step is evaluated via the GN algorithm (which ideally equals 0 if y * has converged to the local optimum):</p><formula xml:id="formula_13">?y = ?(J T J + ?I) ?1 J T F (y * ),<label>(9)</label></formula><p>where</p><formula xml:id="formula_14">F (y * ) = f T 1 (y * ), f T 2 (y * ), ? ? ? , f T N (y * )</formula><p>T is the concatenated weighted reprojection errors of all points, J = ?F (y)/ ?y T y=y * is the Jacobian matrix, and ? is a small value for numerical stability. Note that ?y is analytically differentiable. We therefore design the regularization loss as follows:</p><p>L reg = l(y * + ?y, y gt ),</p><p>where l(?, ?) is a distance metric for pose. We adopt smooth L1 for position and cosine similarity for orientation (see supplementary materials for details). Note that the gradient is only backpropagated through ?y, encouraging the step to be non-zero if y * = y gt . This regularization loss can be also used as a standalone objective to train pose estimators <ref type="bibr" target="#b23">[24]</ref>. However, this objective alone cannot handle pose ambiguity properly, and is thus regarded as a secondary regularization in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Attention-Inspired Correspondence Networks</head><p>As discussed in Section 3.3, the balance between uncertainty and discrimination enables locating important correspondences in an attention-like manner. This inspires us to take elements from attention-related work, i.e., the Softmax layer and the deformable sampling <ref type="bibr" target="#b53">[54]</ref>.</p><p>In this section, we present two networks with EPro-PnP layer for 6DoF pose estimation and 3D object detection, respectively. For the former, EPro-PnP is incorporated into the existing dense correspondence architecture <ref type="bibr" target="#b28">[29]</ref>. For the latter, we propose a radical deformable correspondence network to explore the flexibility of EPro-PnP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dense Correspondence Network</head><p>For a strict comparison against existing PnP-based pose estimators, this paper takes the network from CDPN <ref type="bibr" target="#b28">[29]</ref> as a baseline, adding minor modifications to fit the EPro-PnP.</p><p>The original CDPN feeds cropped image regions within the detected 2D boxes into the pose estimation network, to which two decoupled heads are appended for rotation and translation respectively. The rotation head is PnP-based while the translation head uses direct regression. This paper discards the translation head to focus entirely on PnP.</p><p>Modifications are only made to the output layers. As shown in <ref type="figure" target="#fig_3">Figure 4</ref>, the original confidence map is expanded to two-channel XY weights with spatial Softmax and dynamic global weight scaling. Inspired by the attention mechanism <ref type="bibr" target="#b43">[44]</ref>, the Softmax layer is a vital element for stable training, as it translates the absolute corresponding weights into a relative measurement. On the other hand, the global weight scaling factors represent the global concentration of the predicted pose distribution, ensuring a better convergence of the KL divergence loss.</p><p>The dense correspondence network can be trained solely with the KL divergence loss L KL to achieve decent performance. For top-tier performance, it is still beneficial to utilize additional coordinate regression as intermediate supervision, not to stabilize convergence but to introduce the geometric knowledge from the 3D models. Therefore, we keep the masked coordinate regression loss from CDPN <ref type="bibr" target="#b28">[29]</ref> but leave out its confidence loss. Furthermore, the performance  can be elevated by imposing the regularization loss L reg in Eq. (10).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Deformable Correspondence Network</head><p>Inspired by Deformable DETR <ref type="bibr" target="#b53">[54]</ref>, we propose a novel deformable correspondence network for 3D object detection, in which the entire 2D-3D coordinates and weights are learned from scratch.</p><p>As shown in <ref type="figure">Figure 5</ref>, the deformable correspondence network is an extension of the FCOS3D <ref type="bibr" target="#b46">[47]</ref> framework. The original FCOS3D is a one-stage detector that directly regresses the center offset, depth, and yaw orientation of multiple objects for 4DoF pose estimation. In our adaptation, the outputs of the multi-level FCOS head <ref type="bibr" target="#b40">[41]</ref> are modified to generate object queries instead of directly predicting the pose. Also inspired by Deformable DETR <ref type="bibr" target="#b53">[54]</ref>, the appearance and position of a query is disentangled into the embedding vector and the reference point. A multi-head deformable attention layer <ref type="bibr" target="#b53">[54]</ref> is adopted to sample the key-value pairs from the dense features, with the value projected into point-wise features, and meanwhile aggregated into the object-level features.</p><p>The point features are passed into a subnet that predicts the 3D points and corresponding weights (normalized by Softmax). Following MonoRUn <ref type="bibr" target="#b12">[13]</ref>, the 3D points are set in the normalized object coordinate (NOC) space to handle categorical objects of various sizes.</p><p>The object features are responsible for predicting the object-level properties: (a) the 3D score (i.e., 3D localization confidence), (b) the weight scaling factor (same as in Section 4.1), (c) the 3D box size for recovering the absolute scale of the 3D points, and (d) other optional properties (velocity, attribute) required by the nuScenes benchmark <ref type="bibr" target="#b7">[8]</ref>.</p><p>The deformable 2D-3D correspondences can be learned solely with the KL divergence loss L KL , preferably in conjunction with the regularization loss L reg . Other auxiliary losses can be imposed onto the dense features for enhanced accuracy. Details are given in supplementary materials. (inferring mode) <ref type="figure">Figure 5</ref>. The deformable correspondence network based on the FCOS3D <ref type="bibr" target="#b46">[47]</ref> detector. Note that the sampled point-wise features are shared by the point-level subnet and the deformable attention layer that aggregates the features for object-level predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets and Metrics</head><p>LineMOD Dataset and Metrics The LineMOD dataset <ref type="bibr" target="#b22">[23]</ref> consists of 13 sequences, each containing about 1.2K images annotated with 6DoF poses of a single object. Following <ref type="bibr" target="#b4">[5]</ref>, the images are split into the training and testing sets, with about 200 images per object for training. For data augmentation, we use the same synthetic data as in CDPN <ref type="bibr" target="#b28">[29]</ref>. We use two common metrics for evaluation: ADD(-S) and n?, n cm. The ADD measures whether the average deviation of the transformed model points is less than a certain fraction of the object's diameter (e.g., ADD-0.1d). For symmetric objects, ADD-S computes the average distance to the closest model point. n?, n cm measures the accuracy of pose based on angular/positional error thresholds. All metrics are presented as percentages.</p><p>nuScenes Dataset and Metrics The nuScenes 3D object detection benchmark <ref type="bibr" target="#b7">[8]</ref> provides a large scale of data collected in 1000 scenes. Each scene contains 40 keyframes, annotated with a total of 1.4M 3D bounding boxes from 10 categories. Each keyframe includes 6 RGB images collected from surrounding cameras. The data is split into 700/150/150 scenes for training/validation/testing. The official benchmark evaluates the average precision with true positives judged by 2D center error on the ground plane. The mAP metric is computed by averaging over the thresholds of 0.5, 1, 2, 4 meters. Besides, there are 5 true positive metrics: Average Translation Error (ATE), Average Scale Error (ASE), Average Orientation Error (AOE), Average Velocity Error (AVE) and Average Attribute Error (AAE). Finally, there is a nuScenes detection score (NDS) computed as a weighted average of the above metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation Details</head><p>EPro-PnP Configuration For the PnP formulation in Eq. (1), in practice the actual reprojection costs are robustified by the Huber kernel ?(?):</p><p>arg min</p><formula xml:id="formula_16">y 1 2 N i=1 ? f i (y) 2 .<label>(11)</label></formula><p>The Huber kernel with threshold ? is defined as:</p><formula xml:id="formula_17">?(s) = s, s ? ? 2 , ?(2 ? s ? ?), s &gt; ? 2 .<label>(12)</label></formula><p>We use an adaptive threshold as described in the supplementary materials. For Monte Carlo pose loss, we set the AMIS iteration count T to 4 and the number of samples per iteration K to 128. The loss weights are tuned such that L KL produces roughly the same magnitude of gradient as typical coordinate regression, while the gradient from L reg are kept very low. The weight normalization technique in <ref type="bibr" target="#b12">[13]</ref> is adopted to compute the dynamic loss weight for L KL .</p><p>Training the Dense Correspondence Network General settings are kept the same as in CDPN <ref type="bibr" target="#b28">[29]</ref> (with ResNet-34 <ref type="bibr" target="#b20">[21]</ref> as backbone) for strict comparison, except that we increase the batch size to 32 for less training wall time.</p><p>The network is trained for 160 epochs by RMSprop on the LineMOD dataset <ref type="bibr" target="#b22">[23]</ref>. To reduce the Monte Carlo overhead, 512 points are randomly sampled from the 64?64 dense points to compute L KL .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training the Deformable Correspondence Network</head><p>We adopt the same detector architecture as in FCOS3D <ref type="bibr" target="#b46">[47]</ref>, with ResNet-101-DCN <ref type="bibr" target="#b14">[15]</ref> as backbone. The network is trained for 12 epochs by the AdamW <ref type="bibr" target="#b29">[30]</ref> optimizer, with a batch size of 12 images across 4 GPUs on the nuScenes dataset <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Results on the LineMOD Benchmark</head><p>Comparison to the CDPN baseline with Ablations The contributions of every single modification to CDPN <ref type="bibr" target="#b28">[29]</ref> are revealed in <ref type="table" target="#tab_1">Table 1</ref>. From the results it can be observed that: ? Strong improvement (+5.46) is seen when initialized from A1, because CDPN has been trained with the extra ground truth of object masks, providing a good initial state highlighting the foreground. ? Finally, the performance benefits (+0.97) from more training epochs (160 ep. from A1 + 320 ep.) as equivalent to CDPN-Full <ref type="bibr" target="#b28">[29]</ref> (3 stages ? 160 ep.).</p><p>The results clearly demonstrate that EPro-PnP can unleash the enormous potential of the classical PnP approach, without any fancy network design or decoupling tricks.</p><p>Comparison to the State of the Art As shown in <ref type="table">Table 2</ref>, despite modified from the lower baseline, EPro-PnP easily reaches comparable performance to the top pose refiner RePOSE <ref type="bibr" target="#b23">[24]</ref>, which adds extra overhead to the PnP-based initial estimator PVNet <ref type="bibr" target="#b35">[36]</ref>. Among all these entries, EPro-PnP is the most straightforward as it simply solves the PnP problem itself, without refinement network <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b51">52]</ref>, disentangled translation <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b44">45]</ref>, or multiple representations <ref type="bibr" target="#b39">[40]</ref>.</p><p>Comparison to Implicit Differentiation and Reprojection Learning As shown in <ref type="table">Table 3</ref>, when the coordinate regression loss is removed, both implicit differentiation and reprojection loss fail to learn the pose properly. Yet EPro-PnP manages to learn the coordinates from scratch, even outperforming CDPN without translation head (79.46 vs. 74.54). This validates that EPro-PnP can be used as a general pose estimator without relying on geometric prior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Uncertainty and Discrimination</head><p>In <ref type="table">Table 3</ref>, Reprojection vs. Monte Carlo loss can be interpreted as uncertainty alone vs. uncertainty-discrimination balanced. The results reveal that uncertainty alone exhibits strong performance when intermediate coordinate supervision is available, while discrimination is the key element for learning correspondences from scratch.</p><p>Contribution of End-to-End Weight/Coordinate Learning As shown in <ref type="table" target="#tab_1">Table 1</ref>, detaching the weights from the end-to-end loss has a stronger impact to the performance than detaching the coordinates (?8.69 vs. ?3.08), stressing the importance of attention-like end-to-end weight learning.</p><p>On the Importance of the Softmax Layer Learning the corresponding weights without the normalization denominator of spatial Softmax (so it becomes exponential activation as in <ref type="bibr" target="#b12">[13]</ref>) does not converge, as listed in <ref type="table" target="#tab_1">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Results on the nuScenes Benchmark</head><p>We evaluate 3 variants of EPro-PnP: (a) the basic approach that learns deformable correspondences without geometric prior (enhanced with regularization), (b) adding coordinate regression loss with sparse ground truth extracted from the available LiDAR points as in <ref type="bibr" target="#b12">[13]</ref>, (c) further adding test-time flip augmentation (TTA) for fair comparison against <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48]</ref>  <ref type="table">Table 3</ref>. Comparison between loss functions by experiments conducted on the same dense correspondence network. For implicit differentiation, we minimize the distance metric of pose in Eq. (10) instead of the reprojection-metric pose loss in BPnP <ref type="bibr" target="#b11">[12]</ref>.</p><p>presented in <ref type="table" target="#tab_3">Table 4</ref> with comparison to other approaches. From the validation results it can be observed that:</p><p>? The basic EPro-PnP significantly outperforms the FCOS3D <ref type="bibr" target="#b46">[47]</ref> baseline (NDS 0.425 vs. 0.372). Although it partially benefits from more parameters from the correspondence head, there is still good evidence that: with a proper end-to-end pipeline, PnP can outperform direct pose prediction on a large scale of data.   On the test data, with the advantage in pose accuracy (mATE and mAOE), EPro-PnP achieves the highest NDS score among other task-specific competitors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Qualitative Analysis</head><p>As illustrated in <ref type="figure" target="#fig_6">Figure 7</ref>, the dense weight and coordinate maps learned with EPro-PnP generally capture less details compared to CDPN <ref type="bibr" target="#b28">[29]</ref>, as a result of higher uncertainty around sharp edges. Surprisingly, even though the learned-from-scratch coordinate maps seem to be a mess, the end-to-end pipeline gains comparable pose accuracy to the CDPN baseline (79.46 vs. 79.96). When initialized with pretrained CDPN, EPro-PnP inherits the detailed geometric profile, therefore confining the active weights within the foreground region and achieving the overall best performance. Also note that the weight maps of both derivative regularization and implicit differentiation <ref type="bibr" target="#b11">[12]</ref> are more concentrated, biasing towards discrimination over uncertainty. <ref type="figure" target="#fig_5">Figure 6</ref> shows that the flexibility of EPro-PnP allows predicting multimodal distributions with strong expressive power, successfully capturing the orientation ambiguity without discrete multi-bin classification <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b46">47]</ref> or complicated mixture model <ref type="bibr" target="#b6">[7]</ref>. Owing to the ability to model orientation ambiguity, EPro-PnP outperforms other competitors by a wide margin in terms of the AOE metric in <ref type="table" target="#tab_3">Table 4</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This paper proposes the EPro-PnP, which translates the non-differentiable deterministic PnP operation into a differentiable probabilistic layer, empowering end-to-end 2D-3D correspondence learning of unprecedented flexibility. The connections to previous work <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref> have been thoroughly discussed with theoretical and experimental proofs. For application, EPro-PnP can inspire novel solutions such as the deformable correspondence, or it can be simply integrated into existing PnP-based networks. Beyond the PnP problem, the underlying principles are theoretically generalizable to other learning models with nested optimization layer, known as declarative networks <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Levenberg-Marquardt PnP Solver</head><p>For scalability, we have implemented a PyTorch-based batch Levenberg-Marquardt (LM) PnP solver. The implementation generally follows the Ceres solver <ref type="bibr" target="#b0">[1]</ref>. Here, we discuss some important details that are related to the proposed Monte Carlo pose sampling and derivative regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Adaptive Huber Kernel</head><p>To robustify the weighted reprojection errors of various scales, we adopt an adaptive Huber kernel with a dynamic threshold ? for each object, defined as a function of the weights w 2D i and 2D coordinates x 2D i :</p><formula xml:id="formula_18">? = ? rel w 2D 1 2 1 N ? 1 N i=1 x 2D i ?x 2D 2 1 2 ,<label>(13)</label></formula><p>with the relative threshold ? rel as hyperparameter, and the mean vectorsw</p><formula xml:id="formula_19">2D = 1 N N i=1 w 2D i ,x 2D = 1 N N i=1 x 2D i .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. LM Step with Huber Kernel</head><p>Adding the Huber kernel influences every related element from the likelihood function to the LM iteration step and derivative regularization loss. Thanks to PyTorch's automatic differentiation, the robustified Monte Carlo KL divergence loss does not require much special handling. For the LM solver, however, the residual F (y) (concatenated weighted reprojection errors) and the Jacobian matrix J have to be rescaled before computing the robustified LM step <ref type="bibr" target="#b41">[42]</ref>.</p><p>The rescaled residual blockf i (y) and Jacobian block J i (y) of the i-th point pair are defined as:</p><formula xml:id="formula_20">f i (y) = ? i f i (y),<label>(14)</label></formula><formula xml:id="formula_21">J i (y) = ? i J i (y),<label>(15)</label></formula><p>where</p><formula xml:id="formula_22">? i = ? ? ? 1, f i (y) ? ?, ? f i (y) , f i (y) &gt; ?,<label>(16)</label></formula><formula xml:id="formula_23">J i (y) = ?f i (y) ?y T .<label>(17)</label></formula><p>Following the implementation of Ceres solver <ref type="bibr" target="#b0">[1]</ref>, the robustified LM iteration step is:</p><formula xml:id="formula_24">?y = ? J TJ + ?D 2 ?1J TF ,<label>(18)</label></formula><formula xml:id="formula_25">whereJ = ? ? ?J 1 (y) . . . J N (y) ? ? ?,F = ? ? ?f 1 (y) . . . f N (y) ? ? ?,<label>(19)</label></formula><p>D is the square root of the diagonal of the matrixJ TJ , and ? is the reciprocal of the LM trust region radius <ref type="bibr" target="#b0">[1]</ref>. Note that the rescaled residual and Jacobian affects the derivative regularization (Eq. (10)), as well as the covariance estimation in the next subsection.</p><p>Fast Inference Mode We empirically found that in a well-trained model, the LM trust region radius can be initialized with a very large value, effectively rendering the LM algorithm redundant. We therefore use the simple Gauss-Newton implementation for fast inference:</p><formula xml:id="formula_26">?y = ? J TJ + ?I ?1J TF ,<label>(20)</label></formula><p>where ? is a small value for numerical stability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Covariance Estimation</head><p>During training, the concentration of the AMIS proposal is determined by the local estimation of pose covariance matrix ? y * , defined as:</p><formula xml:id="formula_27">? y * = J TJ + ?I ?1 y=y * ,<label>(21)</label></formula><p>where y * is the LM solution that determines the location of the proposal distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Initialization</head><p>Since the LM solver only finds a local solution, initialization plays a determinant role in dealing with ambiguity. Standard EPnP <ref type="bibr" target="#b26">[27]</ref> initialization can handle the dense correspondence network trained on the LineMOD <ref type="bibr" target="#b22">[23]</ref> dataset, where ambiguity is not noticeable. For the deformable correspondence network trained on the nuScenes <ref type="bibr" target="#b7">[8]</ref> dataset and more general cases, we implement a random sampling algorithm analogous to RANSAC, to search for the global optimum efficiently.</p><p>Given the N -point correspondence set X = x 3D</p><p>i , x 2D i , w 2D i i = 1 ? ? ? N , we generate M subsets consisting of n corresponding points each (3 ? n &lt; N ), by repeatedly sub-sampling n indices without replacement from a multinomial distribution, whose probability mass function p(i) is defined by the corresponding weights:</p><formula xml:id="formula_28">p(i) = w 2D i 1 N i=1 w 2D i 1 .<label>(22)</label></formula><p>From each subset, a pose hypothesis can be solved via the LM algorithm with very few iterations (we use 3 iterations). This is implemented as a batch operation on GPU, and is rather efficient for small subsets. We take the hypothesis of maximum log-likelihood log p(X|y) as the initial point, starting from which subsequent LM iterations are computed on the full set X.</p><p>Training Mode During training, the LM PnP solver is utilized for estimating the location and concentration of the initial proposal distribution in the AMIS algorithm. The location is very important to the stability of Monte Carlo training. If the LM solver fails to find the global optimum and the location of the local optimum is far from the true pose y gt , the balance between the two opposite signed terms in Eq. (5) may be broken, leading to exploding gradient in the worst case scenario. To avoid such problem, we adopt a simple initialization trick: we compare the log-likelihood log p(X|y) of the ground truth y gt and the selected hypothesis, and then keep the one with higher likelihood as the initial state of the LM solver.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Details on Monte Carlo Pose Sampling</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Proposal Distribution for Position</head><p>For the proposal distribution of the translation vector t ? R 3 , we adopt the multivariate t-distribution, with the following probability density function (PDF):</p><formula xml:id="formula_29">q T (t) = ? ?+3 2 ? ? 2 ? 3 ? 3 |?| 1 + 1 ? t ? ? 2 ? ? ?+3 2 , (23) where t ? ? 2 ? = (t ? ?) T ? ?1 (t ? ?)</formula><p>, with the location ?, the 3?3 positive definite scale matrix ?, and the degrees of freedom ?. Following <ref type="bibr" target="#b13">[14]</ref>, we set ? to 3. Compared to the multivariate normal distribution, the t-distribution has a heavier tail, which is ideal for robust sampling.</p><p>The multivariate t-distribution has been implemented in the Pyro <ref type="bibr" target="#b1">[2]</ref> package.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Initial Parameters</head><p>The initial location and scale is determined by the PnP solution and covariance matrix, i.e., ? ? t * , ? ? ? t * , where ? t * is the 3?3 submatrix of the full pose covariance ? p * . Note that the actual covariance of the t-distribution is thus ? ??1 ? t * , which is intentionally scaled up for robust sampling in a wider range.</p><p>Parameter Estimation from Weighted Samples To update the proposal, we let the location ? and scale ? be the first and second moment of the weighted samples (i.e., weighted mean and covariance), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Proposal Distribution for 1D Orientation</head><p>For the proposal distribution of the 1D yaw-only orientation ?, we adopt a mixture of von Mises and uniform distribution. The von Mises is also known as the circular normal distribution, and its PDF is given by:</p><formula xml:id="formula_30">q VM (?) = exp (? cos (? ? ?)) 2?I 0 (?) ,<label>(24)</label></formula><p>where ? is the location parameter, ? is the concentration parameter, and I 0 (?) is the modified Bessel function with order zero. The mixture PDF is thus:</p><formula xml:id="formula_31">q mix (?) = (1 ? ?)q VM (?) + ?q uniform (?),<label>(25)</label></formula><p>with the uniform mixture weight ?. The uniform component is added in order to capture other potential modes under orientation ambiguity. We set ? to a fixed value of 1/4. PyTorch has already implemented the von Mises distribution, but its random sample generation is rather slow. As an alternative we use the NumPy implementation for random sampling.</p><p>Initial Parameters With the yaw angle ? * and its variance ? 2 ? * from the PnP solver, the parameters of the von Mises proposal is initialized by ? ? ? * , ? ? 1</p><formula xml:id="formula_32">3? 2 ? * .</formula><p>Parameter Estimation from Weighted Samples For the location ?, we simply adopt its maximum likelihood estimation, i.e., the circular mean of the weighted samples. For the concentration ?, we first compute an approximated estimation <ref type="bibr" target="#b15">[16]</ref> </p><formula xml:id="formula_33">by:? =r (2 ?r 2 ) 1 ?r 2 ,<label>(26)</label></formula><p>wherer = j v j [sin ? j , cos ? j ] T / j v j is the norm of the mean orientation vector, with the importance weight v j for the j-th sample ? j . Finally, the concentration is scaled down for robust sampling, such that ? ??/3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Proposal Distribution for 3D Orientation</head><p>Regarding the quaternion based parameterization of 3D orientation, which can be represented by a unit 4D vector l, we adopt the angular central Gaussian (ACG) distribution as the proposal. The support of the 4-dimensional ACG distribution is the unit hypersphere, and the PDF is given by:</p><formula xml:id="formula_34">q ACG (l) = (l T ? ?1 l) ?2 S 4 |?| 1 2 ,<label>(27)</label></formula><p>where S 4 = 2? 2 is the 3D surface area of the 4D sphere, and ? is a 4?4 positive definite matrix. The ACG density can be derived by integrating the zeromean multivariate normal distribution N (0, ?) along the radial direction from 0 to inf. Therefore, drawing samples from the ACG distribution is equivalent to sampling from N (0, ?) and then normalizing the samples to unit radius.</p><p>Initial Parameters Consider l * to be the PnP solution and ? ?1 l * to be the estimated 4?4 inverse covariance matrix. Note that ? ?1 l * is only valid in the local tangent space with rank 3, satisfying l * T ? ?1 l * l * = 0. The initial parameters are determined by:</p><formula xml:id="formula_35">? ?? + ?|?| 1 4 I,<label>(28)</label></formula><p>where? = ? ?1 l * + I ?1 , and ? is a hyperparameter that controls the dispersion of the proposal for robust sampling.</p><p>We set ? to 0.001 in the experiments.</p><p>Parameter Estimation from Weighted Samples Based on the samples l j and weights v j , the maximum likelihood estimation? is the solution to the following equation:</p><formula xml:id="formula_36">? = 4 j v j j v j l j l T j l T j? ?1 l j .<label>(29)</label></formula><p>The solution to Eq. (29) can be computed by fixed-point iteration <ref type="bibr" target="#b42">[43]</ref>. The final parameters of the updated proposal is determined the same way as in Eq. (28).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Details on Derivative Regularization Loss</head><p>As stated in the main paper, the derivative regularization loss L reg consists of the position loss L pos and the orientation loss L orient .</p><p>For L pos , we adopt the smooth L1 loss based on the Euclidean distance d t = t * + ?t ? t gt , given by:</p><formula xml:id="formula_37">L pos = ? ? ? d 2 t 2? , d t ? ?, d t ? 0.5?, d t &gt; ?,<label>(30)</label></formula><p>with the hyperparameter ?.</p><p>For L orient , we adopt the cosine similarity loss based on the angular distance d ? . For 1D orientation parameterized by the angle ?, d ? = ? * + ?? ? ? gt . For 3D orientation parameterized by the quaternion vector l, d ? = 2 arccos (l * + ?l) T l gt . The loss function is therefore defined as:</p><formula xml:id="formula_38">L orient = 1 ? cos d ? .<label>(31)</label></formula><p>For 3D orientation, after the substitution, the loss function can be simplified to:</p><formula xml:id="formula_39">L orient = 2 ? 2 (l * + ?l) T l gt 2 .<label>(32)</label></formula><p>For the specific settings of the hyperparameter ? and loss weights, please refer to the experiment configuration code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Details on the Deformable Correspondence Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Network Architecture</head><p>The detailed network architecture of the deformable correspondence network is shown in <ref type="figure" target="#fig_7">Figure 8</ref>. Following deformable DETR <ref type="bibr" target="#b53">[54]</ref>, this paper adopts the multi-head deformable sampling. Let n head be the number of heads and n hpts be the number of points per head, a total number of N = n head n hpts points are sampled for each object. The sampling locations relative to the reference point are generated from the object embedding by a single layer of linear transformation. We set n head to 8, which yields 256/n head = 32 channels for the point features. The point-level branch on the left side of <ref type="figure" target="#fig_7">Figure 8</ref> is responsible for predicting the 3D points x 3D i and corresponding weights w 2D i . The sampled point features are first enhanced by the object-level context, by adding the reshaped head-wise object embedding to the point features. Then, the features of the N points are processed by the self attention layer, for which the 2D points are transformed into positional encoding. The attention layer is followed by standard layers of normalization, skip connection, and feedforward network (FFN).</p><p>Regarding the object-level branch on the right side of <ref type="figure" target="#fig_7">Figure 8</ref>, a multi-head attention layer is employed to aggregate the sampled point features. Unlike the original deformable attention layer <ref type="bibr" target="#b53">[54]</ref> that predicts the attention weights by linear projection of the object embedding, we adopt the full Q-K dot-product attention with positional encoding. After being processed by the subsequent layers, the object-level features are finally transformed into to the object-level predictions, consisting of the 3D localization score, weight scale, 3D bounding box size, and other optional properties (velocity and attribute). Note that the attention layer is actually not a necessary component for object-level predictions, but rather a byproduct of the deformable point samples whose features can be leveraged with little computation overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Loss Functions for Object-Level Predictions</head><p>As in FCOS3D <ref type="bibr" target="#b46">[47]</ref>, we adopt smooth L1 regression loss for 3D box size and velocity, and cross-entropy classification loss for attribute. Additionally, a binary cross-entropy loss is imposed upon the 3D localization score, with the target c tgt defined as a function of the position error: <ref type="bibr" target="#b32">(33)</ref> where t * XZ is the XZ components of the PnP solution, t XZgt is the XZ components of the true pose, and a, b are the linear coefficients. The predicted 3D localization score c pred shall reflect the positional uncertainty of an object, as a faster alternative to evaluating the uncertainty via the Monte Carlo method during inference (Section F.2). The final detection score is defined as the product of the predicted 3D score and the classification score from the base detector.</p><formula xml:id="formula_40">c tgt = Score( t * XZ ? t XZgt ) = max(0, min(1, ?a log t * XZ ? t XZgt + b)),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. Auxiliary Loss Functions</head><p>To regularize the dense features, we append an auxiliary branch that predicts the multi-head dense 3D coordinates and corresponding weights, as shown in <ref type="figure">Figure 9</ref>. Leveraging the ground truth of object 2D boxes, the features within the box regions are densely sampled via RoI Align <ref type="bibr" target="#b19">[20]</ref>, and transformed into the 3D coordinates x 3D and weights w 2D via an independent linear layer. Besides, the attention weights ? are obtained via Q-K dot-product and normalized along the n head dimension and across the overlapping region of multiple RoIs via Softmax.</p><p>During training, we impose the reprojection-based auxiliary loss for the multi-head dense predictions, formulated as the negative log-likelihood (NLL) of the Gaussian mixture model <ref type="bibr" target="#b2">[3]</ref>. The loss function for each sampled point is defined as:</p><formula xml:id="formula_41">L proj = ? log RoI nhead k=1 ? k | diag w 2D k | exp ? 1 2 f k (y gt ) 2 ,<label>(34)</label></formula><p>where k is the head index, f k (y gt ) is the weighted reprojection error of the k-th head at the truth pose y gt . In the above equation, the diagonal matrix diag w 2D k is interpreted as the inverse square root of the covariance matrix of the normal distribution, i.e., diag w 2D k = ? ? 1 2 , and the head attention weight ? k is interpreted as the mixture component weight.</p><p>RoI is a special operation that takes the overlapping region of multiple RoIs into account, formulating a mixture of multiple heads and multiple RoIs (see code for details).</p><p>Another auxiliary loss is the coordinate regression loss that introduces the geometric knowledge. Following MonoRUn <ref type="bibr" target="#b12">[13]</ref>, we extract the sparse ground truth of 3D coordinates x 3D gt from the 3D LiDAR point cloud. The multihead coordinate regression loss for each sampled point with available ground truth is defined as:  <ref type="figure">Figure 9</ref>. Architecture of the auxiliary branch. This branch shares the same weights of Q, K projection with the deformable attention layer in the lower right of <ref type="figure" target="#fig_7">Figure 8</ref>.  where ?(?) is the Huber kernel. L regr is essentially a weighted smooth L1 loss (although we write the Huber kernel for convenience in notation).</p><formula xml:id="formula_42">L regr = nhead k=1 ? k ? x 3D k ? x 3D gt 2 ,<label>(35)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4. Training Strategy</head><p>During training, we randomly sample 48 positive object queries from the FCOS3D <ref type="bibr" target="#b46">[47]</ref> detector for each image, which limits the batch size of the deformable correspondence network to control the computation overhead of the Monte Carlo pose loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Additional Results of the Dense Correspondence Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1. Convergence Behavior</head><p>The convergence behaviors of EPro-PnP and CDPN <ref type="bibr" target="#b28">[29]</ref> are compared in <ref type="figure" target="#fig_9">Figure 10</ref>. The original CDPN-Full is trained in 3 stages (rotation head -translation head -both together) with a total of 480 epochs. In contrast, EPro-PnP with derivative regularization clearly outperforms CDPN-Full within one stage, and goes further when initialized from the pretrained first-stage CDPN.  <ref type="table">Table 5</ref>. Additional results of the deformable correspondence network tested on the nuScenes <ref type="bibr" target="#b7">[8]</ref> benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. Inference Time</head><p>Compared to the inference pipeline of CDPN-Full <ref type="bibr" target="#b28">[29]</ref>, EPro-PnP does not use the RANSAC algorithm or extra translation head, so the overall inference speed is more than twice as fast as CDPN-Full (at a batch size of 32), even though we introduces the iterative LM solver.</p><p>Regarding the LM solver itself, inference takes 7.3 ms for a batch of one object, measured on RTX 2080 Ti GPU, excluding EPnP <ref type="bibr" target="#b26">[27]</ref> initialization. As a reference, the stateof-the-art pose refiner RePOSE <ref type="bibr" target="#b23">[24]</ref> (also based on the LM algorithm) adds 10.9 ms overhead to the base pose estimator PVNet <ref type="bibr" target="#b35">[36]</ref> at the same batch size, measured on RTX 2080 Super GPU, which is slower than ours. Nevertheless, faster inference is possible if the number of points N = 64 ? 64 is reduced to an optimal level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Additional Experiments on the Deformable</head><p>Correspondence Network</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1. On the Auxiliary Reprojection Loss</head><p>As shown in <ref type="table">Table 5</ref>, removing the auxiliary reprojection loss in Eq. 34 lowers the 3D object detection accuracy (NDS 0.408 vs. 0.425). Among the true positive metrics, the orientation metric mAOE is the most affected. The results indicate that, although the deformable correspondences can be learned solely with the end-to-end loss, it is still beneficial to add auxiliary task for further regularization, even if the task itself does not involve extra annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2. On the Uncertainty of Object Pose</head><p>The dispersion of the inferred pose distribution reflects the aleatoric uncertainty of the predicted pose. Previous work <ref type="bibr" target="#b12">[13]</ref> reasons the pose uncertainty by propagating the reprojection uncertainty learned from a surrogate loss through the PnP operation, but that uncertainty requires calibration and is not reliable enough. In our work, the pose uncertainty is learned with the KL-divergence-based pose loss in an end-to-end manner, which is much more reliable in theory.</p><p>To quantitatively evaluate the reliability of the pose un-certainty in terms of measuring the localization confidence, a straightforward approach is to compute the 3D localization score c MC via Monte Carlo pose sampling, and compare the resulting mAP against the standard implementation with 3D score c pred predicted from the object-level branch. With the PnP solution t * , the sampled translation vector t j , and its importance weight v j , the Monte Carlo score is computed by:</p><formula xml:id="formula_43">c MC = 1 j v j j v j Score t * XZ ? t XZj ,<label>(36)</label></formula><p>where the subscript (?) XZ denotes taking the XZ components, and the function Score(?) is the same as in Eq. 33. Furthermore, the final score can also be a mixture of the two sources, defined as:</p><formula xml:id="formula_44">c mix = c ? MC c 1?? pred ,<label>(37)</label></formula><p>where ? is the mixture weight. The evaluation results under different mixture weights are presented in <ref type="table">Table 5</ref>. Regarding the mAP metric, the Monte Carlo score is on par with the standard implementation (0.350 vs. 0.350 vs. 0.349), indicating that the pose uncertainty is a reliable measure of the detection confidence. Nevertheless, due to the much longer runtime of inferring with Monte Carlo pose sampling, training a standard score branch is still a more practical choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3. On the Network Redundancy and Potential for Future Improvement</head><p>Since the main concern of this paper is to propose a novel differentiable PnP layer, we did not have enough time and resources to fine-tune the architecture and parameters of the deformable correspondence network at the time of submitting the manuscript. Therefore, the network described in Sections 4.2 and D.1 was crafted with some redundancy in mind, being not very efficient in terms of FLOP count, memory footprint and inference time, leaving large potential for improvement.</p><p>To demonstrate the potential for improvement, we train a more compact network with lower resolution (stride=8) for the dense feature map, and the number of points per head n hpts reduced from 32 to 16, and squeeze the batch of 12 images into 2 RTX 3090 GPUs. As shown in <ref type="table">Table 5</ref>, the overall performance is actually slightly better than the original version (NDS 0.434 vs. 0.430). Still, a more efficient architecture is yet to be determined in future work.</p><p>Inference Time Regarding the compact network, the average inference time per frame (comprising a batch of 6 surrounding 1600?672 5 images, without TTA) is shown in <ref type="table" target="#tab_7">Table 6</ref>, measured on RTX 3090 GPU and Core i9-10920X CPU. On average, the batch PnP solver processes 625.97 objects per frame before non-maximum suppression (NMS).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Limitation</head><p>EPro-PnP is a versatile pose estimator for general problems, yet it has to be acknowledged that training the network with the Monte Carlo pose loss is inevitably slower than the baseline. At the batch size of 32, training the CDPN (without translation head) takes 143 seconds per epoch with the original coordinate regression loss, and 241 seconds per epoch with the Monte Carlo pose loss, which is about 70% longer time, as measured on GTX 1080 Ti GPU. However, the training time can be controlled by adjusting the number of Monte Carlo samples or the number of 2D-3D corresponding points. In this paper, the choice of these hyperparameters generally leans towards redundancy. <ref type="bibr" target="#b4">5</ref> The original size is 1600?900. We crop the images for efficiency. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Notation</head><p>Notation Description  <ref type="table">Table 7</ref>. A summary of frequently used notations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>The learned corresponding weight can be factorized into inverse uncertainty and discrimination. Typically, inverse uncertainty roughly resembles the foreground mask, while discrimination emphasizes the 3D extremities of the object.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>The 6DoF pose estimation network modified from CDPN<ref type="bibr" target="#b28">[29]</ref>. with spatial Softmax and global weight scaling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>?</head><label></label><figDesc>The original CDPN heavily relies on direct position regression, and the performance drops greatly (-17.46) when reduced to a pure PnP estimator, although the LM solver partially recovers the mean metric (+6.29). ? Employing EPro-PnP with the KL divergence loss significantly improves the metric (+13.84), outperforming CDPN-Full by a clear margin (65.88 vs. 63.21). ? The regularization loss proposed in Eq. (10) further elevates the performance (+1.88).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Visualization of the predicted pose distribution. The orientation density is clearly multimodal, capturing the pose ambiguity of symmetric objects (Barrier, Cone) and uncertain observations (Pedestrian). ? Regarding the mATE and mAOE metrics that reflect pose accuracy, the basic EPro-PnP already outperforms all previous methods, again demonstrating that EPro-PnP is a better pose estimator. The coordinate regression loss helps further reducing the orientation error (mAOE 0.337 vs. 0.363). ? With TTA, EPro-PnP outperforms the state of the art by a clear margin (NDS 0.439 vs. 0.422) on the validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Visualization of the inferred weight and coordinate maps on LineMOD test data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Detailed architecture of the deformable correspondence network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 .</head><label>10</label><figDesc>Testing accuracy vs. training progress on LineMOD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 .Figure 12 .Figure 13 .Figure 14 .</head><label>11121314</label><figDesc>Inferred results on LineMOD test set by EPro-PnP with derivative regularization and pretrained CDPN weights, Part I. input orienta?on Inferred results on LineMOD test set by EPro-PnP with derivative regularization and pretrained CDPN weights, Part II. Inferred orientation on nuScenes validation set by the Basic EPro-PnP. 2D points colored by instance 2D points colored by XY component density of (X: red, Y: green) Inferred 3D bounding boxes Inferred bounding boxes (red), posi on density (blue), and ground truth bounding boxes (green) in bird's eye view Inferred results on nuScenes validation set by the Basic EPro-PnP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>3 R 2 R 2 +R 3 2</head><label>32232</label><figDesc>Coordinate vector of the i-th 3D object point x 2D i ? Coordinate vector of the i-th 2D image point w 2D i ? Weight vector of the i-th 2D-3D point pair X The set of weighted 2D-3D correspondences y Object pose y gt Ground truth of object pose y * Object pose estimated by the PnP solver R 3?3 rotation matrix representation of object orientation ? 1D yaw angle representation of object orientation l Unit quaternion representation of object orientation t ? Translation vector representation of object position ? y * Pose covariance estimated by the PnP solver J Jacobian matrix J Rescaled Jacobian matrix F Concatenated vector of weighted reprojection errors of all points F Concatenated vector of rescaled weighted reprojection errors of all points ?(?) : R 3 ? R 2 Camera projection function f i (y) ? R 2 Weighted reprojection error of the i-th correspondence at pose y r i (y) ? R Unweighted reprojection error of the i-th correspondence at pose y ?(?) Huber kernel function ? i The derivative of the Huber kernel function of the i-th correspondence ? The Huber threshold p(X|y) Likelihood function of object pose p(y) PDF of the prior pose distribution p(y|X) PDF of the posterior pose distribution t(y) PDF of the target pose distribution q(y), q t (y) PDF of the proposal pose distribution (of the t-th AMIS iteration) y j , y t j The j-th random pose sample (of the t-th AMIS iteration) v j , v t j Importance weight of the j-th pose sample (of the t-th AMIS iteration) i Index of 2D-3D point pair j Index of random pose sample t Index of AMIS iteration N Number of 2D-3D point pairs in total K Number of pose samples in total T Number of AMIS iterations K Number of pose samples per AMIS iteration n head Number of heads in the deformable correspondence network n hpts Number of points per head in the deformable correspondence network L KL KL divergence loss for object pose L tgt The component of L KL concerning the reprojection errors at target pose L pred The component of L KL concerning the reprojection errors over predicted pose L reg Derivative regularization loss</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>. All results on the validation/test sets are Full [29] 29.10 69.50 91.03 63.21 A1 CDPN w/o trans. head 15.93 46.79 74.54 45.75 (?17.46) A2 + Batch=32, LM solver 21.17 55.00 79.96 52.04 (+ 6.29) B0 Basic EPro-PnP 32.14 72.83 92.66 65.88 (+13.84) B1 + Regularize derivatives 35.44 74.41 93.43 67.76 (+ 1.88) B2 + Initialize from A1 42.92 80.98 95.76 73.22 (+ 5.46) B3 + Long sched. (320 ep.) 44.81 81.96 95.80 74.19 (+ 0.97) Comparison to the CDPN baseline with Ablation Studies. Results of CDPN are reproduced with the official code.<ref type="bibr" target="#b3">4</ref> In C0/C1 either component is detached individually from the KL loss, while adding a surrogate mask regression loss<ref type="bibr" target="#b28">[29]</ref> in C1.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">ADD(-S)</cell><cell></cell></row><row><cell>ID Method</cell><cell></cell><cell cols="3">0.02d 0.05d 0.1d</cell><cell cols="2">Mean</cell></row><row><cell cols="2">A0 CDPN-C0 B0 ? Detach coords.</cell><cell cols="5">29.57 68.61 90.23 62.80 (? 3.08)</cell></row><row><cell cols="2">C1 B0 ? Detach weights</cell><cell cols="5">22.99 61.31 87.27 57.19 (? 8.69)</cell></row><row><cell cols="3">D0 B0 ? No Softmax denom.</cell><cell cols="3">divergence</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ADD(-S)</cell></row><row><cell>Method</cell><cell cols="3">2?, 2 cm 5?, 5 cm</cell><cell cols="3">0.02d 0.05d 0.1d</cell></row><row><cell>CDPN [29]</cell><cell>-</cell><cell></cell><cell>94.31</cell><cell>-</cell><cell>-</cell><cell>89.86</cell></row><row><cell>HybridPose [40]</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>91.3</cell></row><row><cell>GDRNet* [45]</cell><cell>67.1</cell><cell></cell><cell>-</cell><cell>35.6</cell><cell cols="2">76.0 93.6</cell></row><row><cell>DPOD [52]</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>95.15</cell></row><row><cell>PVNet-RePOSE [24]</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>96.1</cell></row><row><cell>EPro-PnP</cell><cell cols="2">80.99</cell><cell>98.54</cell><cell cols="3">44.81 81.96 95.80</cell></row><row><cell>Main Loss</cell><cell>Coord. Regr.</cell><cell></cell><cell cols="3">2?2 cm 2?, 2 cm</cell><cell>ADD(-S) 0.1d</cell></row><row><cell>Implicit diff. [12]</cell><cell></cell><cell></cell><cell cols="3">divergence</cell></row><row><cell>Reprojection [13]</cell><cell></cell><cell></cell><cell>0.32 42.30</cell><cell cols="2">0.16</cell><cell>14.56</cell></row><row><cell>Monte Carlo (ours)</cell><cell></cell><cell></cell><cell>44.18 81.55</cell><cell cols="2">40.96</cell><cell>79.46</cell></row><row><cell>Implicit diff. [12]</cell><cell></cell><cell></cell><cell>56.13 91.13</cell><cell cols="2">53.33</cell><cell>88.74</cell></row><row><cell>Reprojection [13]</cell><cell></cell><cell></cell><cell>62.79 92.91</cell><cell cols="2">60.65</cell><cell>92.04</cell></row><row><cell>Monte Carlo (ours)</cell><cell></cell><cell></cell><cell>65.75 93.90</cell><cell cols="2">63.80</cell><cell>92.66</cell></row></table><note>Table 2. Comparison to the state-of-the-art geometric meth- ods. BPnP [12] is not included as it adopts a different train/test split. *Although GDRNet [45] only reports the performance in its ablation section, it is still a fair comparison to our method, since both use the same baseline (CDPN).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>3D object detection results on the nuScenes benchmark. Methods with extra pretraining other than ImageNet backbone are not included for comparison.</figDesc><table /><note>? indicates test-time flip augmentation (TTA). ? indicates model ensemble.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Inference time (sec) of the deformable correspondence network on nuScenes object detection dataset [8]. The PnP solver (including the random sampling initialization in Section A.4) works faster (26 ms) with PyTorch v1.8.1, for which the code was originally developed, while the full model works faster (301 ms) with PyTorch v1.10.1.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://git.io/JXZv6</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keir</forename><surname>Mierle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Others</forename><surname>Ceres</surname></persName>
		</author>
		<ptr target="http://ceres-solver.org.11" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pyro: Deep Universal Probabilistic Programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Bingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jankowiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fritz</forename><surname>Obermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neeraj</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theofanis</forename><surname>Karaletsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Szerlip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Horsfall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Mixture density networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dsac -differentiable ransac for camera localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Uncertainty-driven 6d pose estimation of objects and scenes from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Ying</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning less is more -6d camera localization via 3d surface regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Slobodan Ilic, and Nassir Navab. 6d camera relocalization in ambiguous scenes via continuous multimodal inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mai</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haowen</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shadi</forename><surname>Albarqouni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">nuscenes: A multimodal dataset for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venice</forename><forename type="middle">Erin</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anush</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giancarlo</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Solving the blind perspective-n-point problem end-to-end with robust differentiable geometric optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep manta: A coarse-to-fine many-task network for joint 2d and 3d vehicle analysis from monocular image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Chabot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Chaouch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaonary</forename><surname>Rabarisoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?line</forename><surname>Teuli?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Chateau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">End-to-end learnable geometric vision by backpropagating pnp optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Parra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiewei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Jun</forename><surname>Chin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Monorun: Monocular 3d object detection by reconstruction and uncertainty propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hansheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adaptive multiple importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Marie</forename><surname>Cornuet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Michel</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonietta</forename><surname>Mira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><forename type="middle">P</forename><surname>Robert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scandinavian Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Modeling data using directional distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Inderjit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suvrit</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep orientation uncertainty learning based on a bingham loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Gilitschenski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roshni</forename><surname>Sahoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilko</forename><surname>Schwarting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sertac</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep declarative networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><forename type="middle">John</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bounding box regression with uncertainty for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianren</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multimodal templates for real-time detection of texture-less objects in heavily cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Holzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cedric</forename><surname>Cagniart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Repose: Fast 6d object pose refinement via deep texture rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Shun Iwase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rawal</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rio</forename><surname>Khirodkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><forename type="middle">M</forename><surname>Yokota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">What uncertainties do we need in bayesian deep learning for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Epnp: An accurate o(n) solution to the pnp problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal Of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rtm3d: Real-time monocular 3d detection from object keypoints for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaici</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feidao</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cdpn: Coordinates-based disentangled pose network for real-time rgb-based 6-dof object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Overcoming limitations of mixture density networks: A sampling and fitting framework for multimodal future prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osama</forename><surname>Makansi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozgun</forename><surname>Cicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Explaining the ambiguity of object detection and 6d pose from visual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><forename type="middle">Mart?n</forename><surname>Arroyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Busam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">3d bounding box estimation using deep learning and geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsalan</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Kosecka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Is pseudo-lidar needed for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rares</forename><surname>Ambrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitor</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pix2pose: Pixel-wise coordinate regression of objects for 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiru</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Patten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Vincze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pvnet: Pixel-wise voting network for 6dof pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">BB8: A scalable, accurate, robust to partial occlusion method for predicting the 3d poses of challenging objects without using depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Robust pose estimation from a planar target</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Schweighofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Disentangling monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Simonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>L?pez-Antequera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Hybridpose: 6d object pose estimation under hybrid representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaru</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Bundle adjustment: A modern synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">F</forename><surname>Mclauchlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">I</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Vision Algorithms: Theory and Practice</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Statistical analysis for the angular central gaussian distribution on the sphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">E</forename><surname>Tyler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Gdr-net: Geometry-guided direct regression network for monocular 6d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Normalized object coordinate space for category-level 6d object pose and size estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">FCOS3D: Fully convolutional one-stage monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinge</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Probabilistic and geometric depth: Detecting objects in perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinge</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning (CoRL)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Abhinav Gupta, and Kaiming He. Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Detr3d: 3d object detection from multi-view images via 3d-to-2d queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitor</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning (CoRL)</title>
		<imprint>
			<biblScope unit="page" from="2021" to="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Unsupervised learning of probably symmetric deformable 3d objects from images in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangzhe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Dpod: 6d pose object detector and refiner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zakharov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Shugurov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2021</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
