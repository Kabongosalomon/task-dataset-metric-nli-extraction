<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Stereo R-CNN based 3D Object Detection for Autonomous Driving</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiliang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<postCode>2 DJI</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<postCode>2 DJI</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<postCode>2 DJI</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Stereo R-CNN based 3D Object Detection for Autonomous Driving</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a 3D object detection method for autonomous driving by fully exploiting the sparse and dense, semantic and geometry information in stereo imagery. Our method, called Stereo R-CNN, extends Faster R-CNN for stereo inputs to simultaneously detect and associate object in left and right images. We add extra branches after stereo Region Proposal Network (RPN) to predict sparse keypoints, viewpoints, and object dimensions, which are combined with 2D left-right boxes to calculate a coarse 1 3D object bounding box. We then recover the accurate 3D bounding box by a region-based photometric alignment using left and right RoIs. Our method does not require depth input and 3D position supervision, however, outperforms all existing fully supervised image-based methods. Experiments on the challenging KITTI dataset show that our method outperforms the state-of-the-art stereobased method by around 30% AP on both 3D detection and 3D localization tasks. Code has been released at https://github.com/HKUST-Aerial-Robotics/Stereo-RCNN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>3D object detection serves as an essential basis of visual perception, motion prediction, and planning for autonomous driving. Currently, most of the 3D object detection methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">18]</ref> heavily rely on LiDAR data for providing accurate depth information in autonomous driving scenarios. However, LiDAR has the disadvantage of high cost, relatively short perception range (?100 m), and sparse information (32, 64 lines comparing to &gt;720p images). On the other hand, monocular camera provides alternative low-cost solutions <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b27">27]</ref> for 3D object detection. The depth information can be predicted by semantic properties in scenes and object size, etc. However, the inferred depth cannot guarantee the accuracy, especially for unseen scenes. To this end, we propose a stereo-vision based 3D object detection method. Comparing with monocular cam-era, stereo camera provides more precise depth information by left-right photometric alignment. Comparing with Li-DAR, stereo camera is low-cost while achieving comparable depth accuracy for objects with non-trivial disparities. The perception range of stereo camera depends on the focal length and the baseline. Therefore, stereo vision has the potential ability to provide larger-range perception by combining different stereo modules with different focal length and baselines.</p><p>In this work, we study the sparse and dense constraints for 3D objects by fully exploiting the semantic and geometry information in stereo imagery and propose an accurate Stereo R-CNN based 3D object detection method. Our method simultaneously detects and associates objects for left and right images using the proposed Stereo R-CNN. The network architecture can be overviewed in <ref type="figure" target="#fig_0">Fig.1</ref>, which can be divided into three main parts. The first one is a Stereo RPN module (Sect. 3.1) which outputs corresponding left and right RoI proposals. After applying RoIAlign <ref type="bibr" target="#b7">[8]</ref> on left and right feature maps respectively, we concatenate leftright RoI features to classify object categories and regress accurate 2D stereo boxes, viewpoint, and dimensions in the stereo regression (Sect. 3.2) branch. A keypoint (Sect. <ref type="bibr">3.2)</ref> branch is employed to predict object keypoints using only left RoI feature. These outputs form the sparse constraints (2D boxes, keypoints) for the 3D box estimation (Sect. 4), where we formulate the projection relations between 3D box corners with 2D left-right boxes and keypoints.</p><p>The crucial component that ensures our 3D localization performance is the dense 3D box alignment (Sect. 5). We consider 3D object localization as a learning-aided geometry problem rather than an end-to-end regression problem. Instead of directly using the depth input <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b27">27]</ref> which does not explicitly utilize the object property, we treat the object RoI as an entirety rather than independent pixels. For regular-shaped objects, the depth relation between each pixel and the 3D center can be inferred given the coarse 3D bounding box. We warp dense pixels in the left RoI to the right image according to their depth relations with the 3D object center to find the best center depth that minimizes the entire photometric error. The entire object RoI thereby forms the dense constraint for 3D object depth estimation. The 3D box is further rectified using 3D box estimator (Sect. 4) according to the aligned depth and 2D measurements.</p><p>We summarize our main contributions as follows:</p><p>? A Stereo R-CNN approach which simultaneously detects and associates object in stereo images.</p><p>? A 3D box estimator which exploits the keypoint and stereo boxes constraints.</p><p>? A dense region-based photometric alignment method that ensures our 3D object localization accuracy.</p><p>? Evaluation on the KITTI dataset shows we outperform all state-of-the-art image-based methods and are even comparable with a LiDAR-based method <ref type="bibr" target="#b16">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We briefly review recent works of 3D object detection based on the LiDAR data, monocular image and stereo images respectively.</p><p>LiDAR-based 3D Object Detection. Most of the stateof-the-art 3D object detection methods rely on LiDAR to provide accurate 3D information, while process raw LiDAR input in different representations. <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b12">13]</ref> project the point cloud into 2D bird's eye view or front view representations and feed them into the structured convolution network, where <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b12">13]</ref> exploit fusing multiple LiDAR representations with the RGB image to obtain more dense information. <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b31">31]</ref> utilize structured voxel grid representation to quantize the raw point cloud data, then use either 2D or 3D CNN to detect 3D object, while <ref type="bibr" target="#b20">[20]</ref> takes multiple frames as input and generates 3D detection, tracking and motion forecasting simultaneously. Additionally, instead of quantizing the point cloud, <ref type="bibr" target="#b23">[23]</ref> directly takes raw point cloud as input to localize 3D object based on the frustum region reasoned from 2D detection and PointNet <ref type="bibr" target="#b24">[24]</ref>.</p><p>Monocular-based 3D Object Detection. <ref type="bibr" target="#b2">[3]</ref> focuses on 3D object proposals generation using ground-plane assumption, shape prior, contextual feature and instance segmentation from the monocular image. <ref type="bibr" target="#b21">[21]</ref> proposes to estimate 3D box using the geometry relations between 2D box edges and 3D box corners. <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b22">22]</ref> explicitly utilize sparse information by predicting series of keypoints of regular-shape vehicles. The 3D object pose can be constrained by wireframe template fitting. <ref type="bibr" target="#b27">[27]</ref> proposes an end-to-end multilevel fusion approach to detect 3D object by concatenating the RGB image and the monocular-generated depth map. Recently an inverse-graphics framework <ref type="bibr" target="#b13">[14]</ref> is proposed to predict both the 3D object pose and instance-level segmentation by graphic rendering and comparing. However, monocular-based methods unavoidably suffer from the lack of accurate depth information.</p><p>Stereo-based 3D Object Detection. There are surprisingly only a few works exploit utilizing stereo vision for 3D object detection. 3DOP <ref type="bibr" target="#b3">[4]</ref> focuses on generating 3D proposals by encoding object size prior, ground-plane prior and depth information (e.g., free space, point cloud density) into an energy function. 3D Proposals are then used to regress the object pose and 2D boxes using the R-CNN approach. <ref type="bibr" target="#b17">[17]</ref> extends the Structure from Motion (SfM) approach to the dynamic object case and continuously track the 3D object and ego-camera pose by fusing both spatial and temporal information. However, none of the above approaches takes advantage of dense object constraints in raw stereo images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Stereo R-CNN Network</head><p>In this section, we describe the Stereo R-CNN network architecture. Compared with the single frame detector such as Faster R-CNN <ref type="bibr" target="#b25">[25]</ref>, Stereo R-CNN can simultaneously detect and associate 2D bounding boxes for left and right images with minor modifications. We use weight-share ResNet-101 <ref type="bibr" target="#b8">[9]</ref> and FPN <ref type="bibr" target="#b19">[19]</ref> as our backbone network to extract consistent features on left and right images. Benefit from our training target design <ref type="figure" target="#fig_1">Fig. 2</ref>, there is no additional computation for data association.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Stereo RPN</head><p>Region Proposal Network (RPN) <ref type="bibr" target="#b25">[25]</ref> is a slidingwindow based foreground detector. After feature extraction, a 3 ? 3 convolution layer is utilized to reduce channel, followed by two sibling fully-connected layer to classify objectness and regress box offsets for each input location which is anchored with pre-define multiple-scale boxes. Similar with FPN <ref type="bibr" target="#b19">[19]</ref>, we modify origin RPN for pyramid features by evaluating anchors on multiple-scale feature maps. The difference is we concatenate left and right feature maps at each scale, then we feed the concatenated features into the stereo RPN network.</p><p>The key design enables our simultaneous object detection and association is the different ground-truth (GT) box assignment for objectness classifier and stereo box regressor. As illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>, we assign the union of left and right GT boxes (referred as union GT box) as the target for objectness classification. An anchor is assigned a positive label if its Intersection-over-Union (IoU) ratio with one of union GT boxes is above 0.7, and a negative label if its IoU with any of union boxes is below 0.3. Benefit from this design, the positive anchors tend to contain both left and right object regions. We calculate offsets of positive anchors respecting to the left and right GT boxes contained in the target union GT box, then assign offsets to the left and right regression respectively. There are six regressing terms for the stereo regressor: [?u, ?w, ?u , ?w , ?v, ?h], where we use u, v to denote the horizontal and vertical coordinates of the 2D box center in image space, w, h for width and height of the box, and the superscript (?) for corresponding terms in the right image. Note that we use same v, h offsets ?v, ?h for the left and right boxes because we use rectified stereo images. Therefore we have six output channels for stereo RPN regressor instead of four in the origin RPN implementation. Since the left and right proposals are generated from the same anchor and share the objectness score, they can be associated naturally one by one. We utilize Non-Maximum Suppression (NMS) on left and right RoIs separately to reduce redundancy, then choose top 2000 candidates from entries which are kept in both left and right NMS for training. For testing, we choose only top 300 candidates.</p><formula xml:id="formula_0">! " # " # $ ! $ % ! " # " % &amp; ! " # " % &amp; &amp; = 0</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Stereo R-CNN</head><p>Stereo Regression. After stereo RPN, we have corresponding left-right proposal pairs. We apply RoI Align <ref type="bibr" target="#b7">[8]</ref> on the left and right feature maps respectively at appropriate pyramid level. The left and right RoI features are concatenated and fed into two sequential fully-connected layers (each followed by a ReLU layer) to extract semantic information. We use four sub-branches to predict object class, stereo bounding boxes, dimension, and viewpoint angle respectively. The box regression terms are same as defined in Sect. 3.1. Note that the viewpoint angle is not equal to the object orientation which is unobservable from cropped image RoI. An example is illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>, where we use ? to denote the vehicle orientation respecting to the camera frame, and ? to denote the object azimuth respecting to the camera center. Three vehicles have different orientations, however, the projection of them are exactly the same on cropped RoI images. We therefore regress the viewpoint angle ? defined as: ? = ? + ?. To avoid the discontinuity, the training targets are [sin ?, cos ?] pair instead of the raw angle value. With stereo boxes and object dimension, the depth information can be recovered intuitively, and the vehicle orientation can also be solved by decoupling the relations between the viewpoint angle with the 3D position. When sampling the RoIs, we consider a left-right RoI pair as foreground if the maximum IoU between the left RoI with left GT boxes is higher than 0.5, meanwhile the IoU between right RoI with the corresponding right GT box is also higher than 0.5. A left-right RoI pair is considered as background if the maximum IoU for either the left RoI or the right RoI lies in the [0.1, 0.5) interval. For foreground RoI pairs, we assign regression targets by calculating offsets between the left RoI with the left GT box, and offsets between the right RoI with the corresponding right GT box. We still use the same ?v, ?h for left and right RoIs. For dimension prediction, we simply regress the offset between the ground-truth dimension with a pre-set dimension prior.</p><p>Keypoint Prediction. Besides stereo boxes and viewpoint angle, we notice that the 3D box corner which projected in the box middle can provide more rigorous constraints to the 3D box estimation. As <ref type="figure" target="#fig_3">Fig. 4</ref> presents, we define four 3D semantic keypoints which indicate four corners at the bottom of the 3D bounding box. There is only one 3D semantic keypoint can be visibly projected to the box middle (instead of left or right edges). We define the projection of this semantic keypoint as perspective keypoint. We show how the perspective keypoint contributes to the 3D box estimation in Sect. 4 and <ref type="table">Table.</ref> 5. We also predict two boundary keypoints which serve as simple alternatives to instance mask for regular-shaped objects. Only the region between two boundary keypoints belongs to the current object and will be used for the further dense alignment (See Sect. 5).</p><p>We predict the keypoint as proposed in Mask R-CNN <ref type="bibr" target="#b7">[8]</ref>. Only the left feature map is used for keypoint prediction. We feed the 14 ? 14 RoI aligned feature maps to six sequential 256-d 3 ? 3 convolution layers as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, each followed by a ReLU layer. A 2 ? 2 deconvolution layer is used to upsample the output scale to 28 ? 28.</p><p>We notice that only u coordinate of the keypoints provide additional information besides the 2D box. To relax the task, we sum the height channel in the 6 ? 28 ? 28 output to produce 6 ? 28 prediction. As a result, each column in the RoI feature will be aggregated and contribute to the keypoint prediction. The first four channels repre-sent the probability that each of four semantic keypoints is projected to the corresponding u location. The other two channels represent the probability of each u lies in the left and right boundary respectively. Note that only one of four 3D keypoints can be visibly projected to the 2D box middle, thereby softmax is applied to the 4 ? 28 output to encourage that one exclusive semantic keypoint is projected to a single location. This strategy avoids the probable confusion of perspective keypoint type (corresponding to which of semantic keypoints). For the left and right boundary keypoints, we apply softmax on the 1 ? 28 outputs respectively.</p><p>During training, we minimize the cross-entropy loss over 4 ? 28 softmax output for perspective keypoint prediction. Only a single location in the 4 ? 28 output is labeled as perspective keypoint target. We omit the case where no 3D semantic keypoint is visibly projected in the box middle (e.g., truncation and orthographic projection cases). For boundary keypoints, we minimize the cross-entropy loss over two 1 ? 28 softmax outputs independently. Each foreground RoI will be assigned the left and right boundary keypoints according to the occlusion relations between GT boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">3D Box Estimation</head><p>In this section, we solve a coarse 3D bounding box by utilizing the sparse keypoint and 2D box information. States of the 3D bounding box can be represented by x = {x, y, z, ?}, which denotes the 3D center position and horizontal orientation respectively. Given the left-right 2D boxes, perspective keypoint, and regressed dimensions, the 3D box can be solved by minimize the reprojection error of 2D boxes and the keypoint. As detailed in <ref type="figure" target="#fig_4">Fig. 5</ref>, we extract seven measurements from stereo boxes and perspective keypoints: z = {u l , v t , u r , v b , u l , u r , u p }, which represent left, top, right, bottom edges of the left 2D box, left, right edges of the right 2D box, and the u coordinate of the perspective keypoint. Each measurement is normalized by camera intrinsic for simplifying representation. Given the perspective keypoint, the correspondences between 3D box corners and 2D box edges can be inferred (See dotted lines in <ref type="figure" target="#fig_4">Fig. 5</ref>). Inspired from <ref type="bibr" target="#b17">[17]</ref>, we formulate the 3D-2D relations by projection transformations. In such a viewpoint in <ref type="figure" target="#fig_4">Fig. 5</ref>:</p><formula xml:id="formula_1">v t = (y ? h 2 )/(z ? w 2 sin? ? l 2 cos?), u l = (x ? w 2 cos? ? l 2 sin?)/(z + w 2 sin? ? l 2 cos?), u p = (x + w 2 cos? ? l 2 sin?)/(z ? w 2 sin? ? l 2 cos?), . . . u r = (x ? b + w 2 cos? + l 2 sin?)/(z ? w 2 sin? + l 2 cos?).<label>(1)</label></formula><p>We use b to denote the baseline length of the stereo camera, and w, h, l for regressed dimensions. There are to- tal seven equations corresponding to seven measurements, where the sign of { w 2 , l 2 } should be changed appropriately based on the corresponding 3D box corner. Truncated edges are dropped on above seven equations. These multivariate equations are solved via Gauss-Newton method. Different from <ref type="bibr" target="#b17">[17]</ref> using single 2D box and size prior to solve the 3D position and orientation, we recover the 3D depth information more robustly by jointly utilizing the stereo boxes and regressed dimensions. In some cases where less than two side-surfaces can be completely observed and no perspective keypoint u p (e.g., truncation, orthographic projection), the orientation and dimensions are unobservable from pure geometry constraints. We use the viewpoint angle ? to compensate the unobservable states (See <ref type="figure" target="#fig_2">Fig. 3</ref> for the illustration):</p><formula xml:id="formula_2">? = ? + arctan(? x z ).<label>(2)</label></formula><p>Solved from 2D boxes and the perspective keypoint, the coarse 3D box has accurate projection and is well aligned with the image, which enables our further dense alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Dense 3D Box Alignment</head><p>The left and right bounding boxes provide object-level disparity information such that we can solve the 3D bounding box roughly. However, the stereo boxes are regressed by aggregating the high-level information in a 7 ? 7 RoI feature maps. The pixel-level information (e.g., corners, edges) contained in original image is lost due to multiple convolution filters. To achieve sub-pixel matching accuracy, we retrieve the raw image to exploit the pixel-level high-resolution information. Note that our task is different with pixel-wise disparity estimation problem where the result might encounter either discontinuity at ill-posed regions (SGM <ref type="bibr" target="#b9">[10]</ref>), or oversmooth at edge areas (CNN based methods <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b1">2]</ref>). We only solve the disparity of the 3D bounding box center while using the dense object patch, i.e., we use plenty of pixel measurements to solve one single variable.</p><p>Treating the object as a regular-shaped cube, we know the depth relation between each pixel with the center of 3D bounding box solved from Sect. 4. To exclude the pixel belonging to the background or other objects, we define a valid RoI as the region is between the left-right boundary keypoints and lies in the bottom halves of the 3D box since the bottom halves of vehicles fits the 3D box more tightly (See <ref type="figure" target="#fig_0">Fig. 1</ref>). For a pixel located at the normalized coordinate (u i , v i ) in the valid RoI of the left image, the photometric error can be defined as:</p><formula xml:id="formula_3">e i = I l (u i , v i ) ? I r (u i ? b z+?zi , v i ) ,<label>(3)</label></formula><p>where we use I l , I r to denote the 3-channels RGB vector of left and right image respectively; ?z i = z i ? z the depth differences of pixel i with the 3D box center; and b the baseline length. z is the only objective variable we want to solve. We use bilinear interpolation to get sub-pixel value on the right image. The total matching cost is defined as the Sum of Squared Difference (SSD) over all pixels in the valid RoI:</p><formula xml:id="formula_4">E = N i=0 e i .<label>(4)</label></formula><p>The center depth z can be solved by minimizing the total matching cost E, we can enumerate the depth efficiently to find a depth that minimizes the cost. We initially enumerate 50 depth values around the initial value with 0.5-meter interval to get a rough depth and finally enumerate 20 depth values around the rough depth with 0.05-meter interval to get the accurately aligned depth. Afterwards, we rectify the entire 3D box using our 3D box estimator by fixing the aligned depth (See <ref type="table">Table.</ref> 6). Consider the object RoI as a geometric-constraint entirety, our dense alignment method naturally avoids the discontinuity and ill-posed problems in stereo depth estimation, and is robust to intensity variations and brightness dominant since each pixel in the valid RoI will contribute to the object depth estimation. Note that this method is efficient and can be a light-weight plug-in module for any image-based 3D detection to achieve depth rectifying. Although the 3D object does not fit the 3D cube rigorously, relative depth errors caused by the shape variation are much more trivial than the global depth. Therefore our geometry-constraint dense alignment provides accurate depth estimation of object center.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Implementation Details</head><p>Network. As implemented in <ref type="bibr" target="#b25">[25]</ref>  Training. We define the multi-task loss as:</p><formula xml:id="formula_5">L = w p cls L p cls + w p reg L p reg + w r cls L r cls + w r box L r box + w r ? L r ? + w r dim L r dim + +w r key L r key ,<label>(5)</label></formula><p>where we use (?) p , (?) r for representing RPN and R-CNN respectively, and the subscript box, ?, dim, key for the loss of stereo boxes, viewpoint, dimension, and keypotin respectively. Each loss is weighted by their uncertainty following <ref type="bibr" target="#b10">[11]</ref>. We flip and exchange the left and right image, meanwhile mirror the the viewpoint angle and keypoints respectively to form a new stereo imagery. The origin dataset is thereby doubled with different training targets. During training, we keep 1 stereo pair and 512 sampled RoIs in each mini-batch. We train the network using SGD with a weight decay of 0.0005 and a momentum of 0.9. The learning rate is initially set to 0.001 and reduced by 0.1 for every 5 epochs. We train 20 epochs with 2 days in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experiments</head><p>We evaluate our method on the challenging KITTI object detection benchmark <ref type="bibr" target="#b6">[7]</ref>. Following <ref type="bibr" target="#b3">[4]</ref>, we split 7481 training images into training set and validation set with roughly the same amount. To fully evaluate the performance of our Stereo R-CNN based approach, we conduct experiments using the 2D stereo recall, 2D detection, stereo association, 3D detection, and 3D localization metrics by comparing with state-of-the-art and self-ablation. Objects are divided into three difficulty regimes: easy, moderate and hard, according to their 2D box height, occlusion and truncation levels following the KITTI setting.</p><p>Stereo Recall and Stereo Detection. Our Stereo R-CNN aims to simultaneously detect and associate object for the left and right image. Besides evaluating the 2D Average Recall (AR) and 2D Average Precision (AP 2d ) on both the left and right images, we also define the stereo AR and stereo AP metrics, where only querying stereo boxes fulfill the following conditions can be considered as the True Positives (TPs):</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">The maximum IoU of the left box with left GT boxes</head><p>is higher than the given threshold;</p><p>2. The maximum IoU of the right box with right GT boxes is higher than the given threshold;</p><p>3. The selected left and right GT boxes belong to the same object.</p><p>The stereo AR and stereo AP metrics jointly evaluate the 2D detection and association performance together. As <ref type="table">Table.</ref> 1 shows, our Stereo R-CNN has similar proposal recall and detection precision on the single image comparing with Faster R-CNN, while producing high-quality data association in left and right image without additional computation.</p><p>Although the stereo AR is slightly less than left AR in RPN, we observe almost the same left, right, and stereo APs after R-CNN, which indicates the consistent detection performance on the left and right image and nearly all true positive boxes in the left image have corresponding true-positive right boxes. We also test two strategies for left-right feature fusion: element-wise mean and channel concatenation. As reported in <ref type="table">Table.</ref> 1, the channel concatenation shows better performance since it keeps all the information. Accurate 3D Detection and 3D Localization. We evaluate our 3D detection and 3D localization performance using Average Precision for bird's eye view (AP bv ) and 3D box (AP 3d ). Results are shown in <ref type="table">Table.</ref> 2, where our method outperforms state-of-the-art monocular-based methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b27">27]</ref> and stereo-method <ref type="bibr" target="#b3">[4]</ref> by large margins. Specifically, we outperform 3DOP <ref type="bibr" target="#b3">[4]</ref> over 30% for both AP bv and AP 3d across easy and moderate sets. For the hard set, we achieve ?25% improvements. Although Multi-Fusion <ref type="bibr" target="#b27">[27]</ref> obtains significant improvements with stereo input, it still reports much lower AP bv and AP 3d than our geometric method in the moderate set. Since comparing our approach with LiDAR-based approaches is unfair, we only list one LiDAR-based method VeloFCN <ref type="bibr" target="#b16">[16]</ref> for reference, where we outperform it by ?10% AP bv and AP 3d using IoU = 0.5 in the moderate set. We also report evaluation results on the KITTI testing set in <ref type="table">Table.</ref> 3. The detailed performance can be found online. <ref type="bibr" target="#b1">2</ref> Note that the KITTI 3D detection benchmark is difficult for image-based method, for which the 3D performance tends to decrease as objects distance increases. This phenomenon can be observed intuitively in <ref type="figure" target="#fig_6">Fig. 7</ref>, although our method achieves sub-pixel disparity estimation (less than 0.5 pixel), the depth error becomes larger as the object distance increase due to the inversely proportional relation between disparity and depth. For objects with explicit disparity, we achieve high accurate depth estimation based on rigorous geometric constraints. That explains why a higher IoU threshold, an easier regime the object belongs, we obtain more improvements compared with other methods.</p><p>Benefits of the Keypoint. We utilize the 3D box estima-2 http://www.cvlibs.net/datasets/kitti/eval_ object.php?obj_benchmark=3d    calculated from box-level disparity and 2D box size. Even if 1-pixel disparity or 2D box error will cause large distance error for distant objects. In result, although the coarse 3D box has a precise projection on the image as we expected, it is not accurate enough for 3D localization. Detailed statistics can be found in <ref type="table">Table.</ref> 6. After we recover the object depth using the dense alignment and simply scaling the x, y (w/ Alignment, w/o 3D rectify), we obtain major improvements on all the metric. Furthermore, when we using the box estimator (Sect. 4) to rectify the entire 3D box by fixing the aligned depth, the 3D localization and 3D detection performance are further improved by several points.</p><p>Ablation Study. We employ two strategies to enhance our model performance. To validate the contributions of each strategy, we conduct experiments with different combinations and evaluate the detection and localization performance. As <ref type="table">Table.</ref> 4 shows, we use Flip and Uncert to represent the proposed stereo flip augmentation and the uncertainty weight for multiple losses <ref type="bibr" target="#b10">[11]</ref>. Without bells and whistles, we already outperform all state-of-the-art imagebased methods. Each strategy further enhances our network performance by several points. Detailed contributions can be found in <ref type="table">Table.</ref> 4. Balancing the multi-task loss using uncertainty weight yields non-trivial improvements in both 3D detection and localization tasks. With stereo flip augmentation, the left-right images are flipped and exchanged, and the training target for the the perspective keypoint and viewpoint are also changed respectively. Therefore the training set is doubled with different inputs and training targets. Combining two strategies together, our method obtains strongly promising performance in both 3D detection and 3D localization tasks <ref type="table">(Table.</ref> 2).</p><p>Qualitative Results. We show some qualitative results in <ref type="figure" target="#fig_5">Fig. 6</ref>, where we visualize corresponding stereo boxes on the left and right images. The 3D box is projected to the left and bird's eye view image respectively. Our joint sparse and dense constraints ensure the detected box is well aligned on both image and LiDAR point cloud.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion and Future Work</head><p>In this paper, we propose a Stereo R-CNN based 3D object detection method in autonomous driving scenarios. Formulating the 3D object localization as a learning-aided geometry problem, our approach takes the advantage of both semantic properties and dense constraints of objects. Without 3D supervision, we outperform all existing imagebased methods by large margins on 3D detection and 3D localization tasks, and even better than a baseline LiDAR method <ref type="bibr" target="#b16">[16]</ref>.</p><p>Our 3D object detection framework is flexible and practical where each module can be extended and further improved. For example, Stereo R-CNN can be extended for multiple object detection and tracking. We can replace the boundary keypoints with instance segmentation to provide more precise valid RoI selection. By learning the object shape, our 3D detection method can be further applied to general objects.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Network architecture of the proposed Stereo R-CNN (Sect. 3) which outputs stereo boxes, keypoints, dimensions, and the viewpoint angle, followed by the 3D box estimation (Sect. 4) and the dense 3D box alignment module (Sect. 5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Different targets assignment for RPN classification and regression.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Relations between object orientation ?, azimuth ? and viewpoint ? + ?. Only same viewpoints lead to same projections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Illustration of 3D semantic keypoints, the 2D perspective keypoint, and boundary keypoints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Sparse constraints for the 3D box estimation (Sect. 4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Qualitative results. From top to bottom: detections on left image, right image, and bird's eye view image. stereo detection and association provide sufficient box-level constraints for the 3D box estimation (Sect. 4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Relations between the disparity and the depth error with the object distance (best viewed in color</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>, we use five scale anchors of {32, 64, 128, 126, 512} with three ratios {0.5, 1, 2}. The original image is resized to 600 pixels in the shorter side. For Stereo RPN, we have 1024 input channels in the final classification and regression layer instead of 512 layers in the implementation [19] due to the concatenation of the left and right feature maps. Similarly, we have 512 input channels in the R-CNN regress head. The inference time of Stereo R-CNN for one stereo pair is around 0.28s on the Titan Xp GPU. CNN mean 85.50 85.56 74.60 90.58 88.42 71.24 90.59 88.47 71.28 90.53 88.24 71.12 Stereo R-CNN concat 86.20 86.27 75.51 98.73 88.48 71.26 98.71 88.50 71.28 98.53 88.27 71.14 Average recall (AR) (in %) of RPN and Average precision (AP) (in %) of 2D detection, evaluated on the KITTI validation set. We compare two fusion methods for Stereo-RCNN with Faster R-CNN using the same backbone network, hyper-parameters, and augmentation. The Average Recall is evaluated on the moderate set. 74.11 58.93 68.50 48.30 41.47 85.84 66.28 57.24 54.11 36.69 31.07</figDesc><table><row><cell>AP 2d (IoU=0.7)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table /><note>Average precision of bird's eye view (AP bv ) and 3D boxes (AP 3d ) comparison, evaluated on the KITTI validation set.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>3D detection and localization AP on the KITTI test set. tor (Sect. 4) to calculate the coarse 3D box and rectify the actual 3D box after the dense alignment. An accurate 3D box estimator is thereby important for the final 3D detection. To study benefits of the keypoint for 3D box estimator, we evaluate the 3D detection and 3D localization performance without using the keypoint, where we use the regressed viewpoint to determine the relations between 3D box corners and 2D box edges, and employ Eq. 2 to constraint the 3D orientation for all objects. As reported in Table. 5, the usage of the keypoint improve both AP bv and AP 3D across all difficulty regimes by non-trivial margins. As the keypoint provides pixel-level constraints to the 3D box corner in addition to the 2D box-level measurements, it ensures more accurate localization performance.76.82 64.75 54.72 54.38 36.45 29.74 75.05 60.83 47.69 32.30 21.52 17.61 79.78 78.24 65.94 56.01 60.93 40.33 33.89 76.87 61.45 48.18 40.22 28.74 23.96 88.52 84.89 67.02 57.57 60.93 40.91 34.48 78.76 64.99 55.72 47.53 30.36 25.25 88.82 87.13 74.11 58.93 68.50 48.30 41.47 85.84 66.28 57.24 54.11 36.69 31.07</figDesc><table><row><cell>). For each distance range</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Ablation study of using flip augmentations and uncertainty weight, evaluated on KITTI validation set. AP bv (IoU=0.5) 87.10 67.42 58.41 87.13 74.11 58.93 AP bv (IoU=0.7) 59.45 40.44 34.14 68.50 48.30 41.47 AP 3d (IoU=0.5) 85.21 65.23 55.75 85.84 66.28 57.24 AP 3d (IoU=0.7) 46.58 30.29 25.07 54.11 36.69 31.07</figDesc><table><row><cell></cell><cell>w/o Keypoint</cell><cell>w/ Keypoint</cell><cell></cell></row><row><cell>Metric</cell><cell>Easy Mode Hard</cell><cell>Easy Mode</cell><cell>Hard</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Comparing 3D detection and localization AP of w/o and w/ keypoint, evaluated on KITTI validation set. Mode 74.11 48.30 66.28 36.69 Hard 58.93 41.47 57.24 31.07Table 6. Improvements of using our dense alignment and 3D box rectify, evaluated on KITTI validation set.</figDesc><table><row><cell>Config</cell><cell>Set</cell><cell>AP 0.5 bv</cell><cell>AP 0.7 bv</cell><cell>AP 0.5 3d</cell><cell>AP 0.7 3d</cell></row><row><cell></cell><cell cols="5">Easy 45.59 16.87 41.88 11.37</cell></row><row><cell>w/o Alignment</cell><cell cols="4">Mode 33.82 10.40 27.99</cell><cell>7.75</cell></row><row><cell></cell><cell cols="4">Hard 28.96 10.03 22.80</cell><cell>5.74</cell></row><row><cell>w/ Alignment w/o 3D rectify</cell><cell cols="5">Easy 86.15 66.93 83.05 48.95 Mode 73.54 47.35 65.45 32.00</cell></row><row><cell></cell><cell cols="5">Hard 58.66 36.29 56.50 30.12</cell></row><row><cell>w/ Alignment</cell><cell cols="5">Easy 87.13 68.50 85.84 54.11</cell></row><row><cell>w/ 3D rectify</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We use the coarse 3D box to represent one with accurate 2D projection but not necessarily with accurate 3D position.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment. This work was supported by the Hong Kong Research Grants Council Early Career Scheme under project 26201616.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep manta: A coarse-to-fine many-task network for joint 2d and 3d vehicle analysis from monocular image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chabot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chaouch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rabarisoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Teuli?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chateau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit.(CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit.(CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2040" to="2049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pyramid stereo matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-R</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5410" to="5418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2147" to="2156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3d object proposals using stereo imagery for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TPAMI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Vote3deep: Fast object detection in 3d point clouds using efficient convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<editor>Robotics and Automation (ICRA</editor>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1355" to="1361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
	<note>Computer Vision (ICCV</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Stereo processing by semiglobal matching and mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hirschmuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="328" to="341" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>intelligence</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">End-to-end learning of geometry and context for deep stereo regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Martirosyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bry</surname></persName>
		</author>
		<idno>abs/1703.04309</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Joint 3d proposal generation and object detection from view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Waslander</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02294</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">3d-rcnn: Instance-level 3d object reconstruction via render-and-compare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3559" to="3568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">3d fully convolutional network for vehicle detection in point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems</title>
		<imprint>
			<publisher>IROS</publisher>
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<title level="m">IEEE/RSJ International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1513" to="1518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Vehicle detection from 3d lidar using fully convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stereo vision-based semantic 3d object and ego-motion tracking for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="664" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep continuous fusion for multi-sensor 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="663" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast and furious: Real time end-to-end 3d detection, tracking and motion forecasting with a single convolutional net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3569" to="3577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">3d bounding box estimation using deep learning and geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ko?eck?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5632" to="5640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Reconstructing vehicles from a single image: Shape priors for road scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chhaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="724" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Frustum pointnets for 3d object detection from rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08488</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">-d data. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Voting for voting in online point cloud object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-level fusion based 3d object detection from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pixor: Real-time 3d object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7652" to="7660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Stereo matching by training a convolutional neural network to compare image patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="32" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Are cars just 3d boxes?-jointly estimating the 3d shape of multiple objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3678" to="3685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.06396</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
