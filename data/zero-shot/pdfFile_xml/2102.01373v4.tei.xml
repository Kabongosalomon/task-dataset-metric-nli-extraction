<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Improved Baseline for Sentence-level Relation Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-09-22">22 Sep 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhao</forename><surname>Chen</surname></persName>
							<email>muhaoche@usc.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Southern</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Improved Baseline for Sentence-level Relation Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-09-22">22 Sep 2022</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sentence-level relation extraction (RE) aims at identifying the relationship between two entities in a sentence. Many efforts have been devoted to this problem, while the best performing methods are still far from perfect. In this paper, we revisit two problems that affect the performance of existing RE models, namely ENTITY REPRESENTATION and NOISY OR ILL-DEFINED LABELS. Our improved RE baseline, incorporated with entity representations with typed markers, achieves an F 1 of 74.6% on TACRED, significantly outperforms previous SOTA methods. Furthermore, the presented new baseline achieves an F 1 of 91.1% on the refined Re-TACRED dataset, demonstrating that the pretrained language models (PLMs) achieve high performance on this task. We release our code 1 to the community for future research.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As one of the fundamental information extraction (IE) tasks, relation extraction (RE) aims at identifying the relationship(s) between two entities in a given piece of text from a pre-defined set of relationships of interest. For example, given the sentence "Bill Gates founded Microsoft together with his friend Paul Allen in 1975" and an entity pair ("Bill Gates", "Microsoft"), the RE model is expected to predict the relation ORG:FOUNDED BY. On this task, SOTA models based on PLMs <ref type="bibr" target="#b4">(Devlin et al., 2019;</ref><ref type="bibr" target="#b8">Joshi et al., 2020)</ref> have gained significant success.</p><p>Recent work on sentence-level RE can be divided into two lines. One focuses on injecting external knowledge into PLMs. Methods of such, including ERNIE <ref type="bibr" target="#b25">(Zhang et al., 2019)</ref> and Know-BERT <ref type="bibr" target="#b15">(Peters et al., 2019)</ref>, take entity embedding pretrained from knowledge graphs as inputs to the Transformer. Similarly, K-Adapter <ref type="bibr">(Wang et al., 2020)</ref> introduces a plug-in neural adaptor that injects factual and linguistic knowledge into the language model. LUKE <ref type="bibr" target="#b22">(Yamada et al., 2020)</ref> further extends the pretraining objective of masked language modeling to entities and proposes an entity-aware self-attention mechanism. The other line of work focuses on continually pretraining PLMs on text with linked entities using relation-oriented objectives. Specifically, <ref type="bibr">BERT-MTB (Baldini Soares et al., 2019)</ref> proposes a matching-the-blanks objective that decides whether two relation instances share the same entities. Despite extensively studied, existing RE models still perform far from perfect. On the commonly-used benchmark TA-CRED <ref type="bibr" target="#b24">(Zhang et al., 2017)</ref>, the SOTA F 1 result only increases from 70.1% (BERT LARGE ) to 72.7% (LUKE) after applying PLMs to this task. It is unclear what building block is missing to constitute a promising RE system.</p><p>In this work, we discuss two obstacles that have hindered the performance of existing RE models. First, the RE task provides a structured input of both the raw texts and side information of the entities, such as entity names, spans, and types (typically provided by NER models), which are shown important to the performance of RE models <ref type="bibr" target="#b14">(Peng et al., 2020)</ref>. However, existing methods fall short of representing the entity information comprehensively in the text, leading to limited characterization of the entities. Second, humanlabeled RE datasets (e.g., TACRED), may contain a large portion of noisy or ill-defined labels, causing the model performance to be misestimated. <ref type="bibr" target="#b0">Alt et al. (2020)</ref> relabeled the development and test set of TACRED and found that 6.62% of labels are incorrect. <ref type="bibr" target="#b16">Stoica et al. (2021)</ref> refined many ill-defined relation types and further re-annotated the TACRED dataset using an improved annotation strategy to ensure high-quality labels. To this end, we propose an improved RE baseline, where we introduce the typed entity marker to sentencelevel RE, which leads to promising improvement of performance over existing RE models. We evaluate our model on TA-CRED <ref type="bibr" target="#b24">(Zhang et al., 2017)</ref>, TACREV <ref type="bibr" target="#b0">(Alt et al., 2020)</ref>, and Re-TACRED <ref type="bibr" target="#b16">(Stoica et al., 2021)</ref>. Using RoBERTa  as the backbone, our improved baseline model achieves an F 1 of 74.6% and 83.2% on TACRED and TACREV, respectively, significantly outperforming various SOTA RE models. Particularly, our baseline model achieves an F 1 of 91.1% on Re-TACRED, demonstrating that PLMs can achieve much better results on RE than shown in previous work. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>In this section, we first formally define the relation extraction task in Sec. 2.1, and then present our model architecture and entity representation techniques in Sec. 2.2 and Sec. 2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Definition</head><p>In this paper, we focus on sentence-level RE. Specifically, given a sentence x mentioning an entity pair (e s , e o ), referred as the subject and object entities, respectively, the task of sentence-level RE is to predict the relationship r between e s and e o from R ? {NA}, where R is a pre-defined set of relationships of interest. If the text does not express any relation from R, the entity pair will be accordingly labeled NA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Model Architecture</head><p>Our RE classifier is an extension of previous PLMbased RE models <ref type="bibr" target="#b2">(Baldini Soares et al., 2019)</ref>. Given the input sentence x, we first mark the entity spans and entity types using techniques presented in Sec. 2.3, then feed the processed sentence into a PLM to get its contextual embedding. Finally, we feed the hidden states of the subject and object entities in the language model's last layer, i.e., h subj and h obj , into the softmax classifier:</p><formula xml:id="formula_0">z = ReLU W proj h subj , h obj , P(r) = exp(W r z + b r ) r ? ?R?{NA} exp(W r ? z + b r ? ) , where W proj ? R 2d?d , W r , W r ? ? R d , b r , b r ? ? R are model parameters.</formula><p>In inference, the classifier returns the relationship with the maximum probability as the predicted relationship.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Entity Representation</head><p>For sentence-level RE, the names, spans, and NER types of subject and object entities are provided in the structured input. Such composite entity information provides useful clues to the relation types. For example, the relationship ORG:FOUNDED BY is more likely to hold when entity types of subject and object are ORGANIZATION and PERSON, respectively, and is less likely for instances where the entity types do not match. The entity information needs to be represented in the input text, allowing it to be captured by the PLMs. Such techniques have been studied in previous work <ref type="bibr" target="#b24">(Zhang et al., 2017;</ref><ref type="bibr" target="#b2">Baldini Soares et al., 2019;</ref><ref type="bibr">Wang et al., 2020)</ref>, while many of them fall short of capturing all types of the provided information. In this paper, we re-evaluate existing entity representation techniques and also seek to propose a better one. We evaluate the following techniques:</p><p>? Entity mask <ref type="bibr" target="#b24">(Zhang et al., 2017)</ref>. This technique introduces new special tokens [SUBJ-TYPE] or [OBJ-TYPE] to mask the subject or object entities in the original text, where TYPE is substituted with the respective entity type. This technique was originally proposed in the PA-LSTM model <ref type="bibr" target="#b24">(Zhang et al., 2017)</ref>, and was later adopted by PLMs such as SpanBERT <ref type="bibr" target="#b8">(Joshi et al., 2020)</ref>. <ref type="bibr" target="#b24">Zhang et al. (2017)</ref> claim that this technique prevents the RE model from over-fitting specific entity names, leading to more generalizable inference.</p><p>? Entity marker <ref type="bibr" target="#b25">(Zhang et al., 2019;</ref><ref type="bibr" target="#b2">Baldini Soares et al., 2019)</ref>. This technique introduces special tokens pairs [E1], [/E1] and [E2], [/E2] to enclose the subject and object entities, therefore modifying the input text to the format of "</p><formula xml:id="formula_1">[E1] SUBJ [/E1] ... [E2] OBJ [/E2]" 3 .</formula><p>? Entity marker (punct) <ref type="bibr">(Wang et al., 2020;</ref>. This technique is a variant of the previous technique that encloses entity spans using punctuation. It modifies the input text to "@ SUBJ @ ... # OBJ #". The main difference from the previous technique is that this one does not introduce new special tokens into the model's reserved vocabulary.</p><p>? Typed entity marker <ref type="bibr" target="#b26">(Zhong and Chen, 2021</ref> ? Typed entity marker (punct). We propose a variant of the typed entity marker technique that marks the entity span and entity types without introducing new special tokens. This is to enclose the subject and object entities with "@" and "#", respectively. We also represent the subject and object entity types using their label text, which is prepended to the entity spans and is enclosed by "*" for subjects or "?" for objects.</p><p>The modified text is "@ * subj-type * SUBJ @ ... # ? obj-type ? OBJ # ", where subj-type and obj-type is the label text of NER types.</p><p>The embedding of all new special tokens is randomly initialized and updated during fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, we evaluate the proposed techniques based on widely used RE benchmarks. The evaluation starts by first identifying the best-performing entity representation technique (Sec. 3.2), which is further incorporated into our improved RE baseline to be compared against prior SOTA methods (Sec. 3.3). Due to space limits, we study in the Appendix of how the incorporated techniques lead to varied generalizability on unseen entities (Appx. B) and how they perform under annotation errors (Appx. C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head><p>Datasets. The datasets we have used in the experiments include three versions of TA-CRED: the original TACRED <ref type="bibr" target="#b24">(Zhang et al., 2017)</ref>, TACREV <ref type="bibr" target="#b0">(Alt et al., 2020)</ref>, and Re-TACRED <ref type="bibr" target="#b16">(Stoica et al., 2021)</ref>. <ref type="bibr" target="#b0">Alt et al. (2020)</ref> observed that the TACRED dataset contains about 6.62% noisily-labeled instances and relabeled the development and test set. <ref type="bibr" target="#b16">Stoica et al. (2021)</ref> further refined the label definitions in TACRED and relabeled the whole dataset. We provide the statistics of the datasets in Appx. A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compared methods.</head><p>We compare with the following methods.</p><p>PA-LSTM <ref type="bibr" target="#b24">(Zhang et al., 2017)</ref> adopts bi-directional LSTM <ref type="bibr" target="#b7">(Hochreiter and Schmidhuber, 1997)</ref> and positional-aware attention <ref type="bibr" target="#b1">(Bahdanau et al., 2015)</ref> to encode the text into an embedding, which is then fed into a softmax layer to predict the relation. C-GCN <ref type="bibr" target="#b23">(Zhang et al., 2018)</ref> is a graph-based model, which feeds the pruned dependency tree of the sentence into the graph convolutional network <ref type="bibr" target="#b10">(Kipf and Welling, 2017)</ref> to obtain the representation of entities. Span-BERT <ref type="bibr" target="#b8">(Joshi et al., 2020</ref>) is a PLM based on the Transformer <ref type="bibr" target="#b17">(Vaswani et al., 2017)</ref>. It extends BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref> by incorporating a training objective of span prediction and achieves improved performance on RE.</p><p>KnowBERT <ref type="bibr" target="#b15">(Peters et al., 2019)</ref> jointly trains a language model and an entity linker, which allows the subtokens to attend to entity embedding that is pretrained on knowledge bases. LUKE <ref type="bibr" target="#b22">(Yamada et al., 2020)</ref> pretrains the language model on both large text corpora and knowledge graphs. It adds frequent entities into the vocabulary and proposes an entity-aware self-attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model configurations.</head><p>For the compared methods, we rerun their officially released code using the recommended hyperparameters in their papers.</p><p>Our model is implemented based on HuggingFace's Transformers <ref type="bibr" target="#b21">(Wolf et al., 2020)</ref>. Our model is optimized with Adam (Kingma and Ba, 2015) using the learning rate of 5e?5 on BERT BASE , and 3e?5 on BERT LARGE and RoBERTa LARGE , with a linear warm-up <ref type="bibr" target="#b5">(Goyal et al., 2017)</ref> of for the first 10% steps followed by a linear learning rate decay to 0. We use a batch size of 64 and fine-tune the model for 5 epochs on all datasets. For all experiments, we report the median F 1 of 5 runs of training using different random seeds.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Analysis on Entity Representation</head><p>We first provide an analysis on different entity representation techniques. In this analysis, we use the base and large versions of BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref> and the large version of RoBERTa  as the encoder. Tab. 1 shows the performance of the PLMs incorporated with different entity representation techniques. For each technique, we also provide an example of the processed text. We have several observations from the results. First, the typed entity marker and its variants outperform untyped entity representation techniques by a notable margin. Especially, the RoBERTa model achieves an F 1 score of 74.6% using the typed entity marker (punct), which is significantly higher than the SOTA result of 72.7% by LUKE <ref type="bibr" target="#b22">(Yamada et al., 2020)</ref>. This shows that representing all categories of entity information is helpful to the RE task. It also shows that keeping entity names in the input improves the performance of RE models. Second, symbols used in entity markers have an obvious impact on the performance of RE models. Although the original and punct versions of entity representation techniques represent the same categories of entity information, they do lead to a difference in model performance. Particularly, introducing new special tokens hinders the model performance drastically on RoBERTa. On RoBERTa LARGE , the entity marker underperforms the entity marker (punct) by 0.7%, the typed entity marker underperforms the typed entity marker (punct) by 3.6%, while the entity mask gets a much worse result of 60.9%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison with Prior Methods</head><p>The prior experiment has found RoBERTa LARGE with the typed entity marker (punct) to be the bestperforming RE model. We now compare our improved baseline with methods in prior studies.   <ref type="bibr" target="#b0">Alt et al. (2020)</ref>. ? marks reimplemented results from <ref type="bibr" target="#b16">Stoica et al. (2021)</ref>. ? marks our re-implemented results.</p><p>We evaluate all methods on TACRED, TACREV, and Re-TACRED. Incorporated with the typed entity marker (punct) and using RoBERTa LARGE as the backbone, our improved baseline model achieves new SOTA results over previous methods on all datasets. However, we observe that on Re-TACRED, the gain from the typed entity marker is much smaller compared to TACRED and TACREV, decreasing from 3.1 ? 3.9% and 2.0 ? 3.4% to 0.2 ? 0.8% of F 1 . This observation could be attributed to the high noise rate in TACRED, in which the noisy labels are biased towards the side information of entities.</p><p>To assess how the presented techniques contribute to robustness and generalizability of RE, we provide more analyses on varied generalizability on unseen entities (Appx. B) and the performance under annotation errors (Appx. C) in the Appendix.</p><p>In this paper, we present a simple yet strong RE baseline that offers new SOTA performance, along with a comprehensive study to understand its prediction generalizability and robustness. Specifically, we revisit two technical problems in sentence-level RE, namely entity representation and noisy or ill-defined labels. We propose an improved entity representation technique, which significantly outperforms existing sentencelevel RE models. Especially, our improved RE baseline achieves an F 1 score of 91.1% on the Re-TACRED dataset, showing that PLMs already achieve satisfactory performance on this task. We hope the proposed techniques and analyses can benefit future research on RE.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Dataset Statistics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Analysis on Unseen Entities</head><p>Some previous work <ref type="bibr" target="#b23">(Zhang et al., 2018;</ref><ref type="bibr" target="#b8">Joshi et al., 2020)</ref> claims that entity names may leak superficial clues of the relation types, allowing heuristics to hack the benchmark. They show that neural RE models can achieve high evaluation results only based on the subject and object entity names even without putting them into the original sentence. They also suggest that RE models trained without entity masks may not generalize well to unseen entities. However, as the provided NER types in RE datasets are usually coarse-grained, using entity masks may lose the meaningful information of entities. Using entity masks also contradicts later studies' advocacy of injecting entity knowledge into RE models <ref type="bibr" target="#b25">(Zhang et al., 2019;</ref><ref type="bibr" target="#b15">Peters et al., 2019;</ref><ref type="bibr">Wang et al., 2020)</ref>. If RE models should not consider entity names, it is unreasonable to suppose that they can be improved by external knowledge graphs.</p><p>To evaluate whether the RE model trained without entity mask can generalize to unseen entities, we propose a filtered evaluation setting. Specifically, we remove all test instances containing entities from the training set of TACRED, TACREV, and Re-TACRED. This results in filtered test sets of 4,599 instances on TACRED and TACREV, and 3815 instances on Re-TACRED. These filtered test sets only contain instances with unseen entities during training.</p><p>We present the evaluation results on the filtered test set in Tab. 4. We compare the performance of models with entity mask or typed entity marker representations, between which the only difference lies in whether to include entity names in entity representations or not. Note that as the label distributions of the original and filtered test set are different, their results are not directly comparable.   Still, the typed entity marker consistently outperforms the entity mask on all encoders and datasets, which shows that RE models can learn from entity names and generalize to unseen entities. Our finding is consistent with <ref type="bibr" target="#b14">Peng et al. (2020)</ref>, whose work suggests that entity names can provide semantically richer information than entity types to improve the RE model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Analysis on Annotation Errors</head><p>Our model achieves a smaller performance gain on Re-TACRED compared to TACRED and TACREV. We find that this difference can be mainly attributed to the annotation errors in their evaluation sets. Specifically, we create a clean TA-CRED test set by pruning all instances in the TA-CRED test set, of which the annotated relation is different in the Re-TACRED test set. The remaining instances are considered clean. Note that as the label sets of TACRED and Re-TACRED are different, instances of some classes cannot be found in Re-TACRED and are thus completely pruned. We train the model on the original (noisy) training set and show the results on the clean test set in Tab. 5. We observe a similar drop of performance gain on the clean TACRED test set. It shows that the annotation errors in TACRED and TACREV can lead to overestimation of the performance of models depending on the side information of entities. We hypothesize that in data annotation, much noise may be created as some annotators label the relation only based on the two entities without reading the whole sentence. Therefore, integrating NER types into the entity representation can brings larger performance gain. Overall, this experiment shows that the evaluation sets of both TACRED and TACREV are biased and unreliable. We recommend future work on sentence-level RE should use Re-TACRED as the evaluation benchmark.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>TYPE , /S:TYPE , O:TYPE , /O:TYPE , where TYPE is the corresponding NER type given by a named entity tagger. The input text is accordingly modified to " S:TYPE SUBJ /S:TYPE ...</figDesc><table><row><cell></cell><cell>).</cell></row><row><cell cols="2">This technique further incorporates the NER</cell></row><row><cell>types into entity markers.</cell><cell>It introduces</cell></row><row><cell>new special tokens S:</cell><cell></cell></row></table><note>O:TYPE OBJ /O:TYPE ".</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Bill [/E1] was born in [E2] Seattle [/E2].</figDesc><table><row><cell>Method</cell><cell>Input Example</cell><cell></cell><cell cols="3">BERT BASE BERT LARGE RoBERTa LARGE</cell></row><row><cell>Entity mask</cell><cell cols="2">[SUBJ-PERSON] was born in [OBJ-CITY].</cell><cell>69.6</cell><cell>70.6</cell><cell>60.9</cell></row><row><cell>Entity marker</cell><cell cols="3">[E1] 68.4</cell><cell>69.7</cell><cell>70.7</cell></row><row><cell>Entity marker (punct)</cell><cell cols="2">@ Bill @ was born in # Seattle #.</cell><cell>68.7</cell><cell>69.8</cell><cell>71.4</cell></row><row><cell>Typed entity marker</cell><cell>S:PERSON Bill</cell><cell>/S:PERSON was born in</cell><cell>71.5</cell><cell>72.9</cell><cell>71.0</cell></row><row><cell></cell><cell cols="2">O:CITY Seattle /O:CITY .</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Typed entity marker (punct) @ * person * Bill @ was born in # ? city ? Seattle #.</cell><cell>70.9</cell><cell>72.7</cell><cell>74.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Test F 1 (in %) of different entity representation techniques on TACRED. For each technique, we also provide the processed input of an example text "Bill was born in Seattle". Typed entity markers (original and punct) significantly outperforms others.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>The experimental results are shown in Tab. 2.</figDesc><table><row><cell>Model</cell><cell cols="3">TACRED TACREV Re-TACRED</cell></row><row><cell></cell><cell cols="2">Test F 1 Test F 1</cell><cell>Test F 1</cell></row><row><cell>Sequence-based Models</cell><cell></cell><cell></cell><cell></cell></row><row><cell>PA-LSTM (Zhang et al., 2017)</cell><cell>65.1</cell><cell>73.3  ?</cell><cell>79.4  ?</cell></row><row><cell>C-GCN (Zhang et al., 2018)</cell><cell>66.3</cell><cell>74.6  ?</cell><cell>80.3  ?</cell></row><row><cell>Transformer-based Models</cell><cell></cell><cell></cell><cell></cell></row><row><cell>BERT BASE + entity marker</cell><cell>68.4</cell><cell>77.2</cell><cell>87.7</cell></row><row><cell>BERT LARGE + entity marker</cell><cell>69.7</cell><cell>77.9</cell><cell>89.2</cell></row><row><cell cols="2">RoBERTa LARGE + entity marker 70.7</cell><cell>81.2</cell><cell>90.5</cell></row><row><cell>SpanBERT (Joshi et al., 2020)</cell><cell>70.8</cell><cell>78.0  *</cell><cell>85.3  ?</cell></row><row><cell cols="2">KnowBERT (Peters et al., 2019) 71.5</cell><cell>79.3  *</cell><cell>-</cell></row><row><cell>LUKE (Yamada et al., 2020)</cell><cell>72.7</cell><cell>80.6  ?</cell><cell>90.3  ?</cell></row><row><cell>Improved RE baseline</cell><cell></cell><cell></cell><cell></cell></row><row><cell>BERT BASE + typed entity</cell><cell>71.5</cell><cell>79.3</cell><cell>87.9</cell></row><row><cell>marker</cell><cell></cell><cell></cell><cell></cell></row><row><cell>BERT LARGE + typed entity</cell><cell>72.9</cell><cell>81.3</cell><cell>89.7</cell></row><row><cell>marker</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RoBERTa LARGE + typed entity</cell><cell>74.6</cell><cell>83.2</cell><cell>91.1</cell></row><row><cell>marker (punct)</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>F 1 (in %) on the test sets. * marks reimplemented results from</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Statistics of datasets.</figDesc><table><row><cell>The statistics of the datasets are shown</cell></row><row><cell>in Tab. 3.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Test F 1 on the filtered test sets. The typed entity marker consistently outperforms the entity mask, showing that knowledge from entity names can generalize to unseen entities.</figDesc><table><row><cell>Model</cell><cell cols="3">BERT BASE BERT LARGE RoBERTa LARGE</cell></row><row><cell>Entity marker</cell><cell>83.8</cell><cell>86.0</cell><cell>88.6</cell></row><row><cell>Typed entity marker</cell><cell>84.3</cell><cell>87.5</cell><cell>89.4</cell></row><row><cell>(punct for RoBERTa)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Gain</cell><cell>+0.5</cell><cell>+1.5</cell><cell>+0.8</cell></row><row><cell>Gain on TACRED</cell><cell>+3.1</cell><cell>+3.2</cell><cell>+3.9</cell></row><row><cell>Gain on TACREV</cell><cell>+2.1</cell><cell>+3.4</cell><cell>+2.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Test F 1 on the clean test set of TACRED. The gain on the clean test set is smaller than on TACRED and TACREV.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/wzhouad/ RE_improved_baseline</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This work first appeared as a technical report on arXiv in Feb 2021<ref type="bibr" target="#b27">(Zhou and Chen, 2021)</ref>. Since then, the proposed techniques have been incorporated into several followup works Wang et al., 2022b,a;<ref type="bibr" target="#b13">Lu et al., 2022;</ref><ref type="bibr" target="#b6">Han et al., 2021;</ref><ref type="bibr" target="#b11">Kulkarni et al., 2022)</ref> that are published before this version of the paper.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">SUBJ and OBJ are respectively the original token spans of subject and object entities.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We appreciate the reviewers for their insightful comments and suggestions. This work supported by the National Science Foundation of United States Grant IIS 2105329, and a Cisco Research Award.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TACRED revisited: A thorough evaluation of the TACRED relation extraction task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Alt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandra</forename><surname>Gabryszak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonhard</forename><surname>Hennig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1558" to="1569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.0473</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Matching the blanks: Distributional similarity for relation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Livio Baldini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kwiatkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2895" to="2905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Knowprompt: Knowledgeaware prompt-tuning with synergistic optimization for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunzhi</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Web Conference 2022</title>
		<meeting>the ACM Web Conference 2022</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="2778" to="2788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno>abs/1706.02677</idno>
	</analytic>
	<monogr>
		<title level="m">Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.11259</idno>
		<title level="m">Ptr: Prompt tuning with rules for text classification</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">SpanBERT: Improving pre-training by representing and predicting spans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="64" to="77" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>abs/1609.02907</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning rich representation of keyphrases from text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debanjan</forename><surname>Mahata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravneet</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajarshi</forename><surname>Bhowmik</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.findings-naacl.67</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: NAACL 2022</title>
		<meeting><address><addrLine>Seattle, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022" />
			<biblScope unit="page" from="891" to="906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Summarization as indirect supervision for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keming</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Mingyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.09837</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning from Context or Names? An Empirical Study on Neural Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3661" to="3672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Knowledge enhanced contextual word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vidur</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="43" to="54" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Re-tacred: Addressing shortcomings of the tacred dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Emmanouil Antonios Platanios, and Barnab?s P?czos</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruize</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuihong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.01808</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>et al. 2020. K-adapter: Infusing knowledge into pre-trained models with adapters</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Graph-Cache: Message passing as caching for sentencelevel relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Hooi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: NAACL 2022</title>
		<meeting><address><addrLine>Seattle, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="1698" to="1708" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Should we rely on entity mentions for relation extraction? debiasing relation extraction with counterfactual analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dayiheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baosong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juncheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Hooi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Seattle, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="3071" to="3081" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Drame</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Quentin Lhoest, and Alexander Rush</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">LUKE: Deep contextualized entity representations with entityaware self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideaki</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6442" to="6454" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Graph convolution over pruned dependency trees improves relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2205" to="2215" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Positionaware attention and supervised data improve slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="35" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">ERNIE: Enhanced language representation with informative entities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1441" to="1451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A frustratingly easy approach for joint entity and relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zexuan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">An improved baseline for sentence-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhao</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.01373v1</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Document-level relation extraction with adaptive thresholding and localized context pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
