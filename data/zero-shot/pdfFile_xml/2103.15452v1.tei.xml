<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Boosting the Speed of Entity Alignment 10?: Dual Attention Matching Network with Normalized Hard Sample Mining</title>
			</titleStmt>
			<publicationStmt>
				<publisher>ACM</publisher>
				<availability status="unknown"><p>Copyright ACM</p>
				</availability>
				<date type="published" when="2021-04-19">2021. April 19-23, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Mao</surname></persName>
							<email>xmao@stu.ecnu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">East China Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenting</forename><surname>Wang</surname></persName>
							<email>wenting.wang@lazada.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">East China Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Lan</surname></persName>
							<email>mlan@cs.ecnu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">East China Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Mao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">East China Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenting</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">East China Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Lan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">East China Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">ACM Reference Format</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Boosting the Speed of Entity Alignment 10?: Dual Attention Matching Network with Normalized Hard Sample Mining</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the Web Conference 2021 (WWW &apos;21)</title>
						<meeting>the Web Conference 2021 (WWW &apos;21) <address><addrLine>Ljubljana, Slovenia; New York, NY, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>ACM</publisher>
							<biblScope unit="volume">12</biblScope>
							<date type="published" when="2021-04-19">2021. April 19-23, 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3442381.3449897</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS ? Computing methodologies ? Knowledge representation and reasoning</term>
					<term>Natural language processing</term>
					<term>Supervised learning KEYWORDS Graph Neural Networks</term>
					<term>Knowledge Graph</term>
					<term>Entity Alignment</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Seeking the equivalent entities among multi-source Knowledge Graphs (KGs) is the pivotal step to KGs integration, also known as entity alignment (EA). However, most existing EA methods are inefficient and poor in scalability. A recent summary points out that some of them even require several days to deal with a dataset containing 200, 000 nodes (DWY100K). We believe over-complex graph encoder and inefficient negative sampling strategy are the two main reasons. In this paper, we propose a novel KG encoder -Dual Attention Matching Network (Dual-AMN), which not only models both intra-graph and cross-graph information smartly, but also greatly reduces computational complexity. Furthermore, we propose the Normalized Hard Sample Mining Loss to smoothly select hard negative samples with reduced loss shift. The experimental results on widely used public datasets indicate that our method achieves both high accuracy and high efficiency. On DWY100K, the whole running process of our method could be finished in 1, 100 seconds, at least 10? faster than previous work. The performances of our method also outperform previous works across all datasets, where @1 and have been improved from 6% to 13%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>(e.g., DBpedia <ref type="bibr" target="#b0">[1]</ref>, YAGO <ref type="bibr" target="#b18">[19]</ref>) and domain-specific KGs (e.g., Scientific <ref type="bibr" target="#b25">[25]</ref>) have proliferated and been widely used in downstream applications, such as search engines and recommendation systems. In practice, a KG is usually constructed from one single data source. Therefore, it is unlikely to cover the full domain. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>(a), integrating KGs in the same domain but built from different languages can transfer the information from high resource language to low resource language. This in turn will facilitate downstream cross-lingual applications, especially for minority language users. Moreover, consolidating multi-domain KGs <ref type="figure" target="#fig_0">(Figure 1(b)</ref>) can supplement cross-domain information and improve the coverage, thus making KGs more complete.</p><p>Seeking the equivalent entities among multi-source KGs is the pivotal step to KGs integration, also known as entity alignment (EA). Recently, EA attracts enormous attention and progresses rapidly. Dozens of related papers have been published in recent years. In general, these methods all share one core framework: assume that equivalent entities possess similar neighboring structure, apply KG embedding methods (e.g., TransE <ref type="bibr" target="#b1">[2]</ref> or GCN <ref type="bibr" target="#b8">[9]</ref>) to obtain dense embeddings for each entity, then map these embeddings into a unified vector space by alignment module (e.g., Triplet loss and arXiv:2103.15452v1 [cs.AI] <ref type="bibr" target="#b29">29</ref> Mar 2021</p><p>Contrastive Loss <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16]</ref>), and finally the pair-wise distance between entities determines whether they are aligned or not.</p><p>However, previous EA methods are inefficient and poor in scalability, as summarized by Zhao et al. <ref type="bibr" target="#b34">[34]</ref> that most of them require several hours <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">22]</ref> or even days <ref type="bibr" target="#b31">[31]</ref> on a dataset containing 200, 000 nodes (i.e., DWY100K). In reality, KGs usually consist of millions of entities and relations (e.g., the full DBpedia contains 10+ billion entities, 1+ trillion triples). Such large-scale datasets impose huge challenges in the efficiency and scalability of EA methods. Obviously, the high cost in time hinders the feasibility of applying these EA methods to large-scale KGs.</p><p>We believe there are two main reasons that cause the high time complexity of these advanced methods:</p><p>(1) Over-complex graph encoder: Since vanilla GCN is unable to model the heterogeneous relation information in KGs, many relation-aware GNN variants are proposed in EA task. However, some GNN variants are over-complex and inefficient. The running time of the vanilla GCN <ref type="bibr" target="#b27">[27]</ref> is only 10% of those from complex encoders. Every time a complex technique is introduced, e.g., Graph Attention mechanism <ref type="bibr" target="#b26">[26]</ref>, Graph Matching Networks <ref type="bibr" target="#b11">[12]</ref> (GMN), Joint Learning <ref type="bibr" target="#b10">[11]</ref>, the time complexity is dramatically increased. For instance, GM-Align <ref type="bibr" target="#b31">[31]</ref> incorporates GMN and achieves decent performances on a small dataset (DBP15K), but the performance improvement most likely is contributed from the literal information. When moving to a larger dataset (DWY100K), GM-Align needs five days to obtain the results. We believe the graph encoder still has some redundancy in design and its architecture can be further simplified to reduce time consumption.</p><p>(2) Inefficient negative sampling strategy: Almost all existing EA methods rely on the pair-wise loss functions (e.g., TransE, Triplet loss, and Contrastive Loss). In pair-wise loss, the negative samples are constructed via uniform random sampling. In this way, the samples are usually highly redundant and have limited information. The learning process could be hampered by the low-quality negative samples, resulting in slow convergence and model degradation. To alleviate this problem, BootEA <ref type="bibr" target="#b22">[22]</ref> proposes a Truncated Uniform Negative Sampling strategy to choose K-nearest neighbors as negative samples (i.e., hard samples). Such an intuitive and effective strategy has been widely adopted in subsequent studies <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b23">23]</ref>. However, ranking all neighbors to find the K-nearest is highly time-consuming and difficult to be fully parallelized on GPU. For example, Truncated Uniform Negative Sampling Strategy takes more than 25% of the whole time cost of BootEA.</p><p>Instead of trading efficiency for better performance, in this paper, we propose Dual Attention Matching Network (Dual-AMN) to capture dual relational information within a single graph and across two graphs: The Simplified Relational Attention Layer captures relational information within each KG by generating relation-specific embeddings through Relational Anisotropy Attention and Relational Projection. The Proxy Matching Attention Layer treats alignment as a special relation type and explicitly models it via proxy vectors. In addition, to tackle the inefficient sampling issue, we further propose a Normalized Hard Sample Mining Loss. First, LogSumExp operation is used to approximate Max operation to generate hard samples smoothly but efficiently. Then, to resolve the dilemma of hyper-parameter selection in LogSumExp, we introduce a loss normalization strategy adjusting the distribution of loss dynamically. Experiment with the same hardware environment, our method could finish the whole running process in 1, 100 seconds on DWY-100K, including data loading, training, and evaluating, which is 3? faster compared to the fastest existing model (i.e., GCN-Align <ref type="bibr" target="#b27">[27]</ref>) and only takes up 10% of advanced methods. On DBP15K with a smaller scale, our method even could obtain results in less than 40 seconds. More surprisingly, the alignment results obtained by our method have very high accuracy. The experiments show that our method beats all state-of-the-art competitors across all datasets, and the performance improvement on ? @1 and ranges from 6% to 13%. The main contributions are summarized as follows: </p><formula xml:id="formula_0">? Model.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">TASK DEFINITION</head><p>Definition of Knowledge Graph: The formal definition of a KG is a directed graph = ( , , ) comprising three sets -entities , relations , and triples ? ? ? . KG stores the real-world information in the form of triples &lt;entity, relation, entity&gt;, which describe the inherent relation between two entities. In addition, we define N to represent the neighbor set of entity and R represent the set of relations between and . Definition of Entity Alignment: Given two KGs 1 = ( 1 , 1 , 1 ), 2 = ( 2 , 2 , 2 ), and a pre-aligned entity pair set = {( , )| ? 1 , ? 2 ? }, where ? denotes equivalence. EA aims to obtain more potential equivalent entity pairs based on the information of 1 , 2 , and .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RELATED WORK</head><p>As mentioned in Section 1, existing EA methods can be abstracted into one framework containing three major components:</p><p>? Graph embedding module is responsible for encoding entities and relations of KGs into dense embeddings. ? Entity alignment module aims to map the embeddings of multi-source KGs into a unified vector space via pre-aligned entity pairs. ? Information enhancement module is able to generate semi-supervised data or introduce additional literal information for enhancement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Embedding Alignment Enhancement  In this section, we categorize existing EA approaches based on their designs of these three components, as shown in <ref type="table" target="#tab_3">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Embedding Module</head><p>TransE <ref type="bibr" target="#b1">[2]</ref>, GNN, and Hybrid are the three mainstream embedding approaches. TransE interprets relations as the translation from head entities to tail entities and assumes that the embeddings of entities and relations follow the assumption + ? if a triple (?, , ) holds. Based on this hypothesis, many variants (e.g., TransH <ref type="bibr" target="#b28">[28]</ref> and TransR <ref type="bibr" target="#b12">[13]</ref>) are proposed and proven to be effective in subsequent studies. Graph Neural Network (GNN) is famous for its strong modeling capability on the non-Euclidean structure. Different from TransE optimizing triples, GNN generates node-aware embeddings by aggregating the neighboring information of entities. However, vanilla GNN <ref type="bibr" target="#b8">[9]</ref> is unable to encode heterogeneous relational graphs such as KGs. Thus, many subsequent studies focus on modifying GNN to fit into KG. The main direction is to use the anisotropic attention mechanism <ref type="bibr" target="#b26">[26]</ref> to assign different weight coefficients to entities. A GNN model whose node update equation treats every edge direction equally, is considered isotropic (e.g., vanilla GCN); and a GNN model whose node update equation treats every edge direction differently, is considered anisotropic (e.g., GAT <ref type="bibr" target="#b26">[26]</ref>). Hybrid embedding approaches combine TransE and GNN together, which aim to enhance the expression ability of the model. However, for now, the best-performing methods TransEdge <ref type="bibr" target="#b23">[23]</ref> and MRAEA <ref type="bibr" target="#b13">[14]</ref> are not hybrid. The hybrid-based methods do not show necessity while introducing additional complexity. In addition to these three mainstream approaches, RSNs <ref type="bibr" target="#b4">[5]</ref> integrates Recurrent Neural Networks (RNNs) with a skipping mechanism to efficiently capture the long-term relational dependencies within and between KGs. RSNs performs well on sparse KGs, but it is still weaker than SOTA mainstream methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Alignment Module</head><p>The most common alignment methods are as follows: (1) Mapping <ref type="bibr" target="#b3">[4]</ref> uses one or two linear transformation matrices to map the embeddings of entities in different KGs into a unified vector space. This idea is inspired by the cross-lingual word embedding task <ref type="bibr" target="#b9">[10]</ref>, and the first proposed EA method <ref type="bibr" target="#b3">[4]</ref> adopts this alignment module.</p><p>(2) Corpus fusion <ref type="bibr" target="#b22">[22]</ref> swaps the entities in the pre-aligned set and generates new triples to calibrate the embeddings into a unified space. For example, there are two triples ( </p><p>. This approach not only integrates two KGs into one KG but also plays the role of data augmentation.</p><p>(3) Margin-based represents a series of pair-wise margin-based loss functions, such as Triplet loss <ref type="bibr" target="#b15">[16]</ref>, Contrastive loss <ref type="bibr" target="#b5">[6]</ref>, and so on. Margin-based loss functions are often combined with Siamese Neural Network in ranking tasks (e.g., face recognition and text similarity). Actually, GNN-based EA methods are inspired by the Siamese Neural Network and have similar architecture, so most of them use Margin-based loss to be their alignment module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Enhancement Module</head><p>Because manually aligning entities is expensive in practice, prealigned pairs are usually a small part of all entities. Therefore, existing methods usually reserve 30% or even less of the aligned pairs as training data to simulate this situation. Due to the lack of labeled data, some EA methods <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b23">23]</ref> adopt bootstrapping to generate semi-supervised data iteratively. Based on the asymmetric nature of cross-KG alignment, MRAEA <ref type="bibr" target="#b13">[14]</ref> further proposes a bidirectional iterative strategy. These data augmentation techniques have been proved effective in improving alignment performance.</p><p>In addition to structure, some methods <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b32">32]</ref> propose that introducing literal information could provide a multi-aspect view for alignment models and improve accuracy. However, it should be noted that not all datasets contain literal information, especially in practical applications. For example, there are privacy risks when using User Generated Content (UGC). Compared with the literal methods, the structure-only methods are more general. Therefore, these literal methods should be compared among themselves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DUAL ATTENTION MATCHING NETWORK</head><p>As mentioned in Section 1, existing graph encoders, which are overcomplex in certain designs and poor in scalability, are not suitable to be applied to large-scale KG. To address these defects, we propose Dual Attention Matching Network (Dual-AMN). <ref type="figure" target="#fig_1">Figure 2</ref> depicts that Dual-AMN is composed of two major components: Simplified Relational Attention Layer and Proxy Matching Attention Layer. The Simplified Relational Attention Layer captures relational information within each KG by generating relation-specific embeddings through Relational Anisotropy Attention and Relational Projection. By treating alignment as a special relation, our Proxy Matching Attention Layer leverages a list of proxies to explicitly capture the cross-graph information. By combining the outcomes of these two proposed components, our Dual-AMN not only embeds both intragraph and cross-graph relations smartly, but also greatly reduces computational complexity. The experimental results show that the proposed method achieves the SOTA in both performance and efficiency. In this section, we describe the architecture of Dual-AMN in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Simplified Relational Attention Layer</head><p>Since vanilla GCN is unable to model the heterogeneous relation information in KGs, many relation-aware GNN variants are proposed in EA task. Most of them could be described by the following equation:</p><formula xml:id="formula_1">+1 = ?? ?N ?{ }<label>(1)</label></formula><p>where represents the embedding vector of obtained by the -th GNN layer, , represents the weight coefficient between and , represents the transformation matrix. <ref type="table">Table 2</ref> lists some popular GNN encoders. We summarize three findings: (1) Except GCN-Align which first utilizes GCN in EA, all the other methods adopt anisotropic attention mechanism. This indicates that it is necessary to distinguish the importance of entities. (2) There is a tendency that more recent methods are not joint learning based, probably because joint methods are not superior in performance. For example, MRAEA and TransEdge outperform MuGNN, KECG, and NAEA. So joint learning which introduces extra computation complexity is not necessary. (3) We also notice that many methods constrain the transformation matrix of GNN layer to be diagonal or even remove in order to avoid performance degradation. We believe the main reason is that the entity embeddings are all trainable and the standard linear transformation may introduce too many parameters, causing over-fitting when updating these embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Joint GCN-Align <ref type="bibr" target="#b27">[27]</ref> Isotropic ? None MuGNN <ref type="bibr" target="#b2">[3]</ref> Anisotropy ? Diagonal KECG <ref type="bibr" target="#b10">[11]</ref> Anisotropy ? Diagonal NAEA <ref type="bibr" target="#b35">[35]</ref> Anisotropy ? Normal HMAN <ref type="bibr" target="#b32">[32]</ref> Anisotropy ? Diagonal MRAEA <ref type="bibr" target="#b13">[14]</ref> Anisotropy ? None <ref type="table">Table 2</ref>: Categorization of GNN encoders in some popular EA approaches.</p><p>Inspired by these findings, we design a simplified relation-aware GNN layer.</p><p>The inputs of our model are two metrics, ? R | |? represents the initial entity features and ? R | |? represents the initial relation features. Both of them are randomly initialized by He_initializer <ref type="bibr" target="#b6">[7]</ref>. Similar to existing EA methods, we use anisotropic relational attention mechanism to aggregate the neighborhood information around entities. The output embedding of entity at the -th layer is obtained by the following equation:</p><formula xml:id="formula_2">+1 = ? ?? ?N ?? ?R ( ? 2 )<label>(2)</label></formula><p>here we employ ? as the activation function. Instead of standard linear transformation matrix , we utilize Relational Projection operation <ref type="bibr" target="#b14">[15]</ref>. Such operation generates relation-specific embedding for each entity without extra parameters. As for the calculation of , we adopt the meta-path <ref type="bibr" target="#b33">[33]</ref> mechanism to assign weights:</p><formula xml:id="formula_3">= ( ) ? ?N ? ?R ? ( ? )<label>(3)</label></formula><p>where is an attention vector. Softmax operation selects the most critical path from all types of edges connected to the entities (i.e., meta-path), which embeds the relational anisotropy but simplifies the calculation to the greatest extent.</p><p>In previous studies <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">24]</ref>, GNN is able to expand to multi-hop neighboring level information by stacking more layers, thus to create a more global-aware representation of the graph. Following this idea, we concatenate the embeddings from different layers together to obtain the Multi-Hop Embeddings for entity :</p><formula xml:id="formula_4">= [ 0 ? 1 ?...? ]<label>(4)</label></formula><p>where ? represents the concatenate operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Proxy Matching Attention Layer</head><p>So far, the GNN encoders we have discussed only focus on modeling a single KG while leaving the cross-graph information to be learned by the alignment module alone. Graph Matching Network <ref type="bibr" target="#b11">[12]</ref> (GMN) builds a cross-graph attention mechanism to learn similarities, although they view the alignment purely as a node-to-node interaction (as illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>(a)). Formally, GMN measures the difference between ? 1 and its closest neighbor in the other graph as follows:</p><formula xml:id="formula_5">= ( ( , )) ? 2 ( ( , )) (5) = ?? ? 2 ( ? )<label>(6)</label></formula><p>( * ) is a vector space similarity metric, represents the difference of against all entities from 2 . Such node-to-node interaction enforces the embeddings to be learned jointly on a pair, at the cost of massive extra computation efficiency. Since attention weights are required for every pair of nodes across two graphs, this operation has a computation cost of (| 1 || 2 |). As mentioned in Section 1, GM-Align which incorporates GMN needs several days to obtain the results on the large-scale dataset (DWY100K). Driven by similar motivation, but in our interpretation, alignment itself is nothing but a special relation type whose representation can be explicitly learned in early stage.</p><p>Inspired by the above, we propose the Proxy Matching Attention Layer. As shown in <ref type="figure" target="#fig_2">Figure 3</ref> (b), we employ a limited set of proxy vectors to represent the cross-graph alignment relation, similar to use anchor points to present a space. If two entities are equivalent, their similarity distributions associated with these proxy vectors should also be consistent. In this way, the proposed layer is able to capture the cross-graph alignment information without computing node-to-node interaction. The interaction of the Proxy Matching Attention Layer is to calculate the similarity between all entities and limited anchors, which is similar to clustering. On large-scale KGs or dense graphs, this interaction approach can greatly reduce the computational complexity from O(</p><formula xml:id="formula_6">| 1 || 2 |) to O(| 1 | + | 2 |).</formula><p>The inputs of the Proxy Matching Attention Layer are two matrices:</p><p>? R | |? represents the entities embeddings obtained by the Simplified Relational Attention Layer and ? R ? represents proxy vectors with random initialization, where represents the number of proxy vectors. Just like GMN, the first step is to compute the similarity between each entity and all proxy vectors:</p><formula xml:id="formula_7">= ( ( , )) ? ( ( , ))<label>(7)</label></formula><p>represents the set of proxy vectors. Here we use the cosine metric to measure the similarity between embeddings. Then, the cross-graph embedding for entity can be computed as:</p><formula xml:id="formula_8">= ?? ? ( ? )<label>(8)</label></formula><p>intuitively describes the difference between and all proxy vectors. Finally, we employ a gate mechanism <ref type="bibr" target="#b17">[18]</ref> to combine and , controlling the information flow between single graph and multiple graphs:</p><formula xml:id="formula_9">= ( + )<label>(9)</label></formula><formula xml:id="formula_10">= ? + (1 ? ) ?<label>(10)</label></formula><p>and are the gate weight matrix and gate bias vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">NORMALIZED HARD SAMPLE MINING</head><p>Typically, in KGs, only a small portion of cross-graph entity pairs are aligned. So negative sampling is crucial to EA methods. However, the most common approach which selects the K-nearest neighbors, spends a lot of time on candidate ranking in each epoch. In this section, we propose a Normalized Hard Sampling Mining strategy, which is efficient and reduces loss shift.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Smooth Hard Sample Mining</head><p>Both TransE-based and GNN-based EA methods rely on the pairwise loss functions to optimize the similarities between samples. TransE-based methods use TransE loss to encode KGs:</p><formula xml:id="formula_11">= ?? (?, , ) ? + ? + ? ? 2 2 ? ? ? + ? ? ? ? 2 2 +<label>(11)</label></formula><p>GNN-based methods use Triplet loss to map the embeddings from two KGs into a unified space:</p><formula xml:id="formula_12">= ?? ( , ) ? + ( , ) ? ( ? , ? ) +<label>(12)</label></formula><p>where represents a fixed margin, [ ] + represents the operation Max(0, ), ? represents the negative sample of . Initially, the negative samples in pair-wise loss are generated through uniform random sampling, but this kind of samples is highly redundant and comprises too many easy even uninformative samples. Training with such low-quality negative samples may significantly degrade the model's learning capability and slow down the convergence. A simple but effective strategy is to select the K-nearest neighbors around the positive sample to be negative samples. This is also known as Hard Sample Mining. BootEA proposes the Truncated Uniform Negative Sampling (TUNS) based on this strategy and reports that it could significantly reduce the number of training epochs and improve performance. Most of the subsequent works follow this approach, such as KECG <ref type="bibr" target="#b10">[11]</ref>, MuGNN <ref type="bibr" target="#b2">[3]</ref>, TransEdge <ref type="bibr" target="#b21">[21]</ref>, and etc. However, faster convergence cannot shorten the overall training time. Because it has to spent massive time in candidate ranking for the next epoch, and this process is difficult to be fully parallelized on GPU.</p><p>In the field of deep metric learning, some studies <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">20]</ref> propose to use the LogSumExp operation to smoothly generate hard negative samples:</p><formula xml:id="formula_13">= ? ? ? ? ? ? 1 + ?? ? ?? ? ( ( + ? )) ? ? ? ? ? ?<label>(13)</label></formula><p>where represents the positive sample set of the anchor and represents the negative sample set. is a scale factor. If ? ?:</p><formula xml:id="formula_14">= ?? 1 ? ? ? ? ? ? 1 + ?? ? ?? ? ( ( + ? )) ? ? ? ? ? ? = [ + ? ] +<label>(14)</label></formula><p>LogSumExp is approximate to TUNS with K = 1. When is set to an appropriate value, LogSumExp could replace the K-nearest sampling strategy to generate high-quality negative samples, but with better computational efficiency (because this process could be fully parallelized on GPU). More interestingly, when = 1, the loss function is equivalent to Softmax with Cross-Entropy loss. This also indicates that the classification losses and the pair-wise losses are essentially two sides of the same coin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Loss Normalization</head><p>Both TUNS and LogSumExp face the same dilemma of how to select the proper value for their hyper-parameters. In TUNS, the hyperparameter is the number of nearest neighbors K. A small K will  lead to slow convergence in the initial training process, while an overlarge K makes the negative samples too "easy. " In LogSumExp operation, the hyper-parameter is the scalar factor . As illustrated in <ref type="table" target="#tab_5">Table 3</ref>, if is set too large, the weights of samples are greatly affected by the random disturbance at the beginning of training. For example, when = 100, these five pair losses are closer to each other while their corresponding weights vary a lot. In such a case, the model would tend to only focus on a few samples, slowing down the convergence. On the other hand, if is too small, it would be difficult for the model to pick up hard samples in the later stage, which causes model degradation. For example, when = 5, though 1 is seven times larger than 2 , the weights difference is small. Inspired by batch normalization <ref type="bibr" target="#b7">[8]</ref> which reduces the internal covariate shift, we propose to use a normalization step that fixes the mean and variance of sample losses and reduces the dependence on the scale of the hyper-parameter. Our overall loss function is defined as follow:</p><formula xml:id="formula_15">= ?? ( , ) ? ? ? ? ? ? ? 1 + ?? ? ? 2 ( ( , , ? ) + ) ? ? ? ? ? ? + ?? ( , ) ? ? ? ? ? ? ? 1 + ?? ? ? 1 ( ( , , ? ) + ) ? ? ? ? ? ?<label>(15)</label></formula><p>( , , ? ) represents the normalized loss of the triple ( , , ? ). and 2 represent the new mean and the new variance of normalized loss respectively. ( , , ? ) is defined as follow:</p><formula xml:id="formula_16">( , , ? ) = ( , , ? ) ? ( , ) ?? 2 ( , ) ? (16) ( , , ? ) = + ( , ) ? ( , ? )<label>(17)</label></formula><p>where ( , , ? ) represents the original loss of the triple ( , , ? ), and 2 represent the mean and the variance of original loss, which are computed by:</p><formula xml:id="formula_17">( , ) = 1 | 2 | ?? ? ? 2 ( , , ? )<label>(18)</label></formula><formula xml:id="formula_18">2 ( , ) = 1 | 2 | ?? ? ? 2 ( , , ? ) ? ( , ) 2<label>(19)</label></formula><p>The calculation process of ( , , ? ) is similar to ( , , ? ).  During training, we choose L2 distance as the metric to measure the similarity between entities:</p><formula xml:id="formula_19">( , ) = ?? ? ? ? 2 2<label>(20)</label></formula><p>During testing, in order to address the hubness problem in highdimensional space, CSLS <ref type="bibr" target="#b9">[10]</ref> is set to be the distance metric. Note that in training, and won't participate in gradient calculation and backpropagation. This is because our loss normalization is designed to change the weights of the samples, not the gradient direction. If and are updated in the backpropagation step, our loss will fail to converge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>We use the Keras framework for developing our approach. Our experiments are conducted on a workstation with a GeForce GTX TITAN X GPU and 128GB memory, which is consistent with the summary <ref type="bibr" target="#b34">[34]</ref>. The code is now available on GitHub 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Datasets</head><p>To fairly and comprehensively verify the effectiveness, robustness and scalability of our model, we construct experiments on three widely used public datasets:</p><p>(1) DBP15K <ref type="bibr" target="#b21">[21]</ref>: This dataset consists of three cross-lingual subsets constructed from DBpedia: English-French (DBP EN?FR ), English-Chinese (DBP EN?ZH ), English-Japanese (DBP EN?JA ). Each subset contains 15, 000 pre-aligned entity pairs for training and testing. As an early dataset, DBP15K is widely used but has some defects: small scale and dense links. These defects prompt more datasets to be proposed.</p><p>(2) DWY100K <ref type="bibr" target="#b22">[22]</ref>: This dataset comprises two mono-lingual subsets, each containing 100, 000 pre-aligned entities pairs and 1 https://github.com/MaoXinn/Dual-AMN nearly one million triples. DWY DBP?WD represents the subset extracted from DBpedia and Wikidata, and DWY DBP?YG represents DBpedia and YAGO. DWY100K, as the largest dataset of the three, brings challenges to space and time complexity.</p><p>(3) SRPRS <ref type="bibr" target="#b4">[5]</ref>: Compared with the real-world KGs, the above two datasets are too dense, and the degree distribution is quite different from the real. Thus, Guo et al. <ref type="bibr" target="#b4">[5]</ref> propose a sparse dataset, including two cross-lingual subsets (SRPRS FR?EN and SRPRS DE?EN ) and two mono-lingual subsets (SRPRS DBP?WD and SRPRS DBP?WD ). Same with DBP15K, each subset of SRPRS contains 15, 000 prealigned entity pairs for training and testing. This dataset challenges the modeling ability of EA approaches when facing limited information.</p><p>The statistics of these datasets are listed in <ref type="table" target="#tab_7">Table 4</ref>. Consistent with previous studies, we randomly split 30% of the pre-aligned entity pairs for training and developing, while the remaining 70% for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Baselines</head><p>As mentioned in Section 3, many studies adopt enhancement modules. For instance, GM-Align and RDGCN propose to introduce literal information to provide a multi-aspect view. The introduction of additional information leads to unfair comparisons between methods. Thus, existing EA methods will be compared separately according to the enhancement category:</p><p>(1) Basic: This kind of method only uses the original structure information (i.e., triples) in the dataset and does not introduce any extra enhancement module: MTransE <ref type="bibr" target="#b3">[4]</ref>, GCN-Align <ref type="bibr" target="#b27">[27]</ref>, RSNs <ref type="bibr" target="#b4">[5]</ref>, MuGNN <ref type="bibr" target="#b2">[3]</ref>, KECG <ref type="bibr" target="#b10">[11]</ref>.</p><p>(2) Semi-supervised: These methods adopt bootstrapping to generate semi-supervised structure data: BootEA <ref type="bibr" target="#b22">[22]</ref>, NAEA <ref type="bibr" target="#b35">[35]</ref>, TransEdge <ref type="bibr" target="#b23">[23]</ref>, and MRAEA <ref type="bibr" target="#b13">[14]</ref>.</p><p>(3) Literal: To obtain a multi-aspect view, literal methods use literal information (e.g., entity name) of entities as input features: GM-Align <ref type="bibr" target="#b31">[31]</ref>, RDGCN <ref type="bibr" target="#b29">[29]</ref>, HMAN <ref type="bibr" target="#b32">[32]</ref>, HGCN <ref type="bibr" target="#b30">[30]</ref>.</p><p>To make a fair comparison against above three types of methods, our model also has three corresponding versions: (1) Dual-AMN is the basic version without any enhancement module, as described in Section 4. (2) Dual-AMN (Semi) introduces the bi-directional iterative strategy proposed by MRAEA to generate semi-supervised data. (3) Dual-AMN (Lit) adopts a simple strategy to utilize literal information. For ? 1 and ? 2 , we use Dual-AMN (Semi) to obtain the structural similarity . Then, using the cross-lingual word embedding 2 to calculate the literal similarity . Finally, the entities are ranked according to + .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Experimental Settings</head><p>Metrics. Following convention, we use @ and Mean Reciprocal Rank ( ) as our evaluation metrics. The @ score is calculated by measuring the proportion of correctly aligned pairs in the top-k. In particular, @1 represents accuracy. In order to be convincing, the reported performance is the average of five independent training runs. Hyper-parameters. For all dataset, we use a same config: The dimensionality for embeddings = 100; depth of GNN = 2;  </p><formula xml:id="formula_20">Method DBP ZH?EN DBP JA?EN DBP FR?EN SRPRS FR?EN SRPRS DE?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>DWY  number of proxy vectors = 64; margin = 1; new mean and new variance of normalized loss are = 10 and = 30; batch size is 1024; dropout rate is set to 30%. RMSprop is adopted to optimize the model with learning rate set to 0.005.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Main Experiments</head><p>In <ref type="table" target="#tab_8">Table 5</ref> and <ref type="table" target="#tab_9">Table 6</ref>, we report the performances of all methods on cross-lingual datasets and mono-lingual datasets, respectively. We compare the performances within each category. Dual-AMN vs. Basic Methods. Our method consistently achieves the best performance across all datasets. On the small-scale dense dataset (DBP15K), Dual-AMN outperforms other methods by at least 20% in terms of both @1 and . On the large-scale dense dataset (DWY100K), the performances are increased by more than 15% compared to previous SOTA. Experimental results show that the designs of Dual-AMN effectively captures the rich structural information of these two datasets. By cutting down the number of triples, SRPRS challenges EA methods' ability to model sparse KGs. It is not surprising to see that the performances of all methods drop significantly compared to the results on dense datasets. RSNs outperforms the previous SOTA on this dataset, which could be credited to the long-term relational paths it captures. But our Dual-AMN still achieves the best performance, exceeding RSNs by at least 10% on @1 and . All these experimental results demonstrate the effectiveness of Dual-AMN in capturing the structural information. Dual-AMN vs. Semi-supervised Methods. Benefiting from the semi-supervised strategy to generate more labeled data for the next training round, the overall performances of the semi-supervised methods surpass the basic methods. Compared with previous SOTA methods, our method outperforms at least 5% on @1. Compared to its own basic version, the semi-supervised strategy greatly improves the performances on DBP15K and DWY100K. On SRPRS, although the semi-supervised strategy still has some benefit, the  improvement is reduced to 2% ? 3%. We believe the reason for the smaller improvement is because the sparse nature of SRPRS makes its structure information insufficient to generate high-quality semisupervised data. Overall, the semi-supervised strategy performs well on dense datasets, while its improvement is marginal in sparse datasets.</p><p>Dual-AMN vs. Literal Methods. According to Zhao et al. <ref type="bibr" target="#b34">[34]</ref>, because the entity names between mono-lingual KGs are almost identical, the edit distance algorithm could achieve the groundtruth performance. Therefore, the literal methods only experiment on cross-lingual datasets. By combining with cross-lingual embeddings, the performances of Dual-AMN are further improved and surpass the previous SOTA methods across all datasets. From observing <ref type="table" target="#tab_8">Table 5</ref>, we found that the performances of the literal methods vary significantly according to language pairs, which is completely different from the structureonly methods. On DBP15K, the introduction of literal information increases @1 by 6%, 9%, and 11%, which indicates that French is the most similar language to English, while Chinese is the most different. Besides, due to the lack of structural information, the literal information is more critical on SRPRS. Literal information improves the performances by 30% on @1. It must be admitted that our way of utilizing literal information is too simple and crude. Compared with other methods, performance improvement mainly comes from better structural embeddings. How to better integrate literal information is our future work. Efficiency Analysis. Better performance is just the cherry on the cake. Dual-AMN's trump card is superior efficiency. <ref type="table" target="#tab_11">Table 7</ref> reports the overall time costs of existing EA methods on each dataset, including data loading, pre-processing, training, and evaluating. All results are obtained by directly running the source code provided by the authors. And hyper-parameters are set to be the same as reported in their original papers. Certainly, implement details such as learning rate, batch size, and pre-processing might influence the time costs. However, we believe that these experimental results still reflect the overall efficiency of EA methods.  <ref type="table">Table 8</ref>: Ablation experiment of architecture on DBP15K.</p><p>Obviously, the efficiency of Dual-AMN far exceeds competitors. The time costs of complex EA methods are tens or even hundreds of times more than that of Dual-AMN. Even compared with the fastest baseline (i.e., GCN-Align), the speed of Dual-AMN is 3? faster, while the @1 outperforms more than 20%. Comparing Dual-AMN and Dual-AMN (Semi), semi-supervised strategy increases the time consumption about three times. Due to the simple combining strategy, Dual-AMN (Lit) hardly increases the time consumption.</p><p>In particular, the large-scale dense dataset (DWY100K) poses a severe challenge to the space and time complexity of all EA methods. Due to the limitation of GPU memory, MuGNN, KECG, and HMAN have to be run on CPU, resulting in massive time costs. GM-Align is the least efficient method, because it uses GMN and requires a complicated pre-processing. We fail to obtain results for NAEA and RDGCN in our experiment environment because they require extremely high memory space. Benefit from the simplification of the encoder architecture and the Normalized Hard Sample Mining Loss, our model could fully utilize the GPU to obtain high-accuracy results efficiently. Even using the semi-supervised strategy for data augmentation, the proposed method still could obtain results within an hour.</p><p>In summary, the high efficiency of Dual-AMN makes the entity alignment application on large-scale KGs possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Ablation Experiment</head><p>To demonstrate the effectiveness of each design in architecture and loss function, we construct two ablation experiments on DBP15K. Ablation Experiment of architecture. Dual-AMN adopts the following four components to capture multi-aspect information existing in KGs: (1) Relational Attention mechanism (RA) finds the critical path around entities.  <ref type="table">Table 8</ref> reports the performances with Means ?stds after removing these components from Dual-AMN. Among all these components, MHE has the greatest impact on performance. Without MHE, the performance is degraded by at least 6% on @1. Only stacking GNN layers cannot fully capture the global information, it is necessary to concatenate the output embeddings of each layer explicitly. Besides, the remaining three components also show the necessity as our expectation. On average, adopting these technologies improves performance by 2% to   To verify its effectiveness, we compare it with several common loss functions. The results are visualized in <ref type="figure">Fig 4.</ref> Compared with the other three, the proposed loss could make the model converge faster and achieve the best performance. Truncated Uniform Negative Sampling Strategy also has a similar decent performance. However, as we have mentioned, this sampling strategy requires massive time consumption. Since most of the negative samples are redundant, the Triplet loss has the worst efficiency of all loss functions. In our experiments, the Triplet loss function usually needs thousands of epochs to converge, and the performance is lower than the proposed loss about 4%. The performance of Softmax with Cross-Entropy is stronger than Triplet loss, but there is obviously a performance gap with Normalized Hard Sample Mining Loss. These experimental results show that the proposed loss function significantly increases the convergence speed without losing any accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Relation Interpretability</head><p>In addition to the performance and speed advantages, our model also has a certain degree of interpretability. Because the weights of adjacent entities are determined by the relations between them, thus these weights can reflect the importance of different relations Dual-AMN Dual-AMN(Semi) Dual-AMN(Lit) <ref type="figure">Figure 5</ref>: @1 of the entities with different degrees.</p><p>to some extent. The importance of each relation is obtained by the following equation:</p><p>= ?</p><p>We train the model on the DWY YG and output the importance of relations. After clustering the relations according to , we obtain the <ref type="table" target="#tab_14">Table 9</ref>. From the observation, we summarize an interesting phenomenon. The relations with high importance (i.e., meta-path) are usually able to identify the entity from another. For example, if holding an entity and relation, we can reduce the potential options down to a small space. However, relation does not have this ability. A country can have many presidents, so its importance becomes extremely low. Of course, this is inseparable from the characteristics of the DBP YG dataset, which contains a large number of celebrities, especially the president, prime minister, and so on. Therefore, such kinds of relations become unimportant in this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7">Degree Analysis</head><p>The main experiments show that the performances of all EA methods on sparse datasets are much lower than that of standard datasets. In order to further explore the correlation between model performance and dataset density, we design an experiment on DBP ZH?EN . <ref type="figure">Figure 5</ref> shows the @1 of the three variants on different levels of entity degrees. We observe a strong correlation between performance and degree. As the degree increasing, the model performance improves significantly. For Dual-AMN, the @1 of the entities with one neighbor is only 20%. The introduction of semi-supervised strategy improves the overall performance of the model, but it has a limited effect on those entities with extremely sparse local structures. In sparse graphs, it is difficult to make correct inferences only based on limited structural information. On the other hand, Dual-AMN (Lit) has much higher performance when the degree value is small, which proves that the incorporation of literal information effectively improves the accuracy of these sparse entities. However, this strategy cannot work on the datasets without literal information. Therefore, how to better represent these sparse entities without extra information is a key point of future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>Over complex graph encoders and inefficient negative sampling strategies lead to the general inefficiency of existing EA methods, resulting in difficulty for applying on large-scale KGs. In this paper, we propose a novel KG encoder Dual Attention Matching Network (Dual-AMN), which not only models both intra-graph and cross-graph relations smartly but also greatly reduces computational complexity. To replace the inefficient sampling strategy, we propose Normalized Hard Sample Mining Loss to cut down the sampling consumption and accelerate the convergence speed. These two modifications enable the proposed model to achieve the SOTA performance while the speed is several times than other EA methods. The main experiments indicate that our method outperforms competitors across all datasets and metrics. Furthermore, we design auxiliary experiments to demonstrate the effectiveness of each component and the interpretability of the model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Two examples of KGs integration. Figure (a) represents cross-lingual and Figure (b) represents cross-domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The architecture illustration of Dual Attention Matching Network (Dual-AMN), composing of Simplified Relational Attention Layer and Proxy Matching Attention Layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The illustration of two approaches. (a) represents Graph Matching Network (GMN) and (b) represents Proxy Matching Attention Layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>EN H@1 H@10 MRR H@1 H@10 MRR H@1 H@10 MRR H@1 H@10 MRR H@1 H@10 MRR Basic MTransE 0.209 0.512 0.310 0.250 0.572 0.360 0.247 0.577 0.360 0.213 0.447 0.290 0.107 0.248 0.160 GCN-Align 0.434 0.762 0.550 0.427 0.762 0.540 0.411 0.772 0.530 0.243 0.522 0.340 0.385 0.600 0.460 MuGNN 0.494 0.844 0.611 0.501 0.857 0.621 0.495 0.870 0.621 0.131 0.342 0.208 0.245 0.431 0.310 KECG 0.477 0.835 0.598 0.489 0.844 0.610 0.486 0.851 0.610 0.298 0.616 0.403 0.444 0.707 0.540 RSNs 0.508 0.745 0.591 0.507 0.737 0.590 0.516 0.768 0.605 0.350 0.636 0.440 0.484 0.729 0.570 Dual-AMN 0.731 0.923 0.799 0.726 0.927 0.799 0.756 0.948 0.827 0.452 0.748 0.552 0.591 0.820 0.670 Semi BootEA 0.629 0.847 0.703 0.622 0.853 0.701 0.653 0.874 0.731 0.365 0.649 0.460 0.503 0.732 0.580 NAEA 0.650 0.867 0.720 0.641 0.872 0.718 0.673 0.894 0.752 0.177 0.416 0.260 0.307 0.535 0.390 TransEdge 0.735 0.919 0.801 0.719 0.932 0.795 0.710 0.941 0.796 0.400 0.675 0.490 0.556 0.753 0.630 MRAEA 0.757 0.930 0.827 0.758 0.934 0.826 0.781 0.948 0.849 0.460 0.768 0.559 0.594 0.818 0.666 Dual-AMN 0.808 0.940 0.857 0.801 0.949 0.855 0.840 0.965 0.888 0.481 0.778 0.568 0.614 0.823 00.842 0.750 0.763 0.897 0.810 0.873 0.950 0.901 0.672 0.767 0.710 0.779 0.886 0.820 HMAN 0.561 0.859 0.670 0.557 0.860 0.670 0.550 0.876 0.660 0.401 0.705 0.500 0.528 0.778 0.620 HGCN 0.720 0.857 0.760 0.766 0.897 0.810 0.892 0.961 0.910 0.670 0.770 0.710 0.763 0.863 0.801 Dual-AMN 0.861 0.964 0.901 0.892 0.978 0.925 0.954 0.994 0.970 0.802 0.932 0.851 0.891 0.972 0.923</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>DBP?WD DWY DBP?YG SRPRS DBP?WD SRPRS DBP?YG H@1 H@10 MRR H@1 H@10 MRR H@1 H@10 MRR H@1 H@10 MRR Basic MTransE 0.238 0.507 0.330 0.227 0.414 0.290 0.188 0.382 0.260 0.196 0.401 0.270 GCN-Align 0.494 0.756 0.590 0.598 0.829 0.680 0.291 0.556 0.380 0.319 0.586 0.410 MuGNN 0.604 0.894 0.701 0.739 0.937 0.810 0.151 0.366 0.220 0.175 0.381 0.240 KECG 0.631 0.888 0.720 0.719 0.904 0.790 0.323 0.646 0.430 0.350 0.651 0.450 RSNs 0.607 0.793 0.673 0.689 0.878 0.756 0.391 0.663 0.480 0.393 0.665 0.490 Dual-AMN 0.786 0.952 0.848 0.866 0.977 0.907 0.513 0.801 0.609 0.495 0.790 00.429 0.260 0.195 0.451 0.280 TransEdge 0.788 0.938 0.824 0.792 0.936 0.832 0.461 0.738 0.560 0.443 0.699 0.530 MRAEA 0.794 0.930 0.856 0.819 0.951 0.875 0.509 0.795 0.597 0.485 0.768 0.574 Dual-AMN 0.869 0.969 0.908 0.907 0.981 0.935 0.546 0.813 0.635 0.518 0.795 0.613</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>( 2 )</head><label>2</label><figDesc>Relational Projection operation (RP) generates the relation-specific embedding for entities. (3) Multi-Hop Embeddings (MHE) creates a more global-aware representation of the KGs. (4) Proxy Attention Matching Layer (PAM) captures the cross-graph information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>[starring] [act] [born in] [located in] [own by] ???? ??? ??? ?? ??? ???</head><label></label><figDesc></figDesc><table><row><cell>[??]</cell><cell></cell></row><row><cell></cell><cell>[??]</cell></row><row><cell>[??]</cell><cell>[??]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Categorization of some popular EA approaches.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Examples of the dilemma of selecting hyperparameter , where represents the loss of a pair + ? .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Statistical data of DBP15K, DWY100K and SRPRS.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Experimental results on cross-lingual datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Experimental results on mono-lingual datasets. Because of the memory limitation, NAEA cannot work on DWY100K.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Time costs of EA methods (seconds).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>AMN .731?.002 .799?.002 .726?.003 .799?.002 .756?.004 .827?.005 -RA. .705?.001 .781?.003 .706?.002 .785?.004 .741?.004 .817?.002 -RP. .702?.002 .779?.002 .704?.005 .783?.003 .745?.004 .821?.003 -MHE. .656?.003 .743?.002 .658?.002 .748?.001 .698?.004 .783?.003 -PAM. .711?.002 .785?.001 .710?.001 .783?.002 .738?.002 .812?.001</figDesc><table><row><cell>Method</cell><cell cols="2">DBP ZH?EN</cell><cell cols="2">DBP JA?EN</cell><cell cols="2">DBP FR?EN</cell></row><row><cell></cell><cell>Hits@1</cell><cell>MRR</cell><cell>Hits@1</cell><cell>MRR</cell><cell>Hits@1</cell><cell>MRR</cell></row><row><cell>Dual-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>Figure 4: Ablation experiment of loss on DBP ZH?EN . Tri. represents Triplet Loss; TUNS represents Triplet loss with Truncated Uniform Negative Sampling Strategy. For TUNS, top-10 nearest neighbors are selected as negative samples, following the same setting in most EA methods'. NHSM represents our Normalized Hard Sample Mining Loss.</figDesc><table><row><cell>Importance</cell><cell></cell><cell></cell><cell cols="3">Examples</cell><cell></cell><cell></cell></row><row><cell>High</cell><cell>[5, ?)</cell><cell>,</cell><cell>,</cell><cell>,</cell><cell>_</cell><cell>,</cell><cell>? ,</cell></row><row><cell>Medium</cell><cell>[?5, 5)</cell><cell>,</cell><cell>,</cell><cell>_</cell><cell>,</cell><cell></cell><cell>,</cell></row><row><cell>Low</cell><cell>[??, ?5)</cell><cell>, _</cell><cell>,</cell><cell cols="3">?, ? , _</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Relation examples of different importance. 3%. By adopting these new designs, Dual-AMN further breaks the ceiling of EA accuracy. Ablation Experiment of Loss. Besides architecture, the Normalized Hard Sample Mining Loss is also one of our main contributions.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Same with GM-Align<ref type="bibr" target="#b9">[10]</ref>.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">DBpedia: A Nucleus for a Web of Open Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?ren</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgi</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><forename type="middle">G</forename><surname>Ives</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-76298-0_52</idno>
		<ptr target="https://doi.org/10.1007/978-3-540-76298-0_52" />
	</analytic>
	<monogr>
		<title level="m">The Semantic Web, 6th International Semantic Web Conference, 2nd Asian Semantic Web Conference</title>
		<editor>Karl Aberer, Key-Sun Choi, Natasha Fridman Noy, Dean Allemang, Kyung-Il Lee, Lyndon J. B. Nixon, Jennifer Golbeck, Peter Mika, Diana Maynard, Riichiro Mizoguchi, Guus Schreiber, and Philippe Cudr?-Mauroux</editor>
		<meeting><address><addrLine>Busan, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007-11-11" />
			<biblScope unit="volume">4825</biblScope>
			<biblScope unit="page" from="722" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Translating Embeddings for Modeling Multi-relational Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garc?a-Dur?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held</title>
		<editor>Christopher J. C. Burges, L?on Bottou, Zoubin Ghahramani, and Kilian Q. Weinberger</editor>
		<meeting><address><addrLine>Lake Tahoe, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-12-05" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-Channel Graph Neural Network for Entity Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1140</idno>
		<ptr target="https://doi.org/10.18653/v1/p19-1140" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2019-07-28" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1452" to="1461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multilingual Knowledge Graph Embeddings for Cross-lingual Knowledge Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingtao</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Zaniolo</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2017/209</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2017/209" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-19" />
			<biblScope unit="page" from="1511" to="1517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to Exploit Long-term Relational Dependencies in Knowledge Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingbing</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/guo19c.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="2505" to="2514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dimensionality Reduction by Learning an Invariant Mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2006.100</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2006.100" />
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-06" />
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.123</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2015.123" />
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision, ICCV 2015</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v37/ioffe15.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<ptr target="http://arxiv.org/abs/1609.02907" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Word translation without parallel data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=H196sainb" />
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semi-supervised Entity Alignment via Joint Knowledge Embedding Model and Cross-graph Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1274</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-1274" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11-03" />
			<biblScope unit="page" from="2723" to="2732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Graph Matching Networks for Learning the Similarity of Graph Structured Objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenjie</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dullien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/li19d.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="3835" to="3845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning Entity and Relation Embeddings for Knowledge Graph Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
		<ptr target="http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9571" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-01-25" />
			<biblScope unit="page" from="2181" to="2187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">MRAEA: An Efficient and Robust Entity Alignment Approach for Cross-lingual Knowledge Graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3336191.3371804</idno>
		<ptr target="https://doi.org/10.1145/3336191.3371804" />
	</analytic>
	<monogr>
		<title level="m">WSDM &apos;20: The Thirteenth ACM International Conference on Web Search and Data Mining</title>
		<meeting><address><addrLine>Houston, TX, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-02-03" />
			<biblScope unit="page" from="420" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Relational Reflection Entity Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
		<idno type="DOI">10.1145/3340531.3412001</idno>
		<ptr target="https://doi.org/10.1145/3340531.3412001" />
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;20: The 29th ACM International Conference on Information and Knowledge Management</title>
		<meeting><address><addrLine>Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-10-19" />
			<biblScope unit="page" from="1095" to="1104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">FaceNet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298682</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2015.7298682" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015-06-07" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep Metric Learning via Lifted Structured Feature Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun Oh</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.434</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.434" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="4004" to="4012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Rupesh Kumar Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00387</idno>
		<ptr target="http://arxiv.org/abs/1505.00387" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Yago: a core of semantic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gjergji</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on World Wide Web</title>
		<meeting>the 16th International Conference on World Wide Web<address><addrLine>Banff, Alberta, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Carey L</publisher>
			<date type="published" when="2007-05-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ellen</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">F</forename><surname>Zurko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel-Schneider</surname></persName>
		</author>
		<idno type="DOI">10.1145/1242572.1242667</idno>
		<ptr target="https://doi.org/10.1145/1242572.1242667" />
		<editor>Prashant J. Shenoy</editor>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="697" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Circle Loss: A Unified Perspective of Pair Similarity Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changmao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongdao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR42600.2020.00643</idno>
		<ptr target="https://doi.org/10.1109/CVPR42600.2020.00643" />
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-06-13" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="6397" to="6406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cross-Lingual Entity Alignment via Joint Attribute-Preserving Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengkai</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-68288-4_37</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-68288-4_37" />
	</analytic>
	<monogr>
		<title level="m">The Semantic Web -ISWC 2017 -16th International Semantic Web Conference</title>
		<editor>Claudia d&apos;Amato, Miriam Fern?ndez, Valentina A. M. Tamma, Freddy L?cu?, Philippe Cudr?-Mauroux, Juan F. Sequeda, Christoph Lange, and Jeff Heflin</editor>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017-10-21" />
			<biblScope unit="volume">10587</biblScope>
			<biblScope unit="page" from="628" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bootstrapping Entity Alignment with Knowledge Graph Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhong</forename><surname>Qu</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2018/611</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2018/611" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07-13" />
			<biblScope unit="page" from="4396" to="4402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">TransEdge: Translating Relation-contextualized Embeddings for Knowledge Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muchao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingbing</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhong</forename><surname>Qu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.13579</idno>
		<ptr target="https://arxiv.org/abs/2004.13579" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Knowledge Graph Alignment Network with Gated Multi-Hop Neighborhood Aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhong</forename><surname>Qu</surname></persName>
		</author>
		<ptr target="https://aaai.org/ojs/index.php/AAAI/article/view/5354" />
	</analytic>
	<monogr>
		<title level="m">The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-02-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="222" to="229" />
		</imprint>
	</monogr>
	<note>The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Arnet-Miner: extraction and mining of academic social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong</forename><surname>Su</surname></persName>
		</author>
		<idno type="DOI">10.1145/1401890.1402008</idno>
		<ptr target="https://doi.org/10.1145/1401890.1402008" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<editor>Ying Li, Bing Liu, and Sunita Sarawagi</editor>
		<meeting>the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Las Vegas, Nevada, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008-08-24" />
			<biblScope unit="page" from="990" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJXMpikCZ" />
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cross-lingual Knowledge Graph Alignment via Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingsong</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d18-1032</idno>
		<ptr target="https://doi.org/10.18653/v1/d18-1032" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-10-31" />
			<biblScope unit="page" from="349" to="357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Knowledge Graph Embedding by Translating on Hyperplanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="http://www.aaai.org/ocs/index.php/AAAI/AAAI14/paper/view/8531" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Eighth AAAI Conference on Artificial Intelligence<address><addrLine>Qu?bec City, Qu?bec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-07-27" />
			<biblScope unit="page" from="1112" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Relation-Aware Entity Alignment for Heterogeneous Knowledge Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2019/733</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2019/733" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence<address><addrLine>Macao, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-08-10" />
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="5278" to="5284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Jointly Learning Entity and Relation Representations for Entity Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1023</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-1023" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11-03" />
			<biblScope unit="page" from="240" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cross-lingual Knowledge Graph Alignment via Graph Matching Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1304</idno>
		<ptr target="https://doi.org/10.18653/v1/p19-1304" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2019-07-28" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3156" to="3161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Aligning Cross-Lingual Entities with Multi-Aspect Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiu-Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1451</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-1451" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11-03" />
			<biblScope unit="page" from="4430" to="4440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Graph Transformer Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seongjun</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minbyul</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raehyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunwoo</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/9367-graph-transformer-networks" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-14" />
			<biblScope unit="page" from="11960" to="11970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">An Experimental Study of State-of-the-Art Entity Alignment Approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Suchanek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Neighborhood-Aware Attentional Representation for Multilingual Knowledge Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiannan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2019/269</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2019/269" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence<address><addrLine>Macao, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1943" />
			<biblScope unit="volume">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
