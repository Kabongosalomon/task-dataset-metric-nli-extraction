<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Depth-Adapted CNNs for RGB-D Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20211">AUGUST 2021 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">Depth-Adapted CNNs for RGB-D Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20211">AUGUST 2021 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-RGB-D Semantic segmentation</term>
					<term>Attention</term>
					<term>Con- volution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent RGB-D semantic segmentation has motivated research interest thanks to the accessibility of complementary modalities from the input side. Existing works often adopt a two-stream architecture that processes photometric and geometric information in parallel, with few methods explicitly leveraging the contribution of depth cues to adjust the sampling position on RGB images. In this paper, we propose a novel framework to incorporate the depth information in the RGB convolutional neural network (CNN), termed Z-ACN (Depth-Adapted CNN). Specifically, our Z-ACN generates a 2D depthadapted offset which is fully constrained by low-level features to guide the feature extraction on RGB images. With the generated offset, we introduce two intuitive and effective operations to replace basic CNN operators: depth-adapted convolution and depth-adapted average pooling. Extensive experiments on both indoor and outdoor semantic segmentation tasks demonstrate the effectiveness of our approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>As one of the fundamental tasks in computer vision, semantic segmentation aims to understand the pixel-wise label from an input image of a generic target scene. Recent advances in deep neural networks, as well as the GPU, have set new state-of-the-art (SOTA) performance in semantic segmentation. Despite significant progress in the last decade, semantic segmentation based on RGB input remains challenging in many challenging scenarios, i.e., low-contrast light, object occlusion, and separating objects sharing a similar visual appearance.</p><p>Recent developments in RGB-D sensors make RGB-D inputs accessible at a low cost, motivating research interests in designing various fusion strategies to merge multimodal features. A number of works have demonstrated the benefit of spatial cues to improve the accuracy of semantic segmentation, affirming the effectiveness of learning from complementary modalities. In the literature, two main designs have been widely exploited: single-stream design and twostream design. The single-stream often realizes an early fusion where RGB and depth images are simply concatenated from the input side. Different from conventional RGB networks with 3-channel, several works merge multi-modal inputs at the channel axis to form a 4-channel input (RGB-D) or 6channel input (RGB-HHA where HHA is encoded from depth Z. <ref type="bibr">Wu</ref>  referring to disparity, height above ground, and norm angle). However, these networks directly extract features from earlymixed modalities that cannot fully explore the correlation between RGB and depth images. The two-stream strategy adopts parallel encoders that extract multi-modal features separately and further fuse them at different semantic levels.</p><p>Nevertheless, compared to single-stream networks, two-stream designs inevitably increase the computational cost. Furthermore, the fusion mechanism is often pre-defined that cannot adapt to different scenarios without handcraft adjusting.</p><p>In this paper, we explore differently the relationship between RGB and depth by explicitly leveraging the perspective effect. Recent non-local attention <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref> works in vision tasks have proved their effectiveness in modeling contextualized awareness. Several recent works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref> further figure out that aggregating both global and local attention can lead to better performance since neighboring pixels tend to have high similarity and correlation. Sharing the same idea, we seek to improve CNN with local but contextualized awareness which is fully constrained by the geometry. As shown in <ref type="figure">Figure 1(a)</ref>, the conventional convolution is designed to have a regular and fixed structure on the image plane. With additional priors on camera parameters and depth cues, we show that the sliding windows can adapt to the geometry, e.g., the vanishing effect as shown in <ref type="figure">Figure 1</ref> Inspired by this observation, we develop a Depth-Adapted Convolutional Network, denoted Z-ACN. Z stands for the Zaxis of the camera coordinates representing the depth information. The preliminary version has been published as the arXiv:2206.03939v1 [cs.CV] 8 Jun 2022 conference paper <ref type="bibr" target="#b9">[10]</ref>  <ref type="bibr" target="#b0">1</ref> . Specifically, we propose a depthadapted offset that can be integrated into basic functions of CNN, i.e, convolution and pooling, and introduce two new operators: depth-adapted convolution and depth-adapted average pooling.</p><p>Our proposed depth-adapted convolution replaces conventional neighboring pixels with geometrically similar ones. Concretely, we reshape the receptive field to cover pixels sharing the same 3D plane with the center of the kernel, yielding a simple but efficient manner to articulate both photometric and geometric information. The second introduced operator is depth-adapted average pooling. Sharing the same idea as the depth-adapted convolution, we re-define the notion of neighboring pixels for average pooling such that the geometrical relations will be considered while computing the mean of the local region of the feature map. Both operators break the limits on the conventional definition of neighboring pixels, forcing the network to pay attention to a larger and more malleable receptive field.</p><p>Depth-adapted operations are based on the intuition that pixels with the same geometrical character should be more likely to share the same semantic label. One common example is the vanishing effect, as illustrated in <ref type="figure">Figure 1</ref>. We assume that pixels on the same 3D plane tend to share the same class. This 3D plane and depth variance have a high correlation. As shown in <ref type="figure">Figure 1</ref>, we display the projection of the 3D plane of the rail on the image plane as the adapted sampling position. The depth-adapted field should be more correlated to the real scene compared to the conventional neighboring field. Essentially, our method uses depth to transform planes into a canonical pose relative to the camera, such that the extracted feature maps are also in a canonical reference frame and thus invariant to scale changes and out-of-plane rotation. The main advantages of such operations are summarized as follows:</p><p>? We propose a novel depth-adapted convolutional network termed Z-ACN, that can integrate the geometric constraint into the conventional receptive field, hence improving the convolution with depth-aware contextualized attention. ? Our grid adaptation is processed by the non-learning method that does not introduce extra learning parameters compared to conventional counterparts. ? Experiments on both indoor and outdoor RGB-D semantic segmentation benchmarks demonstrate that our method can perform favorably over the baseline performance with large margins and set the new state-of-the-art performance. <ref type="bibr" target="#b0">1</ref> Compared to the conference paper, this journal version has made the following extensions and improvements: a) We newly introduce depth-adapted average pooling (Eq. 10) which further improves the model performance. b) We extend our application to the pre-trained model with both VGG and ResNet encoders. c). We conduct extensive studies on both indoor and outdoor scenarios to demonstrate the effectiveness of our approach. d). We thoroughly compare with concurrent attention-based convolution to demonstrate our superior design. e). An ablation study presents the contributions of different components of our operators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK A. 3D Representation</head><p>How to deal with the complementary depth is a key research topic for RGB-D semantic segmentation. Different from 2D RGB images, RGB-D images provide additional cues on 3D geometry. Therefore, a straightforward motivation is to project the 2D pixels to form the 3D representations such as voxel and point cloud. The volumetric representation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> feeds voxel data into 3D CNN. Despite the demonstrated success, it inefficiently consumes huge memory as data is often sparse on the 3D scene. Different from the voxel representation, <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> propose to directly use the point cloud representation. Different 3D CNN methods are trying to adapt to the irregularity of the point cloud. <ref type="bibr" target="#b14">[15]</ref> integrates an x-transformation to leverage the spatially-local correlation of point cloud <ref type="bibr" target="#b15">[16]</ref> introduces a spatially deformable convolution based on kernel points to study the local geometry. <ref type="bibr" target="#b16">[17]</ref> learns the mapping from geometry relations to high-level relations between points to get a shape awareness. <ref type="bibr" target="#b17">[18]</ref> defines convolution as an SLP (Single-Layer Perceptron) with a nonlinear activator.</p><p>Besides, a number of efforts have been made to reduce the model complexity. <ref type="bibr" target="#b18">[19]</ref> adapts CRF (Conditional Random Fields) to reduce the model parameters. Multi-view methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref> reform 3D CNN to become the combination of 2D CNNs. <ref type="bibr" target="#b19">[20]</ref> profits from Lidar to get bird-view and frontview information in addition to a traditional RGB image. <ref type="bibr" target="#b20">[21]</ref> uses depth image to generate the 3D volumetric representation after which projections on X, Y, and Z planes are learned respectively by 2D CNNs. 3D CNN achieves better results than RGB CNN but requires further development on problems such as memory cost, data resolution, and computing time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. 2D RGB-D Fusion</head><p>Instead of processing the 3D data, an alternative is to consider depth as another 2D image complementary to the RGB image. Deep neural networks for paired 2D RGB-D images have attracted research interests for years and numerous improvements have been achieved. At the early stage, <ref type="bibr" target="#b23">[24]</ref> proposes to encode a depth map to a 3-channel HHA image, which refers to Horizontal disparity, Height above ground, and normal Angle. Afterward, the encoded HHA is widely used in RGB-D tasks since both modalities share the same dimension on both the channel axis and spatial axis <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>. Hence, a straightforward idea is to apply identical but parallel networks on both RGB image and depth maps, and further realize data fusion at different scales <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>. ACNet <ref type="bibr" target="#b24">[25]</ref> fuses RGB-D features at middle level. SA gate <ref type="bibr" target="#b29">[30]</ref> further explicitly leverages spatial and channel cues to firstly calibrate modality in a bi-directional manner and further realize middle fusion. Despite the proven progress, the two-stream design doubles the number of encoder parameters, yielding a higher requirement on the computational cost.</p><p>To address this issue, a number of preliminary works <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref> fuse the RGB-D/RGB-HHA images from the input side and form a 4-or 6-channel input with a single encoder. Based on the early fusion, D-CNN <ref type="bibr" target="#b32">[33]</ref> enhances the network with a depth similarity term which re-weight the standard convolution with the depth-related local context. Since then, various works have been developed on the forms of weight functions. <ref type="bibr" target="#b33">[34]</ref> extends the idea of <ref type="bibr" target="#b32">[33]</ref> to dilated convolution. <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref> develop 2.5 D convolutions with a more generalized weight function. <ref type="bibr" target="#b36">[37]</ref> projects 3D convolution on 2D images to form a depth-aware multi-scale 2D convolution.</p><p>[27] uses depth information to define local neighborhoods by introducing a learned Gaussian kernel. Sharing the same idea of re-weighting the convolution, ShapeConv <ref type="bibr" target="#b37">[38]</ref> integrates the channel attention into the convolution function and forms a more generalized convolution that is not limited to RGB-D context.</p><p>It can be seen that contextualized awareness has played a vital role in RGB-D fusion. For two-stream designs, multi-modal features are often firstly fed into attention module before the data fusion: <ref type="bibr" target="#b38">[39]</ref> with ConvLSTM modules, ACNet <ref type="bibr" target="#b24">[25]</ref> with channel attention <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b26">[27]</ref> with a learned Gaussian convolution kernel, and <ref type="bibr" target="#b29">[30]</ref> with a modified CBAM <ref type="bibr" target="#b3">[4]</ref>. For single-stream design, the contextualized awareness is directly integrated into the basic convolution function to re-calibrate the filter weight: <ref type="bibr" target="#b32">[33]</ref> with depth similarity, <ref type="bibr" target="#b35">[36]</ref> with a malleable depth-aware function, and <ref type="bibr" target="#b37">[38]</ref> with channel attention. Despite the popularity of attention modules in previous works, the capability of modeling long-range dependencies is still limited due to the fixed shape of the convolutional receptive field, i.e., within the 8 neighboring pixels for a conventional 3 ? 3 convolution. In contrast, we propose a depth-adapted sampling position to explicitly leverage both global and local awareness in a simple yet efficient manner. By designing a geometryconstrained offset, we aim to break the conventional receptive field to adapt to the perspective effect, yielding an effective depth-guided 2D CNN to improve the RGB understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Non-local Adaptive Model</head><p>There are extensive surveys on attention modules in the literature. In this section, we briefly review several related deformable attention modules. In the early stages, the spatial transformer <ref type="bibr" target="#b39">[40]</ref> aligns the feature map to alleviate the 2D rotation problem. Dilated operation <ref type="bibr" target="#b40">[41]</ref> enables a larger receptive field for the convolution. Deformable-CNN (Deform-CNN) <ref type="bibr" target="#b0">[1]</ref> learns a deformable and dense receptive fields for convolution to augment spatial sampling location. Sharing a similar idea, non-local neural network <ref type="bibr" target="#b1">[2]</ref> builds convolutional blocks with contextualized awareness.</p><p>The idea with long-range dependencies has also been proved in recent transformer works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b41">42]</ref> and achieves leading performance in various vision tasks. Several works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> discover that the global transformer attention is always a constraint around local regions, demonstrating the interest of further development in converging both global and local attention. CPVT <ref type="bibr" target="#b6">[7]</ref> proposes a conditional positional encoding to enhance the local awareness in the transformer backbone. Deformable Transformer <ref type="bibr" target="#b42">[43]</ref> learns local attention with a small set of key sampling points around a reference. Swin Transformer <ref type="bibr" target="#b5">[6]</ref> presents a hierarchical manner to aggregate local shifted attention through different scales. Meanwhile, the recent ACmix <ref type="bibr" target="#b7">[8]</ref> shows that self-attention and convolution are mutually beneficial and can be elegantly integrated through 1 ? 1 convolutions. From another perspective, ConvNext <ref type="bibr" target="#b8">[9]</ref> re-design a ResNet with a larger receptive field and achieves better performance compared to transformer counterparts.</p><p>Despite the demonstrated promising results in previous works, we observe that the contextualized awareness are or learned through gradient descent, e.g., the positional encoding in CPVT and the offset in Deformable works, or learned through a pre-defined large receptive field, e.g., global attention in transformer and large kernel size in ConvNext. In the case of multi-modal feature learning, we seek to compute the global awareness from the additional prior. This perspective has been widely studied in the field of spherical images where the global attention is computed according to the distortion priors <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47]</ref>. Inspired by these works, we propose to compute the non-local awareness from the depth priors, making the convolution geometry-aware for RGB-D semantic segmentation. A concurrent work SConv <ref type="bibr" target="#b47">[48]</ref> learns the offset from depth image. Our approach resembles the SConv in that both methods belong to depth-adapted convolution frameworks. However, one main difference is that our offset is purely defined by the geometric without requiring any gradient descent, while SConv applies convolutional layers to learn the offset from latent space. Our approach explicitly leverages the scale changes along the Z-axis of camera coordinates and outof-the-plane rotation. Instead of adding extra learning parameters, we show that a simple and intuitive local deformation can contribute to semantic segmentation with minimal cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DEPTH-ADAPTED CONVOLUTIONAL NETWORK</head><p>In this section, two depth-adapted operations are presented: depth-adapted convolution and depth-adapted average pooling. <ref type="figure" target="#fig_2">Figure 2</ref> shows the information propagation in our network.</p><p>First, we take a 2D conventional regular and fixed area on the depth map, which corresponds to a conventional receptive field, e.g., 3 ? 3 convolution. We back-project the pixels to the 3D scene to get the 3D position in the camera coordinate. Second, we compute a depth-aware plane that passes through the real-world position of the kernel center and fits the best to all 3D points. Third, we create a 3D regular grid on this plane with an adapted orientation to fit the geometry. Last, we project this 3D grid on the image plane to form a 2D depth-adapted sampling grid.</p><p>Our model requires 2 inputs: input feature map and depth map (ground truth or estimated). The feature map is denoted as x ? R c i ?h?w , where c i is the number of input feature channel, h and w are the height and weight of the input feature map. The depth map is denoted as D ? R h?w . D is used to adapt the spatial sampling locations by computing the offset, denoted as ?p ? R c o f f ?h 1 ?w 1 , where h 1 and w 1 are the height and weight of the output feature map and c o f f = 2 ? N ? N for a N ? N filter. Different from Deformable ConvNet, our offset does not require gradient during back-propagation. The output feature map is denoted as </p><formula xml:id="formula_0">y ? R c o ?h 1 ?w 1 , where c o is the number of output feature channel. Deformable Back-Projection LMS 2D Grid 3D Points 3D Plane Depth-Adapted 2D Grid Z-ACN (ours) ? Projection ( ) ?( ) ( ) ? (a) Standard (b) Deformable (c) Depth (d) Z-ACN (ours) 3D Grid Depth-Adapted</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Depth-Adapted Convolution</head><p>A standard image convolution is formulated as:</p><formula xml:id="formula_1">y(p) = ? p n ?R(p) w(p n ) ? x(p + p n ),<label>(1)</label></formula><p>where w is the weight matrix. R(p) is the grid for point p.</p><p>Physically it represents a local neighborhood on input feature map, which conventionally has regular shape with certain dilation ?d, such that :</p><formula xml:id="formula_2">R(p) = a u + b v<label>(2)</label></formula><p>where ( u, v) is the pixel coordinate system of input feature map and (a, b) ? (?d ? {?1, 0, 1}) 2 .</p><p>To exploit the 3D planarity, depth-adapted convolution simply adds an adapted deformation term ?p to adjust the spatial sampling locations :</p><formula xml:id="formula_3">y(p) = ? p n ?R(p) w(p n ) ? x(p + p n + ?p n )<label>(3)</label></formula><p>The convolution may be operated on the irregular positions p n + ?p n as the offset ?p n may be fractional. To address the issue, we use the bilinear interpolation which is the same as that proposed in <ref type="bibr" target="#b0">[1]</ref>. In the following subsections, we will present how to process this offset from traditional computer vision algorithms.</p><p>1) 3D Planarity: To compute the offset, firstly we assume that the camera fits the pinhole model. Therefore, with the camera parameters, we can back-project 2D pixels within the conventional field R(p) into camera coordinates, forming the 3D point cloud P i = (X i ,Y i , Z i ) . An analysis of the intrinsic parameters is presented in Section V. Let p = (u 0 , v 0 ) be the center of 2D receptive field and P 0 = (X 0 ,Y 0 , Z 0 ) the associated back-projection on 3D space. The plane ? passing through P 0 and fitting the best to all P i can be extracted by applying the least square method:</p><formula xml:id="formula_4">? ? n = arg min ? ? n =(n 1 ,n 2 ,n 3 ) || ? ? n ||=1 ? i || ? ? n ? ? ? ? P 0 P i || 2<label>(4)</label></formula><p>where ? ? n = (n 1 , n 2 , n 3 ) is an approximation of the normal of the plane ?.</p><p>Basing on the plane ?, we build a new planar and regular grid, denoted as R 3D (P 0 ), which is centered on P 0 . The regular shape is defined by an orthonormal basis ( x , y ) on the plane ?. We fix x as horizontal ( x = (?, 0, ? )). As x is on the plane ? defined by its normal n = (n 1 , n 2 , n 3 ) and ( x , y ) are the orthonormal basis, we have :</p><formula xml:id="formula_5">x ? n = 0; || x || 2 = 1; || n|| 2 = 1; n ? x = y .<label>(5)</label></formula><p>Analytically, we can obtain ( x , y ) as follow:</p><formula xml:id="formula_6">x = ? ? ? ? ? ? ? n 3 ? 1?n 2 2 0 ? n 1 ? 1?n 2 2 ? ? ? ? ? ? ? , y = ? ? ? ? ? ? ? ? n 1 n 2 ? 1?n 2 2 1 ? n 2 2 ? n 2 n 3 ? 1?n 2 2 ? ? ? ? ? ? ?<label>(6)</label></formula><p>To conclude, R 3D (P 0 ) is defined as :</p><formula xml:id="formula_7">R 3D (P 0 ) = a x + b y<label>(7)</label></formula><p>with</p><formula xml:id="formula_8">(a, b) ? (?k u , 0, k u ) ? (?k v , 0, k v ) where (k u , k v ) are scale factors.</formula><p>A conventional 2D convolution on the image plane can be considered as realizing a planar convolution on a frontoparallel plane on the 3D camera basis. While the depth value is constant, our depth-adapted plane ? ? n becomes the same as the fronto-parallel plane. Otherwise, our plane ? ? n can better explore the perspective effect compared to the counterpart, yielding a depth adapted sampling position R 3D (P 0 ) in the camera basis.</p><p>2) Scale Factor: The scale factors are designed to be constant such that the 3D receptive field of each point from the feature map has the same size. In such a way, with the variance of depth, due to the perspective effect, the projected 2D receptive field on the image plane will have different sizes. The value of scale factors can be empirically set in different tasks. In our application, we want the adapted convolution performs the same as a conventional 2D convolution on a particular point p(u 0 , v 0 ) whose associated plane in Eq. 4 is fronto-parallel {Z|Z = Z 0 }. By taking into account the dilation ?d and the camera focal length ( f u , f v ), we have:</p><formula xml:id="formula_9">k u = ?d ? Z 0 f u k v = ?d ? Z 0 f v .<label>(8)</label></formula><p>3) Depth-Adapted Sampling Position: To form the depthadapted sampling position, we denote R'(p) as the projection of R 3D (P 0 ) on the image plane :</p><formula xml:id="formula_10">y(p) = ? p n ?R'(p) w(p) ? x(p + p n ) = ? p n ?R(p) w(p) ? x(p + p n + ?p n ).<label>(9)</label></formula><p>Different from the conventional grid R(p), the newly computed R'(p) breaks the regular size and shape structure with the additional offset. In such a way, the geometry information is incorporated in RGB CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Depth-Adapted Average Pooling</head><p>A standard average pooling is defined as :</p><formula xml:id="formula_11">y(p) = 1 |R(p)| ? p n ?R(p) x(p + p n ).<label>(10)</label></formula><p>This treats every pixel equally regardless of its associated geometry information, e.g. whether they belong to the same plane or not. To address this issue, similar to depth-adapted convolution, we add an extra offset to adjust the pooling field to the geometry. We force pixels sharing the same plane to contribute more to the corresponding output. For each pixel location p 0 , the depth-adapted average pooling operation becomes :</p><formula xml:id="formula_12">y(p) = 1 |R(p)| ? p n ?R(p) x(p + p n + ?p n ).<label>(11)</label></formula><p>C. Understanding Depth-Adapted operations</p><p>In <ref type="figure" target="#fig_2">Figure 2</ref> we show several examples of depth-adapted sampling positions of given input neurons (the center) on an RGB image. We seek to profit from the depth cues to articulate both photometric and geometric information for RGB CNN. Our method integrates the geometry into the convolution by adjusting the 2D sampling grid. This pattern is integrated into Eq. 3. In the case of conventional CNN, the shape of the grid is fixed as regular, which has difficulty adapting to the perspective effect. With the proposed Z-ACN, we can better leverage the geometric constraint in the sampling position. As shown in <ref type="figure" target="#fig_2">Figure 2</ref>, the receptive field for a closer input neuron in the 3D space is larger than that of a geometrically farther neuron. Sampling positions on the same plane also have different shapes that are adapted to the camera-projection effect. These patterns improve 2D CNN's performance with contextualized awareness without complicating the network with extra learning parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental setup</head><p>Dataset and metrics. We evaluate the effectiveness of our approach on both indoor and outdoor RGB-D semantic segmentation benchmarks, including NYUv2 dataset <ref type="bibr" target="#b48">[49]</ref>, SUN RGBD dataset <ref type="bibr" target="#b49">[50]</ref> and KITTI dataset <ref type="bibr" target="#b50">[51]</ref>. For the NYUv2 dataset, it contains 1,449 RGB-D images which are split into 795 training images and 654 testing images. For SUN-RGBD, it contains 37 categories of objects and consists of 10,335 RGB-D images which are split into 5,285 training images and 5,050 testing images. For the KITTI dataset, we use the semantic segmentation annotation provided in <ref type="bibr" target="#b51">[52]</ref>, which contains 70 training and 37 testing images from different scenes, with high-quality pixel annotations in 11 categories. The performance is evaluated with common metrics, i.e., Pixel Accuracy (PixelAcc), Mean Accuracy (mAcc.), Mean Region Intersection Over Union (mIoU), and Frequency Weighted Intersection Over Union (f.w.IoU).</p><p>Implementation details. Our approach requires paired RGB-D images as input. The depth map is first used to generate the geometry-aware offset which is further integrated into the network. As HHA encoding, the offset generation can be also realized during pre-processing since our method does not require gradient descent. We follow the same learning settings for both our proposed network and the baseline counterpart. Experiments are realized with 2 Nvidia V100 GPUs under the PyTorch framework. During inference, we apply a single-scale inference strategy.</p><p>Comparison protocol. We evaluate the generalization capability of our approach with different backbones, including old-fashioned VGG-16 encoders and popular ResNet encoders. We seek to demonstrate that our approach can constantly improve the baseline performance. To purely analyze the gain by applying our approach, we only replace the vanilla convolution and average pooling with our proposed depthadapted operators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. With VGG-16 backbone</head><p>Comparison with D-CNN <ref type="bibr" target="#b32">[33]</ref>: D-CNN is the pioneering work that integrates depth into the basic operations of convolutional networks. The depth is used to compute a similarity term to re-calibrate convolutional weight. We build our approach upon the DeepLab architecture, which is the same as D-CNN <ref type="bibr" target="#b32">[33]</ref>. Note that both the D-CNN and our approach belong GT + -ACN Per Class IoU improvement (%) <ref type="figure">Fig. 3</ref>. On the left we illustrate the qualitative comparison on the NYUv2 dataset. The two first columns are the input RGB and HHA, respectively. Baseline represents the semantic map obtained with early fused RGB-HHA input. + Z-ACN stands for the results obtained by inserting our depth-adapted sampling position into the baseline. It can be seen that by explicitly leveraging non-local attention, our method reasons about semantic maps closer to the ground truth (GT). The black regions in "GT" are the ignoring category. On the right we illustrate the per-class improvement above the baseline. We improve 29/37 classes with 5.2% mean IoU increment. to the depth-aware convolution framework. Unlike the D-CNN model, we update the depth information to break the limitation of a fixed structure, which can better leverage longrange dependencies while D-CNN seeks to refine the sampling position within the conventional receptive field. To evaluate our superior design, we follow the same training settings as D-CNN and conduct experiments on both NYUv2 and SUN-RGBD datasets. Since conventional backbones are pre-trained with RGB input, i.e., ImageNet <ref type="bibr" target="#b52">[53]</ref>, which is not designed for RGB-D tasks. Hence, we follow D-CNN and train our model from scratch. We refer authors to <ref type="bibr" target="#b32">[33]</ref> for more details on the training strategies. The quantitative comparison can be found in <ref type="table" target="#tab_1">Table I</ref>. We only extract features from RGB input images. The baseline model is with VGG-16 encoder under Deeplab <ref type="bibr" target="#b53">[54]</ref> architecture. D-CNN stands for the performance obtained by adding depth-aware re-calibration on both convolution and pooling. Z-ACN is the result obtained with our proposed convolution and pooling where we explicitly integrate the contextualized awareness in the basic operators. Our method can achieve superior performance over the counterpart, validating the effectiveness of our depth guided sampling position which can better model geometric priors compared to D-CNN.</p><p>Comparison with pre-trained methods: We also evaluate our method with pre-trained weights, i.e., we initialize the weight with the pre-trained models and further fine-tune it on NYUv2 datasets. We build our approach upon the DeepLab architecture, which is the same as D-CNN <ref type="bibr" target="#b32">[33]</ref>. We report in <ref type="table" target="#tab_1">Table II</ref> the performances of different methods. It can be seen that our method performs favorably over other methods. Compared to <ref type="bibr" target="#b54">[55]</ref> which adapts a 3D CNN, our model remains a 2D CNN that requires less computational cost but achieves superior performance. Compared to our baseline, i.e., vanilla Deeplab, our Z-ACN enables significant improvements by encoding the depth information into the network. As D-CNN, our approach can also work well with the early fused RGB-HHA input, yielding a further improvement in the performance. The quantitative results validate that our operators are more effective in merging multi-modal features compared to the counterpart and set the new state-of-the-art performance with the VGG-16 encoder. The qualitative comparison can be found in <ref type="figure">Figure 3</ref> which shows the improvement of our approach over the baseline. The two first columns show the input RGB image and input HHA map. Baseline is the result obtained with early fused RGB-HHA input. + Z-ACN denotes that we further apply our approach over the baseline. It can be seen that our approach can favorably improve scene understanding over the counterparts by explicitly leveraging the depth cues, yielding more accurate semantic maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. With ResNet backbones</head><p>Plug in SOTA ESAnet <ref type="bibr" target="#b57">[58]</ref>: The current SOTA CNN performance on RGB-D semantic segmentation is achieved with ESAnet. To evaluate the generalization properties of our approach, we plug our Z-ACN into ESAnet, aiming to further improve the performance with additional depth-awareness. Compared to VGG encoders, ResNet encoders are deeper with more convolutions. Hence, replacing all convolutions with depth-adapted convolutions will yield more computational cost. As suggested in previous work <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b59">60]</ref>, the geometric cues play a more vital role in the first convolutional layers. Therefore, to find the best trade-off between the computational cost and the performance, we simply add a 3?3 depth-adapted convolution before the RGB encoder. This operation can be regarded as a spatial-aware positional encoding that improves the discriminability of RGB features. The gain by further adding our Z-ACN can be found in <ref type="table" target="#tab_1">Table III</ref>. With our depth-adapted operator, the new model performs favorably over the ESAnet baseline under different backbones, demonstrating the generalization capability of our approach which can easily be embedded into any existing backbones. Furthermore, since both our approach and the counterpart shares the same architecture, the improvement is purely attributed to our depth-awareness, validating the effectiveness of our geometry-guided sampling position.</p><p>Comparison with RGB-D attention convolutions: To evaluate our Z-ACN, we compare our approach with two recent RGB-D attention convolutions, ShapeConv <ref type="bibr" target="#b37">[38]</ref> and SConv <ref type="bibr" target="#b47">[48]</ref>. ShapeConv decomposes the features within the receptive field into a base component and the remaining which are then calibrated with two additional learning weights before the convolution. The base component is computed by the mean function to squeeze the spatial resolution, which can be regarded as the additional channel attention for convolution. Different from ShapeConv which is not specially dedicated to RGB-D tasks, we explicitly leverage the depth prior to deform the convolutional sampling position, yielding a simple but efficient manner to integrate the spatial attention into convolution. Meanwhile, the concurrent SConv proposes a learning strategy to infer a depth-aware offset from latent space. However, for the same scene, the learned offset may vary under different settings such as different training strategies or backbones. As shown in <ref type="figure" target="#fig_3">Figure 4</ref>, while the backbone changes, SConv yields different sampling positions. Intuitively, the depth-aware offset should be only dependent on the geometry and independent of the learning factors. Different from SConv, our offset is computed without any learning parameters, making our depthawareness constant under different environments. Further, we show through <ref type="figure" target="#fig_3">Figure 4</ref> that our computed receptive field can favorably describe the perspective effect over the counterpart. Besides, we report in <ref type="table" target="#tab_1">Table IV</ref> the model size for each method. Similar to ShapeConv, our method does not add additional parameters above the baseline and is more efficient compared to SConv which requires additional learning costs. <ref type="table">Table V</ref> illustrates the quantitative comparison with other attention convolutions. Under the consideration of a fair comparison, we embed all the operators into the ESAnet baseline and retrain them under the same settings. It can be seen that our Z-ACN outperforms the concurrent approaches with a large margin under all backbones. This highlights the effectiveness of our depth-constraint attention compared to channel attention (ShapeConv) and learned depth attention (SConv).</p><p>Comparison with SOTA performance: We compare the performance of our Z-ACN with other state-of-the-art models. The quantitative results can be found in <ref type="table" target="#tab_1">Table VI</ref>. Our Z-ACN sets the new state-of-the-art performance in NYUv2 datasets. Compared to ShapeConv when both methods use ResNet-101 as the backbone and adopt single-scale inference, our approach achieves 3.8% mIoU improvement.</p><p>Outdoor scene: We also evaluate our approach to the outdoor scene, e.g., KITTI <ref type="bibr" target="#b50">[51]</ref>. The vanilla KITTI dataset provides RGB and lidar input. We take the dataset presented in <ref type="bibr" target="#b51">[52]</ref> which provides a dense depth map. We validate all methods on the held-out testing set due to the smaller size and lack of a proposed validation split.</p><p>We adopt the same modified ResNet-18 as presented in <ref type="bibr" target="#b36">[37]</ref> as our backbone with skip-connected fully convolutional architecture <ref type="bibr" target="#b25">[26]</ref>. The conventional convolution is replaced by our proposed operator.</p><p>Our model is compared with 3D representation such as PointNet <ref type="bibr" target="#b12">[13]</ref>, Conv3D <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b63">64]</ref> and 2D representation such as DeformCNN <ref type="bibr" target="#b0">[1]</ref> and SurfConv <ref type="bibr" target="#b36">[37]</ref>. Conv3D <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b63">64]</ref> and (e) is the receptive field computed by our approach. SConv adopts a learning diagram to generate the receptive field, resulting in different shapes for different backbones. However, our method explicitly leverages the geometric constraint for the perspective effect. Our whole process is realized without learning parameters, making the depth-adapted sampling position independent from the neural network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone PixelAcc mAcc mIoU f.w.IoU ACNet <ref type="bibr" target="#b24">[25]</ref> ResNet-50 --48.3 -2.5D <ref type="bibr" target="#b34">[35]</ref> ResNet-101 75. PointNet <ref type="bibr" target="#b12">[13]</ref> use the hole-filled dense depth map provided by the dataset to create 3D input. For PointNet, the source code is used to use RGB plus gravity-aligned point cloud (pcl). The recommended configuration <ref type="bibr" target="#b12">[13]</ref> is used to randomly sample points. The sample number is set to be 25k. For Conv3D, the SSCNet architecture <ref type="bibr" target="#b18">[19]</ref> is used and is trained with flipped -TSDF and RGB. The resolution is reduced to 240 ? 144 ? 240 voxel grid. For DeformCNN, RGB images and HHA images are chosen as input for a fair comparison. For SurfConv, we compare with their best performance, which requires a resampling on the input image to be adapted to the 8 levels of depth. For all the above-mentioned models, we follow the same configuration and learning settings as discussed in <ref type="bibr" target="#b36">[37]</ref>. The quantitative result is reported in <ref type="table" target="#tab_1">Table VII</ref> that all methods are trained from scratch following <ref type="bibr" target="#b36">[37]</ref>. While dealing with an outdoor scene, 3D methods such as point cloud suffer from computational costs compared to 2D CNN which extracts features from images. It is also the case for Conv3D <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b63">64]</ref> since voxelizing the whole 3D space is time-consuming. Compared to these 3D methods, our model remains 2D CNN but achieves a better result. DeformCNN <ref type="bibr" target="#b0">[1]</ref> takes into RGB + HHA as input and learns offsets to deform the sampling position. Nevertheless, the offset is learned from the input feature maps which do not explicitly leverage the geometric constraints. In contrast, our model computes the offset from low-level constraint, i.e., 1-channel depth, with traditional algorithms and does not require gradient descent. The result in <ref type="table" target="#tab_1">Table VII</ref> shows that our model performs favorably over DeformCNN without extra learning parameters, validating the effectiveness of our depth-adapted sampling position. Surf-Conv is a concurrent work that incorporation 3D information into 2D CNN. However, it requires additional pre-processing on the input data such that depth-guided image resampling. Instead, we encode the depth into the CNN via the bias of offset. Compared to the concurrent method, our approach achieves large performance gains. We present in <ref type="table" target="#tab_1">Table VIII</ref> the quantitative comparison over the baseline with weight initialization. Baseline 1 and Baseline 2 represent the result obtained with RGB input and early fused RGB-HHA input, respectively. + Z-ACN stands for the results obtained by inserting our depth-adapted offset into the baseline. It can be seen that our methods can significantly enable gains over the baseline performance with improved depth-awareness. We illustrate in <ref type="figure">Figure 5</ref> the per-class IoU improvement with RGB input. Compared to the baseline, our approach enables improvement on 7/11 objects, especially "salient" objects in the urban scene such as the vehicle, cyclists, and pedestrians. However, we also observe that our approach achieves lower performance in detecting lanemark. This is because the lanemark is co-planar as the road that the confusing geometrical information may add noises for our depth-adapted model.</p><p>The qualitative comparison over the baseline is shown in <ref type="figure">Figure 5</ref>. The two first columns show the input RGB image and input HHA map. Baseline is the result obtained with GT + -ACN Per Class IoU improvement (%) <ref type="figure">Fig. 5</ref>. On the left we illustrate the qualitative comparison on the NYUv2 dataset. The two first columns are the input RGB and HHA, respectively. Baseline represents the semantic map obtained with early fused RGB-HHA input. + Z-ACN stands for the results obtained by inserting our depth-adapted sampling position into the baseline. It can be seen that our approach can also improve the baseline performance in outdoor scenes. The black regions in "GT" are the ignoring category. On the right, we illustrate the per-class improvement above the baseline. We improve 7/11 objects with 3.7% mean IoU increment. Segmentation results on the KITTI test dataset. GT stands for ground truth. The black regions in "GT" are the ignoring category. early fused RGB-HHA input. + Z-ACN denotes that we further replace the baseline convolution with our approach. By explicitly leveraging the geometry, our approach constrains the network to pay more attention to boundaries and reason about semantic maps with higher accuracy. We observe that objects like the vehicle, pedestrian, and cyclist are better segmented, as well as the sign. Recognizably, these objects do not share the same depth compared to the background (road or pavement). Hence, our adapted sampling position contributes to improving the discriminability of these salient objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. ADDITIONAL STUDIES AND DISCUSSIONS</head><p>In this section, we conduct additional studies on the NYUv2 dataset to validate the efficiency, robustness, and flexibility of our operators. We choose VGG-16 with Deeplab as the baseline. The features are extracted from RGB images. The depth map is used to guide the sampling position.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Intrinsic Parameters</head><p>Our model requires the intrinsic parameters to back-project the pixels to the 3D scene and project the depth-adapted 3D planar grid to the image plane. This pattern is integrated into Eq. 4 and Eq. 8. Demanding camera parameters as priors can be a strong assumption that limits the application. Therefore, we evaluate the performance with a randomly set camera matrix.</p><p>The intrinsic parameters include the principal point and the focal length. However, most models resize the input image shape, which results in difficulties in using the official principal point value. Hence, we assume that the principal point is the same as the center of the input image and chose a random value for focal length. We set ( f u , f v ) = (100, 100) for NYUv2 dataset, where the official value is around (519, 519). We retrain the new model under the same training setting. The quantitative result is reported in <ref type="table" target="#tab_1">Table IX</ref>. We denote k r , the result obtained with randomly chosen intrinsic parameters, and GT , the result obtained with official values. It can be seen that with an arbitrary value for intrinsic parameters, our model can still achieve favorable performance compared to the baseline. Compared to the result obtained with GT intrinsic value, the loss is only 0.2% for mAcc and 0.9% for mIoU. The result validates that our model can get rid of the assumption of the input intrinsic parameters under the condition that they are logically chosen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ablation Study</head><p>To further verify the functionality of both depth-adapted convolution and depth-adapted average pooling, the following experiments are conducted.  <ref type="table" target="#tab_10">Table X</ref>. While learning from scratch, our operators can effectively extract features with geometric relationships and improve the segmentation performance. By comparing (a) and (b), we only replace deep convolution with our approach, i.e., the first convolution of layer 5 of VGG-16, we achieve a 3.6% gain on mIoU. (c) illustrates the result with the first convolution of all layers replaced by our approach. Our Z-ACN enables a 5.7% gain compared to the baseline (a). Finally, by introducing the depthadapted average pooling (d), we observe that the performance can be further promoted, validating the effectiveness of our depth-adapted pooling method. While learning from the pre-trained model, we firstly want to argue that the existing weight may not be fair nor suitable for our adapted convolution. The existing weight is learned with a fixed size and shape structure, while our adapted convolution breaks this limitation. The most suitable pretrained weight for our operator might require training our depth-adapted model on ImageNet, which is impossible since the depth information is not available on this dataset.</p><p>Nevertheless, we still show that our approach can benefit from the conventional pre-trained weights. By fine-tuning the weights, <ref type="table" target="#tab_10">Table X</ref> illustrates that replacing the first convolution from a deep layer contributes the most to the performance by 1.8% over the baseline. By introducing the depth-adapted average pooling, the performance can be further promoted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>In this paper, we propose a novel 2D CNN to include geometric information in RGB CNN. Different from previous works that integrate the channel or spatial attention into convolution through learning methods, our network fully leverages the geometric constraint additional training parameters, making the depth-awareness independent of the learning settings. We introduce two basic depth-adapted operators that can be easily integrated into the existing CNN model. Extensive studies demonstrate the generalization properties of our methods which perform favorably over the baseline and other convolutions. Experiments on challenging RGB-D datasets demonstrate that our approach performs well over the state-of-the-art methods by large margins. In future works, we will try to extend the application to other popular tasks instance segmentation, object detection, or even passing from image to 3D data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Conventional Receptive Field (b) Adapted to Depth PriorsFig. 1. A sketch of Depth-Adapted Sampling position. We explicitly leverage the depth priors to compute a locally-deformable sampling position, yielding a simple but efficient manner to introduce a global-local attention into CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Illustration of depth-adapted CNN. On the left we show the example of a 3 ? 3 kernel: a) shows a standard 2D convolution with dilation equal to 1. b) shows the offset computed from deformable convolution<ref type="bibr" target="#b0">[1]</ref>. c) is the available depth data. The represented figure shows a linear change with the depth value. From left to right, the scene becomes deeper. d) illustrates offset computed by Z-ACN which is adapted to depth. On the right, we illustrate the overview of our approach. LMS stands for Least Mean Square algorithm. ( x , y ) are the 3D unit axis. Firstly, pixels within the 2D receptive field are back-projected to 3D space to form a point cloud, based on which a 3D plane is computed with normal n . Secondly, a new 3 ? 3 grid on the 3D space is created with the help of 3D axis ( x , y ) which are perpendicular to the normal n. Finally, the 3D grid is projected to the image plane, forming our depth-adapted sampling position. Zoom in for more details on the depth-guided sampling position on the RGB image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Visual comparison with concurrent SConv [48]. (a-b) are the input RGB and Depth. (c-d) illustrates the learned sampling position from SConv for different ResNet backbones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>?</head><label></label><figDesc>For results trained from scratch: we analyze a) baseline performance, b) a deep layer convolution replaced by Z-ACN, c) first convolutions from all layers replaced by Z-ACN, d) CNN replaced by Z-ACN including the average pooling. ? For results trained from pre-trained weight: we analyze a) baseline performance, b) the first convolution from a deep layer replaced by Z-ACN, c) the second convolution from a deep layer replaced by Z-ACN, d) the third convolution from a deep layer replaced by Z-ACN, e) CNN replaced by Z-ACN including the average pooling. Experimental results are reported in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, C. Stolz, and C. Demonceaux are with ImViA, Universit? Bourgogne Franche-Comt?, Dijon, France (e-mail: {zongwei wu@etu.; christophe.stolz@; cedric.demonceaux@}u-bourgogne.fr )</figDesc><table /><note>G. Allibert is with Universit? C?te d'Azur, CNRS, I3S, Nice, France (e- mail: allibert@i3s.unice.fr) Z. Wu and C. Ma are with MOE Key Lab of Artificial Intelligence, AI Insti- tute, Shanghai Jiao Tong University, Shanghai, China (chaoma@sjtu.edu.cn)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I COMPARISON</head><label>I</label><figDesc>WITH D-CNN [33]. BOTH MODELS ARE TRAINED FROM SCRATCH WITH THE SAME TRAINING SETTINGS. OUR METHOD ACHIEVES BETTER PERFORMANCE UNDER DIFFERENT DATASETS, SHOW THAT THE DEPTH PRIORS ARE BETTER EXPLOIT WITH OUR Z-ACN.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>NYUv2</cell><cell></cell><cell></cell><cell cols="2">SUN-RGBD</cell></row><row><cell>Method</cell><cell cols="6">RGB D-CNN Z-ACN RGB D-CNN Z-ACN</cell></row><row><cell cols="2">PixelAcc (%) 50.1</cell><cell>60.3</cell><cell>73.5</cell><cell>66.6</cell><cell>72.4</cell><cell>78.4</cell></row><row><cell>mIoU (%)</cell><cell>15.9</cell><cell>27.8</cell><cell>28.4</cell><cell>22.8</cell><cell>29.7</cell><cell>30.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II QUANTITATIVE</head><label>II</label><figDesc></figDesc><table><row><cell cols="3">COMPARISON WITH VGG-16 BASED METHODS ON</cell></row><row><cell cols="3">NYUV2 DATASET. OUR METHOD SIGNIFICANTLY BOOSTS THE</cell></row><row><cell cols="3">PERFORMANCE OVER THE BASELINE AND SETS A NEW RECORD ON</cell></row><row><cell cols="2">VGG-16-BASED APPROACHES.</cell><cell></cell></row><row><cell>Model</cell><cell>Learned features</cell><cell>mIoU (%)</cell></row><row><cell>SurfConv [37]</cell><cell>RGB + HHA</cell><cell>31.0</cell></row><row><cell>Eigen et al. [56]</cell><cell>RGB + HHA</cell><cell>34.1</cell></row><row><cell>3DGNN [55]</cell><cell>RGB</cell><cell>39.9</cell></row><row><cell>Std2p [57]</cell><cell>RGB + HHA</cell><cell>40.1</cell></row><row><cell>D-CNN [33]</cell><cell>RGB</cell><cell>41.0</cell></row><row><cell>CFN [28]</cell><cell>RGB + HHA</cell><cell>41.7</cell></row><row><cell>D-CNN [33]</cell><cell>RGB + HHA</cell><cell>43.9</cell></row><row><cell>Baseline</cell><cell>RGB + HHA</cell><cell>40.4</cell></row><row><cell>Z-ACN (Ours)</cell><cell>RGB</cell><cell>42.5</cell></row><row><cell>Z-ACN (Ours)</cell><cell>RGB + HHA</cell><cell>45.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III QUANTITATIVE</head><label>III</label><figDesc>COMPARISON WITH THE BASELINE ESANET ON NYUV2 DATASET. BY SIMPLY ADDING AN DEPTH-ADAPTED CONVOLUTION, OUR METHOD PERFORMS FAVORABLY OVER THE BASELINE WITH DIFFERENT BACKBONES, DEMONSTRATING THE GENERALIZATION CAPABILITY OR OUR Z-ACN.</figDesc><table><row><cell>Backbone</cell><cell>Setting</cell><cell>mIoU (%)</cell><cell>Improvement ? (%)</cell></row><row><cell>ResNet-18</cell><cell>ESAnet Ours</cell><cell>46.28 47.02</cell><cell>0.74</cell></row><row><cell>ResNet-34</cell><cell>ESAnet Ours</cell><cell>48.13 49.15</cell><cell>1.02</cell></row><row><cell>ResNet-50</cell><cell>ESAnet Ours</cell><cell>49.02 50.05</cell><cell>1.03</cell></row><row><cell>ResNet-101</cell><cell>ESAnet Ours</cell><cell>49.44 51.24</cell><cell>1.76</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV MODEL</head><label>IV</label><figDesc>SIZE WITH DIFFERENT ATTENTION CONVOLUTIONS. WE CHOOSE RESNET-18 AS THE BACKBONE. SIMILAR TO SHAPECONV, OUR METHOD DO NOT ADD EXTRA LEARNING PARAMETERS ON TOP OF THE BASELINE. DIFFERENT FROM SCONV, WE COMPUTE THE OFFSET IN A NON LEARNING MANNER, YIELDING A EFFICIENT MANNER TO EXPLICITLY LEVERAGE THE DEPTH ATTENTION IN 2D CNN. NYUV2 DATASET. ALL METHODS ARE IMPLEMENTED ON THE ESANET BASELINE AND TRAINED UNDER THE SAME SETTINGS. OUR</figDesc><table><row><cell cols="5">ResNet-18 ESAnet [58] + SConv [48] + ShapeConv [38] + Ours</cell></row><row><cell>Size (Mb)</cell><cell>304</cell><cell>+1</cell><cell>+0</cell><cell>+0</cell></row><row><cell></cell><cell></cell><cell>TABLE V</cell><cell></cell><cell></cell></row><row><cell cols="5">QUANTITATIVE COMPARISON WITH OTHER ATTENTION CONVOLUTION</cell></row><row><cell cols="5">METHODS ON APPROACH ACHIEVES BETTER MIOU COMPARED TO CONCURRENT WORKS</cell></row><row><cell cols="5">UNDER DIFFERENT BACKBONES, VALIDATING THE EFFECTIVE OF OUR</cell></row><row><cell cols="4">GEOMETRY-CONSTRAINED SAMPLING POSITION.</cell><cell></cell></row><row><cell cols="2">Backbone Setting</cell><cell cols="3">PixelAcc mAcc mIoU f.w.IoU</cell></row><row><cell></cell><cell>+SConv</cell><cell>74.19</cell><cell cols="2">60.01 46.93 60.26</cell></row><row><cell>ResNet-18</cell><cell>+ShapeConv</cell><cell>74.11</cell><cell cols="2">59.37 46.38 60.61</cell></row><row><cell></cell><cell>+Ours</cell><cell>74.35</cell><cell cols="2">59.82 47.02 60.73</cell></row><row><cell></cell><cell>+SConv</cell><cell>74.95</cell><cell cols="2">61.08 47.99 61.77</cell></row><row><cell>ResNet-34</cell><cell>+ShapeConv</cell><cell>74.68</cell><cell cols="2">61.07 47.70 61.20</cell></row><row><cell></cell><cell>+Ours</cell><cell>75.78</cell><cell cols="2">62.81 49.15 62.64</cell></row><row><cell></cell><cell>+SConv</cell><cell>76.13</cell><cell cols="2">62.36 49.04 63.00</cell></row><row><cell>ResNet-50</cell><cell>+ShapeConv</cell><cell>76.17</cell><cell cols="2">62.45 49.58 63.02</cell></row><row><cell></cell><cell>+Ours</cell><cell>75.88</cell><cell cols="2">63.55 50.05 62.99</cell></row><row><cell></cell><cell>+SConv</cell><cell>76.49</cell><cell cols="2">63.65 50.43 63.67</cell></row><row><cell>ResNet-101</cell><cell>+ShapeConv</cell><cell>76.45</cell><cell cols="2">63.28 50.10 63.46</cell></row><row><cell></cell><cell>+Ours</cell><cell>77.00</cell><cell cols="2">64.26 51.24 64.32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI PERFORMANCE</head><label>VI</label><figDesc>COMPARISON WITH SOTA METHODS ON NYUV2 DATASET. DENOTES THE MULTI-SCALE STRATEGY. OUR METHOD IS TESTED WITH SINGLE-SCALE INFERENCE STRATEGY AND SETS THE NEW STATE-OF-THE-ART PERFORMANCE AMONG RESNET BASED MODELS.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VII COMPARISON</head><label>VII</label><figDesc>ON KITTI TEST SET. OUR METHODS ACHIEVE BETTER PERFORMANCE COMPARED TO 3D APPROACHES AND THE CONCURRENT SURFCONV. IT IS WORTH NOTING THAT WITH SINGLE RGB INPUT, OUR</figDesc><table><row><cell cols="4">DEPTH-ADAPTED SAMPLING POSITION ENABLES SIGNIFICANT</cell></row><row><cell cols="4">IMPROVEMENT OVER OUR BASELINE, VALIDATING THE EFFECTIVENESS</cell></row><row><cell cols="4">OF DEPTH-GUIDED NON-LOCAL ATTENTION. MODELS ARE TRAINED</cell></row><row><cell></cell><cell>FROM SCRATCH.</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>Learned features</cell><cell cols="2">Acc (%) mIoU (%)</cell></row><row><cell>PointNet [13]</cell><cell>RGB + pcl</cell><cell>55.1</cell><cell>9.4</cell></row><row><cell>Conv3D [19, 64]</cell><cell>RGB + voxel</cell><cell>64.5</cell><cell>17.5</cell></row><row><cell>DeformCNN [1]</cell><cell>RGB + HHA</cell><cell>79.2</cell><cell>34.2</cell></row><row><cell>SurfConv-8 [37]</cell><cell>RGB + HHA</cell><cell>79.4</cell><cell>35.1</cell></row><row><cell>Baseline</cell><cell>RGB</cell><cell>79.3</cell><cell>31.3</cell></row><row><cell>Z-ACN (Ours)</cell><cell>RGB</cell><cell>79.7</cell><cell>33.5</cell></row><row><cell>Z-ACN (Ours)</cell><cell>RGB + HHA</cell><cell>80.1</cell><cell>35.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VIII QUANTITATIVE</head><label>VIII</label><figDesc>COMPARISON ON KITTI TEST SET. NETWORKS ARE TRAINED FROM PRE-TRAINED MODELS.</figDesc><table><row><cell>KITTI</cell><cell>Baseline 1</cell><cell>+ Z-ACN</cell><cell>Baseline 2</cell><cell>+ Z-ACN</cell></row><row><cell>mAcc (%)</cell><cell>48.3</cell><cell>49.5</cell><cell>51.8</cell><cell>55.1</cell></row><row><cell>mIoU (%)</cell><cell>39.1</cell><cell>40.6</cell><cell>41.6</cell><cell>45.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE IX EMPIRICAL</head><label>IX</label><figDesc>ANALYSIS ON THE INFLUENCE OF THE INTRINSIC PARAMETERS. ALL METHODS ARE TRAINED FROM PRE-TRAINED MODEL UNDER THE SAME SETTING.</figDesc><table><row><cell>NYUv2 (%)</cell><cell>mAcc</cell><cell>mIoU</cell></row><row><cell>Baseline</cell><cell>51.9</cell><cell>40.4</cell></row><row><cell>Z-ACN (k r )</cell><cell>53.4</cell><cell>41.6</cell></row><row><cell>Z-ACN (GT)</cell><cell>55.2</cell><cell>42.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE X RESULTS</head><label>X</label><figDesc>OF USING DEPTH-ADAPTED OPERATORS IN DIFFERENT LAYERS. EXPERIMENTS ARE CONDUCTED ON NYUV2 TEST SET. i STANDS FOR THE NUMBER OF CONVOLUTION LAYERS.</figDesc><table><row><cell></cell><cell>Configuration</cell><cell>mIoU (%)</cell></row><row><cell></cell><cell>Result from scratch</cell><cell></cell></row><row><cell>a)</cell><cell>Baseline</cell><cell>24.0</cell></row><row><cell>b)</cell><cell>Z-Conv5 1</cell><cell>27.6</cell></row><row><cell>c)</cell><cell>Z-Convi 1</cell><cell>29.7</cell></row><row><cell>d)</cell><cell>Z-Convi 1 + Z-AvgPool</cell><cell>30.4</cell></row><row><cell></cell><cell>Result from pre-trained</cell><cell></cell></row><row><cell>a)</cell><cell>Baseline</cell><cell>40.4</cell></row><row><cell>b)</cell><cell>Z-Conv5 1</cell><cell>42.2</cell></row><row><cell>c)</cell><cell>Z-Conv5 2</cell><cell>41.7</cell></row><row><cell>d)</cell><cell>Z-Conv5 3</cell><cell>41.7</cell></row><row><cell>e)</cell><cell>Z-Conv5 1 + Z-AvgPool</cell><cell>42.5</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENT This work was supported by the French National Research Agency through ANR CLARA (ANR-18-CE33-0004) and financed by the French Conseil R?gional de Bourgogne -Franche -Comt?.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision (ICCV)</title>
		<meeting>the IEEE international conference on computer vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Nonlocal neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.10882</idno>
		<title level="m">Conditional positional encodings for vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">On the integration of self-attention and convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.14556</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.03545</idno>
		<title level="m">A convnet for the 2020s</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Depth-adapted CNN for RGB-D cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Allibert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stolz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Demonceaux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision (ACCV)</title>
		<meeting>the Asian Conference on Computer Vision (ACCV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">3D Convolutional Neural Networks for landing zone detection from Lidar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">PointCNN: Convolution on x-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Relation-shape convolutional neural network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Densepoint: Learning densely contextual representation for efficient point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Segcloud: Semantic segmentation of 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 international conference on 3D vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="537" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for efficient and robust hand pose estimation from single depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Vehicle detection from 3D lidar using fully convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.07916</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view cnns for object classification on 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning rich features from RGB-D images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European conference on computer vision (ECCV)</title>
		<meeting>European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">ACNet: Attention based network to exploit complementary features for RGB-D semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Variational context-deformable convnets for indoor scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cascaded feature network for semantic segmentation of RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning common and specific features for RGB-D semantic segmentation with deconvolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bi-directional cross-modality feature propagation with separation-and-aggregation gate for RGB-D semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fusenet: Incorporating depth into semantic segmentation via fusion-based CNN architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Domokos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Asian conference on computer vision (ACCV)</title>
		<meeting>Asian conference on computer vision (ACCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning common and specific features for RGB-D semantic segmentation with deconvolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Depth-aware CNN for RGB-D segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">3D neighborhood convolution: Learning depth-aware features for RGB-D and RGB semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">2.5 D convolution for RGB-D semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Malleable 2.5 D convolution: Learning receptive fields along the depth-axis for RGB-D scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Surfconv: Bridging 3d and 2d convolution for RGBD images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Shapeconv: Shape-aware convolutional layer for indoor RGB-D semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">LSTM-CF: Unifying context modeling and fusion with LSTMs for RGB-D scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deformable DETR: deformable transformers for endto-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>K?hler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10130</idno>
	</analytic>
	<monogr>
		<title level="j">Spherical cnns</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Spherenet: Learning spherical representations for detection and classification in omnidirectional images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Coors</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Paul</forename><surname>Condurache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Distortion-aware convolutional filters for dense prediction in panoramic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tateno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Corners for layout: End-to-end layout recovery from 360 images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fernandez-Labrador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Facil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perez-Yus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Demonceaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Civera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Guerrero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1255" to="1262" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Spatial information guided convolution for realtime RGBD semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2313" to="2324" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from RGBD images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">SUN RGB-D: A RGB-D scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Multimodal information fusion for urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Davoine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Denoeux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Vision and Applications</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">3D graph neural networks for RGBD semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision (ICCV)</title>
		<meeting>the IEEE international conference on computer vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">STD2P: RGBD semantic segmentation using spatio-temporal data-driven pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Efficient RGB-D semantic segmentation for indoor scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Seichter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>K?hler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lewandowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wengefeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-M</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Omniflownet: a perspective neural network adaptation for optical flow estimation in omnidirectional images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-O</forename><surname>Artizzu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Allibert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Demonceaux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.13388</idno>
		<title level="m">Panoflow: Learning optical flow for panoramic images</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">3D graph neural networks for RGBD semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">RDFNet: RGB-D multi-level residual feature fusion for indoor semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision (ICCV)</title>
		<meeting>the IEEE international conference on computer vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">RGB-D co-attention network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision (ACCV)</title>
		<meeting>the Asian Conference on Computer Vision (ACCV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Semantic scene completion from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
