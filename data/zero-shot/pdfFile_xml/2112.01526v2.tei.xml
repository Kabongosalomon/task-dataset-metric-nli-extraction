<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MViTv2: Improved Multiscale Vision Transformers for Classification and Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karttikeya</forename><surname>Mangalam</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xiong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MViTv2: Improved Multiscale Vision Transformers for Classification and Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>* equal technical contribution</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we study Multiscale Vision Transformers (MViTv2) as a unified architecture for image and video classification, as well as object detection. We present an improved version of MViT that incorporates decomposed relative positional embeddings and residual pooling connections. We instantiate this architecture in five sizes and evaluate it for ImageNet classification, COCO detection and Kinetics video recognition where it outperforms prior work. We further compare MViTv2s' pooling attention to window attention mechanisms where it outperforms the latter in accuracy/compute. Without bells-and-whistles, MViTv2 has state-of-the-art performance in 3 domains: 88.8% accuracy on ImageNet classification, 58.7 AP box on COCO object detection as well as 86.1% on Kinetics-400 video classification. Code and models are available at https: //github.com/facebookresearch/mvit.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Designing architectures for different visual recognition tasks has been historically difficult and the most widely adopted ones have been the ones that combine simplicity and efficacy, e.g. VGGNet <ref type="bibr" target="#b66">[67]</ref> and ResNet <ref type="bibr" target="#b36">[37]</ref>. More recently Vision Transformers (ViT) <ref type="bibr" target="#b16">[17]</ref> have shown promising performance and are rivaling convolutional neural networks (CNN) and a wide range of modifications have recently been proposed to apply them to different vision tasks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr">21,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b89">90]</ref>.</p><p>While ViT <ref type="bibr" target="#b16">[17]</ref> is popular in image classification, its usage for high-resolution object detection and space-time video understanding tasks remains challenging. The density of visual signals poses severe challenges in compute and memory requirements as these scale quadratically in complexity within the self-attention blocks of Transformerbased <ref type="bibr" target="#b75">[76]</ref> models. The community has approached this burden with different strategies: Two popular ones are <ref type="bibr" target="#b0">(1)</ref> local attention computation within a window <ref type="bibr" target="#b54">[55]</ref> for object detection and <ref type="bibr" target="#b1">(2)</ref> pooling attention that locally aggregates features before computing self-attention in video tasks <ref type="bibr">[21]</ref>.  The latter fuels Multiscale Vision Transformers (MViT) <ref type="bibr">[21]</ref>, an architecture that extends ViT in a simple way: instead of having a fixed resolution throughout the network, it has a feature hierarchy with multiple stages starting from high-resolution to low-resolution. MViT is designed for video tasks where it has state-of-the-art performance.</p><p>In this paper, we develop two simple technical improvements to further increase its performance and study MViT as a single model family for visual recognition across 3 tasks: image classification, object detection and video classification, in order to understand if it can serve as a general vision backbone for spatial as well as spatiotemporal recognition tasks (see <ref type="figure" target="#fig_1">Fig. 1</ref>). Our empirical study leads to an improved architecture (MViTv2) and encompasses the following:</p><p>(i) We create strong baselines that improve pooling attention along two axes: (a) shift-invariant positional embeddings using decomposed location distances to inject position information in Transformer blocks; (b) a residual pooling connection to compensate the effect of pooling strides in attention computation. Our simple-yet-effective upgrades lead to significantly better results.</p><p>(ii) Using the improved structure of MViT, we employ a standard dense prediction framework: Mask R-CNN <ref type="bibr" target="#b35">[36]</ref> with Feature Pyramid Networks (FPN) <ref type="bibr" target="#b52">[53]</ref> and apply it to object detection and instance segmentation.</p><p>We study if MViT can process high-resolution visual input by using pooling attention to overcome the computation and memory cost involved. Our experiments suggest that pooling attention is more effective than local window attention mechanisms (e.g. Swin <ref type="bibr" target="#b54">[55]</ref>). We further develop a simple-yet-effective Hybrid window attention scheme that can complement pooling attention for better accuracy/compute tradeoff.</p><p>(iii) We instantiate our architecture in five sizes of increasing complexity (width, depth, resolution) and report a practical training recipe for large multiscale transformers. The MViT variants are applied to image classification, object detection and video classification, with minimal modification, to study its purpose as a generic vision architecture.</p><p>Experiments reveal that our MViTv2 achieves 88.8% accuracy for ImageNet-1K classification, with pretraining on ImageNet-21K (and 86.3% without), as well as 58.7 AP box on COCO object detection using only Cascade Mask R-CNN <ref type="bibr" target="#b5">[6]</ref>. For video classification tasks, MViT achieves unprecedented accuracies of 86.1% on Kinetics-400, 87.9% on Kinetics-600, <ref type="bibr" target="#b78">79</ref>.4% on Kinetics-700, and 73.3% on Something-Something-v2. Our video code will be open-sourced in PyTorchVideo 1,2 <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>CNNs serve as the primary backbones for computer vision tasks, including image recognition <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b70">71]</ref>, object detection <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b92">93]</ref> and video recognition <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b83">84,</ref><ref type="bibr" target="#b91">92]</ref>.</p><p>Vision transformers have generated a lot of recent enthusiasm since the work of ViT <ref type="bibr" target="#b16">[17]</ref>, which applies a Transformer architecture on image patches and shows very competitive results on image classification. Since then, different works have been developed to further improve ViT, including efficient training recipes <ref type="bibr" target="#b72">[73]</ref>, multi-scale transformer structures [21, <ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b77">78]</ref> and advanced self-attention mechanism design <ref type="bibr" target="#b10">[11,</ref><ref type="bibr">21,</ref><ref type="bibr" target="#b54">55]</ref>. In this work, we build upon the Multiscale Vision Transformers (MViT) and study it as a general backbone for different vision tasks.</p><p>Vision transformers for object detection tasks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b88">89]</ref> address the challenge of detection typically requiring high-resolution inputs and feature maps for accurate object localization. This significantly increases computation complexity due to the quadratic complexity of self-attention operators in transformers <ref type="bibr" target="#b75">[76]</ref>. Recent works develop technology to alleviate this cost, including shifted window attention <ref type="bibr" target="#b54">[55]</ref> and Longformer attention <ref type="bibr" target="#b88">[89]</ref>. Meanwhile, pooling attention in MViT is designed to compute self-attention efficiently using a different perspective <ref type="bibr">[21]</ref>. In this work, we study 1 https://github.com/facebookresearch/pytorchvideo 2 https://github.com/facebookresearch/SlowFast</p><p>MViT for detection and more generally compare pooling attention to local attention mechanisms.</p><p>Vision transformers for video recognition have also recently shown strong results, but mostly <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b58">59]</ref> rely on pre-training with large-scale external data (e.g. ImageNet-21K <ref type="bibr" target="#b13">[14]</ref>). MViTv1 [21] reports a good training-fromscratch recipe for Transformer-based architectures on Kinetics data <ref type="bibr" target="#b43">[44]</ref>. In this paper, we use this recipe and improve the MViT architecture with improved pooling attention which is simple yet effective on accuracy; further, we study the (large) effect of ImageNet pre-training for video tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Revisiting Multiscale Vision Transformers</head><p>The key idea of MViTv1 [21] is to construct different stages for both low-and high-level visual modeling instead of single-scale blocks in ViT <ref type="bibr" target="#b16">[17]</ref>. MViT slowly expands the channel width D, while reducing the resolution L (i.e. sequence length), from input to output stages of the network.</p><p>To perform downsampling within a transformer block, MViT introduces Pooling Attention. Concretely, for an input sequence, X ? R L?D , it applies linear projections W Q , W K , W V ? R D?D followed by pooling operators (P) to query, key and value tensors, respectively:</p><formula xml:id="formula_0">Q = P Q (XW Q ) , K = P K (XW K ) , V = P V (XW V ) ,<label>(1)</label></formula><p>where the lengthL of Q ? RL ?D can be reduced by P Q and K and V length can be reduced by P K and P V . Subsequently, pooled self-attention,</p><formula xml:id="formula_1">Z := Attn(Q, K, V ) = Softmax QK ? / ? D V,<label>(2)</label></formula><p>computes the output sequence Z ? RL ?D with flexible lengthL. Note that the downsampling factors P K and P V for key and value tensors can be different from the ones applied to the query sequence, P Q . Pooling attention enables resolution reduction between different stages of MViT by pooling the query tensor Q, and to significantly reduce compute and memory complexity by pooling the key, K, and value, V , tensors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Improved Multiscale Vision Transformers</head><p>In this section, we first introduce an empirically powerful upgrade to pooling attention ( ?4.1). Then we describe how to employ our generic MViT architecture for object detection ( ?4.2) and video recognition ( ?4.3). Finally, ?4.4 shows five concrete instantiations for MViTv2 in increasing complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Improved Pooling Attention</head><p>We start with re-examining two important implications of MViTv2 for potential improvement and introduce techniques to understand and address them. Decomposed relative position embedding. While MViT has shown promises in their power to model interactions between tokens, they focus on content, rather than structure. The space-time structure modeling relies solely on the "absolute" positional embedding to offer location information. This ignores the fundamental principle of shift-invariance in vision <ref type="bibr" target="#b46">[47]</ref>. Namely, the way MViT models the interaction between two patches will change depending on their absolute position in images even if their relative positions stay unchanged. To address this issue, we incorporate relative positional embeddings <ref type="bibr" target="#b64">[65]</ref>, which only depend on the relative location distance between tokens into the pooled self-attention computation.</p><p>We encode the relative position between the two input elements, i and j, into positional embedding R p(i),p(j) ? R d , where p(i) and p(j) denote the spatial (or spatiotemporal) position of element i and j. <ref type="bibr" target="#b2">3</ref> The pairwise encoding representation is then embedded into the self-attention module:</p><formula xml:id="formula_2">Attn(Q, K, V ) = Softmax (QK ? + E (rel) )/ ? d V, where E (rel) ij = Q i ? R p(i),p(j) .<label>(3)</label></formula><p>However, the number of possible embeddings R p(i),p(j) scale in O(T W H), which can be expensive to compute. To reduce complexity, we decompose the distance computation between element i and j along the spatiotemporal axes:</p><formula xml:id="formula_3">R p(i),p(j) = R h h(i),h(j) + R w w(i),w(j) + R t t(i),t(j) ,<label>(4)</label></formula><p>where R h , R w , R t are the positional embeddings along the height, width and temporal axes, and h(i), w(i), and t(i) <ref type="bibr" target="#b2">3</ref> Note that Q and (K, V ) can reside in different scales due to potentially different pooling. p maps the index of all of them into a shared scale. denote the vertical, horizontal, and temporal position of token i, respectively. Note that R t is optional and only required to support temporal dimension in the video case. In comparison, our decomposed embeddings reduce the number of learned embeddings to O(T + W + H), which can have a large effect for early-stage, high-resolution feature maps.</p><p>Residual pooling connection. As demonstrated [21], pooling attention is very effective to reduce the computation complexity and memory requirements in attention blocks. MViTv1 has larger strides on K and V tensors than the stride of the Q tensors which is only downsampled if the resolution of the output sequence changes across stages. This motivates us to add the residual pooling connection with the (pooled) Q tensor to increase information flow and facilitate the training of pooling attention blocks in MViT.</p><p>We introduce a new residual pooling connection inside the attention blocks as shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. Specifically, we add the pooled query tensor to the output sequence Z. So Eq. (2) is reformulated as:</p><formula xml:id="formula_4">Z := Attn (Q, K, V ) + Q.<label>(5)</label></formula><p>Note that the output sequence Z has the same length as the pooled query tensor Q.</p><p>The ablations in ?6.2 and ?5.3 shows that both the pooling operator (P Q ) for query Q and the residual path are necessary for the proposed residual pooling connection. This change still enjoys the low-complexity attention computation with large strides in key and value pooling as adding the pooled query sequence in Eq. (5) comes at a low cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">MViT for Object Detection</head><p>In this section, we describe how to apply the MViT backbone for object detection and instance segmentation tasks.</p><p>FPN integration. The hierarchical structure of MViT produces multiscale feature maps in four stages, and therefore naturally integrates into Feature Pyramid Networks (FPN) <ref type="bibr" target="#b52">[53]</ref> for object detection tasks, as shown in <ref type="figure" target="#fig_3">Fig. 3</ref>. The top-down pyramid with lateral connections in FPN constructs semantically strong feature maps for MViT at all scales. By using FPN with the MViT backbone, we apply it to different detection architectures (e.g. Mask R-CNN <ref type="bibr" target="#b35">[36]</ref>).</p><p>Hybrid window attention. The self-attention in Transformers has quadratic complexity w.r.t. the number of tokens. This issue is more exacerbated for object detection as it typically requires high-resolution inputs and feature maps. In this paper, we study two ways to significantly reduce this compute and memory complexity: First, the pooling attention designed in attention blocks of MViT. Second, window attention used as a technique to reduce computation for object detection in Swin <ref type="bibr" target="#b54">[55]</ref>.</p><p>Pooling attention and window attention both control the complexity of self-attention by reducing the size of query, key and value tensors when computing self-attention. Their intrinsic nature however is different: Pooling attention pools features by downsampling them via local aggregation, but keeps a global self-attention computation, while window attention keeps the resolution of tensors but performs selfattention locally by dividing the input (patchified tokens) into non-overlapping windows and then only compute local self-attention within each window. The intrinsic difference of the two approaches motivates us to study if they could perform complementary in object detection tasks.</p><p>Default window attention only performs local selfattention within windows, thus lacking connections across windows. Different from Swin <ref type="bibr" target="#b54">[55]</ref>, which uses shifted windows to mitigate this issue, we propose a simple Hybrid window attention (Hwin) design to add cross-window connections. Hwin computes local attention within a window in all but the last blocks of the last three stages that feed into FPN. In this way, the input feature maps to FPN contain global information. The ablation in ?5.3 shows that this simple Hwin performs consistently better than Swin <ref type="bibr" target="#b54">[55]</ref> on image classification and object detection tasks. Further, we will show that combining pooling attention and Hwin achieves the best performance for object detection.</p><p>Positional embeddings in detection. Different from Ima-geNet classification where the input is a crop of fixed resolution (e.g. 224?224), object detection typically encompasses inputs of varying size in training. For the positional embeddings in MViT (either absolute or relative), we first initialize the parameters from the ImageNet pre-training weights corresponding to positional embeddings with 224?224 input size and then interpolate them to the respective sizes for object detection training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">MViT for Video Recognition</head><p>MViT can be easily adopted for video recognition tasks (e.g. the Kinetics dataset) similar to MViTv1 [21] as the upgraded modules in ?4.1 generalize to the spatiotemporal domain. While MViTv1 only focuses on the training-fromscratch setting on Kinetics, in this work, we also study the (large) effect of pre-training from ImageNet datasets.</p><p>Initialization from pre-trained MViT. Compared to the image-based MViT, there are only three differences for videobased MViT: 1) the projection layer in the patchification stem needs to project the input into space-time cubes instead of 2D patches; 2) the pooling operators now pool spatiotemporal feature maps; 3) relative positional embeddings reference space-time locations.</p><p>As the projection layer and pooling operators in 1) and 2) are instantiated by convolutional layers by default 4 , we use an inflation initialization as for CNNs <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">24]</ref>. Specifically, we initialize the conv filters for the center frame with the weights from the 2D conv layers in pre-trained models and initialize other weights as zero. For 3), we capitalize on our decomposed relative positional embeddings in Eq. 4, and simply initialize the spatial embeddings from pre-trained weights and the temporal embedding as zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">MViT Architecture Variants</head><p>We build several MViT variants with different number of parameters and FLOPs as shown in <ref type="table" target="#tab_0">Table 1</ref>, in order to have a fair comparison with other vision transformer works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b80">81]</ref>. Specifically, we design five variants (Tiny, Small, Base, Large and Huge) for MViT by changing the base channel dimension, the number of blocks in each stage and the number of heads in the blocks. Note that we use a smaller number of heads to improve runtime, as more heads lead to slower runtime but have no effect on FLOPs and Parameters.</p><p>Following the pooling attention design in MViT [21], we employ Key and Value pooling in all pooling attention blocks by default and the pooling stride is set to 4 in the first stage and adaptively decays stride w.r.t resolution across stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments: Image Recognition</head><p>We conduct experiments on ImageNet classification <ref type="bibr" target="#b13">[14]</ref> and COCO object detection <ref type="bibr" target="#b53">[54]</ref>. We first show state-of-theart comparisons and then perform comprehensive ablations. More results and discussions are in ?A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Image Classification on ImageNet-1K</head><p>Settings. The ImageNet-1K <ref type="bibr" target="#b13">[14]</ref> (IN-1K) dataset has ?1.28M images in 1000 classes. Our training recipe for MViTv2 on IN-1K is following MViTv1 <ref type="bibr">[21,</ref><ref type="bibr" target="#b71">72]</ref>. We train all MViTv2 variants for 300 epochs without using EMA. We In addition to center crop testing (with a 224/256=0.875 crop ratio), we report a testing protocol that has been adopted recently in the community <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b80">81]</ref>: This protocol takes a full-sized crop of the (resized) original validation images. We observe that full crop testing can increase our MViTv2-L ? 384 2 from 86.0 to 86.3%, which is the highest accuracy on IN-1K to date (without external data or distillation models).</p><p>Results using ImageNet-21K. Results for using the largescale IN-21K pre-training are shown in <ref type="table" target="#tab_16">Table 3</ref>. The IN-21K data adds +2.2% accuracy to MViTv2-L.</p><p>Compared to other Transformers, MViTv2-L achieves better results than Swin-L (+1.2%). We lastly finetune MViTv2-L with 384 2 input to directly compare to prior models of size L: MViTv2-L achieves 88.4%, outperforming other large models. We further train a huge MViTv2-H with accuracy 88.0%, 88.6% and 88.8% at 224 2 , 384 2 and 512 2 resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Object Detection on COCO</head><p>Settings. We conduct object detection experiments on the MS-COCO dataset <ref type="bibr" target="#b53">[54]</ref>. All the models are trained on 118K training images and evaluated on the 5K validation images. We use standard Mask R-CNN <ref type="bibr" target="#b35">[36]</ref> and Cascade Mask R-CNN <ref type="bibr" target="#b5">[6]</ref> detection frameworks implemented in Detec-tron2 <ref type="bibr" target="#b81">[82]</ref>. For a fair comparison, we follow the same recipe as in Swin <ref type="bibr" target="#b54">[55]</ref>. Specifically, we pre-train the backbones on IN and fine-tune on COCO using a 3?schedule (36 epochs) by default. Detailed training recipes are in ?B.3.</p><p>For MViTv2, we take the backbone pre-trained from IN and add our Hybrid window attention (Hwin) by default. The window sizes are set as <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b6">7]</ref> for the four stages, which is consistent with the self-attention size used in IN pre-training which takes 224?224 as input.  Main results. <ref type="table" target="#tab_3">Table 5a</ref> shows the results on COCO using Mask R-CNN. Our MViTv2 surpasses CNN (i.e. ResNet <ref type="bibr" target="#b37">[38]</ref> and ResNeXt <ref type="bibr" target="#b82">[83]</ref>) and Transformer backbones (e.g. Swin <ref type="bibr" target="#b54">[55]</ref>, ViL <ref type="bibr" target="#b88">[89]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablations on ImageNet and COCO</head><p>Different self-attention mechanism. We first study our pooling attention and Hwin self-attention mechanism in MViTv2 by comparing with different self-attention mechanisms on ImageNet and COCO. For a fair comparison, we conduct the analysis on both ViT-B and MViTv2-S networks.</p><p>In <ref type="table" target="#tab_5">Table 4a</ref> we compare different attention schemes on IN-1K. We compare 5 attention mechanisms: global (full), windowed, Shifted window (Swin), our Hybrid window (Hwin) and pooling. We observe the following:</p><p>(i) For ViT-B based models, default win reduces both FLOPs and Memory usage while the top-1 accuracy also drops by 2.0% due to the missing cross-window connection. Swin <ref type="bibr" target="#b54">[55]</ref>   <ref type="table" target="#tab_13">Table 6</ref>. Ablation of positional embeddings on MViTv2-S.</p><p>Positional embeddings. <ref type="table" target="#tab_13">Table 6</ref> compares different positional embeddings. We observe that: (i) Comparing <ref type="formula" target="#formula_1">(2)</ref>   Residual pooling connection. Single-scale vs. multi-scale for detection. As shown in <ref type="table" target="#tab_16">Table 9</ref>, FPN significantly improves performance for both backbones while MViTv2-S is consistently better than ViT-B. Note that the FPN gain for MViTv2-S (+2.9 AP box ) is much larger than those for ViT-B (+1.5 AP box ), which shows the effectiveness of a native hierarchical multi-scale design for dense object detection tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments: Video Recognition</head><p>We apply our MViTv2 on Kinetics-400 <ref type="bibr" target="#b43">[44]</ref> (K400), Kinetics-600 (K600) <ref type="bibr" target="#b7">[8]</ref>, and Kinetics-700 (K700) <ref type="bibr" target="#b6">[7]</ref> and Something-Something-v2 <ref type="bibr" target="#b30">[31]</ref> (SSv2) datasets.</p><p>Settings. By default, our MViTv2 models are trained from scratch on Kinetics and fine-tuned from Kinetics models for SSv2. The training recipe and augmentations follow <ref type="bibr" target="#b18">[19,</ref><ref type="bibr">21]</ref>. When using IN-1K or IN-21K as pre-training, we adopt the initialization scheme introduced in ?4.3 and shorter training.</p><p>For the temporal domain, we sample a T ?? clip from the full-length video which contains T frames with a temporal stride of ? . For inference, we follow testing strategies in <ref type="bibr">[21,</ref><ref type="bibr" target="#b22">23]</ref> and get final score by averaged from sampled temporal clips and spatial crops. Implementation and training details are in ?B.  <ref type="table" target="#tab_0">Table 12</ref>. Comparison with previous work on Kinetics-700.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Main Results</head><p>Kinetics-400. Prior ViT-based models require large-scale pre-training on IN-21K to produce best accuracy on K400. We fine-tune our MViTv2-L with large spatiotemporal input size 40?312 2 (time ?space 2 ) to reach 86.1% top-1 accuracy, showing the performance of our architecture in a large-scale setting.</p><p>Kinetics-600/-700. <ref type="table" target="#tab_0">Table 11</ref> shows the results on K600. We train MViTv2-B, 32?3 from scratch and achieves 85.5% top-1 accuracy, which is better than the MViTv1 counterpart (+1.4%), and even better than other ViTs with IN-21K pre-training(e.g. +1.5% over Swin-B <ref type="bibr" target="#b55">[56]</ref>) while having ?2.2?and ?40% fewer FLOPs and parameters. The larger MViTv2-L 40?3 sets a new state-of-the-art at 87.9%.  <ref type="table" target="#tab_0">Table 13</ref>. Comparison with previous work on SSv2.</p><p>In <ref type="table" target="#tab_0">Table 12</ref>, our MViTv2-L achieves 79.4% on K700 which greatly surpasses the previous best result by +7.1%.</p><p>Something-something-v2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Ablations on Kinetics</head><p>In this section, we carry out MViTv2 ablations on K400. The video ablation our technical improvements share trends with  <ref type="table" target="#tab_0">Table 14</ref>.</p><p>Effect of pre-training on K400.</p><p>We use viewspace?viewtime = 1?10 crops for inference.</p><p>Effect of pre-training datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We present an improved Multiscale Vision Transformer as a general hierarchical architecture for visual recognition.</p><p>In empirical evaluation, MViT shows strong performance compared to other vision transformers and achieves state-ofthe-art accuracy on widely-used benchmarks across image classification, object detection, instance segmentation and video recognition. We hope that our architecture will be useful for further research in visual recognition.</p><p>This appendix provides further details for the main paper:</p><p>?A contains further results for COCO object detection ( ?A.1) AVA action detection ( ?A.2) and ImageNet classification ( ?A.3), as well as ablations for ImageNet classification and COCO object detection ( ?A.4) and Kinetics action classification ( ?A.5).</p><p>?B contains additional MViTv2 upgrade details ( ?B.1), and additional implementation details for: ImageNet classification ( ?B.2), COCO object detection ( ?B.3), Kinetics action classification ( ?B.4), SSv2 action classification ( ?B.5), and AVA action detection ( ?B.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Results: COCO Object Detection</head><p>System-level comparsion on COCO. <ref type="table" target="#tab_16">Table A</ref>.1 shows the system-level comparisons on COCO data. We compare our results with previous state-of-the-art models. We adopt Soft-NMS <ref type="bibr" target="#b3">[4]</ref> during inference, following <ref type="bibr" target="#b54">[55]</ref>. MViTv2-L * achieves 58.7 AP box with multi-scale testing, which is already +0.7 AP better than the best results of Swin-L * that relies on the improved HTC++ detector <ref type="bibr" target="#b54">[55]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Results: ImageNet Classification</head><p>Results of ImageNet-1K <ref type="table" target="#tab_16">. Table A.</ref>3 shows the comparison of our MViTv2 with more prior work (without external data or distillation models) on ImageNet-1K. As shown in the <ref type="table" target="#tab_16">Table,</ref> our MViTv2 achieves better results than any previously published methods for a variety of model complexities. We note that our improvements to pooling attention bring significant gains over the MViTv1 [21] counterparts which use exactly the same training recipes (for all datasets we compare on); therefore the gains over MViTv1 stem solely from our technical improvements in ?4.1 of the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Ablations: ImageNet and COCO</head><p>Decomposed relative position embeddings. As introduced in Sec. 4.1, our Relative position embedding is only applied for Q i by default. We could further extend it to all Q, K and V terms for attention layers: And the rel pos terms are defined as:</p><formula xml:id="formula_5">Attn(Q, K, V ) = AV + E (relv) , where A = Softmax (QK ? + E (relq) + E (rel k ) )/ ? d .</formula><formula xml:id="formula_6">E (relq) ij =Q i ? R q p(i),p(j) , E (rel k ) ij =R k p(i),p(j) ? K i , E (relv) i = j A ij * R v p(i),p(j)</formula><p>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. Ablations: Kinetics Action Classification</head><p>In ?5.3 of the main paper we ablated the impact of our improvements to pooling attention, i.e. decomposed relative positional embeddings &amp; residual pooling connections, for image classification and object detection. Here, we ablate the effect of our improvements for video classification.</p><p>Positional embeddings for video. positional embeddings by ?0.6% comparing <ref type="formula" target="#formula_1">(2)</ref> and <ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b5">6)</ref>.</p><p>Comparing <ref type="formula" target="#formula_4">(5)</ref> to <ref type="formula">(6)</ref>, our decomposed space/time rel. positional embeddings achieve nearly the same accuracy as the joint space rel. embeddings while being ?2? faster in training. For joint space/time rel. (5 vs. 7), our decomposed space/time rel. is even ?8?faster with ?2?fewer parameters. This demonstrates the effectiveness of our decomposed design for relative positional embeddings.</p><p>Residual pooling connection for video. <ref type="table" target="#tab_16">Table A</ref>.7 studies the effect of residual pooling connections on K400. We observe similar results as for image classification and object detection ( <ref type="table" target="#tab_6">Table 7</ref> of the main paper), that: both Q pooling blocks and residual paths are essential in our improved MViTv2 and combining them together leads to +1.7% accuracy on K400 while using them separately only improves slightly (+0.4%). We follow the training recipe of MViTv1 <ref type="bibr">[21,</ref><ref type="bibr" target="#b71">72]</ref> for IN-1K training. We train for 300 epochs with 64 GPUs. The batch size is 32 per GPU by default. We use truncated normal distribution initialization <ref type="bibr" target="#b34">[35]</ref> and adopt synchronized AdamW <ref type="bibr" target="#b57">[58]</ref> optimization with a base learning rate of 2 ? 10 ?3 for batch size of 2048. We use a linear warm-up strategy in the first 70 epochs and a decayed half-period cosine schedule <ref type="bibr" target="#b71">[72]</ref>.</p><p>For regularization, we set weight decay to 0.05 for MViTv2-T/S/B and 0.1 for MViTv2-L/H and labelsmoothing <ref type="bibr" target="#b69">[70]</ref> to 0.1. Stochastic depth <ref type="bibr" target="#b40">[41]</ref> (i.e. drop-path or drop-connect) is also used with rate 0.1 for MViTv2-T &amp; MViTv2-S, rate 0.3 for MViTv2-B, rate 0.5 for MViTv2-L and rate 0.8 for MViTv2-H. Other data augmentations have the same (default) hyperparameters as in <ref type="bibr">[21,</ref><ref type="bibr" target="#b72">73]</ref>, including mixup <ref type="bibr" target="#b87">[88]</ref>, cutmix <ref type="bibr" target="#b86">[87]</ref>, random erasing <ref type="bibr" target="#b90">[91]</ref> and rand augment <ref type="bibr" target="#b11">[12]</ref>.</p><p>For 384?384 input resolution, we fine-tune the models trained on 224?224 resolution. We decrease the batch size to 8 per GPU and fine-tune 30 epochs with a base learning rate of 4 ? 10 ?5 per 256 batch-size samples. For MViTv2-L and MViTv2-H, we disable mixup and fine-tune with a learning rate of 5 ? 10 ?4 per batch of 64. We linearly scale learning rates with the number of overall GPUs (i.e. the overall batch-size). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IN-21K</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Details: COCO Object Detection</head><p>For object detection experiments, we adopt two typical object detection framework: Mask R-CNN <ref type="bibr" target="#b35">[36]</ref> and Cascade Mask R-CNN <ref type="bibr" target="#b5">[6]</ref> in Detectron2 <ref type="bibr" target="#b81">[82]</ref>. We follow the same training settings from <ref type="bibr" target="#b54">[55]</ref>: multi-scale training (scale the shorter side in [480, 800] while longer side is smaller than 1333), AdamW optimizer <ref type="bibr" target="#b57">[58]</ref> (? 1 , ? 2 = 0.9, 0.999, base learning rate 1.6?10 ?4 for base size of 64, and weight decay of 0.1), and 3?schedule (36 epochs). The drop path rate is set as 0.1, 0.3, 0.4, 0.5 and 0.6 for MViTv2-T, MViTv2-S, MViTv2-B, MViTv2-L and MViTv2-H, respectively. We use PyTorch's automatic mixed precision during training.</p><p>For the stronger recipe for MViTv2-L and MViTv2-H in <ref type="table" target="#tab_16">Table.</ref> 5 of the main paper, we use large-scale jittering (1024?1024 resolution) as the training augmentation <ref type="bibr" target="#b25">[26]</ref> and a longer schedule (50 epochs) with IN-21K pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Details: Kinetics Action Classification</head><p>Training from scratch. We follow the training recipe and augmentations from <ref type="bibr" target="#b18">[19,</ref><ref type="bibr">21]</ref> when training from scratch for Kinetics datasets. We adopt synchronized AdamW <ref type="bibr" target="#b57">[58]</ref> and train for 200 epochs with 2 repeated augmentation <ref type="bibr" target="#b39">[40]</ref> on 128 GPUs. The mini-batch size is 4 clips per GPU. We adopt a half-period cosine schedule <ref type="bibr" target="#b56">[57]</ref> of learning rate decaying. The base learning rate is set as 1.6 ? 10 ?3 for 512 batch-size. We use weight decay of 0.05 and set drop path rate as 0.2 and 0.3 for MViTv2-S and MViTv2-B.</p><p>For the input clip, we randomly sample a clip (T frames with a temporal stride of ? ; denoted as T ? ? <ref type="bibr" target="#b22">[23]</ref>) from the full-length video during training. For the spatial domain, we use Inception-style <ref type="bibr" target="#b68">[69]</ref> cropping (randomly resize the input area between a [min, max], scale of [0.08, 1.00], and jitter aspect ratio between 3/4 to 4/3). Then we take an H ? W = 224?224 crop as the network input.</p><p>During inference, we apply two testing strategies following [21, 23]: (i) Temporally, uniformly samples K clips (e.g. K=5) from a video. (ii) in spatial axis, scales the shorter spatial side to 256 pixels and takes a 224?224 center crop or 3 crops of 224?224 to cover the longer spatial axis. The final score is averaged over all predictions.</p><p>For the input clips, we perform the same data augmentations across all frames, including random horizontal flip, mixup <ref type="bibr" target="#b87">[88]</ref> and cutmix <ref type="bibr" target="#b86">[87]</ref>, random erasing <ref type="bibr" target="#b90">[91]</ref>, and rand augment <ref type="bibr" target="#b11">[12]</ref>.</p><p>For Kinetics-600 and Kinetics-700, all hyper-parameters are identical to K400.</p><p>Fine-tuning from ImageNet. When using IN-1K or IN-21K as pre-training, we adopt the initialization scheme introduced in ?4.3 of the main paper and shorter training schedules. For example, we train 100 epochs with base learning rate as 4.8 ? 10 ?4 for 512 batch-size when fine-tuning from IN-1K for MViTv2-S and MViTv2-B, and 75 epochs with base learning as 1.6 ? 10 ?4 when fine-tuning from IN-21K. For long-term models with 40?3 sampling, we initialize from the 16?4 counterparts, disable mixup, train for 30 epochs with learning rate of 1.6 ? 10 ?5 at batch-size of 128, and use a weight decay of 10 ?8 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5. Details: Something-Something V2 (SSv2)</head><p>The SSv2 dataset <ref type="bibr" target="#b30">[31]</ref> contains 169k training, and 25k validation videos with 174 human-object interaction classes. We fine-tune the pre-trained Kinetics models and take the same recipe as in <ref type="bibr">[21]</ref>. Specifically, we train for 100 epochs (40 epochs for MViTv2-L) using 64 or 128 GPUs with 8 clips per GPU and a base learning rate of 0.02 (for batch size of 512) with half-period cosine decay <ref type="bibr" target="#b56">[57]</ref>. We adopt synchronized SGD and use weight decay of 10 ?4 and drop path rate of 0.4. The training augmentation is the same as Kinetics in ?B.4, except we disable random flipping and repeated augmentations in training.</p><p>We use the segment-based input frame sampling [21, 52] (split each video into segments, and sample one frame from each segment to form a clip). During inference, we take a single clip with 3 spatial crops to form predictions over a single video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.6. Details: AVA Action Detection</head><p>The AVA action detection dataset <ref type="bibr" target="#b31">[32]</ref> assesses the spatiotemporal localization of human actions in videos. It has 211k training and 57k validation video segments. We evaluate methods on AVA v2.2 and use mean Average Precision (mAP) metric on 60 classes as is standard in prior work <ref type="bibr" target="#b22">[23]</ref>.</p><p>We use MViTv2 as the backbone and follow the same detection architecture in [21,23] that adapts Faster R-CNN <ref type="bibr" target="#b63">[64]</ref> for video action detection. Specifically, we extract region-ofinterest (RoI) features <ref type="bibr" target="#b28">[29]</ref> by frame-wise RoIAlign <ref type="bibr" target="#b35">[36]</ref> on the spatiotemporal feature maps from the last MViTv2 layer. The RoI features are then max-pooled and fed to a per-class, sigmoid classifier for action prediction.</p><p>The training recipe is identical to [21] and summarized next. We pre-train our MViTv2 models on Kinetics. The region proposals are identical to the ones used in <ref type="bibr">[21,</ref><ref type="bibr" target="#b22">23]</ref>. We use proposals that have overlaps with ground-truth boxes by IoU &gt; 0.9 for training. The models are trained with synchronized SGD training on 64 GPUs (8 clips per GPU). The base learning rate is set as 0.6 with a half-period cosine schedule of learning rate decaying. We train for 30 epochs with linear warm-up <ref type="bibr" target="#b29">[30]</ref> for the first 5 epochs and use a weight decay of 1 ? 10 ?8 and drop-path rate of 0.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional Discussions</head><p>Societal impact. Our MViTv2 is a general vision backbone for various vision tasks, including image recognition, object detection, instance segmentation, video classification and video detection. Though we are not providing any direct applications, it could potentially apply to a wide range of vision-related applications, which then might have a wide range of societal impacts. On the positive side, the better vision backbone could potentially improve the performance of many different computer vision applications, e.g. visual inspection and quality management in manufacturing, cancer and tumor detection in healthcare, and vehicle re-identification and pedestrian detection in transportation.</p><p>On the other hand, the advanced vision recognition technologies could also have potential negative societal impact if they are adopted by harmful or mismanaged applications, e.g. usage in surveillance systems that violate privacy. It is important to be aware when vision technologies are deployed in practical applications.</p><p>Limitations. Our MViTv2 is a general vision backbone and we demonstrate its effectiveness on various recognition tasks. To reduce the full hyperparameter tuning space for MViTv2 on different datasets and tasks, we mainly follow the existing standard recipe for each task from the community (e.g. [21, <ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b72">73]</ref>) with lightweight tuning (e.g. learning rate, weight decay). Therefore, the choice of hyperparameters for different MViTv2 variants may be suboptimal.</p><p>In addition, MViTv2 provides five different variants from tiny to huge models with different complexity as a general backbone. In the future, we think there are two potential interesting research directions: scaling down MViTv2 to even smaller models for mobile applications, and scaling up MViTv2 to even larger models for large-scale data scenarios.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>(a )</head><label>)</label><figDesc>Image classification (b) Object detection (c) Video recognition</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Our MViTv2 is a multiscale transformer with state-ofthe-art performance across three visual recognition tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>The improved Pooling Attention mechanism that incorporating decomposed relative position embedding, R p(i),p(j) , and residual pooling connection modules in the attention block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>MViT backbone used with FPN for object detection. The multiscale transformer features naturally integrate with standard feature pyramid networks (FPN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>to ( 1 )</head><label>1</label><figDesc>, absolute position only slightly improves over no pos.. This is because the pooling operators (instantiated by conv layers) already model positional information. (ii) Comparing<ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b3">4)</ref> and<ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b1">2)</ref>, relative positions can bring performance gain by introducing shift-invariance priors to pooling attention. Finally, our decomposed relative position embedding train 3.9? faster than joint relative position on COCO.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>pre-training and fine-tuning on IN-1K. We download the latest winter-2021 version of IN-21K from the official website. The training recipe follows the IN-1K training introduced above except for some differences described next. We train the IN-21K models on the joint set of IN-21K and 1K for 90 epochs (60 epochs for MViTv2-H) with a 6.75 ? 10 ?5 base learning rate for MViTv2-S and MViTv2-B, and 10 ?4 for MViTv2-L and MViTv2-H, per batch-size of 256. The weight decay is set as 0.01 for MViTv2-S and MViTv2-B, and 0.1 for MViTv2-L and MViTv2-H. When fine-tuning IN-21K MViTv2 models on IN-1K for MViTv2-L and MViTv2-H, we disable mixup and fine-tune for 30 epochs with a learning rate of 7 ? 10 ?5 per batch of 64. We use a weight decay of 5 ? 10 ?2 . The MViTv2-H ? 512 2 model is initialized from the 384 2 variant and trained for 3 epochs with mixup enabled and weight decay of 10 ?8 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Configuration for MViT variants. #Channels, #Blocks and #Heads specify the channel width, number of MViT blocks and heads in each block for the four stages, respectively. FLOPs are measured for image classification with 224 ? 224 input. The stage resolutions are [56 2 , 28 2 , 14 2 , 7 2 ].</figDesc><table><row><cell>Model</cell><cell>#Channels</cell><cell>#Blocks</cell><cell cols="2">#Heads FLOPs Param</cell></row><row><cell cols="4">MViT-T [96-192-384-768] [1-2-5-2] [1-2-4-8]</cell><cell>4.7</cell><cell>24</cell></row><row><cell cols="4">MViT-S [96-192-384-768] [1-2-11-2] [1-2-4-8]</cell><cell>7.0</cell><cell>35</cell></row><row><cell cols="5">MViT-B [96-192-384-768] [2-3-16-3] [1-2-4-8] 10.2</cell><cell>52</cell></row><row><cell cols="5">MViT-L [144-288-576-1152] [2-6-36-4] [2-4-8-16] 39.6</cell><cell>218</cell></row><row><cell cols="5">MViT-H [192-384-768-1536] [4-8-60-8] [3-6-12-24] 120.6</cell><cell>667</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 Table 3 .</head><label>23</label><figDesc>Table 2shows our MViTv2 and state-of-the-art CNNs and Transformers (without external data or distillation models<ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b85">86]</ref>). The models are split into groups based on computation and compared next. Compared to MViTv1 [21], our improved MViTv2 has better accuracy with fewer flops and parameters. For example, MViTv2-S (83.6%) improves +0.6% over MViTv1-B-16 (83.0%) with 10% fewer flops. On the base model size, MViTv2-B (84.4%) improves +1.0% over MViTv1-B-24 (83.4%) while even being lighter. This shows clear effectiveness of the MViTv2 improvements in ?4.1. ImageNet-1K fine-tunning results using IN-21K data. Fine-tuning is with 224 2 input size (default) or with ? 384 2 size. Center denotes testing with a center crop, while resize is scaling the full image to the inference resolution (including more context).Our MViTv2 outperforms other Transformers, including DeiT<ref type="bibr" target="#b71">[72]</ref> and Swin<ref type="bibr" target="#b54">[55]</ref>, especially when scaling up models. For example, MViTv2-B achieves 84.4% top-1 accuracy, surpassing DeiT-B and Swin-B by 2.6% and 1.1% respectively. Note that MViTv2-B has over 33% fewer flops and parameters comparing DeiT-B and Swin-B. The trend is similar with 384?384 input and MViTv2-B has further +0.8% gain from the high-resolution fine-tuning under center crop testing.</figDesc><table><row><cell></cell><cell>Acc</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Acc</cell><cell></cell></row><row><cell>model</cell><cell cols="4">center resize FLOPs (G) Param (M)</cell><cell>model</cell><cell cols="3">center resize FLOPs (G) Param (M)</cell></row><row><cell>RegNetZ-4GF [15]</cell><cell>83.1</cell><cell></cell><cell>4.0</cell><cell>28</cell><cell>Swin-L [55]</cell><cell>86.3</cell><cell>34.5</cell><cell>197</cell></row><row><cell>EfficientNet-B4 ? 380 2 [71]</cell><cell>82.9</cell><cell></cell><cell>4.2</cell><cell>19</cell><cell>MViTv2-L</cell><cell>87.5</cell><cell>42.1</cell><cell>218</cell></row><row><cell>DeiT-S [72]</cell><cell>79.8</cell><cell></cell><cell>4.6</cell><cell>22</cell><cell>MViTv2-H</cell><cell>88.0</cell><cell>120.6</cell><cell>667</cell></row><row><cell>TNT-S [33]</cell><cell>81.5</cell><cell></cell><cell>5.2</cell><cell>24</cell><cell>ViT-L/16 ? 384 2 [17]</cell><cell>85.2</cell><cell>190.7</cell><cell>307</cell></row><row><cell>PVTv2-V2 [77]</cell><cell>82.0</cell><cell></cell><cell>4.0</cell><cell>25</cell><cell cols="2">ViL-B-RPB ? 384 2 [89] 86.2</cell><cell>43.7</cell><cell>56</cell></row><row><cell>CoAtNet-0 [13]</cell><cell>81.6</cell><cell></cell><cell>4.2</cell><cell>25</cell><cell>Swin-L ? 384 2 [55]</cell><cell>87.3</cell><cell>103.9</cell><cell>197</cell></row><row><cell>XCiT-S12 [18]</cell><cell>82.0</cell><cell></cell><cell>4.8</cell><cell>26</cell><cell>CSwin-L ? 384 2 [16]</cell><cell>87.5</cell><cell>96.8</cell><cell>173</cell></row><row><cell>Swin-T [55]</cell><cell>81.3</cell><cell></cell><cell>4.5</cell><cell>29</cell><cell>CvT-W24 ? 384 2 [81]</cell><cell>87.6</cell><cell>193.2</cell><cell>277</cell></row><row><cell>CSWin-T [16]</cell><cell>82.7</cell><cell></cell><cell>4.3</cell><cell>23</cell><cell>CoAtNet-4 [13] ? 512 2</cell><cell>88.4</cell><cell>360.9</cell><cell>275</cell></row><row><cell>MViTv2-T</cell><cell>82.3</cell><cell></cell><cell>4.7</cell><cell>24</cell><cell>MViTv2-L ? 384 2</cell><cell>88.2 88.4</cell><cell>140.7</cell><cell>218</cell></row><row><cell>RegNetY-8GF [62] EfficientNet-B5 ? 456 2 [71]</cell><cell>81.7 83.6</cell><cell></cell><cell>8.0 9.9</cell><cell>39 30</cell><cell>MViTv2-H ? 384 2 MViTv2-H ? 512 2</cell><cell>88.3 88.6 88.3 88.8</cell><cell>388.5 763.5</cell><cell>667 667</cell></row><row><cell>Twins-B [11]</cell><cell>83.2</cell><cell></cell><cell>8.6</cell><cell>56</cell><cell></cell><cell></cell><cell></cell></row><row><cell>PVTv2-V2-B3 [77]</cell><cell>83.2</cell><cell></cell><cell>6.9</cell><cell>45</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Swin-S [55]</cell><cell>83.0</cell><cell></cell><cell>8.7</cell><cell>50</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CSWin-S [16]</cell><cell>83.6</cell><cell></cell><cell>6.9</cell><cell>35</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MViT-v1-B-16 [21]</cell><cell>83.0</cell><cell></cell><cell>7.8</cell><cell>37</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MViTv2-S</cell><cell>83.6</cell><cell></cell><cell>7.0</cell><cell>35</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RegNetZ-16GF [15]</cell><cell>84.1</cell><cell></cell><cell>15.9</cell><cell>95</cell><cell></cell><cell></cell><cell></cell></row><row><cell>EfficientNet-B6 ? 528 2 [71]</cell><cell>84.2</cell><cell></cell><cell>19</cell><cell>43</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DeiT-B [72]</cell><cell>81.8</cell><cell></cell><cell>17.6</cell><cell>87</cell><cell></cell><cell></cell><cell></cell></row><row><cell>PVTv2-V2-B5 [77]</cell><cell>83.8</cell><cell></cell><cell>11.8</cell><cell>82</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CaiT-S36 [74]</cell><cell>83.3</cell><cell></cell><cell>13.9</cell><cell>68</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CoAtNet-2 [13]</cell><cell>84.1</cell><cell></cell><cell>15.7</cell><cell>75</cell><cell></cell><cell></cell><cell></cell></row><row><cell>XCiT-M24 [18]</cell><cell>82.7</cell><cell></cell><cell>16.2</cell><cell>84</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Swin-B [55]</cell><cell>83.3</cell><cell></cell><cell>15.4</cell><cell>88</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CSWin-B [16]</cell><cell>84.2</cell><cell></cell><cell>15.0</cell><cell>78</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MViTv1-B-24 [21]</cell><cell>83.4</cell><cell></cell><cell>10.9</cell><cell>54</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MViTv2-B</cell><cell>84.4</cell><cell></cell><cell>10.2</cell><cell>52</cell><cell></cell><cell></cell><cell></cell></row><row><cell>EfficientNet-B7 ? 600 2 [71]</cell><cell>84.3</cell><cell></cell><cell>37.0</cell><cell>66</cell><cell></cell><cell></cell><cell></cell></row><row><cell>NFNet-F1 ? 320 2 [5]</cell><cell>84.7</cell><cell></cell><cell>35.5</cell><cell>133</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DeiT-B ? 384 2 [72]</cell><cell>83.1</cell><cell></cell><cell>55.5</cell><cell>87</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CvT-32 ? 384 2 [81]</cell><cell></cell><cell>83.3</cell><cell>24.9</cell><cell>32</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CaiT-S36? 384 2 [74]</cell><cell></cell><cell>85.0</cell><cell>48</cell><cell>68</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Swin-B ? 384 2 [55]</cell><cell></cell><cell>84.2</cell><cell>47.0</cell><cell>88</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MViT-v1-B-24 ? 320 2 [21]</cell><cell>84.8</cell><cell></cell><cell>32.7</cell><cell>73</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MViTv2-B ? 384 2</cell><cell>85.2</cell><cell>85.6</cell><cell>36.7</cell><cell>52</cell><cell></cell><cell></cell><cell></cell></row><row><cell>NFNet-F2 ? 352 2 [5]</cell><cell>85.1</cell><cell></cell><cell>62.6</cell><cell>194</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CoAtNet-3 [13]</cell><cell>84.5</cell><cell></cell><cell>34.7</cell><cell>168</cell><cell></cell><cell></cell><cell></cell></row><row><cell>XCiT-M24 [18]</cell><cell>82.9</cell><cell></cell><cell>36.1</cell><cell>189</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MViTv2-L</cell><cell>85.3</cell><cell></cell><cell>42.1</cell><cell>218</cell><cell></cell><cell></cell><cell></cell></row><row><cell>NFNet-F4 ? 512 2 [5]</cell><cell>85.9</cell><cell></cell><cell>215.3</cell><cell>316</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CoAtNet-3 [13] ? 384 2</cell><cell></cell><cell>85.8</cell><cell>107.4</cell><cell>168</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MViTv2-L ? 384 2</cell><cell>86.0</cell><cell>86.3</cell><cell>140.2</cell><cell>218</cell><cell></cell><cell></cell><cell></cell></row></table><note>. Comparison to published work on ImageNet-1K. Input images are 224?224 by default and ? denotes using different sizes. MViT is trained for 300 epochs without any external data or models. We report ? 384 2 MViT tested with center crop or a resized view of the original image, to compare to prior work. Full Table in A.3 also explore pre-training on ImageNet-21K (IN-21K) with ?14.2M images and ?21K classes. See ?B for details. Results using ImageNet-1K.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>61.7 44.9 37.1 58.4 40.1 260 44 PVT-S [78] 43.0 65.3 46.9 39.9 62.5 42.8 245 44 Swin-T [55] 46.0 68.2 50.2 41.6 65.1 44.8 264 48 ViL-S-RPB [89] 47.1 68.7 51.5 42.7 65.9 46.2 277 45 MViTv1-T [21] 45.9 68.7 50.5 42.1 66.0 45.58.7 76.7 64.3 50.5 74.2 55.9 -270</figDesc><table><row><cell></cell><cell>(a) Mask R-CNN</cell><cell></cell><cell></cell></row><row><cell>model</cell><cell>AP box AP box 50 AP box 75 AP mask AP mask 50</cell><cell>AP mask 75</cell><cell cols="2">FLOPs Param</cell></row><row><cell>Res50 [38]</cell><cell cols="2">41.0 4</cell><cell>326</cell><cell>46</cell></row><row><cell>MViTv2-T</cell><cell cols="2">48.2 70.9 53.3 43.8 67.9 47.2</cell><cell>279</cell><cell>44</cell></row><row><cell>Res101 [38]</cell><cell cols="2">42.8 63.2 47.1 38.5 60.1 41.3</cell><cell>336</cell><cell>63</cell></row><row><cell>PVT-M [78]</cell><cell cols="2">44.2 66.0 48.2 40.5 63.1 43.5</cell><cell>302</cell><cell>64</cell></row><row><cell>Swin-S [55]</cell><cell cols="2">48.5 70.2 53.5 43.3 67.3 46.6</cell><cell>354</cell><cell>69</cell></row><row><cell cols="3">ViL-M-RPB [89] 48.9 70.3 54.0 44.2 67.9 47.7</cell><cell>352</cell><cell>60</cell></row><row><cell cols="3">MViTv1-S [21] 47.6 70.0 52.2 43.4 67.3 46.9</cell><cell>373</cell><cell>57</cell></row><row><cell>MViTv2-S</cell><cell cols="2">49.9 72.0 55.0 45.1 69.5 48.5</cell><cell>326</cell><cell>54</cell></row><row><cell>X101-64 [83]</cell><cell cols="2">44.4 64.9 48.8 39.7 61.9 42.6</cell><cell cols="2">493 101</cell></row><row><cell>PVT-L [78]</cell><cell cols="2">44.5 66.0 48.3 40.7 63.4 43.7</cell><cell>364</cell><cell>81</cell></row><row><cell>Swin-B [55]</cell><cell cols="2">48.5 69.8 53.2 43.4 66.8 46.9</cell><cell cols="2">496 107</cell></row><row><cell cols="3">ViL-B-RPB [89] 49.6 70.7 54.6 44.5 68.3 48.0</cell><cell>384</cell><cell>76</cell></row><row><cell cols="3">MViTv1-B [21] 48.8 71.2 53.5 44.2 68.4 47.6</cell><cell>438</cell><cell>73</cell></row><row><cell>MViTv2-B</cell><cell cols="2">51.0 72.7 56.3 45.7 69.9 49.6</cell><cell>392</cell><cell>71</cell></row><row><cell>MViTv2-L</cell><cell cols="4">51.8 72.8 56.8 46.2 70.4 50.0 1097 238</cell></row><row><cell>MViTv2-L ?</cell><cell cols="4">52.7 73.7 57.6 46.8 71.4 50.8 1097 238</cell></row><row><cell></cell><cell>(b) Cascade Mask R-CNN</cell><cell></cell><cell></cell></row><row><cell>model</cell><cell>AP box AP box 50 AP box 75 AP mask AP mask 50</cell><cell>AP mask 75</cell><cell cols="2">FLOPs Param</cell></row><row><cell>R50 [38]</cell><cell cols="2">46.3 64.3 50.5 40.1 61.7 43.4</cell><cell>739</cell><cell>82</cell></row><row><cell>Swin-T [55]</cell><cell cols="2">50.5 69.3 54.9 43.7 66.6 47.1</cell><cell>745</cell><cell>86</cell></row><row><cell>MViTv2-T</cell><cell cols="2">52.2 71.1 56.6 45.0 68.3 48.9</cell><cell>701</cell><cell>76</cell></row><row><cell>X101-32 [83]</cell><cell cols="2">48.1 66.5 52.4 41.6 63.9 45.2</cell><cell cols="2">819 101</cell></row><row><cell>Swin-S [55]</cell><cell cols="2">51.8 70.4 56.3 44.7 67.9 48.5</cell><cell cols="2">838 107</cell></row><row><cell>MViTv2-S</cell><cell cols="2">53.2 72.4 58.0 46.0 69.6 50.1</cell><cell>748</cell><cell>87</cell></row><row><cell>X101-64 [83]</cell><cell cols="2">48.3 66.4 52.3 41.7 64.0 45.1</cell><cell cols="2">972 140</cell></row><row><cell>Swin-B [55]</cell><cell cols="2">51.9 70.9 56.5 45.0 68.4 48.7</cell><cell cols="2">982 145</cell></row><row><cell>MViTv2-B</cell><cell cols="2">54.1 72.9 58.5 46.8 70.6 50.8</cell><cell cols="2">814 103</cell></row><row><cell>MViTv2-B ?</cell><cell cols="2">54.9 73.8 59.8 47.4 71.5 51.6</cell><cell cols="2">814 103</cell></row><row><cell>MViTv2-L</cell><cell cols="4">54.3 73.1 59.1 47.1 70.8 51.7 1519 270</cell></row><row><cell>MViTv2-L ? ?</cell><cell cols="4">55.8 74.3 60.9 48.3 71.9 53.2 1519 270</cell></row><row><cell>MViTv2-H ? ?</cell><cell cols="4">56.1 74.6 61.0 48.5 72.4 53.2 3084 718</cell></row><row><cell>MViTv2-L ? ?</cell><cell></cell><cell></cell><cell></cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Results on COCO object detection with (a) Mask R-CNN<ref type="bibr" target="#b35">[36]</ref> and (b) Cascade Mask R-CNN<ref type="bibr" target="#b5">[6]</ref>. ? indicates that the model is initialized from IN-21K pre-training. ? ? denotes using a stronger large-scale jittering training<ref type="bibr" target="#b25">[26]</ref> and longer schedule (50 epochs) with IN-21K pre-training</figDesc><table /><note>* indicates using SoftNMS and multiscale testing. FLOPs / Params are in Giga (10 9 ) / Mega (10 6 ).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>and MViTv1 [21]<ref type="bibr" target="#b4">5</ref> ). E.g., MViTv2-B outperforms Swin-B by +2.5/+2.3 in AP box /AP mask , with lower compute and smaller model size. When scaling up, our deeper MViTv2-L improves over MViTv2-B by +0.8 AP box and using IN-21K pre-training further adds +0.9 to achieve 52.7 AP box with Mask R-CNN and a standard 3?schedule.InTable 5bwe observe a similar trend among backbones for Cascade Mask R-CNN<ref type="bibr" target="#b5">[6]</ref> which lifts Mask R-CNN accuracy (5a). We also ablate the use of a longer training schedule with large-scale jitter that boosts our AP box to 55.8. MViTv2-H increases this to 56.1 AP box and 48.5 AP mask .We further adopt two inference strategies (SoftNMS<ref type="bibr" target="#b3">[4]</ref> and multi-scale testing) on MViTv2-L with Cascade Mask R-CNN for system-level comparison (SeeTable ?A.1). They boosts our AP box to 58.7, which is already better than the best results from Swin (58.0 AP box ), even MViTv2 does not use the improved HTC++ detector<ref type="bibr" target="#b54">[55]</ref> yet.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc>attention can recover 0.4% over default win. While<ref type="bibr" target="#b4">5</ref> We adapt MViTv1 [21] as a detection baseline combined with Hwin. Hybrid window (Hwin) attention fully recovers the performance and outperforms Swin attention by +1.7%. Finally, pooling attention achieves the best accuracy/computation trade-off by getting similar accuracy for ViT-B with significant compute reduction (?38% fewer FLOPs).(ii) For MViTv2-S, pooling attention is used by default. We study if adding local window attention can improve MViT. We observe that adding Swin or Hwin both can reduce the model complexity with slight performance decay. However, directly increasing the pooling stride (from 4 to 8) achieves the best accuracy/compute tradeoff.Table 4bshows the comparison of attention mechanisms on COCO: (i) For ViT-B based models, pooling and pooling + Hwin achieves even better results (+0.6/0.3 AP box ) than standard full attention with ?2? test speedup. (ii) For MViTv2-S, directly increasing the pooling stride (from 4 to 8) achieves better accuracy/computation tradeoff than adding Swin. This result suggests that simple pooling attention can be a strong baseline for object detection. Finally, combining our pooling and Hwin achieves the best tradeoff.</figDesc><table><row><cell></cell><cell cols="3">(a) ImageNet-1K classification</cell><cell></cell><cell></cell><cell cols="3">(b) Mask R-CNN on COCO detection</cell><cell></cell><cell></cell></row><row><cell>variant</cell><cell>attention</cell><cell>Acc</cell><cell cols="2">FLOPs (G) Mem (G)</cell><cell>variant</cell><cell>attention</cell><cell cols="4">AP box Train(iter/s) Test(im/s) Mem(G)</cell></row><row><cell></cell><cell>full</cell><cell>82.0</cell><cell>17.5</cell><cell>12.4</cell><cell></cell><cell>full</cell><cell>46.6</cell><cell>2.3</cell><cell>4.6</cell><cell>24.7</cell></row><row><cell></cell><cell>fixed win</cell><cell>80.0</cell><cell>17.0</cell><cell>9.7</cell><cell></cell><cell>fixed win</cell><cell>43.4</cell><cell>3.3</cell><cell>7.8</cell><cell>5.6</cell></row><row><cell>ViT-B</cell><cell>Swin [55] Hwin</cell><cell>80.4 82.1</cell><cell>17.0 17.1</cell><cell>9.7 10.4</cell><cell>ViT-B</cell><cell>Swin [55] Hwin</cell><cell>45.1 46.1</cell><cell>3.1 3.1</cell><cell>7.5 6.8</cell><cell>5.7 11.0</cell></row><row><cell></cell><cell>pooling</cell><cell>81.9</cell><cell>10.9</cell><cell>8.3</cell><cell></cell><cell>pooling</cell><cell>47.2</cell><cell>2.9</cell><cell>7.9</cell><cell>8.8</cell></row><row><cell></cell><cell>pooling</cell><cell>83.6</cell><cell>7.0</cell><cell>6.8</cell><cell></cell><cell>pooling + Hwin</cell><cell>46.9</cell><cell>3.1</cell><cell>8.8</cell><cell>5.5</cell></row><row><cell>MViTv2-S</cell><cell cols="2">pooling (stride=8) pooling + Swin [55] 82.8 83.2 pooling + Hwin 83.0</cell><cell>6.3 6.4 6.5</cell><cell>5.5 6.0 6.2</cell><cell>MViTv2-S</cell><cell cols="2">pooling pooling (stride=8) pooling + Swin [55] 48.9 50.8 50.0</cell><cell>1.5 2.5 2.6</cell><cell>4.2 8.3 9.2</cell><cell>19.5 7.8 4.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>pooling + Hwin</cell><cell>49.9</cell><cell>2.7</cell><cell>9.4</cell><cell>5.2</cell></row></table><note>. Comparison of attention mechanisms on ImageNet and COCO using ViT-B and MViTv2-S backbones. fixed win: non- overlapping window-attention in all Transformer blocks. Swin: shifted window attention [55]. Hwin: our Hybrid window attention. Pooling: our pooling attention, the K, V pooling stride is 2 (ViT-B) and 4 on the first stage of MViTv2, or pooling (stride=8). Accuracy, FLOPs and peak training memory are measured on IN-1K. For COCO, we report AP box , average training iterations per-second, average testing frames per-second and peak training memory, which are measured in Detectron2 [82] with 8 V100 GPUs under the same settings. Default is in gray.our</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Ablation of residual pooling connections on MViTv2-S.</figDesc><table><row><cell>residual pooling</cell><cell cols="4">IN-1K Acc AP box Train(iter/s) Test(im/s) Mem(G) COCO</cell></row><row><cell>(1) w/o</cell><cell>83.3 48.5</cell><cell>3.0</cell><cell>10.0</cell><cell>4.7</cell></row><row><cell>(2) residual</cell><cell>83.6 49.3</cell><cell>2.9</cell><cell>9.8</cell><cell>4.7</cell></row><row><cell cols="2">(3) full Q pooling + residual 83.6 49.9</cell><cell>2.7</cell><cell>9.4</cell><cell>5.2</cell></row><row><cell>(4) full Q pooling</cell><cell>83.1 48.5</cell><cell>2.8</cell><cell>9.5</cell><cell>5.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 Table 8 .</head><label>78</label><figDesc>Runtime comparison on IN-1K and COCO. We report accuracy and throughput on IN-1K, measured with a V100 GPU as in<ref type="bibr" target="#b54">[55]</ref>. COCO models are measured similarly and also for training throughput and memory. Batch size for all measures is identical.</figDesc><table><row><cell>studies the impor-</cell></row><row><cell>tance of our residual pooling connection. We see that simply</cell></row><row><cell>adding the residual path (2) can improves results on both</cell></row><row><cell>IN-1K (+0.3%) and COCO (+0.8 for AP box ) with negligible</cell></row><row><cell>cost. (3) Using residual pooling and also adding Q pooling</cell></row><row><cell>to all other layers (with stride=1) leads to a significant boost,</cell></row><row><cell>especially on COCO (+1.4 AP Runtime comparison. We conduct a runtime compari-</cell></row></table><note>box ). This suggests both Q pooling blocks and residual paths are necessary in MViTv2. (4) just adding (without residual) more Q pooling layers with stride=1 does not help and even decays (4) vs. (1).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 Table 9</head><label>99</label><figDesc></figDesc><table><row><cell>G)</cell></row></table><note>com- pares the default multi-scale (FPN) detector with the single- scale detector for ViT-B and MViTv2-S. As ViT produces feature maps at a single scale in the backbone, we adopt a simple scheme [50] to up-/downsample features to integrate with FPN. For single-scale, we directly apply the detection heads to the last Transformers block.variant FPN AP box AP mask FLOPs (. Single-scale vs. Multi-scale (FPN) on COCO. ViT-B and MViTv2-S models are equipped with or w/o a feature pyramid network (FPN). Both FPN models outperforms their single-scale variant while while MViTv2 achieves even larger gains.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 11 .</head><label>11</label><figDesc>Comparison with previous work on Kinetics-600.</figDesc><table><row><cell>model</cell><cell cols="3">pre-train top-1 top-5 FLOPs?views Param</cell></row><row><cell>SlowFast 16?8 +NL [23]</cell><cell>-</cell><cell cols="2">79.8 93.9 234?3?10 59.9</cell></row><row><cell>X3D-XL [22]</cell><cell>-</cell><cell cols="2">79.1 93.9 48.4?3?10 11.0</cell></row><row><cell>MoViNet-A6 [45]</cell><cell>-</cell><cell>81.5 95.3</cell><cell>386?1?1 31.4</cell></row><row><cell>MViTv1, 16?4 [21]</cell><cell>-</cell><cell>78.4 93.5</cell><cell>70.3?1?5 36.6</cell></row><row><cell>MViTv1, 32?3 [21]</cell><cell>-</cell><cell>80.2 94.4</cell><cell>170?1?5 36.6</cell></row><row><cell>MViTv2-S, 16?4</cell><cell>-</cell><cell>81.0 94.6</cell><cell>64?1?5 34.5</cell></row><row><cell>MViTv2-B, 32?3</cell><cell>-</cell><cell>82.9 95.7</cell><cell>225?1?5 51.2</cell></row><row><cell>ViT-B-VTN [59]</cell><cell></cell><cell cols="2">78.6 93.7 4218?1?1 114.0</cell></row><row><cell>ViT-B-TimeSformer [3]</cell><cell></cell><cell cols="2">80.7 94.7 2380?3?1 121.4</cell></row><row><cell>ViT-L-ViViT [1] Swin-L? 384 2 [56]</cell><cell>IN-21K</cell><cell cols="2">81.3 94.7 3992?3?4 310.8 84.9 96.7 2107?5?10 200.0</cell></row><row><cell>MViTv2-L? 312 2 , 40?3</cell><cell></cell><cell cols="2">86.1 97.0 2828?3?5 217.6</cell></row><row><cell cols="4">Table 10. Comparison with previous work on Kinetics-400. We</cell></row><row><cell cols="4">report the inference cost with a single "view" (temporal clip with</cell></row><row><cell cols="4">spatial crop) ? the number of views (FLOPs?viewspace?viewtime).</cell></row><row><cell cols="4">Magnitudes are Giga (10 9 ) for FLOPs and Mega (10 6 ) for Param.</cell></row><row><cell>model</cell><cell cols="3">pretrain top-1 top-5 FLOPs?views Param</cell></row><row><cell>SlowFast 16?8 +NL [23]</cell><cell>-</cell><cell>81.8 95.1</cell><cell>234?3?10 59.9</cell></row><row><cell>X3D-XL [22]</cell><cell>-</cell><cell>81.9 95.5</cell><cell>48.4?3?10 11.0</cell></row><row><cell>MoViNet-A6 [45]</cell><cell>-</cell><cell>84.8 96.5</cell><cell>386?1?1 31.4</cell></row><row><cell>MViTv1-B-24, 32?3 [21]</cell><cell>-</cell><cell>84.1 96.5</cell><cell>236?1?5 52.9</cell></row><row><cell>MViTv2-B, 32?3</cell><cell>-</cell><cell>85.5 97.2</cell><cell>206?1?5 51.4</cell></row><row><cell>ViT-L-ViViT [1]</cell><cell></cell><cell>83.0 95.7</cell><cell>3992?3?4 310.8</cell></row><row><cell>Swin-B [56]</cell><cell></cell><cell>84.0 96.5</cell><cell>282?3?4 88.1</cell></row><row><cell>Swin-L? 384 2 [56]</cell><cell>IN-21K</cell><cell>86.1 97.3</cell><cell>2107?5?10 200.0</cell></row><row><cell>MViTv2-L? 312 2 , 32?3</cell><cell></cell><cell>87.2 97.6</cell><cell>2063?3?4 217.6</cell></row><row><cell>MViTv2-L? 312 2 , 40?3</cell><cell></cell><cell>87.5 97.8</cell><cell>2828?3?4 217.6</cell></row><row><cell>MViTv2-L? 352 2 , 40?3</cell><cell></cell><cell>87.9 97.9</cell><cell>3790?3?4 217.6</cell></row><row><cell>model</cell><cell cols="3">pretrain top-1 top-5 FLOPs?views Param</cell></row><row><cell cols="3">SlowFast 16?8 +NL [23] K600 71.0 89.6</cell><cell>234?3?10 59.9</cell></row><row><cell>MoViNet-A6 [45]</cell><cell cols="2">N/A 72.3 N/A</cell><cell>386?1?1 31.4</cell></row><row><cell>MViTv2-B, 32?3</cell><cell>-</cell><cell>76.6 93.2</cell><cell>206?3?3 51.4</cell></row><row><cell cols="3">MViTv2-L? 312 2 , 40?3 IN-21K 79.4 94.9</cell><cell>2828?3?3 217.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 compares</head><label>10</label><figDesc>MViTv2 to prior work, including state-of-the-art CNNs and ViTs. When training from scratch, our MViTv2-S &amp; B models produce 81.0% &amp; 82.9% top-1 accuracy which is +2.6% &amp; +2.7% higher than their MViTv1 [21] counterparts. These gains stem solely from the improvements in ?4.1, as the training recipe is identical.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 13</head><label>13</label><figDesc>9% while using ?30% and 40% fewer FLOPs and parameters and only K400. With IN-21K pretraining, MViTv2-B boosts accuracy by 1.6% and achieves 72.1%. MViTv2-L achieves 73.3% top-1 accuracy.</figDesc><table><row><cell>compares methods on</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 6</head><label>6</label><figDesc>&amp; 7 and are in ?A.5.</figDesc><table><row><cell>model</cell><cell cols="6">T?? scratch IN1k IN21k FLOPs Param</cell></row><row><cell>MViTv2-S</cell><cell>16?4</cell><cell>81.2</cell><cell>82.2</cell><cell>82.6</cell><cell>64</cell><cell>34.5</cell></row><row><cell>MViTv2-B</cell><cell>32?3</cell><cell>82.9</cell><cell>83.3</cell><cell>84.3</cell><cell>225</cell><cell>51.2</cell></row><row><cell>MViTv2-L</cell><cell>40?3</cell><cell>81.4</cell><cell>83.4</cell><cell>84.5</cell><cell cols="2">1127 217.6</cell></row><row><cell cols="2">MViTv2-L? 312 2 40?3</cell><cell>81.8</cell><cell>84.4</cell><cell>85.7</cell><cell cols="2">2828 217.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 14</head><label>14</label><figDesc>compares the effect different pre-training schemes on K400. We observe that: (i) For MViTv2-S and MViTv2-B models, using either IN1K or IN21k pre-training boosts accuracy compared to training from scratch, e.g.MViTv2-S gets +1.0% and 1.4% gains with IN1K and IN21K pre-training. (ii) For large models, ImageNet pre-training is necessary as they are heavily overfitting when trained from scratch (cf .Table 10).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table A</head><label>A</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">val mAP</cell><cell></cell><cell></cell></row><row><cell>model</cell><cell>pretrain</cell><cell cols="4">center full FLOPs Param</cell></row><row><cell>SlowFast, 4?16, R50 [23]</cell><cell></cell><cell>21.9</cell><cell>-</cell><cell>52.6</cell><cell>33.7</cell></row><row><cell>SlowFast, 8?8, R101 [23]</cell><cell></cell><cell>23.8</cell><cell>-</cell><cell>137.7</cell><cell>53.0</cell></row><row><cell>MViTv1-B, 16?4 [21] MViTv1-B, 64?3 [21]</cell><cell>K400</cell><cell>24.5 27.3</cell><cell>--</cell><cell>70.5 454.7</cell><cell>36.4 36.4</cell></row><row><cell>MViTv2-S, 16?4</cell><cell></cell><cell cols="3">26.8 27.6 64.5</cell><cell>34.3</cell></row><row><cell>MViTv2-B, 32?3</cell><cell></cell><cell cols="3">28.1 29.0 225.2</cell><cell>51.0</cell></row><row><cell>SlowFast, 8?8 R101+NL [23]</cell><cell></cell><cell>27.1</cell><cell>-</cell><cell>146.6</cell><cell>59.2</cell></row><row><cell>SlowFast, 16?8 R101+NL [23]</cell><cell></cell><cell>27.5</cell><cell>-</cell><cell>296.3</cell><cell>59.2</cell></row><row><cell>X3D-XL [22]</cell><cell></cell><cell>27.4</cell><cell>-</cell><cell>48.4</cell><cell>11.0</cell></row><row><cell>Object Transformer [80] ACAR 8?8, R101-NL [60]</cell><cell>K600</cell><cell>31.0 -</cell><cell cols="2">-31.4 N/A 243.8</cell><cell>86.2 N/A</cell></row><row><cell>MViTv1-B, 16?4 [21]</cell><cell></cell><cell>26.1</cell><cell>-</cell><cell>70.4</cell><cell>36.3</cell></row><row><cell>MViTv1-B-24, 32?3 [21]</cell><cell></cell><cell>28.7</cell><cell></cell><cell>236.0</cell><cell>52.9</cell></row><row><cell>MViTv2-B, 32?3</cell><cell></cell><cell cols="3">29.9 30.5 225.2</cell><cell>51.0</cell></row><row><cell>ACAR 8?8, R101-NL [60]</cell><cell>K700</cell><cell>-</cell><cell cols="2">33.3 N/A</cell><cell>N/A</cell></row><row><cell>MViTv2-B, 32?3</cell><cell>K700</cell><cell cols="3">31.3 32.3 225.2</cell><cell>51.0</cell></row><row><cell>MViTv2-L? 312 2 , 40?3</cell><cell cols="5">IN21K+K700 33.5 34.4 2828 213.0</cell></row><row><cell cols="6">Table A.2. Comparison with previvous work on AVA v2.2. We</cell></row><row><cell cols="6">adopt two test strategies: 1) center (single center crop): we resize</cell></row><row><cell cols="6">the shorter spatial side to 224 pixels and takes a 224 2 center crop</cell></row><row><cell cols="6">for inference. 2) full (full-resolution): we resize the shorter spatial</cell></row><row><cell cols="6">side to 224 pixels and take the full image for inference. We report</cell></row><row><cell cols="6">inference cost with the center testing strategy (i.e. 224 2 input).</cell></row><row><cell cols="6">Magnitudes are Giga (10 9 ) for FLOPs and Mega (10 6 ) for Param.</cell></row><row><cell>.1. System-level comparison on COCO object detection</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>and segmentation. The detection frameworks include Cascade</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mask R-CNN [6] (Cascade), the improved Hybrid Task Cascade</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(HTC++) [55] and Cascade Mask R-CNN with NAS-FPN [27].</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>A.2. Results: AVA Action Detection</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Results on AVA. Table A.2 shows the results of our MViTv2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>models compared with prior state-of-the-art works on the</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>AVA dataset [32] which is a dataset for spatiotemporal-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>localization of human actions.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>We observe that MViT consistently achieves better re-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>sults compared to MViTv1 [21] counterparts. For example,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MViTv2-S 16?4 (26.8 mAP) improves +2.3 over MViTv1-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>B 16?4 (24.5 mAP) with fewer flops and parameters (both</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>with the same recipe and default K400 pre-training). For</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>K600 pre-training, MViTv2-B 32?3 (29.9 mAP) improves</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>+1.2 over MViTv1-B-24 32?3. This again validates the ef-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>fectiveness of the proposed MViTv2 improvements in  ?4.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>of the main paper. Using full-resolution testing (without</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>cropping) can further improve MViTv2-B by +0.6 to achieve</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>30.5 mAP. Finally, the larger MViTv2-L 40?3 achieves the</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* indicates multi-scale testing. FLOPs and Params are in Giga (10 9 ) and Mega (10 6 ).state-of-the-art results at 34.4 mAP using IN-21K and K700 pre-training.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>Comparison to previous work on ImageNet-1K. Input images are 224?224 by default and ? denotes using different sizes. MViT is trained for 300 epochs without any external data or models. We report our ? 384 2 models tested using a center crop or a resized full crop of the original image, to compare to prior work.</figDesc><table><row><cell></cell><cell>Acc</cell><cell></cell><cell></cell></row><row><cell>model</cell><cell cols="3">center resize FLOPs (G) Param (M)</cell></row><row><cell>RegNetY-4GF [62]</cell><cell>80.0</cell><cell>4.0</cell><cell>21</cell></row><row><cell cols="4">RegNetZ-4GF [15] EfficientNet-B4 ? 380 2 [71] DeiT-S [72] PVT-S [78] TNT-S [33] T2T-ViTt-14 [85] CvT-13 [81] Twins-S [11] ViL-S-RPB [89] PVTv2-V2 [77] CrossViTc-15 [9] XCiT-S12 [18] Swin-T [55] CSWin-T [16] MViTv2-T RegNetY-8GF [62] EfficientNet-B5 ? 456 2 [71] PVT-M [78] T2T-ViTt-19 [85] CvT-21 [81] Twins-B [11] ViL-M-RPB [89] PVTv2-V2-B3 [77] CrossViTc-18 [9] XCiT-S24 [18] Swin-S [55] CSWin-S [16] MViT-v1-B-16 [21] MViTv2-S RegNetY-16GF [62] RegNetZ-16GF [15] EfficientNet-B6 ? 528 2 [71] NFNet-F0 ? 256 2 [5] DeiT-B [72] PVT-L [78] T2T-ViTt-21 [85] TNT-B [33] Twins-L [11] ViL-B-RPB [89] PVTv2-V2-B5 [77] CaiT-S36 [74] XCiT-M24 [18] Swin-B [55] CSWin-B [16] MViTv1-B-24 [21] MViTv2-B EfficientNet-B7 ? 600 2 [71] NFNet-F1 ? 320 2 [5] DeiT-B ? 384 2 [72] TNT-B ? 384 2 [33] CvT-32 ? 384 2 [81] CaiT-S36? 384 2 [74] Swin-B ? 384 2 [55] MViT-v1-B-24 ? 320 2 [21] MViTv2-B ? 384 2 NFNet-F2 ? 352 2 [5] XCiT-M24 [18] CoAtNet-3 [13] MViTv2-L NFNet-F4 ? 512 2 [5] CoAtNet-3 [13] ? 384 2 MViTv2-L ? 384 2 Table A.3. rel pos 83.1 4.0 28 82.9 4.2 19 79.8 4.6 22 79.8 3.8 25 81.5 5.2 24 81.7 6.1 22 81.6 4.5 20 81.7 2.9 24 82.4 4.9 25 82.0 4.0 25 82.3 6.1 28 82.0 4.8 26 81.3 4.5 29 82.7 4.3 23 82.3 4.7 24 81.7 8.0 39 83.6 9.9 30 81.2 6.7 44 82.4 9.8 39 82.5 7.1 32 83.2 8.6 56 83.5 8.7 40 83.2 6.9 45 82.8 9.5 44 82.6 9.1 48 83.0 8.7 50 83.6 6.9 35 83.0 7.8 37 83.6 7.0 35 82.9 15.9 84 84.1 15.9 95 84.2 19 43 83.6 12.4 72 81.8 17.6 87 81.7 9.8 61 82.6 15.0 64 82.9 14.1 66 83.7 15.1 99 83.7 13.4 56 83.8 11.8 82 83.3 13.9 68 82.7 16.2 84 83.3 15.4 88 84.2 15.0 78 83.4 10.9 54 84.4 10.2 52 84.3 37.0 66 84.7 35.5 133 83.1 55.5 87 83.9 N/A 66 83.3 24.9 32 85.0 48 68 84.2 47.0 88 84.8 32.7 73 85.2 85.6 36.7 52 85.1 62.6 194 82.9 36.1 189 84.5 34.7 168 85.3 42.1 218 85.9 215.3 316 85.8 107.4 168 86.0 86.3 140.2 218 relq rel k relv Acc Mem(G) Test (im/s) AP box AP mask IN-1K COCO ? ? ? 83.6 6.2 316 49.9 45.0 ? ? ? 83.4 6.2 321 49.7 44.8 ? ? ? 83.6 6.4 300 50.0 45.0 ? ? ? 83.6 30.8 109 OOM OOM ? ? ? 83.7 30.9 104 OOM OOM ? ? ? 83.6 30.9 103 OOM OOM Table A.4. Ablation of rel pos embeddings on ImageNet-1K and COCO with MViT-S.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table A .</head><label>A</label><figDesc>4 shows the ablation experiments: different variants achieve similar accuracy on ImageNet and COCO. However rel v requires more GPU memory (e.g. 30.8G vs 6.2G on ImageNet and out-of-memory (OOM) on COCO) and has a ?2.9?lower test throughput on ImageNet. For simplicity and efficiency, we use only rel q by default.Effect of pre-training datasets for detection. In ?6.2 of the main paper we observe that ImageNet pre-training can have very different effects for different model sizes for video classification. Here, we are interested in the impact of pretraining on the larger IN-21K vs. IN-1K for COCO object detection tasks.Table A.5 shows our ablation: The largescale IN-21K pre-training is more helpful for larger models, e.g. MViT-B and MViT-L have +0.5 and +0.9 gains in AP box .Table A.5. Effect of pre-training datasets for COCO. Detection methods are initialized from IN-1K or IN-21K pre-trained weights.</figDesc><table><row><cell>variant</cell><cell cols="2">AP box IN-1k IN-21k IN-1k IN-21k AP mask</cell></row><row><cell cols="2">MViTv2-S 49.9 50.2</cell><cell>45.1 45.1</cell></row><row><cell cols="2">MViTv2-B 51.0 51.5</cell><cell>45.7 46.4</cell></row><row><cell cols="2">MViTv2-L 51.8 52.7</cell><cell>46.2 46.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head></head><label></label><figDesc>Table A.6 compares different positional embeddings for MViTv2 on K400. Similar to image classification and object detection (Table 6of the main paper), relative positional embeddings surpass absolute rel. pos. abs. pos.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Top-1 Train Param</cell></row><row><cell></cell><cell>space time</cell><cell cols="3">(%) (clip/s) (M)</cell></row><row><cell>(1) no pos.</cell><cell></cell><cell cols="3">80.1 91.5 34.4</cell></row><row><cell>(2) abs. pos.</cell><cell>?</cell><cell cols="3">80.4 91.0 34.7</cell></row><row><cell>(3) time-only rel.</cell><cell>?</cell><cell cols="3">80.8 80.5 34.4</cell></row><row><cell>(4) space-only rel.</cell><cell>dec.</cell><cell cols="3">80.6 76.2 34.5</cell></row><row><cell cols="2">(5) dec. space rel. + time rel. dec. ?</cell><cell cols="3">81.0 66.6 34.5</cell></row><row><cell cols="2">(6) joint space rel. + time rel. joint ?</cell><cell cols="3">81.1 33.6 37.1</cell></row><row><cell>(7) joint space/time rel.</cell><cell>joint</cell><cell>-</cell><cell>8.4</cell><cell>73.7</cell></row><row><cell cols="5">Table A.6. Ablation of positional embeddings on K400 with</cell></row><row><cell cols="5">MViTv2-S 16?4. Training throughput is measured by average clips</cell></row><row><cell cols="5">per-second with 8 V100 GPUs. Our (5) decomposed space/time</cell></row><row><cell cols="5">rel. positional embeddings are accurate and significantly faster</cell></row><row><cell cols="5">than other joint versions. Note that we do not finish the full train-</cell></row><row><cell cols="5">ing for (7) joint space/time rel. as the training speed is too slow</cell></row><row><cell cols="5">(?8? slower than ours) and (6) joint space rel. already shows large</cell></row><row><cell cols="4">drawbacks (?2? slower) of joint rel. positional embeddings.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head></head><label></label><figDesc>Besides the technical improvements introduced in ?4.1 of the main paper, MViT entails two further changes: (i) We conduct the channel dimension expansion in the attention computation of the first transformer block of each stage, instead of performing it in the last MLP block of the prior stage as in MViTv1[21]. This change has similar accuracy (?0.1%) to the original version, while reducing parameters and FLOPs. (ii) We remove the class token in MViT by default as this has no advantage for image classification tasks. Instead, we average the output tokens from the last transformer block and apply the final classification head upon it. In practice, we find this modification could reduce the training time by ?8%.</figDesc><table><row><cell></cell><cell></cell><cell>B.2. Details: ImageNet Classification</cell></row><row><cell></cell><cell></cell><cell>IN-1K training.</cell></row><row><cell>residual pooling</cell><cell cols="2">Top-1 FLOPs</cell></row><row><cell>(1) w/o</cell><cell>79.3</cell><cell>64</cell></row><row><cell>(2) full Q pooling</cell><cell>79.7</cell><cell>65</cell></row><row><cell>(3) residual</cell><cell>79.7</cell><cell>64</cell></row><row><cell cols="2">(4) full Q pooling + residual 81.0</cell><cell>65</cell></row><row><cell cols="3">Table A.7. Ablation of residual pooling connections on K400</cell></row><row><cell>with MViTv2-S 16?4 architecture.</cell><cell></cell><cell></cell></row><row><cell cols="3">B. Additional Implementation Details</cell></row><row><cell>B.1. Other Upgrades in MViT</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Note that no initialization is needed if using max-pooling variants.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Vivit: A video vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15691</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Toward transformer-based object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Beal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kislyuk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09958</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Is space-time attention all you need for video understanding?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05095</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Soft-nms-improving object detection with one line of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navaneeth</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06171</idno>
		<title level="m">High-performance large-scale image recognition without normalization</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06987</idno>
		<title level="m">A short note on the kinetics-700 human action dataset</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Crossvit: Cross-attention multi-scale vision transformer for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Fu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV, 2021</title>
		<meeting>ICCV, 2021</meeting>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Drop an octave: Reducing spatial redundancy in convolutional neural networks with octave convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.05049</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Twins: Revisiting the design of spatial attention in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS, 2021</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Coatnet: Marrying convolution and attention for all data sizes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04803</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast and accurate model scaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR, 2021</title>
		<meeting>CVPR, 2021</meeting>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Cswin transformer: A general vision transformer backbone with cross-shaped windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.00652</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09681</idno>
		<title level="m">Cross-covariance image transformers</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Haoqi Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pyslowfast</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/slowfast,2020.2" />
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">PyTorchVideo: A deep learning library for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tullie</forename><surname>Haoqi Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Murrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Vasudev Alwala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhila</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haichuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Feiszli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Adcock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feichtenhofer</surname></persName>
		</author>
		<ptr target="https://pytorchvideo.org/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multiscale vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karttikeya</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">X3D: Expanding architectures for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">SlowFast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Simple copy-paste is a strong data augmentation method for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Nas-fpn: Learning scalable feature pyramid architecture for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Video action transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: training ImageNet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Something Something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heuna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Mueller-Freitag</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">AVA: A video dataset of spatiotemporally localized atomic visual actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhui</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanna</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Transformer in transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Resnest: Split-attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Hang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongruo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">How to start training: The effect of initialization and architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Hanin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Rolnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01719</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Augment your batch: Improving generalization through instance repetition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niv</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Giladi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Stm: Spatiotemporal and motion encoding for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Token labeling: Training a 85</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daquan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10858</idno>
	</analytic>
	<monogr>
		<title level="m">5% top-1 accuracy vision transformer with 56m parameters on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">MoViNets: Mobile video networks for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Kondratyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangzhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Ima-geNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Handwritten digit recognition with a back-propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donnie</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donnie</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Tea: Temporal excitation and aggregation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="909" to="918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Benchmarking detection transfer learning with vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.11429</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">VideoLSTM convolves, attends and flows for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirill</forename><surname>Gavrilyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihir</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">166</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="41" to="50" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13230</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Video swin transformer. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">SGDR: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Fixing weight decay regularization in adam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Neimark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Bar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.00719</idno>
		<title level="m">Maya Zohar, and Dotan Asselmann. Video transformer network</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Actor-context-actor relation network for spatio-temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junting</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><forename type="middle">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR, 2021</title>
		<meeting>CVPR, 2021</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02155</idno>
		<title level="m">Selfattention with relative position representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Segmenter: Transformer for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Strudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.05633</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.00567</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<title level="m">Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<title level="m">Alexandre Sablayrolles, and Herv? J?gou. DeiT: Data-efficient image transformers</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.17239</idno>
		<title level="m">Alexandre Sablayrolles, Gabriel Synnaeve, and Herv? J?gou. Going deeper with image transformers</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Video classification with channel-separated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13797</idno>
		<title level="m">Pvtv2: Improved baselines with pyramid vision transformer</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Long-term feature banks for detailed video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Towards long-form video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Chao-Yuan Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR, 2021</title>
		<meeting>CVPR, 2021</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15808</idno>
		<title level="m">Cvt: Introducing convolutions to vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron2</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Rethinking spatiotemporal feature learning for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.04851</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi-Hang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><forename type="middle">E H</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Volo: Vision outlooker for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Multi-scale vision longformer: A new vision transformer for high-resolution image encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Objects as points. arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
