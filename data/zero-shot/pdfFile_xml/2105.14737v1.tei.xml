<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semi-orthogonal Embedding for Efficient Unsupervised Anomaly Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saehoon</forename><surname>Yi</surname></persName>
							<email>saehoon.yi@sk.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taehoon</forename><surname>Lee</surname></persName>
							<email>taehoonlee@sk.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<country>Telecom Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<country>Telecom Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<country>Telecom Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<address>
									<country>Telecom Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Semi-orthogonal Embedding for Efficient Unsupervised Anomaly Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Do-Hyeong Kim</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present the efficiency of semi-orthogonal embedding for unsupervised anomaly segmentation. The multi-scale features from pre-trained CNNs are recently used for the localized Mahalanobis distances with significant performance. However, the increased feature size is problematic to scale up to the bigger CNNs, since it requires the batch-inverse of multi-dimensional covariance tensor. Here, we generalize an ad-hoc method, random feature selection, into semi-orthogonal embedding for robust approximation, cubically reducing the computational cost for the inverse of multi-dimensional covariance tensor. With the scrutiny of ablation studies, the proposed method achieves a new state-of-the-art with significant margins for the MVTec AD, KolektorSDD, KolektorSDD2, and mSTC datasets. The theoretical and empirical analyses offer insights and verification of our straightforward yet cost-effective approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Unsupervised anomaly segmentation is to localize the anomaly regions in the test sample while only anomaly-free samples are available in training. So, anomaly segmentation is a more challenging task than anomaly detection, which is generally referred to detection whether a given sample has anomaly. Anomaly segmentation is to give the visual explanation for the detected anomalies and the location for manual inspection. In real-world problems, anomaly-free samples are usually redundant; however, anomaly samples are scarce due to manufacturing and annotation costs. For this reason, unsupervised methods are favored while it can also provide the robustness towards unknown anomaly forms.</p><p>The reconstruction error-based methods are explored for autoencoders <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> and GANs <ref type="bibr" target="#b2">[3]</ref>. The common idea is to train generative networks to minimize reconstruction errors learning low-dimensional features, and expect the higher error for the anomalies not presented in training than the anomaly-free. However, the networks with a sufficient capacity could restore even anomalies causing performance degradation, although the perceptual loss function for the generative networks <ref type="bibr" target="#b0">[1]</ref> or the knowledge distillation loss for teacher-student pairs of networks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> achieves a limited success.</p><p>An inspiring advance comes from a tradition, the Mahalanobis distance <ref type="bibr" target="#b5">[6]</ref>. It measures how many standard deviations away a given sample is from the mean of a distribution of normal samples. First, one update is to use the features extracted from the CNNs pre-trained by many natural images. Second, for unsupervised anomaly segmentation, the localized Mahalanobis distance using the feature maps outperform the comparative methods <ref type="bibr" target="#b6">[7]</ref>, which exploits a separate covariance for each location in the feature map. This method is in accordance with the assumption of a Gaussian distribution having a single mode since the distribution for every locations of feature maps tends to be a multi-modal distribution. Third, the multi-scale features enable to detect the anomalies in the interactions among the different stages <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7]</ref>, along with various sizes of receptive-fields in the CNNs.</p><p>However, the precision matrices for the Mahalanobis distances are required to compute for every locations in the feature map, which formulates the batch-inverse of multi-dimensional tensor where the batch size is for every locations, H ? W . As an ad-hoc method, Defard et al. <ref type="bibr" target="#b6">[7]</ref> propose to use randomly sampled features to reduce the covariance size. Our study reveals that it incurs the rank reduction when redundant features are selected with a limited budget of covariance size.</p><p>In this paper, we generalize the random feature selection to semi-orthogonal embedding as a low-rank approximation of precision matrix for the Mahalanobis distance. The uniformly generated semiorthogonal matrix <ref type="bibr" target="#b7">[8]</ref> can avoid the singular case retaining the better performance while cubically reducing the computational cost for batch-inverse. We achieve new state-of-the-art results for the benchmark datasets, MVTec AD <ref type="bibr" target="#b8">[9]</ref>, KolektorSDD <ref type="bibr" target="#b9">[10]</ref>, KolektorSDD2 <ref type="bibr" target="#b10">[11]</ref>, and mSTC <ref type="bibr" target="#b11">[12]</ref> while outperforming the competitive methods using reconstruction error-based <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref> or knowledge distillation-based <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> methods with substantial margins. Moreover, we show that our method decoupled with the pre-trained CNNs can exploit the advances of discriminative models without a fine-tuning procedure.</p><p>2 Mahalanobis distance with multi-scale visual features 2.1 Mahalanobis distance for anomaly segmentation Let X i,j ? R F ?N be the {i,j}-th feature vectors from the H ? W feature map extracted from a pre-trained CNNs, where F is the feature size and N is the number of training samples. Without loss of generality, the mean of feature vectors is zero. For each position, the covariance matrix is C i,j = 1 N X i,j X i,j , then, the squared Mahalanobis distance for a feature vector x i,j is defined as:</p><formula xml:id="formula_0">d 2 i,j = x T i,j C ?1 i,j x i,j ? R +<label>(1)</label></formula><p>where d i,j indicates the anomaly score for x i,j . The Gaussian assumption of Mahalanobis distance is blessed by the localized statistics since it has a better chance to have unimodal distributions assuming that the spatial alignment of samples is done by pre-processing. Additionally, the multi-scale visual features are helpful to detect various sizes of anomalies <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b12">13]</ref> where the feature maps from the layers with different sizes of receptive field are considered after their spatial dimensions are matched to the largest H ? W by interpolation. However, the inverse of covariance matrices requires O(HW F 3 ), which prohibits to efficiently compute with a large F of multi-scale features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Low-rank approximation of precision matrix</head><p>The feature data X is subject to low-rank approximation due to the narrower target domain for anomaly-free images than the ImageNet dataset's. The multi-scale features from different layers may also contribute to it due to the inter-dependency among the features from the layers. Inspired by the truncated SVD of a precision matrix, a low-rank embedding of input features with W ? R F ?k , where F &gt; k, is considered as follows:</p><formula xml:id="formula_1">d 2 i,j = x W(W C i,j W) ?1 W x<label>(2)</label></formula><p>where the below Theorem 1 shows the optimal W is the eigenvectors related to the k-smallest eigenvalues of C i,j . Notice that 1) the computational complexity of the equation is cubically reduced to O(HW k 3 ) set aside the cost of SVD, although which is the concern, 2) PCA embedding would fail to minimize approximation error since it uses the k-largest eigenvectors <ref type="bibr" target="#b13">[14]</ref>, and 3) near-zero eigenvalues may induce substantial anomaly scores. For the last, a previous work suggests to use C + I for the inverse to avoid a possible numerical problem <ref type="bibr" target="#b6">[7]</ref>, what we follow. Lemma 1. (Truncated SVD) For any k ? 1, ..., min(F, N ) and B ? R F ?k , let U k ? R F ?k be the last k columns of U, which is the eigenvectors of C, and ? k be a k ? k diagonal matrix containing the smallest k eigenvalues of a Hermitian positive definite matrix C. Then, we have that:</p><formula xml:id="formula_2">min B C ?1 ? BB 2 = C ?1 ? U k ? ?1 k U k 2<label>(3)</label></formula><p>where ? denotes the spectral norm or Frobenius norm.</p><p>Proof. The proof is through the well-known Eckart-Young theorem <ref type="bibr" target="#b14">[15]</ref>, noting that C ?1 has the same eigenvectors and the inverse of eigenvalues of C, where C ?1 = U? ?1 U .</p><formula xml:id="formula_3">Theorem 1. (Low-rank embedding of precision matrix) Let W ? R F ?k , where k ? min(F, N ),</formula><p>be a low-rank embedding matrix. Then, we have that:</p><formula xml:id="formula_4">U k ? arg min W C ?1 ? W(W CW) ?1 W 2 .<label>(4)</label></formula><p>Proof. Because W is a non-square matrix, it does not have a right inverse when F &gt; k. If the right inverse exists, it satisfies with any W. Letting W = U k , we have:</p><formula xml:id="formula_5">W(W CW) ?1 W = U k (U k U?U U k ) ?1 U k = U k ? ?1 k U k .<label>(5)</label></formula><p>Letting (W CW) ?1 = V?V and B = WV? 1 2 , and using Lemma 1, we conclude the proof.</p><p>However, it still needs to compute SVD or other algorithms to find the k-smallest eigenvectors for the multi-dimensional covariance matrix. In the following section, we will discuss a simple yet effective solution for anomaly segmentation tasks.</p><p>3 Semi-orthogonal embedding for low-rank approximation Orthogonal invariance. The eigenvalues of a matrix C, the essence for the precise anomaly detection, are invariant concerning left or right unitary transformations of C. Moreover, Theorem 1 states the invariant property of an orthogonal matrix for the inverse of C. But, since W is a square matrix, there is no computational advantage. Proposition 1. (Orthogonal invariance) With a consistent notation except a random orthogonal matrix W F ? R F ?F having orthonormal column vectors, we have that:</p><formula xml:id="formula_6">C ?1 = W F (W F CW F ) ?1 W F .<label>(6)</label></formula><p>Proof. The inverse of W F is W F . Therefore, the right term is</p><formula xml:id="formula_7">W F W F C ?1 W F W F = C ?1 .</formula><p>Semi-orthogonal. Based on this observation of the orthogonal invariance, we propose to use uniformly distributed k-orthonormal vectors to embed feature vectors. These vectors consist of the unitary transformation, but having at most k-rank constraint. The uniformly-distributed <ref type="bibr" target="#b15">[16]</ref> orthonormal vectors are generated from Gaussian distributed random variables ? ? R F ?k ? N (0, 1), while the QR decomposition gives ? = QR. Since we use the same embedding to every locations of the feature map, the cost is neglectable. We get W following the method of Mezzadri's <ref type="bibr" target="#b7">[8]</ref>:</p><formula xml:id="formula_8">W = Q ? sign diag(R) ? R F ?k<label>(7)</label></formula><p>where diag(?) returns a diagonal matrix of a given matrix and sign(?) returns the sign of elements in the same shape of matrix, which corrects its distribution to be uniform. In linear algebra, the matrix W is called a semi-orthogonal matrix, where</p><formula xml:id="formula_9">W W = I k ? R k?k , but WW = I F ? R F ?F . Proposition 2. (Expectation of low-rank Mahalanobis distances) Let C = 1 N XX , X ? R F ?N , W ? R F ?k is a matrix having k-orthonormal columns, where k ? min(F, N )</formula><p>, and x is a column vector of X. Then, we have that:</p><formula xml:id="formula_10">E x [x W(W CW) ?1 W x] = k.<label>(8)</label></formula><p>The proof is shown in Appendix A. The expectation of the low-rank Mahalanobis distance is the same for any semi-orthogonal matrix W. This property is helpful to adjust the threshold depending on k using the semi-orthogonal embedding. But, be cautious that the approximation error of the precision matrix is critical to the generalization of normal samples, which impacts on the performance of anomaly segmentation.</p><p>Approximation error of the precision matrix. The lower bound of the approximation error of a precision matrix C ?1 using a semi-orthogonal matrix is related to the eigenvectors corresponding k-smallest eigenvalues of C as in Theorem 1. Furthermore, Theorem 2 states the upper bound of error using the convexity of eigenvalues in the approximation (Lemma 2 in Appendix) and the Cauchy interlacing theorem. Please refer to Appendix A for the proof.</p><p>Theorem 2. (Error bounds of low-rank precision matrix). For ? k and U k are the diagonal matrix having the k-smallest eigenvalues of C and the corresponding eigenvectors, respectively, and, ? ?k and U ?k have the k-largest eigenvalues of C and the corresponding eigenvectors, respectively, the error bounds of the semi-orthogonal approximation of a precision matrix are that:</p><formula xml:id="formula_11">C ?1 ? U k ? ?1 k U k 2 ? C ?1 ? W(W CW) ?1 W 2 ? C ?1 ? U ?k ? ?1 ?k U ?k 2<label>(9)</label></formula><p>Interestingly, the assumption of flat-eigenvalues of C where C = ?I gives an opportunity to assess the error of the randomized approximation. Corollary 1. (Approximation error of the flat-eigenvalues) With a consistent notation, and the flat-eigenvalues assumption of C = ?I where ? ? R + . Then, we have that:</p><formula xml:id="formula_12">C ?1 ? W(W CW) ?1 W 2 = 1 ? I F ?k 2 .<label>(10)</label></formula><p>Proof. Since the all eigenvalues are ?, we can rewrite the equation in Theorem 2 as follows:</p><formula xml:id="formula_13">1 ? I F ?k 2 ? C ?1 ? W(W CW) ?1 W 2 ? 1 ? I F ?k 2<label>(11)</label></formula><p>which concludes the proof.</p><p>In Corollary 1, any semi-orthogonal matrix has the same result while the interval between the bounds of approximation error are shrinked to zero for the uniform eigenvalues. Although, in real-world problems, we do not expect the assumption is true, we cautiously remind that the batch normalization regularizes CNNs to have intermediate outputs be independently normalized <ref type="bibr" target="#b16">[17]</ref>. To evaluate our method, we show the empirical efficacy of the proposed approximation in Section 4.</p><p>Random feature selection is a special case of the semi-orthogonal. If we randomly select kcolumn vectors of an identity matrix I F , it is a special case of semi-orthogonal matrices. This is equivalent to randomly selecting k features to calculate the precision matrix for Mahalanobis distance; however, it is severely vulnerable to the redundancy in features. If there are only l of F independent features where the rank is min(N, l) instead of min(N, F ), the random selection of k features among F features has a chance that the rank is less than k for the selected redundant features. Please see the ablation study in the Experiment section and <ref type="figure">Figure 3</ref>.</p><p>Computational complexity. The major part of the computational cost comes from the inverse of a multi-dimensional tensor. Since the use of a semi-orthogonal matrix decreases the size of the matrix from F ? F to k ? k, the computational cost is cubically reduced to O(HW k 3 ). For example, the batch-SVD (the same complexity with batch-matrix inversion) of a tensor of 64 ? 64 ? 100 ? 100 takes 191.5 seconds, while a tensor of 64 ? 64 ? 448 ? 448 takes 1464.6 seconds. In the case of F is 1,796 of Wide ResNet-50-2, the computation is infeasible due to out of memory. We measure the time elapse using PyTorch 1.5 for a NVIDIA Quadro RTX 6000 with 24GB memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>Datasets. The MVTec AD dataset <ref type="bibr" target="#b8">[9]</ref>, with the CC BY-NC-SA 4.0 license, consists of five texture and ten object categories with a totally 3,629 images for training and 1,725 images for testing. The emerging dataset for anomaly segmentation offers the real-world categories of textures and objects having multiple types of anomalies. The test images have single or multiple types of defects, or defectfree, while the other splits only have defect-free images. We split for validation to have 10%, while 90% for training. We resize the images to 256x256, evaluate on this scale, and we do not apply any data augmentation strategy being consistent with the previously published works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9]</ref>. The Kolektor surface-defect dataset (KolektorSDD) <ref type="bibr" target="#b9">[10]</ref> consists of the 399 images of electrical commutators, where 52 defected images are annotated for microscopic fractions or cracks on the surface of the plastic embedding in electrical commutators. The dataset is publicly available 2 for research and non-commercial use only. The dataset is split by three folds, where we use only anomaly-free images for unsupervised training. The Kolektor surface-defect dataset 2 (KolektorSDD2) <ref type="bibr" target="#b10">[11]</ref> is similar with the previous one, but having more samples. The train set has 2,085 negative and 246 positive  images while the test set with 894 negative and 110 positive images. For the two Kolektor datasets, we resize the images to 704x256, evaluate on this scale, and do not apply any data augmentation, for the consistent comparison in <ref type="table">Table 3</ref>. The mSTC dataset <ref type="bibr" target="#b17">[18]</ref> is the modified ShanghaiTech Campus (STC) dataset <ref type="bibr" target="#b11">[12]</ref> consisting of 13 scenes with complex light conditions and camera angles having 130 abnormal events 3 . They extract every 5-th frame of the video from each scene for training (274,515 frames) and test (42,883 frames) for unsupervised anomaly segmentation task. We randomly sample 5,000 training samples following the previous work <ref type="bibr" target="#b6">[7]</ref>, and use the same test split. We resize the images to 256x256, evaluate on this scale, and we do not apply any data augmentation strategy being consistent with the previously published works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>Metric. The previous work <ref type="bibr" target="#b8">[9]</ref> proposes a threshold-free metric based on the per-region overlap (PRO). This metric is the area under the receiver operating characteristic curve (ROC) while it takes the average of true positive rates for each connected component 4 in the ground truth. Because the score of a single large region can overwhelm those of small regions, the PRO promotes multiple regions' sensitivity. It calculates up to the false-positive rate of 30% (100% for ROC, of course). The ROC is a natural way to cost-and-benefit analysis of anomaly decision making.</p><p>Multi-scale features. For ResNet-18, we select the layer 1, 2, and 3, having the feature sizes of 64, 128, and 256, respectively, for Wide ResNet-50-2, the feature sizes of the layer 1, 2, and 3 are 256, 512, and 1024, respectively. The feature maps from the corresponding layers are concatenated for the channel dimension after interpolating spatial dimensions to 64 ? 64. The output map of anomaly scores using the approximated Mahalanobis distance is interpolated to 256 ? 256 and applied the Gaussian filter with the kernel size of 4 following the previous works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>Ablation study 1. In the first part of <ref type="table" target="#tab_1">Table 1</ref>, the alternatives using the Mahalanobis distance are compared. First, we confirm that the localized precision matrices, full precision (local), outperforms a global precision matrix, full precision (global) for both texture and object categories with a significant margin, in spite of batch-matrix inversion. Second, the truncated SVD using the k-smallest eigenvalues of C where k ? [1, min(F, N )], eigenvectors (lower), retains the majority of original performance, compared to its counter part using the k-largest eigenvalues, eigenvectors (higher) as predicted in Theorem 1. These two alternatives provide the lower and upper bounds of the semiorthogonal approximation error. Notice that F =1,792 for Wide ResNet-50-2, with the multi-scale consideration, is prohibitively expensive in both computation and memory for batch-matrix inversion.</p><p>Ablation study 2. The second set of experiments is on the choice of layers providing input features. For ResNet-18, the feature sizes of layer 1, 2, and 3 are 64, 128, and 256, respectively, denoted by F a , F b , and F c in <ref type="table" target="#tab_1">Table 1</ref>. Notice that F b and F c are larger than k = 100. The results confirm the multi-scale approach using the multiple layers is crucial in anomaly segmentation. Especially, the layer 2 and 3 underperform our semi-orthogonal method with a higher computational complexity.</p><p>Ablation study 3. The third part of <ref type="table" target="#tab_1">Table 1</ref> compares the alternatives with the embedding size k of 100. The Gaussian random-valued embeddings, Gaussian, significantly deteriorates the performance. The PaDiM <ref type="bibr" target="#b6">[7]</ref> uses the same approximation as in sampled features, however, our careful  implementation gets a stronger baseline with 0.912 compared with their 0.905. One of reasons is the pre-processing where we do not use the center cropping since the center area does not perfectly cover the anomalies in some cases unlike their assumption. Notice that our evaluation protocol is consistent with previous works. Surprisingly, the approximation using our semi-orthogonal matrix outperforms the comparative methods with 0.924 considerably retaining the performance of full precision (local) (0.934) with 1% of computation and 5% of memory complexities of those. For Wide ResNet-50-2, the ratio is more conversing to zero. Also, we verify that the average of the standard deviations per category with five random semi-orthogonal matrices is lower than that of sampled features.</p><p>State-of-the-art of MVTec AD. We achieve a new state-of-the-art for the MVTec AD in the two major metrics, PRO and ROC, in the comparison with competing methods, with significant margins. The results consistently show that Mahalanobis distance-based methods <ref type="bibr" target="#b6">[7]</ref> outperform the reconstruction error-based <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9]</ref> and knowledge distillation-based methods <ref type="bibr" target="#b3">[4]</ref>. Notably, using Wide ResNet-50-2, our method significantly outperforms PaDiM with less computational and memory complexities with <ref type="table">Table 3</ref>: The ROC results for the unsupervised anomaly segmentation task using the KolektorSDD and KolektorSDD2 datasets. We report the score for each fold of the KolektorSDD dataset, their mean and standard deviation (Std.), and the mean score for the KolektorSDD2 dataset with the standard deviation with three random seeds. Notice that we reproduce the scores of PaDiM <ref type="bibr" target="#b6">[7]</ref> with the same setting with ours except the method of approximation. We use ResNet-18 with k=100 for the setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Fold 1 Fold 2 Fold 3 Mean ? Std. KolektorSDD2</p><p>Uninformed student <ref type="bibr" target="#b3">[4]</ref> .904 .883 .902 .896 ? .012 .950 ? .005 PaDiM <ref type="bibr" target="#b6">[7]</ref> .  k=300. It suggests that our method can readily exploit more powerful backbone networks while the computational cost is efficiently controlled by k, without any fine-tuning of backbone networks. The scores of the total 15 categories can be referred in <ref type="table" target="#tab_5">Table 5</ref>, Appendix.</p><p>State-of-the-art of KolektorSDD. In <ref type="table">Table 3</ref>, we compare our method with the other comparative methods using the KolektorSDD and KolektorSDD2 datatsets. Although the third fold of the KolektorSDD dataset tends to have slightly higher score than others which impacts on the standard deviation, our method consistently outperforms the others across all folds. Using the more samples in the KolektorSDD2, the performance of the uninformed student <ref type="bibr" target="#b3">[4]</ref> is notably improved; however, the localized Mahalanobis-based methods outperform it, and our method shows the consistence. For this comparison, we reproduce the uninformed student <ref type="bibr" target="#b3">[4]</ref> and PaDiM <ref type="bibr" target="#b6">[7]</ref>. Notice that we use the same setting of ResNet-18 with k=100 for the PaDiM except the approximation method. For the uninformed student <ref type="bibr" target="#b3">[4]</ref>, we use their model with the receptive size of 33 ? 33 for the best result. We follow the other training settings.</p><p>State-of-the-art of mSTC. In <ref type="table" target="#tab_4">Table 4</ref>, our method consistently outperforms the comparative methods in the unsupervised abnormal event segmentation task, while achieving a new state-of-the-art. Notice that this dataset has different domain from the other datasets since the manufacturing product images tend to have a similar shape and are center-aligned.</p><p>Visualization. <ref type="figure" target="#fig_0">Figure 1</ref> visualizes an example from the Grid category. In this case, our method shows a better prediction than the previous state-of-the-art <ref type="bibr" target="#b6">[7]</ref>, detecting three small regions of anomalies. Please remind that the metric of PRO emphasizes to detect all small regions in the ground-truth. More examples can be found in <ref type="figure">Figure 5</ref>, Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Parsimonious of low-rank approximations. The average PRO scores from the fifteen cateogries of MVTec AD with respect to the embedding size of k using the backbone of ResNet-18 are shown in <ref type="figure">Figure 2</ref>. When the k is smaller than 200 the performance gaps are distinctive among comparative methods. Among them, the approximation with our semi-orthogonal embedding achieves the best performance with a significant margin when the k is less than 100. The worst method is the Gaussian which fails to retain the optimal performance even with the full embedding size of F . The dashed line denotes the best achievable score with the full precision matrix C ?1 .</p><p>Rank collapse in feature selection. In <ref type="table" target="#tab_1">Table 1</ref>, the sampled features <ref type="bibr" target="#b6">[7]</ref> is a special case of ours where the semi-orthogonal matrix W is randomly selected k columns of the F ? F identity matrix. This strategy has a drawback when input features have some degree of redundancy, which is prone to have the rank lower than k for the chance of redundant selection. For which, <ref type="figure">Figure 3</ref> shows the expectation of eigenvalues with respect to the location of a feature map for Grid and Metal nut. Normal Anomaly</p><p>The root of anomaly score In defense of using the localized Mahalanobis distances. Compared to the other approaches, the Mahalanobis distance considers the second moment (covariance) of feature statistics to measure the degree of anomaly. The assumption of Gaussian distribution is backed by the localized statistics in the feature map extracted by pre-trained CNNs. The computational cost of inverse for the multidimensional precision tensor is cubically reduced by the proposed semi-orthogonal embedding successfully retaining the empirical performances as in <ref type="table" target="#tab_1">Table 1</ref> and 2. The decoupled with the backbone networks is the important advantage for small datasets that does not require the fine-tuning of feature extractor. Moreover, it can be shown that the knowledge distillation loss is related to the Mahalanobis distance if linear models are used (Appendix B). This speculation weights on the future work to advance efficient Mahalanobis distance-based methods for anomaly segmentation tasks.</p><p>Limitation. The localized Mahalanobis distance for anomaly segmentation requires the batch-inverse of multi-dimensional tensor. Although it significantly outperforms the global Mahalanobis distance in the ablation study 1 (Section 4) for both texture and object categories, the computational cost may increase depending on the size of a feature map, e.g., 4K high-resolution images.</p><p>Social impact. The automation may induce the reduction of workers, although it can improve the quality assurance of manufacturing and relieve the workers from repetitive labors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related work</head><p>With growing attention to anomaly detection and segmentation tasks, comprehensive reviews on this topic, including recent deep learning approaches, are available <ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref>. Notice that we focus on unsupervised anomaly segmentation, locating any anomaly region in a test image where only anomaly-free images are available while training. Therefore, in this section, we would like to highlight the following related works cohesively.</p><p>Reconstruction errors. Generative models such as autoencoders and GANs are employed for the task <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>. These methods are based on an optimistic view on reconstruction errors that anomalies cause higher reconstruction errors than the others. Notice that a model can generate even anomalies for its robust reconstruction capability. A perceptual loss function <ref type="bibr" target="#b0">[1]</ref> proposed to overcome this limitation; however, it requires domain knowledge to design the function and has shown a little improvement compared with the other recent approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>Embedding feature similarity. One of the breakthroughs comes from the utilization of pre-trained CNNs. An early work <ref type="bibr" target="#b25">[26]</ref> uses the VGG networks <ref type="bibr" target="#b26">[27]</ref> for anomaly image detection tasks. Especially, a work <ref type="bibr" target="#b24">[25]</ref> explicitly shows that learned discriminative embeddings are better than generative models.</p><p>One of successful approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> is to use the knowledge distillation <ref type="bibr" target="#b27">[28]</ref>. This method utilizes the knowledge distillation loss instead of reconstruction errors assuming that the knowledge distillation loss would increase when anomalies appear. However, with a sufficient network capacity, the loss would vanish when the student networks behavior similarly with the teacher networks. They made pre-designed small CNNs for the teacher networks, distilled from the pre-trained ResNet-18 networks.</p><p>Mahalanobis distance. Mahalanobis distance is a metric that measures the distance between two points discounted by a covariance. Notably, PaDiM <ref type="bibr" target="#b6">[7]</ref>, the current state-of-the-art anomaly segmentation method for the MVTec AD dataset <ref type="bibr" target="#b8">[9]</ref>, utilizes 1) the discriminative features from pre-trained CNNs, 2) multi-resolution features, similarly to SPADE <ref type="bibr" target="#b12">[13]</ref>, for robust detection, and 3) uses separate statistics per feature location for the unimodal Gaussian assumption of Mahalanobis distance. Since the separate statistics require the inverse of multi-dimensional covariance tensor, the key problem in this approach was the approximation of the inverse of covariance tensor. They proposed the random selection of features, while we argue the shortcoming of rank collapse and propose a better solution to use a uniformly generated semi-orthogonal matrix.</p><p>Orthogonal embedding. The Johnson-Lindenstrauss lemma <ref type="bibr" target="#b28">[29]</ref> examines the embeddings from high-dimensional into low-dimensional Euclidean space, in a way that the distances among the samples are virtually preserved, e.g., orthogonal projection. A line of works uses random orthogonal matrices to approximate a Gram matrix in the kernel methods <ref type="bibr" target="#b29">[30]</ref> or proposes the orthogonal lowrank embedding loss to reduce intra-class variance and enforce inter-class margin simultaneously for classification tasks <ref type="bibr" target="#b30">[31]</ref>. Related to anomaly detection, the LRaSMD <ref type="bibr" target="#b31">[32]</ref> is proposed to solve a hyperspectral anomaly detection problem using a low-rank and sparse matrix decomposition of data. However, it does not consider a large multi-dimensional covariance matrix, which needs to approximate the Mahalanobis distance for anomaly segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We propose the semi-orthogonal embedding method for the low-rank approximation of the localized Mahalanobis distance reducing the computational cost for the batch-inverse of covariance matrices without batch-SVD computation. We show that the proposed method is the generalization of the random feature selection method in the previous work <ref type="bibr" target="#b6">[7]</ref>, while retaining the better performance by avoiding redundant sampling. We achieve new state-of-the-arts for the benchmark dataset, MVTec AD <ref type="bibr" target="#b8">[9]</ref>, KolektorSDD <ref type="bibr" target="#b9">[10]</ref>, KolektorSDD2 <ref type="bibr" target="#b10">[11]</ref>, and mSTC <ref type="bibr" target="#b11">[12]</ref>, outperforming the competitive methods using reconstruction error-based <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref> or knowledge distillation-based methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> with significant margins. We emphasize that our method 1) implicitly considers multi-scale receptive fields exploiting the feature maps from multiple layers of CNNs, 2) uses the localized Mahalanobis distance for fine-grained anomaly segmentation via an interpretable metric, and 3) is decoupled with the pre-trained CNNs that can exploit the advances of discriminative models without fine-tuning.</p><p>A Theoretical analysis Proposition 2. restated. (Expectation of low-rank Mahalanobis distances). Let C = 1 N XX , X ? R F ?N , W ? R F ?k is a matrix having k-orthonormal columns, where k ? min(F, N ), and x is a column vector of X. Then, we have that:</p><formula xml:id="formula_14">E x [x W(W CW) ?1 W x] = k.<label>(12)</label></formula><p>Proof. First, we simplify the equation using F = X W:</p><formula xml:id="formula_15">E x [x W(W CW) ?1 W x] = 1 N tr X W W ( 1 N XX )W ?1 W X = tr X W(W XX W) ?1 W X = tr(FF ? )<label>(13)</label></formula><p>where F ? is the left Moore-Penrose inverse of F where FF ? = I. Using the singular value decomposition of F = U?V and the invariant of trace,</p><formula xml:id="formula_16">tr(FF ? ) = tr(U?V V? ? U ) = tr(?? ? ) = k.<label>(14)</label></formula><p>where ? has k non-zero elements.</p><p>Notice that W consists of any k-eigenvectors of C, which are orthonormal, holds the same result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 2. (Convex combination of eigenvalues)</head><p>For the semi-orthogonal matrix W ? R F ?k , any eigenvalue of W CW is the convex combination of the eigenvalues of C:</p><formula xml:id="formula_17">? =v W CWv = i a i ? i<label>(15)</label></formula><p>where? is an eigenvalue of W CW,v is the corresponding unit eigenvector to?, {? i } are the eigenvalues of C, i a i = 1, and a i ? 0.</p><p>Proof. The eigenvalue decomposition of C yields U?U as follows:</p><p>v W CWv =v W U?U Wv</p><formula xml:id="formula_18">(16) = i (v W U ?v W U) i ? i<label>(17)</label></formula><p>while the weight for ? i is non-negative by square, and the sum of weight is one as follows:</p><formula xml:id="formula_19">i (v W U ?v W U) i =v W UU Wv = 1<label>(18)</label></formula><p>where UU = I F and W W = I k , which concludes the proof.</p><p>Theorem 2. restated. (Error bounds of low-rank precision matrix). For ? k and U k are the diagonal matrix having the k-smallest eigenvalues of C and the corresponding eigenvectors, respectively, and, ? ?k and U ?k have the k-largest eigenvalues of C and the corresponding eigenvectors, respectively, the error bounds of the semi-orthogonal approximation of a precision matrix are that:</p><formula xml:id="formula_20">C ?1 ? U k ? ?1 k U k 2 ? C ?1 ? W(W CW) ?1 W 2 ? C ?1 ? U ?k ? ?1 ?k U ?k 2<label>(19)</label></formula><p>Proof. The lower bound comes from Theorem 1.</p><p>For the upper bound, we start with the case of the Frobenius norm,</p><formula xml:id="formula_21">C ?1 ? W(W CW) ?1 W 2 = C ?1 2 + W(W CW) ?1 W 2 ? 2tr(C ?1 W(W CW) ?1 W )<label>(20)</label></formula><p>where the last term is the Frobenius inner product. Now, we concern about the function of W as follows:</p><formula xml:id="formula_22">W(W CW) ?1 W 2 ? 2tr(C ?1 W(W CW) ?1 W ) = (W CW) ?1 2 ? 2tr(C ?1 W(W CW) ?1 W ) (cyclic invariance) = (W CW) ?1 2 ? 2tr(W C ?1 W(W CW) ?1 ) (cyclic invariance) = tr (W CW) ?2 ? 2tr W C ?1 W(W CW) ?1 . (by definition)<label>(21)</label></formula><p>By the way, letting (W CW) ?1 =VLV , the last trace can be rewritten as:</p><formula xml:id="formula_23">tr W C ?1 W(W CW) ?1 = tr W C ?1 WVLV = tr V W C ?1 WVL (cyclic invariance) = tr V W (U? ?1 U )WVL (by definition)<label>(22)</label></formula><p>where, using Lemma 2, the eigenvalue of (W CW) ?1 inL is the harmonic weighted sum of the eigenvalues of C ?1 . Moreover, the diagonal elements ofV W (U? ?1 U )WV is the arithmetic weighted sum of the eigenvalues of C ?1 as follows:</p><formula xml:id="formula_24">V W (U? ?1 U )WV ii =v i W (U? ?1 U )Wv i = j (v i W U ?v i W U) i ? ?1 i<label>(23)</label></formula><p>where {v i } are the eigenvectors of (W CW) ?1 and {? i } are the eigenvalues of C. Note that the weights for the harmonic weighted sum and the arithmetic weighted sum are the same. Therefore, we use the weighted arithmetic-harmonic inequality <ref type="bibr" target="#b32">[33]</ref> as follows:</p><formula xml:id="formula_25">tr V W (U? ?1 U )WVL ? tr(LL) = tr(VLV VLV ) = tr (W CW) ?1 (W CW) ?1<label>(24)</label></formula><p>Now, we use tr (W CW) ?2 ? tr W C ?1 W(W CW) ?1 to simplify as follows:</p><formula xml:id="formula_26">tr (W CW) ?2 ? 2tr W C ?1 W(W CW) ?1 ? tr (W CW) ?2 ? 2 tr (W CW) ?2 = ?tr (W CW) ?2 .<label>(25)</label></formula><p>The Cauchy interlacing theorem states that the i-th eigenvalue of W CW is less than or equal to i-th eigenvalue of C and greater than or equal to the F ? k + i-th eigenvalue of C, where the eigenvalues are in descending order. For the upper bound, W is chosen for the corresponding eigenvectors to the k-largest eigenvalues of C. Therefore, the upper bound is as follows:</p><formula xml:id="formula_27">C ?1 2 ? tr (U ?k CU ?k ) ?2 = tr(C ?2 ) ? tr (U ?k CU ?k ) ?2 = tr C ?2 ? U ?k ? ?2 ?k U ?k = tr C ?2 ? 2C ?1 U ?k ? ?1 ?k U ?k + U ?k ? ?2 ?k U ?k = tr (C ?1 ? U ?k ? ?1 ?k U ?k ) 2 = C ?1 ? U ?k ? ?1 ?k U ?k 2 .<label>(26)</label></formula><p>In the case of the spectral norm,</p><formula xml:id="formula_28">C ?1 ? W(W CW) ?1 W 2 = max v =1 v C ?1 v ? v W(W CW) ?1 W v 2 ? max v =1 v C ?1 v 2<label>(27)</label></formula><p>where the inequality comes from the Cauchy interlacing theorem. The difference in the max function is greater than or equal to zero since there exists at least one instance that, if we choose v for the spectral norm of C ?1 , v W(W CW) ?1 W v is less than or equal to the spectral norm of C ?1 . The approximation has non-negative eigenvalues. Therefore, the upper bound is the square of the smallest eigenvalue of C. Remind that the smallest eigenvalue of C is the largest eigenvalue of C ?1 . Now, we conclude the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Knowledge distillation to Mahalanobis distance</head><p>The uninformed students <ref type="bibr" target="#b3">[4]</ref> use the knowledge distillation loss <ref type="bibr" target="#b27">[28]</ref> to detect anomaly regions. We speculate that the knowledge distillation loss, when early stopping is used, connects to the Mahalanobis distance in linear models.</p><p>Let the training data E[x] = 0, the teacher model is w , the student model is w n at the n-iteration of learning. The mean squared error is define as:</p><formula xml:id="formula_29">L (n) = E x?D 1 2 w x ? w n x 2 .<label>(28)</label></formula><p>Then, the gradient with respect to the parameter of the student is as follows:</p><formula xml:id="formula_30">?L (0) ?w 0 = E x(w 0 x ? w x) = E[xx ](w 0 ? w ) = C(w 0 ? w )<label>(29)</label></formula><p>where the SGD with the learning rate ? updates the parameter as follows:</p><formula xml:id="formula_31">w 1 ? w 0 ? ?C(w 0 ? w )<label>(30)</label></formula><p>The analytical solution of the student parameter at the n-step is available via the Neumann series, which is the geometric series for matrices.</p><formula xml:id="formula_32">w n = I ? (I ? ?C) n w + (I ? ?C) n w 0<label>(31)</label></formula><p>Then, the loss is rewritten as:</p><formula xml:id="formula_33">L (n) = E x?D 1 2 x (I ? ?C) n (w ? w 0 ) 2 .<label>(32)</label></formula><p>With the near-zero initialization of the parameter of student and letting w = (XX ) ?1 Xt = N C ?1 Xt, where t is the target used in the pre-training of the teacher model, the loss is defined as:</p><formula xml:id="formula_34">L (n) = E x?D N 2 x (I ? ?C) n C ?1 Xt 2 .<label>(33)</label></formula><p>Here, (I ? ?C) n C ?1 =C ?1 is decomposed by SVD where the i-th eigenvalue is defined as:</p><formula xml:id="formula_35">? i = (1 ? ?? i ) n ? ?1 i .<label>(34)</label></formula><p>With the appropriate n using early stopping, the eigenvalue of the precision matrix C ?1 is filtered out for large eigenvalues while preserving small eigenvalues. Notice that this interpretation coincides with the notion of Theorem 1. The early stopping using a validation split may help to minimize the perturbation from small eigenvalues; however, it does not directly relate to anomaly detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Now, Equation 33</head><p>can be seen as the difference between the squared Mahalanobis distance of x and the squared Mahalanobis distance between a sample x and a fixed point Xt using the filtered precision matrixC ?1 , using the equation as follows:</p><p>x C ?1 Xt = 1 2</p><formula xml:id="formula_36">x C ?1 x ? 1 2 (x ? Xt) C ?1 (x ? Xt) + c<label>(35)</label></formula><p>where the constant c is 1 2 (Xt) C ?1 (Xt). If n ? ?, the loss is quickly conversed to zero with a learning rate of sufficiently small ?.</p><p>C Comparison with the state-of-the-art <ref type="table" target="#tab_5">Table 5</ref> compares with the previous state-of-the-art method <ref type="bibr" target="#b6">[7]</ref> varying the backbone networks and the hyper-parameter k for each category. R18 and WR50 stand for ResNet-18 and Wide ResNet-50-2, respectively. The tailing number after a dash indicates k. Our method outperforms the competitive method in the majority of categories or shows competitive performances for some categories.  <ref type="figure">Figure 4</ref> shows extended examples from <ref type="figure" target="#fig_0">Figure 1</ref>, where the second and third columns are the anomaly prediction from PaDiM <ref type="bibr" target="#b6">[7]</ref> and our method using ResNet-18 and the k of 100, respectively. <ref type="figure">Figure 5</ref> show the visualization of the fourteen categories of the MVTec AD <ref type="bibr" target="#b8">[9]</ref> except Grid, which is previously shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The details can be referred in the caption of <ref type="figure">Figure 5</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Visualization</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The visualization of anomaly prediction using the jet color map after the anomaly score is clamped in [0, 10]. The green lines in the fourth image indicate the ground-truth regions. More examples can be found inFigure 4and 5, Appendix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 Figure 2 :</head><label>12</label><figDesc>The plots of sampled features consistently show rank collapses starting from the around 80 to 90-th 40 The average PRO with respect to the embedding size k.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 Figure 3 :</head><label>13</label><figDesc>The eigenvalues of the comparative embedded covariance matrices W CW of the Grid and Metal nut categories. eigenvalues (for k is 100) compared with those of semi-orthogonal, which is the empirical evidence indicating the vulnerability of feature sampling strategy. The Grid and Metal nut categories have only 81.13% (?0.71) and 79.34% (?1.85) of all eigenvalues (which are not expected values) higher than 1e-4, respectively. Whereas, Gaussian and semi-orthogonal have 100% of eigenvalues (full-rank) higher than 1e-4. Although Gaussian does not induce the rank collapse, the eigenvalues are randomly scaled deviating from the original distribution of eigenvalues, which may interfere the detection of anomalies. Note that the dashed line indicates the plot of the expected k-smallest eigenvalues with respect to the location of a feature map as a reference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1 Figure 4 : 1 Figure 5 :</head><label>1415</label><figDesc>The visualization using the jet color map is shown where the anomaly score is clamped in [0, 10]. The green lines in the fourth image indicate the ground-truth regions. From the top row, Grid, Tile, Cable, and Transistor are shown to compare with the state-of-the-art. Our method shows better detection of small or narrow regions, or reduces false-positive regions for these examples. The visualization of anomaly predictions for the MVTec AD. The first and fourth columns are test samples, the second and fifth columns are the anomaly prediction from our method using ResNet-18 and the k of 100, and the third and sixth are the test samples with the boundaries (green solid lines) of ground-truth regions. The anomaly scores are clamped in [0, 10] and visualized using the jet color map (blue-yellow-red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Ablation study on low-rank methods for the anomaly segmentation task of the MVTec AD using the per-region-overlap (PRO). We use ResNet-18 to extract features and the target rank k is 100 where F a &lt; k &lt; F b &lt; F c &lt; F . The computational complexity is cubically dependent on F , F a?c , or k. Std. indicates the average of the standard deviations per category with five random seeds.</figDesc><table><row><cell>Model</cell><cell cols="5">Complexity Texture Object Overall Std.</cell></row><row><cell>Full-rank (global)</cell><cell>O(F 3 )</cell><cell>.866</cell><cell>.899</cell><cell>.888</cell><cell>-</cell></row><row><cell>Full-rank (local)</cell><cell>O(HW F 3 )</cell><cell>.920</cell><cell>.941</cell><cell>.934</cell><cell>-</cell></row><row><cell>Eigenvectors (higher)</cell><cell>O(HW F 3 )</cell><cell>.886</cell><cell>.896</cell><cell>.893</cell><cell>-</cell></row><row><cell>Eigenvectors (lower)</cell><cell>O(HW F 3 )</cell><cell>.921</cell><cell>.939</cell><cell>.933</cell><cell>-</cell></row><row><cell>Layer 1 Layer 2 Layer 3</cell><cell>O(HW F 3 a ) O(HW F 3 b ) O(HW F 3 c )</cell><cell>.880 .903 .883</cell><cell>.901 .921 .916</cell><cell>.894 .915 .905</cell><cell>---</cell></row><row><cell>Gaussian</cell><cell>O(HW k 3 )</cell><cell>.915</cell><cell>.872</cell><cell>.886</cell><cell>.003</cell></row><row><cell>Sampled features [7]</cell><cell>O(HW k 3 )</cell><cell>.888</cell><cell>.924</cell><cell>.912</cell><cell>.009</cell></row><row><cell cols="2">Semi-orthogonal (ours) O(HW k 3 )</cell><cell>.909</cell><cell>.931</cell><cell>.924</cell><cell>.002</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>PRO</cell><cell>ROC</cell></row></table><note>Comparison with the state-of-the-art for the anomaly segmentation task of the MVTec AD dataset using the two metrics, PRO and ROC. Please see the text for details.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>The ROC results for the unsupervised anomaly segmentation task using the mSTC dataset. Our method use ResNet-18 with k=100 for the fair comparison.</figDesc><table><row><cell cols="5">Model CAVGA-RU [18] SPADE [13] PaDiM [7] Ours</cell></row><row><cell>ROC</cell><cell>.85</cell><cell>.899</cell><cell>.912</cell><cell>.921</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparison with the state-of-the-art for the anomaly segmentation task of the MVTec AD dataset using the PRO. Please see the text for details.</figDesc><table><row><cell>Model</cell><cell cols="2">PaDiM [7]</cell><cell></cell><cell>Ours</cell><cell></cell></row><row><cell>Category</cell><cell cols="5">R18-100 WR50-550 R18-100 WR50-100 WR50-300</cell></row><row><cell>Carpet</cell><cell>.960</cell><cell>.962</cell><cell>.970</cell><cell>.973</cell><cell>.974</cell></row><row><cell>Grid</cell><cell>.909</cell><cell>.946</cell><cell>.898</cell><cell>.908</cell><cell>.941</cell></row><row><cell>Leather</cell><cell>.979</cell><cell>.978</cell><cell>.985</cell><cell>.985</cell><cell>.987</cell></row><row><cell>Tile</cell><cell>.816</cell><cell>.860</cell><cell>.788</cell><cell>.850</cell><cell>.859</cell></row><row><cell>Wood</cell><cell>.903</cell><cell>.911</cell><cell>.896</cell><cell>.908</cell><cell>.906</cell></row><row><cell>Bottle</cell><cell>.939</cell><cell>.948</cell><cell>.956</cell><cell>.961</cell><cell>.962</cell></row><row><cell>Cable</cell><cell>.862</cell><cell>.888</cell><cell>.897</cell><cell>.896</cell><cell>.915</cell></row><row><cell>Capsule</cell><cell>.919</cell><cell>.935</cell><cell>.945</cell><cell>.946</cell><cell>.952</cell></row><row><cell>Hazelnut</cell><cell>.914</cell><cell>.926</cell><cell>.966</cell><cell>.966</cell><cell>.970</cell></row><row><cell>Metal nut</cell><cell>.819</cell><cell>.856</cell><cell>.913</cell><cell>.930</cell><cell>.930</cell></row><row><cell>Pill</cell><cell>.906</cell><cell>.927</cell><cell>.916</cell><cell>.925</cell><cell>.936</cell></row><row><cell>Screw</cell><cell>.913</cell><cell>.944</cell><cell>.940</cell><cell>.928</cell><cell>.953</cell></row><row><cell>Toothbrush</cell><cell>.923</cell><cell>.931</cell><cell>.958</cell><cell>.953</cell><cell>.957</cell></row><row><cell>Transistor</cell><cell>.802</cell><cell>.845</cell><cell>.907</cell><cell>.924</cell><cell>.929</cell></row><row><cell>Zipper</cell><cell>.947</cell><cell>.959</cell><cell>.957</cell><cell>.956</cell><cell>.960</cell></row><row><cell>Mean</cell><cell>.901</cell><cell>.921</cell><cell>.926</cell><cell>.934</cell><cell>.942</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://www.vicos.si/Downloads/KolektorSDD</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://svip-lab.github.io/dataset/campus_dataset.html<ref type="bibr" target="#b3">4</ref> One can exploit the max pooling with a 3 ? 3 kernel for the breadth-first search to batch-compute the markers of connected components.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Improving unsupervised defect segmentation by applying structural similarity to autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sindy</forename><surname>L?we</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications</title>
		<meeting>the 14th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent space autoregression for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Abati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelo</forename><surname>Porrello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="481" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly detection with generative adversarial networks to guide marker discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Seeb?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><forename type="middle">M</forename><surname>Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ursula</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Langs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on information processing in medical imaging</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Uninformed Students: Student-Teacher Anomaly Detection with Discriminative Latent Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multiresolution Knowledge Distillation for Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Salehi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niousha</forename><surname>Sadjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soroosh</forename><surname>Baselizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><forename type="middle">R</forename><surname>Mohammad Hossein Rohban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rabiee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the generalised distance in statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasanta</forename><surname>Chandra Mahalanobis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Institute of Sciences of India</title>
		<meeting>the National Institute of Sciences of India</meeting>
		<imprint>
			<date type="published" when="1936" />
			<biblScope unit="page" from="49" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">PaDiM: a Patch Distribution Modeling Framework for Anomaly Detection and Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Defard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandr</forename><surname>Setkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelique</forename><surname>Loesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romaric</forename><surname>Audigier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 1st International Workshop on Industrial Machine Learning</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="475" to="489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">How to generate random matrices from the classical compact groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Mezzadri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Notices of the AMS</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="592" to="604" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">MVTec AD -A Comprehensive Real-World Dataset for Unsupervised Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9592" to="9600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Segmentation-Based Deep-Learning Approach for Surface-Defect Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Domen</forename><surname>Tabernik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samo</forename><surname>?ela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Skvar?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danijel</forename><surname>Sko?aj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent Manufacturing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mixed supervision for surface-defect detection: from weakly to fully supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Bo?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Domen</forename><surname>Tabernik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danijel</forename><surname>Sko?aj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in Industry</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Future frame prediction for anomaly detection-a new baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongze</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6536" to="6545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niv</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yedid</forename><surname>Hoshen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.02357</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Sub-Image Anomaly Detection with Deep Pyramid Correspondences. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Modeling the Distribution of Normal Data in Pre-Trained Deep Features for Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Mertens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dorit</forename><surname>Merhof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 25th International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The approximation of one matrix by another of lower rank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Eckart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gale</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="218" />
			<date type="published" when="1936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Der massbegriff in der theorie der kontinuierlichen gruppen</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfred</forename><surname>Haar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of mathematics</title>
		<imprint>
			<biblScope unit="page" from="147" to="169" />
			<date type="published" when="1933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">32nd International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attention Guided Anomaly Localization in Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashanka</forename><surname>Venkataramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat Vikram</forename><surname>Kuan Chuan Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mahalanobis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="485" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Explainable Deep One-Class Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Liznerski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Billy</forename><forename type="middle">Joe</forename><surname>Franks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Klaus-Robert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihun</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16067</idno>
		<title level="m">Patch SVDD: Patch-level SVDD for Anomaly Detection and Segmentation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A review of novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A F</forename><surname>Marco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Pimentel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lionel</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tarassenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="215" to="249" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Longbing Cao, and Anton van den Hengel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guansong</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.02500</idno>
	</analytic>
	<monogr>
		<title level="m">Deep Learning for Anomaly Detection: A Review</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pramuditha</forename><surname>Perera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Poojan</forename><surname>Oza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishal</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.03064</idno>
		<title level="m">One-Class Classification: A Survey</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Where&apos;s Wally Now? Deep Generative and Discriminative Embeddings for Novelty Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Burlina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Others</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11507" to="11516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Transfer representationlearning for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerone</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Tanay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewis D</forename><surname>Morton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Griffin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Anomaly Detection Workshop at ICML. JMLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distilling the Knowledge in a Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2014 Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Extensions of lipschitz mappings into a hilbert space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joram</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lindenstrauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Contemporary mathematics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The Unreasonable Effectiveness of Structured Random Orthogonal Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Rowland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Weller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">OLE: Orthogonal Low-rank Embedding, A Plug and Play Geometric Loss for Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos?</forename><surname>Lezama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Mus?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8109" to="8118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A low-rank and sparse matrix decomposition-based mahalanobis distance method for hyperspectral anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shugen</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1376" to="1389" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A note on the weighted harmonic-geometric-arithmetic means inequalities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?rard</forename><surname>Maze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urs</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Inequalities and Applications</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
