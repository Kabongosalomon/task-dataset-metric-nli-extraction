<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Differentiable Relaxation of Graph Segmentation and Alignment for AMR Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunchuan</forename><surname>Lyu</surname></persName>
							<email>chunchuan.lv@gmail.comscohen@inf.ed.ac.ukititov@inf.ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">ILCC</orgName>
								<orgName type="department" key="dep2">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">ILCC</orgName>
								<orgName type="department" key="dep2">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">ILCC</orgName>
								<orgName type="department" key="dep2">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">ILLC</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Differentiable Relaxation of Graph Segmentation and Alignment for AMR Parsing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Meaning Representations (AMR) are a broad-coverage semantic formalism which represents sentence meaning as a directed acyclic graph. To train most AMR parsers, one needs to segment the graph into subgraphs and align each such subgraph to a word in a sentence; this is normally done at preprocessing, relying on hand-crafted rules. In contrast, we treat both alignment and segmentation as latent variables in our model and induce them as part of end-to-end training. As marginalizing over the structured latent variables is infeasible, we use the variational autoencoding framework. To ensure end-to-end differentiable optimization, we introduce a differentiable relaxation of the segmentation and alignment problems. We observe that inducing segmentation yields substantial gains over using a 'greedy' segmentation heuristic. The performance of our method also approaches that of a model that relies on the segmentation rules of Lyu and Titov (2018), which were hand-crafted to handle individual AMR constructions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Abstract Meaning Representation (AMR; <ref type="bibr" target="#b5">Banarescu et al. 2013</ref>) is a broad-coverage semantic formalism which represents sentence meaning as rooted labeled directed acyclic graphs. The representations have been exploited in a wide range of tasks, including text summarization <ref type="bibr" target="#b30">(Liu et al., 2015;</ref><ref type="bibr" target="#b15">Dohare and Karnick, 2017;</ref><ref type="bibr" target="#b19">Hardy and Vlachos, 2018)</ref>, machine translation <ref type="bibr" target="#b23">(Jones et al., 2012;</ref><ref type="bibr" target="#b48">Song et al., 2019)</ref>, paraphrase detection <ref type="bibr" target="#b21">(Issa et al., 2018)</ref> and question answering <ref type="bibr" target="#b37">(Mitra and Baral, 2016)</ref>.</p><p>An AMR graph can be regarded as consisting of multiple concept subgraphs, which can be individually aligned to sentence tokens <ref type="bibr" target="#b16">(Flanigan et al., 2014)</ref>. In <ref type="figure" target="#fig_0">Figure 1</ref>, each dashed box represents the boundary of a single semantic subgraph. Red arrows represent the alignment between subgraphs and tokens. For example, '(o / opine-01: ARG1 (t / thing))' refers to a combination of the predicate 'opine-01' and a filler of its semantic role ARG1. Intuitively, this subgraph needs to be aligned to the token 'opinion'. Similarly, '(b / boy)' should be aligned to the token 'boy'. Given such an alignment and segmentation, it is straightforward to construct a simple parser: parsing can be framed as tagging input tokens with subgraphs (including empty subgraphs), followed by predicting relations between the subgraphs. The key obstacle to training such an AMR parser is that the segmentation and alignment between AMR subgraphs and words are latent, i.e. not annotated in the data.</p><p>Most previous work adopts a pipeline approach to handling this obstacle. They rely on a prelearned aligner (e.g., <ref type="bibr" target="#b44">(Pourdamghani et al., 2014)</ref>) to produce the alignment, and apply a rule system to segment the AMR subgraph <ref type="bibr" target="#b16">(Flanigan et al., 2014;</ref><ref type="bibr" target="#b51">Werling et al., 2015;</ref><ref type="bibr" target="#b14">Damonte et al., 2017;</ref><ref type="bibr" target="#b4">Ballesteros and Al-Onaizan, 2017;</ref><ref type="bibr" target="#b41">Peng et al., 2015;</ref><ref type="bibr" target="#b2">Artzi et al., 2015;</ref><ref type="bibr" target="#b18">Groschwitz et al., 2018)</ref>. While  jointly optimize the parser and the alignment model, the rules handling specific constructions still needed to be crafted to segment the graph. The segmentation rules are relatively complex -e.g., the rules of  targeted 40 different AMR subgraph typesand language-dependent. AMR has never been intended to be used as an interlingua <ref type="bibr" target="#b5">(Banarescu et al., 2013;</ref><ref type="bibr" target="#b21">Damonte and Cohen, 2018)</ref> and AMR banks for individual languages substantially diverge from English AMR. For example, Spanish AMR represents pronouns and ellipsis differently from the English one <ref type="bibr" target="#b36">(Migueles-Abraira et al., 2018)</ref>. As new AMR sembanks in languages other than English are being developed <ref type="bibr" target="#b47">Song et al., 2020)</ref>, domain-specific AMR extensions get developed <ref type="bibr">(Bonn et al., 2020;</ref><ref type="bibr">Bonial et al., 2020)</ref>, and extra constructions are getting introduced to AMRs <ref type="bibr" target="#b8">(Bonial et al., 2018)</ref>, eliminating the need for rules while learning graph segmentation from scratch is becoming an important problem to solve.</p><p>We propose to optimize a graph-based parser that treats the alignment and graph segmentation as latent variables. The graph-based parser consists of two parts: concept identification and relation identification. The concept identification model generates the AMR nodes, and the relation identification component decides on the labeled edges. During training, both components rely on latent alignment and segmentation, which are being induced simultaneously. Importantly, at test time, the parser simply tags the input with the subgraphs and predicts the relations, so there is no test-time overhead from using the latent-structure apparatus. An extra benefit of this approach, in contrast to encoder-decoder AMR models <ref type="bibr" target="#b26">(Konstas et al., 2017;</ref><ref type="bibr" target="#b50">van Noord and Bos, 2017;</ref><ref type="bibr" target="#b10">Cai and Lam, 2020</ref>) is its transparency, as one can readily see which input token triggers each subgraph. <ref type="bibr">1</ref> To develop our parser, we frame the alignment and segmentation problems as choosing a generation order of concept nodes, as we explain in Section 2.2. As marginalization over the latent generation orders is infeasible, we adopt the variational auto-encoder (VAE) framework <ref type="bibr" target="#b25">(Kingma and Welling, 2014)</ref>. Intuitively, a trainable neural module (an encoder in the VAE) is used to sample a plausible generation order (i.e., a segmentation plus an alignment), which is then used to train the parser (a decoder in the VAE). As one cannot 'differentiate through' a sample of discrete variables to train the encoder, we introduce a differentiable relaxation which makes our objective end-to-end differentiable.</p><p>We experiment on the AMR 2.0 and 3.0 datasets. We compare to a greedy segmentation heuristic, inspired by , that produces a seg-mentation deterministically and provides a strong baseline to our segmentation induction method. We also use a version of our model with segmentation induction replaced by a hand-crafted rule-based segmentation system from previous work; 2 it can be thought of as an upper bound on how well induction can work. On AMR 2.0 (LDC2016E25), we found that our VAE system obtained a competitive Smatch score of 76.1, reducing the gap between using the segmentation heristic (75.2) and the rules exploiting the prior knowledge about AMR (76.8). On AMR 3.0 (LDC2020T02), the VAE system gets even closer to the rule-based system (75.5 vs 75.7), possibly because the rules were designed for AMR 2.0. Our main contributions are:</p><p>? we frame the alignment and segmentation problems as inducing a generation order, and provide a continuous relaxation to this discrete optimization problem;</p><p>? we empirically show that our method outperforms a strong heuristic baseline and approaches the performance of a complex handcrafted rule system.</p><p>Our method makes very few assumptions about the nature of the graphs, so it may be effective in other tasks that can be framed as graph prediction (e.g., executable semantic parsing, <ref type="bibr" target="#b28">Liang 2016</ref><ref type="bibr">, or scene graph prediction, Xu et al. 2017</ref>).</p><p>2 Casting Alignment and Segmentation as Choosing a Generation Order</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preliminaries</head><p>We start by introducing the basic concepts and notation. We refer to words in a sentence as x = (x 0 , . . . , x n?1 ), where n is the sentence length. The concepts (i.e. labeled nodes) are v = (v 0 , v 1 , . . . , v m ), where m is the number of concepts. In particular, v m = ? denotes a dummy terminal node; its purpose will be clear in Section 2.2 where we will define the generative model. We refer to all nodes, except for the terminal node (?), as concept nodes. A relation between 'predicate concept' i and 'argument concept' j is denoted by E ij . It is set to ? if j is not an argument of i. We will use E to denote all edges (i.e. relations) in the graph. In addition, we refer to the whole AMR graph as G = (v, E).</p><p>Our goal is to associate each input token with a (potentially empty) subset of the concept nodes in the AMR graph, while making sure that we get a partition of the node set. In other words, each node in the original AMR graph belongs to exactly one subset. In that way, we deal with both segmentation and alignment. Each subset uniquely corresponds to a vertex-induced subgraph (i.e., the subset of nodes together with any edges whose both endpoints are in this subset). For this reason, we will refer to the problem as graph decomposition 3 and to each subset as a subgraph. We will explain how we deal with edges of the AMR graph in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Generation Order</head><p>We choose a subset of nodes for each token by assigning an order to which the nodes are selected for such subset. In <ref type="figure">Figure 2</ref>, dashed red arrows point from every node to the subsequent node to be selected. For example, given the word 'opinion', the node 'opine-01' is chosen first, and then it is followed by another node 'thing'. After this node, we have an arrow pointing to the node ?, signifying <ref type="bibr">3</ref> We slightly abuse the terminology as, in graph theory, graph decomposition usually refers to a partition of edgesrather than nodes -of the original graph. that we finished generating nodes aligned to the word 'opinion'. We refer to these red arrows as a generation order.</p><p>A generation order determines a graph decomposition. To recover it from a generation order, we assign connected nodes (excluding the terminal node) to the same subgraph. Then, a subgraph will be aligned to the token that generated those nodes. In our example, 'opine-01' and 'thing' are connected, and, thus, they are both aligned to the word 'opinion'. The alignment is encoded by arrows between tokens and concept nodes, while the segmentation is represented by arrows between concept nodes.</p><p>From a modeling perspective, the nodes will be generated with an autoregressive model, which is easy to use at test time <ref type="figure">(Figure 3</ref>). From each token, a chain of nodes is generated until the stop symbol ? is predicted. It is more challenging to see how to induce the order and train the autoregressive model at the same time; we will discuss this in Sections 3 and 4. Constraints While in <ref type="figure">Figure 2</ref> the red arrows determine a valid generation order, in general, the arrows have to obey certain constraints. Formally, we denote alignment by A ? {0, 1} n?(m+1) , where A ki = 1 means that for token k we start by generating node i. As the token can only point to one node, we have a constraint i A ki = 1. Similarly, for a segmentation S ? {0, 1} m?(m+1) we have a constraint j S ij = 1. Setting S ij = 1 indicates that node i is followed by node j. In <ref type="figure">Figure 2</ref>, we have A 03 = A 10 = A 23 = A 33 = A 42 = 1 and S 01 = S 13 = S 23 = 1; the rest is 0. Now, we have the full generation order as their concatenation O = [A; S] ? {0, 1} (n+m)?(m+1) . As one node can only be generated once (except for ?), we have a joint constraint: ?j = m, l O lj = 1. Furthermore, the graph defined by O should be acyclic, as it represents the generative process. We denote the set of all valid generation orders as O. In the following sections, we will discuss how this generation order is used in the model and how to infer it as a latent variable while enforcing the above constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Model</head><p>Formally, we aim at estimating P ? (v, E|x), the likelihood of an AMR graph given the sentence. Our graph-based parser is composed of two parts: concept identification P ? (v|x, O) and relation identification P ? <ref type="bibr">(E|x, O, v)</ref>. The concept iden-  </p><formula xml:id="formula_0">log P ? (v, E|x) (1) = log O P ? (O)P ? (v|x, O)P ? (E|x, O, v) ,<label>(2)</label></formula><p>where P ? (O) is a prior on the generation orders, discussed in Section 4.2. To efficiently optimize this objective end-to-end, as will be discussed in Section 4, we need to ensure that both concept and relation identification models admit relaxation, i.e., they should be well-defined for real-valued O.</p><p>In the following subsections, we go through concept identification, relation identification, and their corresponding relaxations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Concept Identification</head><p>As shown in <ref type="figure" target="#fig_13">Figure 4</ref>, our neural model first encodes the sentence with BiLSTM, producing token representations h token k (k ? [0, . . . n?1]), then generates nodes autoregressively at each token with another LSTM.</p><p>In training, we need to be able to run the models with any potential generation order and compute P ? (v|x, O). If we take the order defined in <ref type="figure">Figure</ref> 2, the node 1 ('thing') is predicted relying on the corresponding hidden representation; we refer to this representation as h node 1 where is 1 is the node index. With the discrete generation order defined by red arrows in <ref type="figure">Figure 2</ref>, h node 1 is just the LSTM state of its parent (i.e. 'opine-01'). However, to admit relaxations, our computation should be well-defined when the generation order O is soft (i.e. attention-like). In that case, h node 1 will be a weighted sum of LSTM representations of other nodes and input tokens, where the weights are defined by O. Similarly, the termination symbol ? for the token 'opinion' is predicted from its hidden representation; we refer to this representation as h tail 1 , where 1 is the position of 'opine' in the sentence. With the hard generation order of <ref type="figure">Figure 2</ref>, h tail 1 is just the LSTM state computed after choosing the preceding node (i.e. 'thing'). In the relaxed case, it will again be a weighted sum with the weights defined by O.</p><p>Formally, the probability of concept identification step can be decomposed into probability of generating m concepts nodes and n terminal nodes (one for each token):</p><formula xml:id="formula_1">P ? (v|x,O)= m?1 i=0 P ? (v i |h node i ) n?1 k=0 P ? (?|h tail k ) (3)</formula><p>Representation h node i is computed as the weighted sum of the LSTM states of preceding nodes as defined by O (recall that O = [A; S]):</p><formula xml:id="formula_2">h node i := m?1 j=0 S ji LSTM(h node j , v j ) + n?1 k=0 A ki h token k .</formula><p>(4)</p><p>Note that the preceding node can be either a concept node (then the output of the LSTM, consuming the preceding node, is used) or a word (then we use its contextualized encoding). The first term in Equation 4 corresponds to the former situation, and the second one to the latter. Note that this expression is 'recursive' -each node's representation h node i is computed based on representations of all the nodes h node j ; i, j ? 1, . . . m ? 1. Iterating the assignment defined by Equation 4 for a valid discrete generation order (i.e., a DAG, like the one given in <ref type="figure">Figure 2</ref>), will converge to a stationary point. Crucially, in this discrete case, the stationary point will be equal to the result of applying the autoregressive model (as used in test time, see <ref type="figure" target="#fig_13">Figure 4</ref>). The stationary point will be reached after T steps, where T is the number of nodes in the largest subgraph. 4 This 'message passing' process is fully differentiable and, importantly, well-defined for a relaxed generation order where A ki and S ji are non-binary. The equivalence between the train-time message passing and the test-time autoregressive computation with discrete O prevents the gap between training and testing, as long as the optimization converges to a near-discrete solution.</p><p>The representations h tail k , needed for the terms P ? (?|h tail k ) in Equation 3, are computed as:</p><formula xml:id="formula_3">h tail k = m?1 j=0 B jk LSTM(h node j , v j ) + (1 ? m?1 j=0 B jk )h token k ,<label>(5)</label></formula><p>where B jk = 1 denotes that the concept node j is the last concept node before generating ? for the token k, else B jk = 0. E.g. in <ref type="figure">Figure 2</ref>, we have B 11 = B 42 = 1, and others are 0. Again, in the discrete case, the result will be exactly equivalent to what is obtained by running the corresponding autoregressive model (as in test time, <ref type="figure" target="#fig_13">Figure 4</ref>), but the computation is also well-defined and differentiable in the relaxed cases, where B jk are realvalued. ) and P ? (?|h tail i ) are also defined there.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Relation Identification</head><p>Similarly to , we use an arcfactored model for relation identification (i.e. predicting AMR edges): </p><formula xml:id="formula_4">P ? (E|x, O, v) = m i,j=1 P ? (E ij |h edge i , h edge j ) (6) where P ? (E ij |h edge i , h edge</formula><formula xml:id="formula_5">h edge i = NN edge (h node i ? n?1 k=0 A ? ki h token k ),<label>(7)</label></formula><p>where ? denotes concatenation, h node i is defined in section 3.1, and A ? ki determines whether node i is in a subgraph aligned to token k or not. Note that this is different from A ki which encodes that the node i is the first node in the subgraph (e.g., in <ref type="figure">Figure 2</ref>, A 11 = 0 but A ? 11 = 1). In the continuous case, as used during training, A ? ki can be thought of as the alignment probability that can be computed from O (see Appendix C).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Estimating Latent Generation Order</head><p>We show how to estimate the latent generation order jointly with the parser, as also illustrated in <ref type="figure" target="#fig_4">Figure 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Variational Inference</head><p>In Equation 2, marginalization over O is intractable due to the use of neural parameterization in P ? (v|x, O) and P ? <ref type="bibr">(E|x, O, v)</ref>. Instead, we resort to the variational auto-encoder (VAE) framework <ref type="bibr" target="#b25">(Kingma and Welling, 2014)</ref>. VAEs optimize a lower bound on the marginal likelihood:</p><formula xml:id="formula_6">log O P ? (O)P ? (v|x, O)P ? (E|x, O, v) ? E O?Q ? (O|G,x) log P ? (v|x, O)P ? (E|x, O, v) ? KL(Q ? (O|G, x)||P ? (O)) ,<label>(8)</label></formula><p>where KL is the KL divergence, and Q ? (O|G, x) (the encoder, aka the inference network) is a distribution parameterized with a neural network. The lower bound is maximized with respect to both the original parameters ? and the variational parameters ?. The distribution Q ? (O|G, x) can be thought of as an approximation to the intractable posterior distribution P ? (O|G, x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Stochastic Softmax</head><p>In order to estimate the gradient with respect to the encoder parameters ?, we use the perturb-and-MAP framework <ref type="bibr" target="#b39">(Papandreou and Yuille, 2011;</ref><ref type="bibr" target="#b20">Hazan and Jaakkola, 2012)</ref>, specifically the stochastic softmax , which is a generalization the Gumbel-softmax trick <ref type="bibr" target="#b22">(Jang et al., 2016;</ref><ref type="bibr" target="#b33">Maddison et al., 2017)</ref> to the structured case.</p><p>With Stochastic Softmax, instead of sampling O directly, we independently compute logits W ? R (n+m)?(m+1) for all the potential edges in the generation order, and perturb them:</p><formula xml:id="formula_7">W = F ? (G, x) (9) W = W + , where ij ? G(0, 1) (10)</formula><p>where F ? is a neural module computing the logits (see Section 4.2.2), G(0, 1) is the standard Gumbel distribution, and ? R (n+m)?(m+1) . Then, those perturbed logits W are fed into a constrained convex optimization problem:</p><formula xml:id="formula_8">O( W, ? ) := arg max O?0 W, O ? ? O, log O s.t.?j &lt; m n+m?1 i=0 O ij = 1; ?i m j=0 O ij = 1 (11)</formula><p>This is a linear programming (LP) relaxation of constraints discussed in Section 2.2, where we permit continuous-valued O. Importantly, this LP relaxation is 'tight', and ensures that O( W, 0) is a valid generation order. <ref type="bibr">5</ref> Now, as we will show in the next section, the solution to this optimization O( W, ? ) can be obtained with a differentiable computation, thus, we write:</p><formula xml:id="formula_9">O ? ( , G, x) = O( W, ? )</formula><p>(12) The entropy regularizor, weighted by ? &gt; 0 ('the temperature'), ensures differentiability with respect to W and, thus, with respect to ?, as needed to train the encoder.</p><p>We still need to handle the KL term in Equation 8. We define the prior probability P ? (O) implicitly by having W = 0 in the stochastic softmax framework. Even then, KL(Q ? (O|G, x)||P ? (O)) cannot be easily computed. Following <ref type="bibr" target="#b35">Mena et al. (2018)</ref>, we upper bound it by replacing it with KL(G(W, 1)||G(0, 1)), which is available in closed form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Bregman's Method</head><p>To optimize objective (11) we iterate over the following steps of optimization: the logits W, without taking constraints into account, and then alternating optimization is used to 'fit' the constraints on columns and rows.</p><formula xml:id="formula_10">O (0) = exp W ? (13) ?j &lt; m, O (t+ 1 2 ) :,j = T (O (t) :,j ) (14) O (t+ 1 2 ) :,m = O (t) :,m (15) ?i, O (t+1) i,: = T (O (t+ 1 2 ) i,: )<label>(16)</label></formula><formula xml:id="formula_11">Proposition 1. lim t?? O (t) = O( W, ? ) where O( W, ? ) is defined in Equation 11.</formula><p>See Appendix I for a proof based on the proof for the Bregman method . In practice, we take T = 50, and have O ? ( , G, x) = O (T ) . Importantly, this algorithm is highly parallelizable and amendable to batch implementation on GPU. We compute the gradients with unrolled optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Neural Parameterization</head><p>We introduce the neural modules used for estimating logits W = F ? (G, x) and also the masking mechanism that both ensures acyclicity and enables the use of the copy mechanism. We have W = W raw + W mask . First, we define the unmasked logits, W raw = A raw ? S raw :</p><formula xml:id="formula_12">h g = RelGCN(G; ?) ? R m?d A raw = BiAffine align (h token , h g ? h end ; ?) S raw = BiAffine segment (h g , h g ? h end ; ?)</formula><p>where RelGCN is a relational graph convolutional network <ref type="bibr" target="#b45">(Schlichtkrull et al., 2018)</ref> that takes an AMR graph G and produces embeddings of its nodes informed by their neighbourhood in G. h end ? R 1?d is the trainable embedding of the terminal node, and h token ? R n?d is the BiLSTM encoding of a sentence from Section 3.1.</p><p>The masking also consists of two parts, the alignment mask and the segmentation mask, W mask = A mask ? S mask . If a node is copy-able from at least one token, the alignment mask prohibits alignments from other tokens by setting the corresponding components A mask ij to ??. Acyclicity is ensured by setting S mask so that generation order with circles will get negative infinity in Equation 11. While there may be more general ways to encode acyclicity <ref type="bibr" target="#b34">(Martins et al., 2009)</ref>, we simply perform a depth-first search (DFS) from the root node 6 and permit an edge from node i and j only if i precedes j (not necessarily immediately) in the traversal. In other words, S mask ij is set to ?? for edges (i, j) violating this constraint. The rest of components in S mask are set to 0. Note that this masking approach does not require changes in the optimization method.</p><p>While we relied on the latent variable machinery to train the parser, we do not use it at test time. In fact, the encoder Q ? (O|G, x) is discarded after training. At test time, the first step is to predict sets of concept nodes for every token using the concept identification model P ? (v|x, O) (as shown in <ref type="figure" target="#fig_13">Figure 4)</ref>. Note that the token-specific autoregressive models can be run in parallel across tokens. The second step is predicting relations between all the nodes, relying on the relation identification model P ? (E|x, O, v).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We experiment on LDC2016E25 (AMR2.0) and LDC2020T02 (AMR3.0). The evaluation is based on Smatch <ref type="bibr" target="#b11">(Cai and Knight, 2013)</ref>, and the evaluation tool of <ref type="bibr" target="#b14">Damonte et al. (2017)</ref>. We compare our generation-order induction framework to pre-set segmentations, i.e., producing the segmentation on a preprocessing step. We vary the segmentation methods while keeping the rest of the model identical to our full model (i.e., the same autoregressive model and the learned alignment). We provide ablation studies for our induction framework. We further provide visualization of the induced generation order, along with extra details, in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rule-based Segmentation</head><p>We introduce a hand-crafted rule-based segmentation method, which relies on rules designed to handle specific AMR constructions. In particular, we use the hand-crafted segmentation system of , or, more specifically, its re-implementation by <ref type="bibr" target="#b54">Zhang et al. (2019a)</ref>. Arguably, this can be thought of as an upper bound for how well an induction method can do. This fixed segmentation can be incorporated into our latent-generation-order framework, so that the alignment between concept nodes and the tokens will still be induced. This is achieved by fixing S, while still inducing A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Greedy Segmentation</head><p>We provide a greedy strategy for segmentation that serves as a deterministic baseline. Many nodes are aligned to tokens with the copy mechanism. We could force the unaligned nodes to join its neighbors. This is very similar to the forced alignment of unaligned nodes used in the transition parser of . Again, the segmentation can be incorporated into our latent-generation-order framework by enforcing S and inducing A. See Appendix E for extra details about the strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>In <ref type="table">Table 1</ref>, we compare our models with recent AMR parsers <ref type="bibr">(Xu et al., 2020a;</ref><ref type="bibr">Lam, 2020, 2019;</ref><ref type="bibr" target="#b54">Zhang et al., 2019a;</ref><ref type="bibr" target="#b29">Lindemann et al., 2020;</ref><ref type="bibr" target="#b27">Lee et al., 2020)</ref>, as well as , which we build on, and (van Noord and Bos, 2017), the earliest model which does not exploit any rules. Overall, our model ('full') performs competitively, but lags behind scores reported by some of the very recent parsers. 7 However, except for a no-rule version of <ref type="bibr" target="#b10">Cai and Lam (2020)</ref>, all these models either use rules <ref type="bibr" target="#b27">(Lee et al., 2020</ref>) (see Section 7) or specialized pretraining <ref type="bibr">(Xu et al., 2020a)</ref>. Both our VAE model and the rule-based segmentation achieve high concept identification scores <ref type="bibr" target="#b14">(Damonte et al., 2017)</ref>. The relation identification component is however weaker than, e.g., <ref type="bibr" target="#b10">(Cai and Lam, 2020)</ref>. This may not be surprising, as we, following , score edges independently, whereas <ref type="bibr" target="#b10">(Cai and Lam, 2020)</ref> perform iterative refinement which is known to boost performance on relations <ref type="bibr">(Lyu et al., 2019)</ref>. Also, we use BiLSTM encoders, which -while cheaper to train and easier to tune -is likely weaker than Transformer encoders used by Astudillo et al.; Lee et al. While these modifications, along with using extra pre-training techniques and data augmentation, may further boost performance of our model, we believe that our model is strong enough for our purposes, i.e. demonstrating that informative segmentation can be induced without relying on any rules.</p><p>Indeed, our approach beats the greedy baseline and approaches the rule-based system. The performance gap between the rule-based system and VAE is smaller on AMR 3.0 (0.2 Smatch), possibly because the rules were developed for AMR 2.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Alignment Analysis</head><p>We analyzed the alignment induced by our full model and the model which uses rule-based segmentation. The alignments were evaluated at the level of individual concepts: if a subgraph was aligned to a token, all its concepts were considered aligned to that token. The evaluation was done on 40 sentences. The alignment error rates were 12%, 15% and 14% for the full model, greedy methods and the rulebased method, respectively. This suggests that our   method is able to induce relatively accurate alignments, and joint induction of alignments with segmentation may be beneficial, or, at the very least, not detrimental to alignment quality. Ablations To reconfirm that it is important to learn the segmentation and alignment, rather than to sample it randomly, we perform further ablations. In our parameterization, discussed in Section 4.2.2, it is possible to set A raw = 0 and/or S raw = 0, which corresponds to sampling from the prior in training (i.e. quasi-uniformly while respecting the constraints defined by masking) rather than learning them. We consider 4 potential options, from sampling everything uniformly to learning everything (as in our method). The results are summa-rized in <ref type="table" target="#tab_5">Table 3</ref>. As expected, the full model performs the best, demonstrating that it is important to learn both alignments and segmentation. Interestingly, both 'segmentation learned' and 'alignment learned' obtain reasonable performance, but the 'nothing learned' model fails badly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>A wide range of approaches for AMR parsing have been explored, including graph-based models <ref type="bibr" target="#b16">(Flanigan et al., 2014;</ref><ref type="bibr" target="#b51">Werling et al., 2015;</ref><ref type="bibr" target="#b54">Zhang et al., 2019a)</ref>, transitionbased models <ref type="bibr" target="#b14">(Damonte et al., 2017;</ref><ref type="bibr"></ref> Ballesteros and Al-Onaizan, 2017), grammar-based models <ref type="bibr" target="#b41">(Peng et al., 2015;</ref><ref type="bibr" target="#b2">Artzi et al., 2015;</ref><ref type="bibr" target="#b18">Groschwitz et al., 2018;</ref><ref type="bibr" target="#b29">Lindemann et al., 2020)</ref> and neural autoregressive models <ref type="bibr" target="#b26">(Konstas et al., 2017;</ref><ref type="bibr" target="#b50">van Noord and Bos, 2017;</ref><ref type="bibr" target="#b55">Zhang et al., 2019b;</ref><ref type="bibr" target="#b10">Cai and Lam, 2020;</ref><ref type="bibr">Xu et al., 2020b)</ref>. The majority of strong parsers rely on explicit graph segmentation in training. Typically, the segmentation is dealt with hand-crafted rules, with rule templates developed by studying training set statistics and ensuring the necessary level of coverage. Alternatively, <ref type="bibr" target="#b2">Artzi et al. (2015)</ref>; <ref type="bibr" target="#b17">Groschwitz et al. (2017</ref><ref type="bibr" target="#b18">Groschwitz et al. ( , 2018</ref>  <ref type="formula" target="#formula_0">2020)</ref> -while not not relying on graph recategorization rules -use a rule system to 'pack' and 'unpack' nodes. In recent work, strong results were obtained without using any explicit segmentation and alignment, relying on sequence-sequence models <ref type="bibr">(Xu et al., 2020b;</ref><ref type="bibr" target="#b10">Cai and Lam, 2020)</ref>, still the rules appear useful even with these strong models <ref type="bibr" target="#b10">(Cai and Lam, 2020)</ref>.</p><p>More generally, outside of AMR parsing, differentiable relaxations of latent structure representations have received attention in NLP <ref type="bibr" target="#b24">(Kim et al., 2017;</ref><ref type="bibr" target="#b31">Liu and Lapata, 2018)</ref>, including previous applications of the perturb-and-MAP framework <ref type="bibr" target="#b13">(Corro and Titov, 2019)</ref>. From a more general goal perspective -inducing a segmentation of a linguistic structure -our work is related to tree-substitution grammar induction <ref type="bibr" target="#b46">(Sima'an et al., 1995;</ref><ref type="bibr" target="#b12">Cohn et al., 2010)</ref>, the DOP paradigm <ref type="bibr" target="#b7">(Bod et al., 2003)</ref> and unsupervised semantic parsing <ref type="bibr" target="#b43">(Poon and Domingos, 2009;</ref><ref type="bibr" target="#b49">Titov and Klementiev, 2011)</ref>, though the methods used in that previous work are very different from ours.</p><p>To eliminate hand-crafted segmentation systems used in previous AMR parsers, we cast the alignment and segmentation as generation-order induction. We propose to treat this generation order as a latent variable in a VAE framework. Our method outperforms a simple segmentation heuristic and approaches the performance of a method using rules designed to handle specific AMR constructions. Importantly, while the latent variable modeling machinery is used in training, the parser is very simple at test time. It tags the input words with AMR concept nodes with autoregressive models and then predicts relations between the nodes independently from each other.</p><p>Vanilla sequence-to-sequence models are known to struggle with out-of-distribution generalization (Lake and <ref type="bibr">Baroni, 2018;</ref><ref type="bibr">Bahdanau et al., 2019)</ref>, and, in the future work, it would be interesting to see if this holds for AMR and if such more constrained and structured methods as ours can better deal with this more challenging but realistic setting. We need to model the identification of the root node of the AMR graph. We specify the root identification as:</p><formula xml:id="formula_13">P ? (i|x, O, v) = exp( h root , h e i ) m?1 j=0 exp( h root , h e j )<label>(17)</label></formula><p>where h root is a trainable vector. Inspired by <ref type="bibr" target="#b54">Zhang et al. (2019a)</ref>, who rely on AMR graphs being closely related to dependency trees, we first decode the AMR graph as a maximum spanning tree with log probability of most likely arc-label as edge weights. The reentrancy edges are added afterwards, if their probability is larger than 0.5. We add at most 5 reentrancy edges, based on the empirical founding of <ref type="bibr" target="#b67">Szubert et al. (2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Concept Identification Detail</head><p>Now, we specify P ? (v i |h node i ) and P ? (v m |h tail i ) with a copy mechanism. Formally, we have a small set of candidate nodes V(x i ) for each token x i , and a shared set of candidate nodes V share , which contain v copy . This, however, depends on the token, yet we are learning a latent alignment. During training, we consider all the union of candidate nodes from all possible tokensV(v i ) = ? j:v i ?V(x j ) V(x j ). We abuse notation slightly, and denote the embedding of node i by v i . At training time, for node v i , we have</p><formula xml:id="formula_14">h c i = NN node (h node i ; ?) (18) P ? (v i |h node i ) = [[v i ? V share ]] exp( v i , h c i ) v?v exp( v, h c i ) + [[v i ? V(v i )]] exp( v copy , h c i ) v?v exp( v, h c i ) ? exp(S(v i , h c i )) v?V(v i ) exp(S(v, h c i )<label>(19)</label></formula><p>where NN is a standard one-layer feedforward neural network, and [[. . .]] denotes the indicator function. S(v, h c i ) assigns a score to candidate nodes given the hidden state. To use pre-trained word embedding <ref type="bibr" target="#b66">(Pennington et al., 2014)</ref>, the representation of v is decomposed into primitive category embedding (C(v) 8 and surface lemma embedding. The score function is then a biaffine scoring based on the embeddings and hidden states(</p><formula xml:id="formula_15">L(v) S(v, h c i ) = Biaffine(C(v) ? L(v), h c i ; ?)</formula><p>. For the terminal nodes, we have:</p><formula xml:id="formula_16">h t i = NN node (h tail i ; ?) (20) P ? (v m |h tail i ) = exp( v m , h t i ) v?v exp( v, h t i )<label>(21)</label></formula><p>At testing time, we perform greedy decoding to generate nodes from each token in parallel until either terminal node or T nodes are generated. (22) where S :,:m takes the submatrix of S, excluding the last column, and Diag(S :,m is the diagonal matrix whose diagonal entries are the last column of S. Intuitively, [S :,:m + Diag(S :,m )] can be thought as a Markov transition matrix that passes down the alignment along the generation order, but keeps the alignment mass if the node will generate ?. We truncate the transition at T = 4, as we do not expect a subgraph containing more than 4 nodes.</p><p>To obtain A ? , we observe A ? should obey the following self-consistency equation:</p><formula xml:id="formula_17">A ? = A ? S :,:m + A<label>(23)</label></formula><p>This means, node j is generated from token k iff node i is is generated from token k and node i generates node j or node j is directly generated from token k. This A ? can be computed by initializing A ? = A, and repeating Equation 23 as assignment for T = 4 times. Intuitively, the A ? alignment is passed down along the generation order, while keeping getting alignment mass from the first node alignment. As a result, all nodes get assigned an alignment. As an alternative motivation, the above algorithmic assignment works as a truncated power series expansion of self-consistency equation solution A ? = [I ? S :,:m ] ?1 A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Ablation on Stochastic Softmax</head><p>Our full model uses the Straight-Through (ST) gradient estimator and the Free Bits trick with ? = 10 (Kingma et al., 2017). <ref type="bibr">9</ref> We perform an analysis of different variations of the stochastic softmax: (1) the soft stochastic softmax is the original one with the entropic regularizer (see Section 4.2);</p><p>(2) the rounded stochastic softmax, which selects the highest scored next node from each tokens and  concept nodes based on the soft stochastic softmax; 10 (3) our full model with the ST estimator. All those models use Free Bits (? = 10), while for 'no free bits' ? = 0. As we can see in <ref type="table" target="#tab_8">Table 4</ref>, there is a substantial gap between using structured ST and the two other versions. This illustrates the need for exposing the parsing model to discrete structures in training. Also, the Free Bits trick appears crucial as it prevents the (partial) posterior collapse in our model. We inspected the logits after training and observed that, without free-bits, the learned W are very small, in the [?0.01, +0.01] range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Greedy Segmentation</head><p>We present a greedy strategy for segmentation that serves as a deterministic baseline. This greedy segmentation can be used in the same way as the rule-based segmentation by setting S mask . Many nodes are aligned to tokens with the copy mechanism. We could force the unaligned nodes to join their neighbors. This is very similar to the forced alignment of unaligned nodes used in the transition parser of . We traversal the AMR graph the same way as we do when we produce the masking (Section 4.2.2). During the traversal, we greedily combine subgraphs until one of the constraints is violated: (1) the combined subgraph will have more than 4 nodes; (2) the combined subgraph will have more than 2 copy-able nodes. We present the algorithm recursively (see Algorithm 1). Variable z i indicates whether node i is copy-able and T = 4 represent the maximum subgraph size; n denotes the current subgraph size; z indicates whether the current subgraph contains a copy-able node; k is the last node in the current subgraph, which is used to generate to future nodes in a subgraph. The condition n + n ? T ? z + z ? 1 determines whether we combine the current sub-10 Such rounding does not provide any guarantee of being a valid generation order, but serves as a baseline. In general, a threshold function (at 0.5) can be applied if the constraints have no structure.</p><p>Input: graph G, node index i Result: segmentation S, n, z, k</p><formula xml:id="formula_18">S = 0, k = i, n = 1, z = z i ; forall j ? Child[i] do if j notvisited then S , n , z , k = Greedy(G, j) ; S = S + S ; if n + n ? T ? z + z ? 1 then S kj = 1, n = n + n , z = z + z k = k ; end end end</formula><p>Algorithm 1: Greedy Segmentation graph rooted at node i and the subgraph rooted at node j. Running the algorithm on an AMR graph and the root index will get us the entire segmentation. This greedy method does not require any expert knowledge about AMR, so this should serve as a baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Visualizing Generation Order</head><p>In <ref type="figure">Figures 6, 7</ref> <ref type="bibr">, 8, 9, 11</ref>  As we can see, the standard stochastic softmax indeed produces soft latent structure that might result in large training/testing gap. Furthermore, the rounding strategy does not satisfy the constraint that every concept node can only be generated from one token or another concept node (i.e. [poster-01] is generated twice, and [thing] is never generated.). Meanwhile, the straight through stochastic softmax produce a valid generation order. In Appendix J, we will show the validity formally. It is worth to note that our learned generation order differs from the rule based one. When producing the rule-based segmentation, '(t2 / thing :ARG0-of (e / express-01 )' took precedence over '(t2 / thing :ARG2-of (p / poster-01)' due to the order over traversal edges. The learned model, however, figured out that the poster is the thing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Hyper-Parameters</head><p>We use RoBERTa-large <ref type="bibr" target="#b61">(Liu et al., 2019)</ref>   <ref type="formula" target="#formula_0">, 2015)</ref> is used with learning rate 3e ? 4 and beta=0.9, 0.99.</p><p>Early stopping is used with maximum 60 epochs of training. Dropout is set at 0.33. Those hyperparameters are selected manually, we basically followed the standard model size as in <ref type="bibr" target="#b54">Zhang et al., 2019a)</ref>. We will release the code based on the AllenNLP framework <ref type="bibr" target="#b58">(Gardner et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Pre-and-Post processing</head><p>We follow  for pre-and-post processing. We use CoreNLP  for tokenization and lemmatization. The copy-able dictionary is built with the rules based on string matching between lemmas and concept node string as in . For post-processing, wiki tags are added after the named entity being produced in the graph via a look-up table built from the training set or provided by CoreNLP. We also collapse nodes that represent the same pronouns as heuristics for co-reference resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Proof of Proposition 1</head><p>We prove Proposition 1 based on the Bregman method . The Bregman's method solves convex optimization with a set of linear equalities, the setting is as follows:</p><formula xml:id="formula_19">min x?? F (x) s.t. Ax = b,<label>(24)</label></formula><p>where F is strongly convex and continuously differentiable. Note that A is not our alignment, but denotes a matrix that represents constraints. Two important ingredients are Bregman's diver-gence D F (x, y) = F (x) ? F (y) ? ?F (y), x ? y , and Bregman's projection: P ?,F (y) = arg min x?? D F (x, y), where ? represents constraint. Now, the Bregman's method works as: Intuitively, Bregman's method iteratively performs</p><formula xml:id="formula_20">pick y 0 ? {y ? ?|?F (y) = uA, u ? R m }; for t ? 1 to ? do y t 0 ? y t?1 ; for i ? 1 to m do y t i ? P A i x=b i ,F (y t i?1 ) ; end y t ? y t m ; end</formula><p>Algorithm 2: Bregman's method for solving convex optimization over linear constraints alternating projections w.r.t. each constraint. After each projection, the score F is lowered by the construction of Bregman's projection. Such alternating projections eventually converge, and with careful initialization solve the optimization problem.</p><p>Theorem 1 . lim t?? y t solves the optimization problem 24.</p><p>Proof of Proposition 1. We show Proposition 1 by showing the Algorithm defined by equations 13, 14, 15 and 16 implements Bregman's method. Then, Proposition 1 follows from Theorem 1. Now, we build Bregman's method for our optimization problem 11. For simplicity, we focus on the linear algebraic structure, but do not strictly follow the standard matrix notation. We have O as variable, and F</p><formula xml:id="formula_21">(O) = ? W, O + ? O, log O ? 1 12 . For initialization, we have ?F (O) = ? W + ? log O. Take u = 0, we have O (0) = exp( W ? ) ?? log O (0) = W ? .</formula><p>This corresponds to the initialization step as in our Equation 13. Then, we iterate through constraints to perform Bregman's projection. First, the column normalization constraints ?j &lt; m, n+m?1 i=0 O ij = 1. Take a j &lt; m, we need to compute</p><formula xml:id="formula_22">P n+m?1 i=0 O ij =1,F (O ( t)). A very important prop- erty is that our F (O) = ij f ij (O ij ), where f ij (O ij ) = ? W ij O ij +? O ij (log O ij ?1). More- over, D F (x, y) = 0 ?? x = y.</formula><p>Therefore, for variables that are not involved in the constraints, they are kept the same. To simplify notation, we extend the domain of F to parts of the variable. e.g., F (O :,j ) = i f ij (O ij ). Now, let us focus on column j, we have:</p><p>arg min</p><formula xml:id="formula_23">x: i x i =1 F (x) ? F (O :,j ) ? ?F (O :,j ), x ? O :,j (25) = arg min x: i x i =1 ? W :,j , x + ? x, log x ? 1 ? ?F (O :,j ), x<label>(26)</label></formula><p>= arg min</p><formula xml:id="formula_24">x: i x i =1 ? W :,j , x + ? x, log x ? 1 ? ? W :,j + ? log O :,j , x<label>(27)</label></formula><p>= arg min</p><formula xml:id="formula_25">x: i x i =1 ? x, log x ? 1 + ? log O :,j , x<label>(28)</label></formula><p>= arg min</p><formula xml:id="formula_26">x: i x i =1 x, log x ? 1 + log O :,j , x<label>(29)</label></formula><p>=Softmax(log O :,j ) (30) since when iterating over these mutually nonoverlapping constraints, the non-focused variables are always kept the same. It is hence equivalent to computing them in parallel, which is expressed in our column normalization step 14. Similarly, we can derive row normalization step 16. Therefore, our algorithm is an implementation of Bregman's method, and Proposition 1 follows from Theorem 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J Generation Order is Discrete by LP</head><p>If O( W, 0) is integral valued, it belongs to O by definition. In most cases, there is no guarantee that the linear programming in the relaxed space yields a solution that is also an integer. However, in our cases, we have the following result:</p><formula xml:id="formula_27">Proposition 2. With probability 1, a unique O( W, 0) ? {0, 1} (n+m)?(m+1) , where O( W, 0) is defined in Equation 11.</formula><p>Intuitively, this is a generalization of a classical result about perfect matching on bipartite graph <ref type="bibr" target="#b57">(Conforti et al., 2014)</ref>. To prove this, we need the following theorems from integer linear programming.</p><p>Theorem 2 <ref type="bibr">(Conforti et al. 2014, page 130,133)</ref>. l Let A be an q ? p integral matrix. For all integral vectors d, l, u and c ? R p , max{ c, x : Ax = d, l ? x ? u} is attained by an integral vector x if and only if A is totally unimodular. 13 Note that this theorem does not claim at all the solution is integer, nor that it is unique. However, one should understand this limitation as some degenerate case of c. However, a total unimodular matrix does characterize the convex hull of its integral points. To prove this, we need an additional lemma. where Conv(S) is the convex hull of S. Now we have the following proposition:</p><formula xml:id="formula_28">Proposition 3. Let A be an q ? p integral matrix. For all integral vectors d, l, u ,and c ? R p such that {x ? {0, 1} p |Ax = d, l ? x ? u} is a finite set, {x ? |Ax = d, l ? x ? u} = Conv({x ? {0, 1} p |Ax = d, l ? x ? u}) if and only if A is totally unimodular.</formula><p>In other words, we know the LP relaxation is the convex hull.</p><p>Proof. By Theorem 2, A is totally unimodular is equivalent to maximum is attained by an integer solution. Clearly, the LP relaxation contains the convex hull. So, we only need to show that the LP relaxation does not contain any more points. Now suppose the LP relaxation contains another point x that's not in the convex hull. Since, we restrict our discussion on finite set of integer, both the {x } and the convex hull is closed set. Then by the separation theorem, we have a vector c s.t.</p><p>c, x &gt; c, x ?x ? Conv({x ? {0, 1} p |Ax = d, l ? x ? u}), which contradicts Lemma 1.</p><p>Theorem 3 <ref type="bibr">(Conforti et al. 2014, page 133,134)</ref>. A 0, ?1 matrix A with at most two nonzero elements in each column is totally unimodular if and only if rows of A can be partitioned into two sets, red and blue, such that the sum of the red rows minus the sum of the blue rows is a vector whose entries are 0, ?1 (admits row-bicoloring).</p><p>Our O should be the column vector x, and constraints should be represented by a matrix A. In particular, we view O as a column vector, but still access the item by O ij . 14 The matrix A ? {0, ?1} (m+(m+n))?((n+m)(m+1)) . A :,ij denotes the constraints involving O ij . The first m rows 14 Alternatively, one could have a vector x and x i(m+1)+j = Oij. However, this will gets clumsy. of A correspond to ?j &lt; m, n+m?1 i=0 O ij = 1, and the remaining m + n rows correspond to ?i, m j=0 O ij = 1. Therefore, we have ?k &lt; m, j &lt; m, i, A k,ij = ? j,k and ?k ? m, j, iA k,ij = ? i,k?m , else A k,ij = 0, where ? j,k = [[j == k]]. We have the linear constraints in standard form as AO = 1. Lemma 2. The A defined above is totally unimodular.</p><p>Proof. First, we show A admits row-bicoloring. We color the first m rows red, and remaining n + m rows blue. The sum of red rows is:</p><formula xml:id="formula_29">R ij = m?1 k=0 A k,ij = m?1 k=0 ? j,k = [[j &lt; m]] and the sum of blues is B ij = 2m+n?1 k=m A k,ij = 2m+n?1 k=m ? i,k?m = 1. Therefore, R ij ? B ij = [[j == m]] ? {0, ?1}</formula><p>, and A admits a rowbicoloring. Since A has only 0, ?1 value, and one variable in O at most participates in two constraints (incoming and outgoing), by Theorem 3, A is totally unimodular. Now, we prove Proposition 2.</p><p>Proof. We have A being totally unimodular. We have c = W , l = 0, u = 1, by Theorem 2, the LP solutions contain an integer vector. Since the Gumbel distribution has a positive and differentiable density, by <ref type="bibr">(Paulus et al., 2020, Proposition 3)</ref>, arg max O?O W, O yields a unique solution with probability 1. Clearly, this solution is the only integer solution in our LP solutions. Now, suppose another non-integer solution exists. We know the linear programming domain is the convex hull by Proposition 3. Clearly, another integer solution exists, which contradicts the uniqueness of the integer solution. Hence, the O( W, 0) yields a unique integer solution with probability 1. and Alignment for AMR Parsing </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Abstract Meaning Representations (AMR) are a broad-coverage semantic formalism which represents sentence meaning as a directed acyclic graph. To train most AMR parsers, one needs to segment the graph into subgraphs and align each such subgraph to a word in a sentence; this is normally done at preprocessing, relying on hand-crafted rules. In contrast, we treat both alignment and segmentation as latent variables in our model and induce them as part of end-to-end training. As marginalizing over the structured latent variables is infeasible, we use the variational autoencoding framework. To ensure end-to-end differentiable optimization, we introduce a differentiable relaxation of the segmentation and alignment problems. We observe that inducing segmentation yields substantial gains over using a 'greedy' segmentation heuristic. The performance of our method also approaches that of a model that relies on Lyu and Titov (2018)'s segmentation rules, which were hand-crafted to handle individual AMR constructions.  <ref type="bibr" target="#b21">(Issa et al., 2018)</ref> and question answering <ref type="bibr" target="#b37">(Mitra and Baral, 2016</ref>). An AMR graph can be regarded as consisting of multiple concept subgraphs, which can be individually aligned to sentence tokens <ref type="bibr" target="#b16">(Flanigan et al., 2014)</ref>. In <ref type="figure" target="#fig_0">Figure 1</ref> and tokens. For example, '(o / opine-01: ARG1 (t / thing))' refer to a combination of the predicate 'opine-01' and a filler of its semantic role ARG1. Intuitively, this subgraph needs to be aligned to the token 'opinion'. Similarly, '(b / boy)' should be aligned to the token 'boy'. Given such an alignment and segmentation, it is straightforward to construct a simple graph-based parser; for example, parsing can be framed as tagging input tokens with subgraphs (including empty subgraphs), followed by predicting relations between the subgraphs. The key obstacle to training an AMR parser is that the segmentation and alignment between AMR subgraphs and words are latent, i.e. not annotated in the data.</p><p>Most previous work adopts a pipeline approach to handling this obstacle. They rely on a prelearned aligner <ref type="bibr" target="#b44">(Pourdamghani et al., 2014)</ref> to produce the alignment, and apply a rule system to segment the AMR subgraph <ref type="bibr" target="#b16">(Flanigan et al., 2014;</ref><ref type="bibr" target="#b51">Werling et al., 2015;</ref><ref type="bibr" target="#b14">Damonte et al., 2017;</ref><ref type="bibr" target="#b4">Ballesteros and Al-Onaizan, 2017;</ref><ref type="bibr" target="#b41">Peng et al., 2015;</ref><ref type="bibr" target="#b2">Artzi et al., 2015;</ref><ref type="bibr" target="#b18">Groschwitz et al., 2018)</ref>. While  jointly optimize the parser and the alignment model, the rules handling specific constructions still needed to be crafted to segment the graph. As more constructions are getting introduced to AMRs <ref type="bibr" target="#b8">(Bonial et al., 2018)</ref> and AMR sembanks in languages other than English are being developed <ref type="bibr" target="#b36">Migueles-Abraira et al., 2018;</ref><ref type="bibr" target="#b47">Song et al., 2020)</ref>, getting rid of the rules and learning graph segmentation from scratch becomes a compelling problem to tackle.</p><p>We propose to optimize a graph-based parser while treating alignment and graph segmentation as latent variables. A graph-based parser consists of two parts: concept identification and relation identification. The concept identification model generates the AMR nodes, and the relation identification component decides on the labeled edges. During training, both components rely on latent alignment and segmentation, which is being induced simultaneously. Importantly, at test time, the parser simply tags the input with the subgraphs and predicts the relations, so there is no test-time overhead from using the latent-structure apparatus. An extra benefit of this approach, in contrast to encoder-decoder AMR models <ref type="bibr" target="#b26">(Konstas et al., 2017;</ref><ref type="bibr" target="#b50">van Noord and Bos, 2017;</ref><ref type="bibr" target="#b10">Cai and Lam, 2020</ref>) is its transparency, as one can readily see which input token triggered each subgraph.</p><p>To achieve our goal, we frame the alignment and segmentation problems as choosing a generation order of concept nodes, as we explain in Section 2.2. As marginalization over the latent generation orders is infeasible, we adopt the variational auto-encoder (VAE) framework <ref type="bibr" target="#b25">(Kingma and Welling, 2014)</ref>. Loosely speaking, a trainable neural module (an encoder in the VAE) is used to sample a plausible generation order (i.e., a segmentation plus an alignment), which is then used to train the parser (a decoder in the VAE). However, one cannot 'differentiate through' a sample of discrete variables to train the encoder. We adopt the stochastic softmax  technique to address this challenge. Furthermore, to efficiently apply the stochastic softmax, we derive an inference algorithm for our problem; it can be regarded as an instance of the Bregman's method . As a result, our model is end-to-end differentiable.</p><p>We experiment on the AMR 2.0 and 3.0 datasets. In particular, we present a greedy segmentation heuristic, inspired by , that produces a segmentation deterministically and provides a strong baseline to our segmentation induction method. We also compare against a handcrafted rule-based segmentation system that is used in recent work. 1 On AMR 2.0 (LDC2016E25), we found that our VAE system obtained a competitive Smatch score of 76.1, while the greedy segmentation and the hand-crafted rule-based segmentation obtain 75.2 and 76.8, respectively. On AMR 3.0 (LDC2020T02), the VAE system, the greedy segmentation, and the rule-based segmentation yield <ref type="bibr">75.5, 74.7, and 75.7, respectively.</ref> In other words, our method approaches the performance of the rulebased technique, even though it does not exploit the prior knowledge about AMR used to construct the rules. Our main contributions are:</p><p>? we frame the alignment and segmentation problems as inducing a generation order, and provide a continuous relaxation to this discrete optimization problem;</p><p>? we adopt the stochastic softmax  to estimate the latent generation order and derive an efficient optimizer, an instance of Bregman's method );</p><p>? we empirically show that our method outperforms a strong heuristic baseline and approaches the performance of a hand-crafted rule system.</p><p>Our method makes very few assumptions about the nature of the graphs, so it may be effective in other tasks that can be framed as graph prediction (e.g., executable semantic parsing <ref type="bibr" target="#b28">(Liang, 2016)</ref> or scene graph prediction <ref type="bibr" target="#b52">(Xu et al., 2017)</ref>) but we leave this for future work.</p><p>2 Casting Alignment and Segmentation as Choosing a Generation Order</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preliminaries</head><p>We introduce the basic concepts and notation here. We refer to words in a sentence as x = (x 0 , . . . , x n?1 ), where n is the sentence length. The concepts (i.e. labeled nodes) are v = (v 0 , v 1 , . . . , v m ), where m is the number of concepts. In particular, v m = ? denotes a dummy terminal node (its purpose will be clear later); we refer to all nodes, except for the terminal node (?), as concept nodes. A relation between 'predicate concept' i and 'argument concept' j is denoted by E ij ? E; it is set to ? if j is not an argument of i. We will use E to denote all edges (i.e. relations) in the rules <ref type="bibr" target="#b54">(Zhang et al., 2019a;</ref><ref type="bibr" target="#b10">Cai and Lam, 2020)</ref>. We adopt the code from <ref type="bibr" target="#b54">Zhang et al. (2019a)</ref>. graph. In addition, we refer to the whole graph as G = (v, E).</p><p>Our goal is to associate each input token with a (potentially empty) subset of the nodes of the graph, while making sure that we get a partition of the node set. In other words, each node in the original graph belongs to exactly one subset. In that way, we deal with both segmentation and alignment. Each subset uniquely corresponds to a vertexinduced subgraph (i.e., the subset of nodes together with any edges whose both endpoints are in this subset). For this reason, we will refer to the problem as graph decomposition 2 and to each subset as a subgraph. We will explain how we deal with edges in subsequent sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Generation Order</head><p>Instead of only selecting a subset of nodes for each token, we also select the order in which nodes in the subset are chosen. In <ref type="figure">Figure 2</ref>, dashed red arrows point from every node to the subsequent node to be selected. For example, given the word 'opinion', the node 'opine-01' is chosen first, and then it is followed by another node 'thing'. After this node, we have an arrow pointing to the node ?, signifying the termination. We refer to these red arrows as a generation order. To recover a graph decomposition from a generation order, we assign connected nodes (excluding the terminal node) to the same subgraph. Then, a subgraph will be aligned to the token that generated those nodes. In our example, 'opine-01' and 'thing' are connected, and, thus, they are both aligned to the word 'opinion'. The alignment is encoded by arrows between tokens and concept nodes, while the segmentation is represented by arrows between concept nodes.</p><p>From a modeling perspective, the nodes will be generated with an autoregressive model. Thus, we do not have to represent and store all potential subgraphs in our model explicitly. It is easily possible to apply such an autoregressive concept identification model at test time <ref type="figure">(Figure 3)</ref>. From each token, a chain of nodes is generated until a ? node is predicted. It is more challenging to see how to induce the order and train the autoregressive model at the same time; we will discuss this in Sections 3 and 4.</p><p>While in <ref type="figure">Figure 2</ref> the red arrows yield a valid generation order, the arrows need to obey certain constraints. Formally, we denote alignment by A ? {0, 1} n?(m+1) , where A ki = 1 means that for token k we start by generating node i. As the token can only point to one node, we have a constraint i A ki = 1. Similarly, we have segmentation S ? {0, 1} m?(m+1) with a constraint j S ij = 1. Here, S ij = 1 encodes that node i is followed by node j. In <ref type="figure">Figure 2</ref>, we have A 03 = A 10 = A 23 = A 33 = A 42 = 1 and S 01 = S 13 = S 23 = 1; the rest is 0. Now, we have the full generation order as their concatenation O = A ? S ? {0, 1} (n+m)?(m+1) . As one node can only be generated once (except for ?), we have a joint constraint: ?j = m, l O lj = 1. Furthermore, the graph defined by O should be acyclic, as it represents the generative process. We denote the set of all valid generation orders as O. In the following sections, we will discuss how this generation order is used in the model and how to infer it as a latent variable while enforcing the above constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Model</head><p>Formally, we aim at estimating P ? (v, E|x), the likelihood of an AMR graph given the sentence. Our graph-based parser is composed of two parts: concept identification P ? (v|x, O) and relation identification P ? <ref type="figure">(E|x, O, v)</ref>. The concept identification model generates concept nodes, and the relation identification model assigns relations be- tween them. Both require the latent generation order at the training time, denoted by O. Overall, we have the following objective:</p><formula xml:id="formula_30">log P ? (v, E|x) (1) = log O P ? (O)P ? (v|x, O)P ? (E|x, O, v) ,<label>(2)</label></formula><p>where P ? (O) is a prior on the generation order (we discuss it in Section 4.2). To efficiently optimize this objective end-to-end, we will use tools from variational inference and also stochastic softmax, which will be explained in Section 4. One important requirement for applying the stochastic softmax is that both concept and relation identification models admit relaxation, i.e., they should be well-defined for real-valued O. In the following subsections, we go through concept identification, relation identification, and their corresponding relaxations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Concept Identification</head><p>As shown in <ref type="figure" target="#fig_13">Figure 4</ref>, our neural model first encodes the sentence with BiLSTM, producing token representations h token k (k ? [0, . . . n ? 1]), 3 then generates nodes autoregressively at each token with another LSTM.</p><p>For training, we need to be able to run the models with any potential generation order and compute P ? (v|x, O). If we take the order defined in <ref type="figure" target="#fig_0">Figure 2, the node 1 ('thing')</ref> is predicted relying on the corresponding hidden representation; we refer to this representation as h node 1 where is 1 is the node index. With the discrete generation order defined by red arrows in <ref type="figure">Figure 2</ref>, h node 1 is just the LSTM state of its parent (i.e. 'opine-01'). Im-portantly, our computation should be well-defined when the generation order O is soft (i.e. attentionlike rather than pointer-like). In that case, h node 1 will be a weighted sum of LSTM representations of other nodes and input tokens, where the weights are defined by O. Similarly, the termination symbol ? for the token 'opinion' is predicted from its hidden representation; we refer to this representation as h tail 1 , where 1 is the position of 'opine' in the sentence. With the hard generation order of <ref type="figure">Figure 2</ref>, h tail 1 is just the LSTM state computed after choosing the preceding node (i.e. 'thing'). In the relaxed case, it will again be a weighted sum with the weights defined by O.</p><p>Formally, the probability of concept identification step can be decomposed into probability of generating m concepts nodes and n terminal nodes (one for each token):</p><formula xml:id="formula_31">P ? (v|x,O)= m?1 i=0 P ? (v i |h node i ) n?1 k=0 P ? (?|h tail k ) (3)</formula><p>Representation h node i is computed as the weighted sum of the LSTM states of preceding nodes as defined by O (recall that O = A ? S):</p><formula xml:id="formula_32">h node i := m?1 j=0 S ji LSTM(h node j , v j ) + n?1 k=0 A ki h token k .</formula><p>(4)</p><p>Note that the preceding node can be either a concept node (then the output of the LSTM, consuming the preceding node, is used) or a word (then we use its contextualized encoding). Note that this expression is 'recursive' -each node's representation h node i is computed based on representations of all the nodes h node j ; i, j ? 1, . . . m ? 1. Iterating the assignment defined by Expression (4) for a valid discrete generation order (i.e., a DAG, like the one given in <ref type="figure">Figure 2)</ref>, will converge to a stationary point. Crucially, the stationary point will be equal to the result of applying the autoregressive model (as in test time, see <ref type="figure" target="#fig_13">Figure 4</ref>). It will be reached after T steps, where T is the number of nodes in the largest subgraph. 4 This 'message passing' process is fully differentiable and, importantly, well-defined for a relaxed (i.e., continuous) generation order. The equivalence between the train-time message passing and the test-time autoregressive computation with discrete O should ensure that the gap between training and testing is minimal if the optimization converges to a near-discrete solution.</p><p>The representations h tail k , needed for the terms P ? (?|h tail k ) in Equation <ref type="formula" target="#formula_45">(3)</ref>, are computed as:</p><formula xml:id="formula_33">h tail k = m?1 j=0 B jk LSTM(h node j , v j ) + (1 ? m?1 j=0 B jk )h token k ,<label>(5)</label></formula><p>where B jk = 1 denotes that the concept node j is the last concept node before generating ? for the token k, else B jk = 0. (e.g. in <ref type="figure">Figure 2</ref>, we have B 11 = B 42 = 1, and others are 0). These B jk can be computed from O = A ? S, as explained in Appendix ??. Again, in the discrete case, the result will be exactly equivalent to what is obtained by running the corresponding autoregressive model (as in test time, <ref type="figure" target="#fig_13">Figure 4</ref>), but the computation is also well-defined and differentiable in the continuous case. Intuitively, if we are to think of relaxed O as transition probabilities in a Markov process, then B jk can thought of as the probability of having node j as the last concept node in the chain, when starting from token k. We specify P ? (v i |h node i ) and P ? (?|h tail i ) in Appendix ??.</p><p>We use the copy mechanism to address the data sparsity <ref type="bibr" target="#b42">(Peng et al., 2017)</ref>, where the copy action will trigger the usage of the nominalization dictionary (e.g., encoding that word 'meeting' can correspond to a node 'meet').</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Relation Identification</head><p>Similarly to , we use an arcfactored model for relation identification: </p><formula xml:id="formula_34">P ? (E|x, O, v) = m i,j=1 P ? (E ij |h edge i , h edge j ) (6) where P ? (E ij |h edge i , h edge</formula><formula xml:id="formula_35">h edge i = NN edge (h node i ? n?1 k=0 A ? ki h token k ),<label>(7)</label></formula><p>where ? denotes concatenation, h node i is defined in section 3.1, and A ? ki indicates whether i is in a subgraph aligned to token k or not. Not that this is different from A ki which encodes that the node i is the first node in the subgraph (e.g., in <ref type="figure">Figure 2</ref>, A 11 = 0 but A ? 11 = 1). In the continuous case, as used during training, A ? ki can be thought of as the alignment probability that can be computed from O (see Appendix ?? for details), and it is differentiable with respect to O.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Estimating Latent Generation Order</head><p>We show how to estimate the latent generation order jointly with the parser.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Variational Inference</head><p>In Equation <ref type="formula" target="#formula_0">(2)</ref>, marginalization over O is intractable due to the use of neural parameterization in P ? (v|x, O) and P ? <ref type="figure">(E|x, O, v)</ref>. Instead, we resort to the variational auto-encoder (VAE) framework <ref type="bibr" target="#b25">(Kingma and Welling, 2014)</ref>. VAEs optimize a lower bound on the marginal likelihood objective:</p><formula xml:id="formula_36">log O P ? (O)P ? (v|x, O)P ? (E|x, O, v) ? E O?Q ? (O|G,x) log P ? (v|x, O)P ? (E|x, O, v) ? KL(Q ? (O|G, x)||P ? (O)) ,<label>(8)</label></formula><p>where KL is the KL divergence, and Q ? (O|G, x) (the encoder, aka the inference network) is a distribution parameterized with a neural network. The lower bound is maximized with respect to both the original parameters ? and the variational parameters ?. The distribution Q ? (O|G, x) can be thought of as an approximation to the intractable posterior distribution P ? (O|G, x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Stochastic Softmax</head><p>In order to estimate the gradient with respect to the encoder parameters ?, we use the stochastic softmax . The stochastic softmax is a generalization the Gumbel-softmax trick <ref type="bibr" target="#b22">(Jang et al., 2016;</ref><ref type="bibr" target="#b33">Maddison et al., 2017)</ref> to the structured case, and belongs to the class of perturband-MAP methods <ref type="bibr" target="#b39">(Papandreou and Yuille, 2011;</ref><ref type="bibr" target="#b20">Hazan and Jaakkola, 2012)</ref>. It lets us use the reparameterization trick (Kingma and Welling, 2014) with structured discrete latent variables. Instead of sampling O directly, we sample a random variable from a fixed distribution G. Then, we apply a deterministic parameterized function O ? to get O = O ? ( , G, x). More concretely, we independently compute logits W ? R (n+m)?(m+1) for the all the potential edges in the generation order, and perturb them:</p><formula xml:id="formula_37">W = F ? (G, x)<label>(9)</label></formula><p>W = W + , where ij ? G(0, 1) (10) where F ? is a neural module that we will define in Section 4.2.2, G(0, 1) is the standard Gumbel dis-tribution <ref type="bibr" target="#b22">(Jang et al., 2016;</ref><ref type="bibr" target="#b33">Maddison et al., 2017)</ref>, and ? R (n+m)?(m+1) . Then, those perturbed logits W are fed into a constrained convex optimization problem:</p><formula xml:id="formula_38">O( W, ? ) := arg max O?0 W, O ? ? O, log O s.t.?j &lt; m n+m?1 i=0 O ij = 1; ?i m j=0 O ij = 1 (11)</formula><p>this is the linear programming (LP) relaxation of constraints discussed in Section 2.2, where we allowed continuous-valued O. Importantly, this LP relaxation is 'tight', and ensures that O( W, 0) is a valid generation order. 5 This allows straightthrough (ST) estimator <ref type="bibr" target="#b6">(Bengio et al., 2013)</ref> to forward pass a valid generation order. We will use ST estimator in our model. <ref type="bibr">6</ref> There is an extra acyclicity constraint, which is enforced by masking on W, see Section 4.2.2. Now, as we will show in the next section, the solution to this optimization O( W, ? ) can be obtained with a differentiable computation, thus, we write:</p><formula xml:id="formula_39">O ? ( , G, x) = O( W, ? )</formula><p>(12) The entropy regularizor, weighted by ? &gt; 0 ('the temperature'), ensures differentiability with respect to W and, thus, with respect to ?.</p><p>We still need to handle the KL term in Equation (8). We define the prior probability P ? (O) implicitly by having W = 0 in the stochastic softmax framework. Even then, KL(Q ? (O|G, x)||P ? (O)) cannot be easily computed. Following <ref type="bibr" target="#b35">Mena et al. (2018)</ref>, we upper bound it by replacing it with KL(G(W, 1)||G(0, 1)), which is available in closed form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Bregman's Method</head><p>To optimize objective (11) we iterate over the following steps of optimization:</p><formula xml:id="formula_40">O (0) = exp W ? (13) ?j &lt; m, O (t+ 1 2 ) :,j = T (O (t) :,j )<label>(14)</label></formula><formula xml:id="formula_41">O (t+ 1 2 ) :,m = O (t) :,m (15) ?i, O (t+1) i,: = T (O (t+ 1 2 ) i,: )</formula><p>(16) where {i, :} index ith row, {:, j} index jth column and T = x i x i normalize the vectors. Intuitively, the alignment scores are initially computed from the logits W, without taking constraints into account, and then alternating optimization is used to 'fit' subsets of the constraints. 7 Proposition 1.</p><formula xml:id="formula_42">lim t?? O (t) = O( W, ? ) where O( W, ? ) is defined in Equation (11).</formula><p>See Appendix ?? for a proof based on the Bregman method . In practice, we take T = 50, and have O ? ( , G, x) = O (T ) . Importantly, this algorithm is highly parallelizable and amendable to batch implementation on GPU. We compute the gradient with unrolled optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Neural Parameterization</head><p>We introduce the neural modules used for estimating logits W = F ? (G, x) and also the masking mechanism that both ensures acyclicity and enables the use of the copy mechanism. We have W = W raw + W mask . First, we define the unmasked logits, W raw = A raw ? S raw :</p><formula xml:id="formula_43">h g = RelGCN(G; ?) ? R m?d A raw = BiAffine align (h token , h g ? h end ; ?) S raw = BiAffine segment (h g , h g ? h end ; ?)</formula><p>where RelGCN is a relational graph convolutional network <ref type="bibr" target="#b45">(Schlichtkrull et al., 2018)</ref> that takes an AMR graph G and produces embeddings of its nodes informed by their neighbourhood in G. h end ? R 1?d is the trainable embedding of the terminal node, and h token ? R n?d is the BiLSTM encoding of a sentence from Section 3.1. <ref type="bibr">8</ref> The masking also consists of two parts, the alignment mask and the segmentation mask, W mask = A mask ? S mask . If a node is copy-able from at least one token, the alignment mask prohibits alignments from other tokens by setting the corresponding components A mask ij to ??. Acyclicity is ensured by setting S mask so that generation order with circles will get negative infinity in Equation <ref type="formula" target="#formula_44">(11)</ref>. While there may be better and more general ways to encode acyclicity <ref type="bibr" target="#b34">(Martins et al., 2009)</ref>, we simply perform a depth-first search (DFS) from the root node 9 and permit an edge from node i and j only if i precedes j (not necessarily immediately) in the traversal. In other words, S mask ij is set to ?? for edges (i, j) violating 7 Our formulation is very similar to the Gumbel-Sinkhorn <ref type="bibr" target="#b35">(Mena et al., 2018)</ref>, which models bijective alignment. The difference made our formulation useful for modeling non-square injective alignment. this constraint. The rest of components in S mask are set to 0. Note that this masking approach does not require changes in the optimization method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Parsing</head><p>While we relied on the latent variable machinery to train the parser, we do not use it at test time. In fact, the encoder Q ? (O|G, x) is discarded after training. At test time, the first step is to predict sets of concept nodes for every token using the concept identification model P ? (v|x, O) (as shown in <ref type="figure" target="#fig_13">Figure 4</ref>). Note that the token-specific autoregressive models can be run in parallel across tokens. The second step is predicting relations between all the nodes, relying on the relation induction model P ? <ref type="bibr">(E|x, O, v)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We experiment on LDC2016E25 (AMR2.0) and LDC2020T02 (AMR3.0). The evaluation is based on Smatch <ref type="bibr" target="#b11">(Cai and Knight, 2013)</ref>, and the evaluation tool of <ref type="bibr" target="#b14">Damonte et al. (2017)</ref>. We compare our generation-order induction framework to fixed-based segmentations, i.e., producing the segmentation on a preprocessing step. We vary the segmentation methods while keeping the rest of the model identical to our full model (i.e., the same autoregressive model and the learned alignment). We provide ablation studies for our induction framework. We further provide visualization of the induced generation order, along with extra details, in Appendix. Rule-based Segmentation We introduce a handcrafted rule-based segmentation, which relies on rules designed to handle specific AMR constructions. In particular, we consider the hand-crafted segmentation system of , or more specifically, its re-implementation by <ref type="bibr" target="#b54">Zhang et al. (2019a)</ref>. Arguably, this can be thought of as an upper bound for how well an induction method can do. This fixed segmentation can be incorporated into our latent-generation-order framework, so the alignment between concept nodes and the tokens will still be induced. This is achieved by having the fixed S, then set S mask ij = ?(S ij ? 0.5) if j = m. Greedy Segmentation We provide a greedy strategy for segmentation that serves as a deterministic baseline. Many nodes are aligned to tokens with the copy mechanism. We could force the unaligned nodes to join its neighbors. This is very similar to  the forced alignment of unaligned nodes used in the transition parser of . This greedy segmentation can be used in the same way as the rule-based segmentation by setting S mask . The details are in Appendix ??.</p><p>Results We compare our models with other recent AMR parsers in <ref type="table">Table 1</ref>. Overall, our models performs competitively, but lags behind those very recent parsers from <ref type="bibr" target="#b10">Cai and Lam (2020)</ref>; <ref type="bibr" target="#b27">Lee et al. (2020)</ref>. 10 Importantly, both our VAE model and the rule-based segmentation achieve high concept identification scores <ref type="bibr" target="#b14">(Damonte et al., 2017)</ref>. This suggests that the bottleneck of our graph-based parser is on the relation identification stage, which we largely borrowed from  and is rather basic. For example, independent scoring of the edges may be too restrictive.  the segmentation and alignment, rather than to sample it randomly, we perform further ablations. In our parameterization, discussed in Section 4.2.2, it is possible to set A raw = 0 and S raw = 0, which corresponds to sampling from the prior in training (i.e. quasi-uniformly while respecting the constraints defined by masking) rather than learning this importance sampling distribution with the VAE encoder. There are 4 potential options: (1) 'only prior', A raw = 0 and S raw = 0;</p><p>(2) 'alignment prior', A raw = 0 (while S raw is learned) (3) 'segmentation prior', S raw = 0 (while A raw is learned); (4) our full model where both are learned, i.e. they constitute an output of a trained VAE encoder. The results are summarized in <ref type="table" target="#tab_5">Table 3</ref>. As expected, the full model performs the best, demonstrating that it is important to learn both alignments and segmentation. Interestingly, both 'segmentation prior' and 'alignment prior' obtain reasonable performance, but the 'all prior' model fails badly. One possible explanation is that given one of them being learned, the variations remaining in the other can be controlled, so the overall sampling variance will be small. In Appendix, we further examine some technical variations of stochastic softmax.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>As AMR does not specify a procedure for parsing, a wide range of approaches for AMR parsing have been explored, including graph-based models <ref type="bibr" target="#b16">(Flanigan et al., 2014;</ref><ref type="bibr" target="#b51">Werling et al., 2015;</ref><ref type="bibr" target="#b54">Zhang et al., 2019a)</ref>, transition-based models <ref type="bibr" target="#b14">(Damonte et al., 2017;</ref><ref type="bibr" target="#b4">Ballesteros and Al-Onaizan, 2017)</ref>, grammar-based models <ref type="bibr" target="#b41">(Peng et al., 2015;</ref><ref type="bibr" target="#b2">Artzi et al., 2015;</ref><ref type="bibr" target="#b18">Groschwitz et al., 2018;</ref><ref type="bibr" target="#b29">Lindemann et al., 2020)</ref> and neural autoregressive models <ref type="bibr" target="#b26">(Konstas et al., 2017;</ref><ref type="bibr" target="#b50">van Noord and Bos, 2017;</ref><ref type="bibr" target="#b55">Zhang et al., 2019b;</ref><ref type="bibr" target="#b10">Cai and Lam, 2020;</ref><ref type="bibr" target="#b53">Xu et al., 2020)</ref>. In terms of AMR subgraph segmentation, most strong parsers use subgraph segmentation. They typically rely on hand-crafted rules, with rule templates developed by studying training set statistics and ensuring the necessary level of coverage. Alternatively, <ref type="bibr" target="#b2">Artzi et al. (2015)</ref>; <ref type="bibr" target="#b17">Groschwitz et al. (2017</ref><ref type="bibr" target="#b18">Groschwitz et al. ( , 2018</ref>; <ref type="bibr" target="#b29">Lindemann et al. (2020)</ref>; <ref type="bibr" target="#b41">Peng et al. (2015)</ref> rely on existing grammar formalisms to segment the AMR graphs. Importantly, the state-ofthe-art neural autoregressive model still benefits from an explicit subgraph segmentation (Graph Recatergorization) <ref type="bibr" target="#b10">(Cai and Lam, 2020)</ref>. ; <ref type="bibr" target="#b27">Lee et al. (2020)</ref> use the term 'graph recategorization' to refer to a system that involves an external NER tagger, but they also have a system to 'pack' and 'unpack' nodes.</p><p>More generally, outside of AMR parsing, differentiable relaxations of latent structure representations have received attention in NLP <ref type="bibr" target="#b24">(Kim et al., 2017;</ref><ref type="bibr" target="#b31">Liu and Lapata, 2018)</ref>, including previous applications of the perturb-and-MAP framework <ref type="bibr" target="#b13">(Corro and Titov, 2019)</ref>. From the general goal perspective -inducing a segmentation of a linguistic structure -our work is related to treesubstitution grammar induction <ref type="bibr" target="#b46">(Sima'an et al., 1995;</ref><ref type="bibr" target="#b12">Cohn et al., 2010)</ref>, the DOP paradigm <ref type="bibr" target="#b7">(Bod et al., 2003)</ref> and 'unsupervised semantic parsing' <ref type="bibr" target="#b43">(Poon and Domingos, 2009;</ref><ref type="bibr" target="#b49">Titov and Klementiev, 2011)</ref>, though the methods used in that previous work are very different from ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>To get rid of hand-crafted segmentation systems used in previous AMR parsers, we cast the alignment and segmentation as generation-order induction. We propose to treat this generation order as a latent variable in a variational auto-encoder framework. With the stochastic softmax, the model is end-to-end differentiable, and our method outperforms a technique relying on a simple segmentation heuristic and approaches the performance of a method using rules designed to handle specific AMR constructions.</p><p>Importantly, while the latent variable modeling machinery is used in training, the parser is very simple at test time. It tags the input words with AMR concept nodes with autoregressive models and then predicts relations between the nodes independent from each other. As we achieved high performance on concept identification, one way to further improve the graph-based AMR parser is to replace the fully factorized relation identification component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>A Decoding AMR graph is a rooted directed acyclic graph, and traversal on a connected graph from the root gives the directed acyclic graph. Therefore, we need another root identifier that chooses the root and a decoding algorithm to obtain a connected graph. We specify the root identification as:</p><formula xml:id="formula_44">P ? (i|x, O, v) = exp( h root , h e i ) m?1 j=0 exp( h root , h e j )<label>(1)</label></formula><p>where h root is a trainable vector. Inspired by , who utilizes the fact that AMR graph is very closely related to dependency tree, we first decode the AMR graph as a maximum spanning tree with log probability of most likely arc-label as edge weights. The reentrancy edges are added afterwards, if their probability is larger than 0.5. We add at most 5 reentrancy edges, based on the empirical founding of <ref type="bibr" target="#b67">Szubert et al. (2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Concept Identification Detail</head><p>Now, we specify P ? (v i |h node i ) and P ? (v m |h tail i ) with copy mechanism. Formally, we have a small set of candidate nodes V(x i ) for each token x i , and a shared set of candidate nodes V share , which contains v copy . This, however, depends on the token, yet we are learning a latent alignment. During training, we consider all the union of candidate nodes from all possible tokensV(v i ) = ? j:v i ?V(x j ) V(x j ). We abuse the notation slightly, we denote embedding of node i by v i . In all, at training time, for node v i , we have</p><formula xml:id="formula_45">h c i = NN node (h node i ; ?) (2) P ? (v i |h node i ) = [[v i ? V share ]] exp( v i , h c i ) v?v exp( v, h c i ) + [[v i ? V(v i )]] exp( v copy , h c i ) v?v exp( v, h c i ) ? exp(S(v i , h c i )) v?V(v i ) exp(S(v, h c i )<label>(3)</label></formula><p>where NN is a standard one-layer feedforward neural network, and [[. . .]] denotes the indicator function. S(v, h c i ) assigns a score to candidate nodes given the hidden state. To utilize pre-trained word embedding <ref type="bibr" target="#b66">(Pennington et al., 2014)</ref>, the representation of v is decomposed into primitive category embedding (C(v) 1 and surface lemma embedding. The score function is then a biaffine scoring based on embeddings and hidden states(L(v)</p><formula xml:id="formula_46">S(v, h c i ) = Biaffine(C(v) ? L(v), h c i ; ?).</formula><p>For the terminal nodes, we have:</p><formula xml:id="formula_47">h t i = NN node (h tail i ; ?) (4) P ? (v m |h tail i ) = exp( v m , h t i ) v?v exp( v, h t i )<label>(5)</label></formula><p>At the testing time, we perform greedy decoding to generate nodes from each token in parallel until either terminal node or T nodes are generated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Computing B and A ?</head><p>We obtain B by having:</p><formula xml:id="formula_48">B = A[S :,:m + Diag(S :,m )] T ;<label>(6)</label></formula><p>where S :,:m takes the submatrix of S, excluding the last column, and Diag(S :,m is the diagonal matrix whose diagonal entries are the last column of S. Intuitively, [S :,:m + Diag(S :,m )] can be thought as a Markov transition matrix that pass down the alignment along the generation order, but keep the alignment mass if the node will generate ?. We truncate the transition at T = 4, as we do not expect a subgraph containing more than 4 nodes.</p><p>To obtain A ? , we observe A ? should obey the following self-consistency equation:</p><formula xml:id="formula_49">A ? = A ? S :,:m + A<label>(7)</label></formula><p>This means, node j is generated from token k iff node i is is generated from token k and node i </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Ablation on Stochastic Softmax</head><p>Our full model uses the Straight-Through (ST) gradient estimator and the Free Bits trick with ? = 10 (Kingma et al., 2017). <ref type="bibr">2</ref> We perform analysis of different variations of the stochastic softmax:</p><p>(1) the soft stochastic softmax is the original one with the entropic regularizer (see Section ??); (2) the rounded stochastic softmax, which selects the highest scored next node from each tokens and concept nodes based on the soft stochastic softmax; 3 (3) our full model with the ST estimator. All those models use Free Bits (? = 10), while for 'no free bits' ? = 0. As we can see in <ref type="table">Table 1</ref>, there is a substantial gap between using structured ST and the two other versions. This illustrates the need for exposing the parsing model to discrete structures in training. Also, the Free Bits trick appears crucial as it prevents the (partial) posterior collapse in our model. We inspected the logits after training and observed that, without free-bits, the learned W are very small, in the [?0.01, +0.01] range. <ref type="bibr">2</ref> The Free Bits trick is used to prevent 'the posterior collapse <ref type="bibr" target="#b60">' (Kingma et al., 2017)</ref>. In other words, we use max(?, KL(G(W, 1)||G(0, 1))) for the KL divergence regularizer.</p><p>3 Such rounding does not provide any guarantee of being a valid generation order, but serves as a baseline. In general, a threshold function (at 0.5) can be applied if the constraints have no structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Greedy Segmentation</head><p>We present a greedy strategy for segmentation that serves as a deterministic baseline. This greedy segmentation can be used in the same way as the rule-based segmentation by setting S mask . Many nodes are aligned to tokens with the copy mechanism. We could force the unaligned nodes to join its neighbors. This is very similar to the forced alignment of unaligned nodes used in the transition parser of . We traversal the AMR graph the same way as we do when we produce the masking (Section ??). During the traversal, we greedily combine subgraphs until one of the constraints is violated: (1) the combined subgraph will have more than 4 nodes; (2) the combined subgraph will have more than 2 copy-able nodes. We present the algorithm recursively (see Algorithm 1). Variable z i indicates whether node Input: graph G, node index i Result: segmentation S, n, z, k</p><formula xml:id="formula_50">S = 0, k = i, n = 1, z = z i ; forall j ? Child[i] do</formula><p>if j notvisited then S , n , z , k = Greedy(G, j) ; S = S + S ; if n + n ? T ? z + z ? 1 then S kj = 1, n = n + n , z = z + z k = k ; end end end Algorithm 1: Greedy Segmentation i is copy-able and T = 4 represent the maximum subgraph size; n denotes the current subgraph size; z indicates whether the current subgraph contains a copy-able node; k is the last node in the current subgraph, which is used to generate to future nodes in a subgraph. The condition n + n ? T ? z + z ? 1 determines whether we combine the current subgraph rooted at node i and the subgraph rooted at node j. Running the algorithm on an AMR graph and the root index will get us the entire segmentation. This greedy method does not require any expert knowledge about AMR, so this should serve as a baseline. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Visualizing Generation Order</head><p>In <ref type="figure" target="#fig_0">Figures 1, 2, 3</ref>, 4, 4 we present one example of the induced learned for our three variations of stochastic softmax, and one with rule-based segmentation. The nodes are represented in []. Their gold AMR is:</p><p>(m / make ?01 : ARG0 ( t / t h e y ) : ARG1 ( t 2 / t h i n g : ARG2? o f ( p / p o s t e r ? 0 1) : ARG0? o f ( e / e x p r e s s ?01 : ARG1 ( t 3 / t h i n g : ARG1? o f ( o / o p i n e ?01 : ARG0 t ) ) ) ) )</p><p>As we can see, the standard stochastic softmax indeed produce soft latent structure that might result in large training/testing gap. Furthermore, the rounding strategy does not satisfy the constraint that every concept node can only be generated from one token or another concept node (i.e. [poster-01] is generated twice, and [thing] is nevery generated.). Meanwhile, the straight through stochastic softmax produce a valid generation order. In Appendix J, we will show the validity formally. It is worth to note that our learned generation order differ from the rule based one. When producing the rule-based segmentation, '(t2 / thing :ARG0-of (e / express-01 )' took precedence over '(t2 / thing :ARG2-of (p / poster-01)' due to the order over traversal edges. The learned model, however, figured out that the poster is the thing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Hyper-Parameters</head><p>We use RoBERTa-large <ref type="bibr" target="#b61">(Liu et al., 2019)</ref> from <ref type="bibr" target="#b68">Wolf et al. (2019)</ref> as contextualised embedding before LSTMs. BiLSTM for concept identification has 1 layer, and BiLSTM for relation identification has 2 layers. Both have hidden size 1024. Their averaged representation is used for alignment. Rel-GCN used 128 hidden units and 1 hidden layer (plus one input layer and output layer). Relation identification used 128 hidden units. The LSTM for the locally auto-regressive model is one layer with 1024 hidden units. Adam <ref type="bibr" target="#b59">(Kingma and Ba, 2015)</ref> is used with learning rate 3e ? 4 and beta=0.9, 0.99. Early stopping is used with maximum 60 epochs of training. Dropout is set at 0.33. Those hyperparameters are selected manually, we basically followed the standard model size as in . We will release the code based on the Allennlp framework <ref type="bibr" target="#b58">(Gardner et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Pre-and-Post processing</head><p>We follow  for pre-and-post processing. We use CoreNLP  for tokenization and lemmatization. The copy-able dictionary is built with the rules based on string matching between lemmas and concept node string as in . For post-processing, wiki tags are added after the named entity being produced in the graph via a look-up table built from the training set or provided by CoreNLP. We also collapse nodes that represent the same pronouns as heuristics for co-reference resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Proof of Proposition ??</head><p>We prove Proposition ?? based on the Bregman method . The Bregman's method solves convex optimization with a set of linear equalities, the setting is as follows:</p><formula xml:id="formula_51">min x?? F (x) s.t. Ax = b,<label>(8)</label></formula><p>where F is strongly convex and continuously differentiable. Note that A is not our alignment, but denotes a matrix that represents constraints. Two important ingredients are Bregmans' divergence D F (x, y) = F (x) ? F (y) ? ?F (y), x ? y , and Bregman's projection: P ?,F (y) = arg min x?? D F (x, y), where ? represents constraint. Now, the Bregman's method works as: Intuitively, Bregman's method performs alternating pick y 0 ? {y ? ?|?F (y) = uA, u ? R m }; for t ? 1 to ? do y t 0 ? y t?1 ; for i ? 1 to m do y t i ? P A i x=b i ,F (y t i?1 ) ; end y t ? y t m ; end Algorithm 2: Bregman's method for solving convex optimization over linear constraints projections w.r.t. each constraints iteratively. After each projection, the score F is lowered by the construction of Bregman's projection. Such alternating projections eventually converge, and with careful initialization solves the optimization problem.</p><p>Theorem 1 ( ). lim t?? y t solves the optimization problem 8.</p><p>Proof of Proposition ??. We show Proposition ?? by showing the Algorithm defined by equations ??, ??, ?? and ?? implements Bregman's method. Then, Proposition ?? follows from Theorem 1. Now, we build Bregman's method for our optimization problem ??. For simplicity, we focus on the linear algebraic structure, but do not strictly follow the standard matrix notation. We have O as variable, and ?. Then, we iterate through constraints to perform Bregman's projection. First, the column normalization constraints ?j &lt; m, n+m?1 i=0 O ij = 1. Take a j &lt; m, we need to compute P n+m?1 i=0 O ij =1,F (O ( t)). A very important property is that our F</p><formula xml:id="formula_52">(O) = ij f ij (O ij ), where f ij (O ij ) = ? W ij O ij + ? O ij (log O ij ?1)</formula><p>. Moreover, D F (x, y) = 0 ?? x = y. Therefore, for variables that is not involved in the constraints, they are kept the same. To simplify notation, we extend the domain of F to parts of the variable. e.g., F (O :,j ) = i f ij (O ij ). Now, let's focus on column j, we have:</p><p>arg min </p><p>since when iterating over those mutually nonoverlapping constraints, the non-focused variables are always kept the same. It is hence equivalent solution. Clearly, the LP relaxation contains the convex hull. So, we only need to show that the LP relaxation does not contain any more points. Now suppose the LP relaxation contains another point x that's not in the convex hull. Since, we restrict our discussion on finite set of integer, both the {x } and the convex hull is closed set. Then by separation theorem, we have a vector c s.t. c, x &gt; c, x ?x ? Conv({x ? {0, 1} p |Ax = d, l ? x ? u}), which contradicts to lemma 1.</p><p>Theorem 3 ( <ref type="bibr">(Conforti et al., 2014, page 133,134)</ref>). A 0, ?1 matrix A with at most two nonzero elements in each column is totally unimodular if and only if rows of A can be partitioned into two sets, red and blue, such that the sum of the red rows minus the sum of the blue rows is a vector whose entries are 0, ?1 (admits row-bicoloring).</p><p>Our O should be the column vector x, and constraints should be represented by a matrix A. In particular, we view O as a column vector, but still access the item by O ij . 7 The matrix A ? {0, ?1} (m+(m+n))?((n+m)(m+1)) . A :,ij denotes the constraints involving O ij . The first m rows of A correspond to ?j &lt; m, n+m?1 i=0 O ij = 1, and the remaining m + n rows correspond to ?i, m j=0 O ij = 1. Therefore, we have ?k &lt; m, j &lt; m, i, A k,ij = ? j,k and ?k ? m, j, iA k,ij = ? i,k?m , else A k,ij = 0, where ? j,k = [[j == k]]. We have the linear constraints in standard form as AO = 1. Lemma 2. The A defined above is totally unimodular.</p><p>Proof. First, we show A admits row-bicoloring. We color the first m rows red, and remianing n + m rows blue. The sum of red rows is:</p><formula xml:id="formula_54">R ij = m?1 k=0 A k,ij = m?1 k=0 ? j,k = [[j &lt; m]]</formula><p>and the sum of blues is B ij = 2m+n?1 k=m A k,ij = 2m+n?1 k=m ? i,k?m = 1. Therefore, R ij ? B ij = [[j == m]] ? {0, ?1}, and A admits a rowbicoloring. Since A has only 0, ?1 value, and one variable in O at most participates in two constrains (in-coming and out-going), by theorem 3, A is totally unimodular. Now, we prove proposition 1.</p><p>Proof. We have A being totally unimodular. We have c = W , l = 0, u = 1, by theorem 2, the LP solutions contain an integer vector. Since the Gumbel distribution has a positive and differentiable density, by <ref type="bibr">(Paulus et al., 2020, Proposition 3)</ref>, arg max O?O W, O yields a unique solution with probability 1. Clearly, this solution is the only integer solution in our LP solutions. Now, suppose another non-integer solution exist. We know the linear programming domain is the convex hull by proposition 2. Clearly, another integer solution exist, which contradicts the uniqueness of integer solution. Hence, the O( W, 0) yields a unique integer solution with probability 1.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An example of AMR, the dashed red arrows mark latent alignment. Dashed blue boxes represent the latent segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>AMR concept identification model generates nodes following latent generation order at training time. At test time, the AMR concept identification model generates nodes autoregressively, starting from each sentence token. Importantly, it is just an 'unrolled' form of the order shown inFigure 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>j)</head><label></label><figDesc>is the softmax of the biaffine function of node representations h edge i and h edge j . The node representations are defined as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Overview of the computation graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>where {i, :} index ith row, {:, j} index jth column and T = x i x i normalize the vectors. Intuitively, the alignment scores are initially computed from 5 See proof in Appendix J.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>; Lindemann et al. (2020); Peng et al. (2015) using existing grammar formalisms to segment the AMR graphs. Astudillo et al. (2020); Lee et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>C</head><label></label><figDesc>Computing B and A ? We obtain B by having: B = A[S :,:m + Diag(S :,m )] T ;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>we present one example of the induced learned for our three variations of stochastic softmax, and one with rule-based segmentation. The nodes are represented in []. Their gold AMR is: (m / make ?01 : ARG0 ( t / t h e y ) : ARG1 ( t 2 / t h i n g : ARG2? o f ( p / p o s t e r ? 0 1) : ARG0? o f ( e / e x p r e s s ?01 : ARG1 ( t 3 / t h i n g : ARG1? o f ( o / o p i n e ?01 : ARG0 t ) ) ) ) )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Example of Hard (straight-through) Stochastic Softmax Latent Generation Order. Example of Rule-Segmentation Stochastic Softmax Latent Generation Order.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Lemma 1 (</head><label>1</label><figDesc>Conforti et al. 2014, page 21). l Let S ? R n and c ? R n . Then sup{ c, s : s ? S} = sup{ c, s : s ? Conv(S)}. Furthermore, the supremum of c, s is attained over S if and only if it is attained over Conv(S).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 1 :</head><label>1</label><figDesc>, each dashed box represents the boundary of a single semantic subgraph. Red arrows represent the alignment between subgraphs 2 An example of AMR, the dashed red arrows mark latent alignment. Dashed blue boxes represent the latent segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>AMR concept identification model generates nodes following latent generation order at training time. At test time, the AMR concept identification model generates nodes autoregressively, starting from each sentence token.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 4 :</head><label>4</label><figDesc>AMR concept identification model runs several independent LSTMs to generate nodes autoregressively at test time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>j)</head><label></label><figDesc>is the softmax of the biaffine function of node representations h edge i and h edge j . The node representations are defined as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Example of Hard (straight-through) Stochastic Softmax Latent Generation Order. Example of Rule-Segmentation Stochastic Softmax Latent Generation Order.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>F (O) = ? W, O + ? O, log O ? 1 5 . For initialization, we have ?F (O) = ? W + ? log O. Take u = 0, we have O (0) = exp( W ? ) ?? log O (0) = W ? .This corresponds to initialization step as in our Equation ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>x: i x i =1 F</head><label>=1</label><figDesc>(x) ? F (O :,j ) ? ?F (O :,j ), x ? O :,j(9)= arg minx: i x i =1 ? W :,j , x + ? x, log x ? 1 ? ?F (O :,j ), x(10)= arg minx: i x i =1 ? W :,j , x + ? x, log x ? 1 ? ? W :,j + ? log O :,j , x(11)= arg minx: i x i =1 ? x, log x ? 1 + ? log O :,j , x(12)= arg minx: i x i =1 x, log x ? 1 + log O :,j , x(13)=Softmax(log O :,j )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="4">: AMR 3.0 test set, averaged over 2 runs.</cell></row><row><cell></cell><cell cols="3">Concept SRL Smatch</cell></row><row><cell>nothing learned</cell><cell>81.7</cell><cell>62.6</cell><cell>61.9</cell></row><row><cell>segmentation learned</cell><cell>86.0</cell><cell>69.1</cell><cell>70.5</cell></row><row><cell>alignment learned</cell><cell>87.6</cell><cell>71.1</cell><cell>74.4</cell></row><row><cell>full (all learned)</cell><cell>88.3</cell><cell>73.0</cell><cell>76.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Scores with different versions of latent seg-mentation on the AMR 2.0 test set, averaged over 2 runs</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Yoav Artzi, Kenton<ref type="bibr" target="#b2">Lee, and Luke Zettlemoyer. 2015.</ref> Broad-coverage ccg semantic parsing with amr. In EMNLP.Miguel Ballesteros and Yaser Al-Onaizan. 2017. Amr parsing using stack-lstms. ArXiv, abs/1707.07755. Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract Meaning Representation for Sembanking. Rens Bod, Remko Scha, Khalil Sima'an, et al. 2003. Data-oriented parsing. University of Chicago Press. Conforti, Gerard Cornuejols, and Giacomo Zambelli. 2014. Integer Programming. Springer Publishing Company, Incorporated. Caio Corro and Ivan Titov. 2019. Differentiable perturb-and-parse: Semi-supervised parsing with a structured variational autoencoder. ArXiv, abs/1807.09875. Marco Damonte and Shay B. Cohen. 2018. Crosslingual abstract meaning representation parsing. In Proceedings of NAACL. Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. CoRR, abs/1412.6980. Diederik P. Kingma, Tim Salimans, and M. Welling. 2017. Improved variational inference with inverse autoregressive flow. ArXiv, abs/1606.04934. Diederik P Kingma and Max Welling. 2014. Autoencoding variational bayes. International Conference on Learning Representations. Roukos. 2020. Pushing the limits of amr parsing with self-learning. ArXiv, abs/2010.10673. Percy Liang. 2016. Learning executable semantic parsers for natural language understanding. Communications of the ACM, 59(9):68-76. Matthias Lindemann, Jonas Groschwitz, and Alexander Koller. 2020. Fast semantic parsing with welltypedness guarantees. ArXiv, abs/2009.07365. Chris J. Maddison, Andriy Mnih, and Yee Whye Teh. 2017. The concrete distribution: A continuous relaxation of discrete random variables. ArXiv, abs/1611.00712. Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David Mc-Closky. 2014. The Stanford CoreNLP natural language processing toolkit. In Association for Computational Linguistics (ACL) System Demonstrations, pages 55-60. Poon and Pedro Domingos. 2009. Unsupervised semantic parsing. In Proceedings of the 2009 conference on empirical methods in natural language processing, pages 1-10. Nima Pourdamghani, Yang Gao, Ulf Hermjakob, and Kevin Knight. 2014. Aligning English strings with Abstract Meaning Representation graphs. In ArXiv, abs/1910.03771. Danfei Xu, Yuke Zhu, Christopher B Choy, and Li Fei-Fei. 2017. Scene graph generation by iterative message passing. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5410-5419. Dongqin Xu, Junhui Li, Muhua Zhu, Min Zhang, and Guodong Zhou. 2020a. Improving AMR parsing with sequence-to-sequence pre-training.</figDesc><table><row><cell>Ram?n Fern?ndez Astudillo, Miguel Ballesteros, Tahira Naseem, A. Blodgett, and Radu Florian. 2020. Transition-based parsing with stack-transformers. ArXiv, abs/2010.10669. Michele Marco Damonte, Shay B. Cohen, and Giorgio Satta. 2017. An incremental parser for abstract meaning representation. In EACL. Shibhansh Dohare and Harish Karnick. 2017. Text Summarization using Abstract Meaning Representa-tion. arXiv preprint arXiv:1706.01678. Andr? FT Martins, Noah A Smith, and Eric Xing. 2009. Concise integer linear programming formulations for dependency parsing. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natu-ral Language Processing of the AFNLP, pages 342-350. Gonzalo E. Mena, David Belanger, Scott W. Linder-man, and Jasper Snoek. 2018. Learning latent per-Noelia Migueles-Abraira, Rodrigo Agerri, and Arantza Diaz de Ilarraza. 2018. Annotating Abstract Mean-ing Representations for Spanish. In Proceedings of the Eleventh International Conference on Language (ELRA). Arindam Mitra and Chitta Baral. 2016. Addressing a ing. In AAAI. Tahira Naseem, Abhishek Shah, Hui Wan, Radu parsing with reinforcement learning. ArXiv, abs/1905.13370. George Papandreou and Alan L. Yuille. 2011. Perturb-ternational Conference on Computer Vision, pages 193-200. estimation with stochastic softmax tricks. ArXiv, abs/2006.08063. mar based approach for AMR parsing. In Proceed-ings of the Nineteenth Conference on Computational China. Association for Computational Linguistics. Natural Language Learning, pages 32-41, Beijing, 2015. A synchronous hyperedge replacement gram-Xiaochang Peng, Linfeng Song, and Daniel Gildea. Krause, and Chris J. Maddison. 2020. Gradient Max B. Paulus, Dami Choi, Daniel Tarlow, Andreas to learn and sample from energy models. 2011 In-and-map random fields: Using discrete optimization 2019. Rewarding smatch: Transition-based amr Florian, Salim Roukos, and Miguel Ballesteros. cal methods with inductive rule learning and reason-question answering challenge by combining statisti-Japan. European Language Resources Association Resources and Evaluation (LREC 2018), Miyazaki, abs/1802.08665. mutations with gumbel-sinkhorn networks. ArXiv, In Proceed-ings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2501-2511, Online. Association for Computational Linguistics. Dongqin Xu, Junhui Li, Muhua Zhu, Min Zhang, and Guodong Zhou. 2020b. Improving amr pars-ing with sequence-to-sequence pre-training. ArXiv, abs/2010.01771. Sheng Zhang, Xutai Ma, Kevin Duh, and Ben-jamin Van Durme. 2019a. Amr parsing as sequence-to-graph transduction. ArXiv, abs/1905.08704.</cell><cell>Claire Bonial, Bianca Badarau, Kira Griffitt, Ulf Her-mjakob, Kevin Knight, Tim O'Gorman, Martha Palmer, and Nathan Schneider. 2018. Abstract Meaning Representation of constructions: The more we include, the better the representation. In Proceed-ings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources As-sociation (ELRA). Ioannis Konstas, Srini Iyer, Mark Yatskar, Yejin Choi, and Luke Zettlemoyer. 2017. Neural AMR: Sequence-to-sequence models for parsing and gen-eration. In ACL. Brenden Lake and Marco Baroni. 2018. Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. In Pro-ceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Ma-chine Learning Research, pages 2873-2882. PMLR. Youngsuk Lee, Ram?n Fern?ndez Astudillo, Tahira Naseem, Revanth Gangi Reddy, Radu Florian, and S. Fei Liu, Jeffrey Flanigan, Sam Thomson, Norman M. Sadeh, and Noah A. Smith. 2015. Toward Ab-stractive Summarization Using Semantic Represen-tations. In HLT-NAACL. Yang Liu and Mirella Lapata. 2018. Learning struc-tured text representations. Transactions of the Asso-ciation for Computational Linguistics, 6:63-75. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining ap-proach. ArXiv, abs/1907.11692. Chunchuan Lyu, Shay B. Cohen, and Ivan Titov. 2019. Semantic role labeling with iterative structure refine-ment. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-ral Language Processing (EMNLP-IJCNLP), pages 1071-1082, Hong Kong, China. Association for Computational Linguistics. ric Cistac, Tim Rault, R?mi Louf, Morgan Funtow-Chaumond, Clement Delangue, Anthony Moi, Pier-Thomas Wolf, Lysandre Debut, Victor Sanh, Julien ACL. proves abstract meaning representation parsing. In Manning. 2015. Robust subgraph generation im-Keenon Werling, Gabor Angeli, and Christopher D. abs/1705.09980. ments with abstract meaning representations. ArXiv, tic parsing by character-based translation: Experi-Rik van Noord and Johan Bos. 2017. Neural seman-Ivan Titov and Alexandre Klementiev. 2011. Language Technologies, pages 1445-1455. Association for Computational Linguistics: Human In Proceedings of the 49th Annual Meeting of the Bayesian model for unsupervised semantic parsing. A ings of EMNLP. abstract meaning representation parsing. In Find-Mark Steedman. 2020. The role of reentrancies in Ida Szubert, Marco Damonte, Shay B. Cohen, and sociation for Computational Linguistics, 7:19-31. chine translation using amr. Transactions of the As-Wang, and Jinsong Su. 2019. Semantic neural ma-Linfeng Song, Daniel Gildea, Yue Zhang, Zhiguo France. European Language Resources Association. uation Conference, pages 2962-2969, Marseille, ceedings of The 12th Language Resources and Eval-icate lexicon for Chinese AMR corpus. In Pro-Qu. 2020. Construct a sense-frame aligned pred-Li Song, Yuling Dai, Yihuan Liu, Bin Li, and Weiguang Hoifung Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 425-429, Doha, Qatar. Association for Com-putational Linguistics. M. Schlichtkrull, Thomas Kipf, P. Bloem, R. V. Berg, Ivan Titov, and M. Welling. 2018. Modeling rela-tional data with graph convolutional networks. In ESWC. Khalil Sima'an, Rens Bod, Steven Krauwer, and Remko Scha. 1995. Efficient disambiguation by means of stochastic tree substitution grammars. In Recent Advances in NLP, volume 136.</cell></row></table><note>Dzmitry Bahdanau, Shikhar Murty, Michael Noukhovitch, Thien Huu Nguyen, Harm de Vries, and Aaron Courville. 2019. Systematic general- ization: what is required and can it be learned? ICLR.Claire Bonial, Lucia Donatelli, Mitchell Abrams, Stephanie M. Lukin, Stephen Tratz, Matthew Marge, Ron Artstein, David Traum, and Clare Voss. 2020. Dialogue-AMR: Abstract Meaning Representation for dialogue. In Proceedings of the 12th Lan- guage Resources and Evaluation Conference, pages 684-695, Marseille, France. European Language Re- sources Association. Julia Bonn, Martha Palmer, Zheng Cai, and Kristin Wright-Bettner. 2020. Spatial AMR: Expanded spatial annotation in the context of a grounded Minecraft corpus. In Proceedings of the 12th Lan- guage Resources and Evaluation Conference, pages 4883-4892, Marseille, France. European Language Resources Association.L. M. Bregman. 1967. The relaxation method of find- ing the common point of convex sets and its applica- tion to the solution of problems in convex program- ming. Ussr Computational Mathematics and Math- ematical Physics, 7:200-217. Deng Cai and Wai Lam. 2019. Core semantic first: A top-down approach for AMR parsing. In Proceed- ings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Inter- national Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), pages 3799-3809, Hong Kong, China. Association for Computational Lin- guistics. Deng Cai and Wai Lam. 2020. Amr parsing via graph- sequence iterative inference. In ACL. Shu Cai and K. Knight. 2013. Smatch: an evaluation metric for semantic feature structures. In ACL. Trevor Cohn, Phil Blunsom, and Sharon Goldwater. 2010. Inducing tree-substitution grammars. The Journal of Machine Learning Research, 11:3053- 3096.Jeffrey Flanigan, Sam Thomson, Jaime G. Carbonell, Chris Dyer, and Noah A. Smith. 2014. A discrim- inative graph-based parser for the abstract meaning representation. In ACL. Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson F. Liu, Matthew E. Peters, M. Schmitz, and Luke Zettlemoyer. 2018. Allennlp: A deep semantic natural language process- ing platform. ArXiv, abs/1803.07640. Jonas Groschwitz, Meaghan Fowlie, Mark Johnson, and Alexander Koller. 2017. A constrained graph algebra for semantic parsing with AMRs. In IWCS 2017 -12th International Conference on Computa- tional Semantics -Long papers. Jonas Groschwitz, Matthias Lindemann, Meaghan Fowlie, Mark Johnson, and Alexander Koller. 2018. Amr dependency parsing with a typed semantic al- gebra. In ACL. Hardy and Andreas Vlachos. 2018. Guided neural lan- guage generation for abstractive summarization us- ing abstract meaning representation. In EMNLP. Tamir Hazan and Tommi Jaakkola. 2012. On the parti- tion function and random maximum a-posteriori per- turbations. arXiv preprint arXiv:1206.6410. Fuad Issa, Marco Damonte, Shay B. Cohen, Xiaohui Yan, and Yi Chang. 2018. Abstract meaning repre- sentation for paraphrase detection. In NAACL-HLT. Eric Jang, Shixiang Gu, and Ben Poole. 2016. Cat- egorical reparameterization with gumbel-softmax. ArXiv, abs/1611.01144. Bevan K. Jones, Jacob Andreas, Daniel Bauer, Karl Moritz Hermann, and Kevin Knight. 2012. Semantics-based machine translation with hyper- edge replacement grammars. In COLING. Yoon Kim, Carl Denton, Luong Hoang, and Alexan- der M Rush. 2017. Structured attention networks. arXiv preprint arXiv:1702.00887.Chunchuan Lyu and Ivan Titov. 2018. AMR parsing as graph prediction with latent alignment. In Proceed- ings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa- pers), pages 397-407, Melbourne, Australia. Asso- ciation for Computational Linguistics.Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global vectors for word rep- resentation. In Empirical Methods in Natural Lan- guage Processing (EMNLP), pages 1532-1543.icz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2019. Huggingface's transformers: State-of-the-art natural language processing.Sheng Zhang, Xutai Ma, Kevin Duh, and Ben- jamin Van Durme. 2019b. Broad-coverage semantic parsing as transduction. ArXiv, abs/1909.02607.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Scores with different versions of latent segmentation on the AMR 2.0 test set. Scores are averaged over 2 runs</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Chunchuan Lyu 1</head><label>Lyu</label><figDesc>Shay B. Cohen 1 Ivan Titov 1,2 ILCC, School of Informatics, University of Edinburgh 1 ILLC, University of Amsterdam 2 chunchuan.lv@gmail.com scohen@inf.ed.ac.uk ititov@inf.ed.ac.uk</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>2 runs.</cell><cell>: Scores on the AMR 3.0 test set, averaged over</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Scores with different latent segmentation on the AMR 2.0 test set, averaged over 2 runs</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>Table 1: Scores with different latent segmentation on the AMR 2.0 test set. Scores are averaged over 2 runsgenerates node j or node j is directly generated from token k. This A ? can be computed by initializing A ? = A, and repeating Equation 7 as assignment for T = 4 times. Intuitively, the A ?</figDesc><table><row><cell>Metric</cell><cell cols="2">Concept SRL Smatch</cell></row><row><cell>no free bits</cell><cell>83.5</cell><cell>66.3 66.1</cell></row><row><cell>soft</cell><cell>84.9</cell><cell>68.1 70.3</cell></row><row><cell>rounding</cell><cell>87.7</cell><cell>71.8 74.5</cell></row><row><cell cols="2">straight-through 88.3</cell><cell>73.0 76.1</cell></row><row><cell cols="3">alignment is passed down along the generation or-</cell></row><row><cell cols="3">der, while keep getting alignment mass from the</cell></row><row><cell cols="3">first node alignment. As a result, all nodes get</cell></row><row><cell cols="3">assigned alignment. As an alternative motivation,</cell></row><row><cell cols="3">the above algorithmic assignment works as a trun-</cell></row><row><cell cols="3">cated power series expansion of self-consistency</cell></row><row><cell>equation solution A</cell><cell></cell><cell></cell></row></table><note>? = [I ? S :,:m ] ?1 A.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The code is available at https://github.com/ ChunchuanLv/graph-parser.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We adopt the code from<ref type="bibr" target="#b54">Zhang et al. (2019a)</ref>, which is an extension of the system introduced by, and also used by<ref type="bibr" target="#b10">Cai and Lam (2020)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We use T = 4, as we do not expect subgraphs with more than 4 nodes.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">We use lexicographic ordering of edge labels in DFS.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">Results from Lee et al. replace Roberta-large with Roberta-base in Astudillo et al.. With semi-supervised learning, Lee et al. (2020) achieved 81.3 Smatch score.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">AMR nodes have primitive category, including string, number, frame, concept and special nodes (e.g. polarity).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">The Free Bits trick is used to prevent 'the posterior collapse<ref type="bibr" target="#b60">' (Kingma et al., 2017)</ref>. In other words, we use max(?, KL(G(W, 1)||G(0, 1))) for the KL divergence regularizer.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">Incidentally, the greedy segementation produces the same segmentation as rule-based in this example.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">This regularizer differs from the original one in 11 by a constant m + n, due to the constraints. So, the optimization problem is equivalent.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13">A is totally unimodular if every square submatrix has determinant 0, ?1. We combined a few theorems and definitions from<ref type="bibr" target="#b57">Conforti et al. (2014)</ref> into this theorem.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This is the graph-recategorization system first developed by. The rules are crafted for individual AMR constructions. Then, the subsequent work extends the</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We slightly abuse the terminology as, in graph theory, graph decomposition usually refers to a partition of edges of the original graph.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Technically, we have two token encoders, one for concept identification and one for relation identification. For simplicity, we refer to the states of both as h token , but it is clear from the context which one is used.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We use T = 4, as we do not expect subgraphs with more than 4 nodes.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We provide the proof in Appendix ??. 6 Concretely, in ST estimator, we use O( W, 0) in the forward pass, and set ? W O( W, 0) := ? W O( W, ? ).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">Technically, this is the average of the two token encoders mentioned in footnote 3.9  Arbitrarily, we use lexicographic ordering of edge labels in DFS.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">Results from Lee et al. replace Roberta-large with Roberta-base in Astudillo et al.. With semi-supervised learning, Lee et al. (2020) achieved 81.3 Smatch score. 11 There is no published result on AMR 3.0 yet.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">AMR nodes have primitive category, including string, number, frame, concept and special node (e.g. polarity).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Incidentally, the greedy segementation produce the same segmentation as rule-based in this example.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">This regularizor differ from the original one in ?? by a constant m+n, due to constraints. So, the optimization problem is equivalent.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">A is totally unimodular if every square submatrix has determinant 0, ?1. We combined a few theorems and definitions from<ref type="bibr" target="#b57">Conforti et al. (2014)</ref> into this theorem.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">Alternatively, one could have a vector x and x i(m+1)+j = Oij. However, this will gets clumsy.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the reviewers for their useful feedback and comments. The project was supported by the European Research Council (ERC StG BroadSem 678254), the Dutch National Science Foundation (NWO VIDI 639.022.518) and Bloomberg L.P.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>to compute them in parallel, which is expressed in our column normalization step ??. Similarly, we can derive row normalization step ??. Therefore, our algorithm is an implementation of Bregman's method, and Proposition ?? follows from Theorem 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J Generation Order is Discrete by LP</head><p>If O( W, 0) is integral valued, it belongs to O by definition. In most cases, there is no guarantee that the linear programming in the relaxed space yields a solution that is also an integer. However, in our cases, we have the following result:</p><p>Intuitively, this is a generalization of classical result about perfect matching on bipartite graph <ref type="bibr" target="#b57">(Conforti et al., 2014)</ref>. To prove this, we need the following theorems from integer linear programming. Theorem 2 ( <ref type="bibr">(Conforti et al., 2014, page 130,133)</ref>). l Let A be an q ? p integral matrix. For all integral vectors d, l, u and c ? R p , max{ c, x : Ax = d, l ? x ? u} is attained by an integral vector x if and only if A is totally unimodular. 6 Note that this theorem does not say all the solution is integer, nor it's unique. However, one should understand this limitation as some degenerate case of c. However, a total unimodular matrix does characterize the convex hull of its' integral points. To prove this, we need an additional lemma. Lemma 1 ( <ref type="bibr">(Conforti et al., 2014, page 21)</ref>). l Let S ? R n and c ? R n . Then sup{ c, s : s ? S} = sup{ c, s : s ? Conv(S)}. Furthermore, the supremum of c, s is attained over S if and only if it is attained over Conv(S).</p><p>where Conv(S) is the convex hull of S. Now we have the following proposition: Proposition 2. Let A be an q ? p integral matrix. For all integral vectors d, l, u ,and c ? R p such that {x ? {0,</p><p>In other words, we know the LP relaxation is the convex hull.</p><p>Proof. By theorem 2, A is totally unimodular is equivalent to maximum is attained by an integer</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards AMR-BR: A SemBank for Brazilian Portuguese language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Anchi?ta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thiago</forename><surname>Pardo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)<address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards AMR-BR: A SemBank for Brazilian Portuguese language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Anchi?ta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thiago</forename><surname>Pardo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)<address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Broad-coverage ccg semantic parsing with amr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Transition-based parsing with stack-transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ram?n Fern?ndez Astudillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tahira</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Naseem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Blodgett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Florian</surname></persName>
		</author>
		<idno>abs/2010.10669</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Amr parsing using stack-lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Al-Onaizan</surname></persName>
		</author>
		<idno>abs/1707.07755</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Banarescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Bonial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madalina</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<title level="m">Abstract Meaning Representation for Sembanking</title>
		<meeting><address><addrLine>Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Estimating or propagating gradients through stochastic neurons for conditional computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>L?onard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<idno>abs/1308.3432</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Data-oriented parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rens</forename><surname>Bod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remko</forename><surname>Scha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Sima&amp;apos;an</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>University of Chicago Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Abstract Meaning Representation of constructions: The more we include, the better the representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Bonial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><surname>Badarau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Tim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Gorman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)<address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bregman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ussr Computational Mathematics and Mathematical Physics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="200" to="217" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Amr parsing via graphsequence iterative inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Smatch: an evaluation metric for semantic feature structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Inducing tree-substitution grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3053" to="3096" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Differentiable perturb-and-parse: Semi-supervised parsing with a structured variational autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caio</forename><surname>Corro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<idno>abs/1807.09875</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An incremental parser for abstract meaning representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Damonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Satta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibhansh</forename><surname>Dohare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harish</forename><surname>Karnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.01678</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Text Summarization using Abstract Meaning Representation. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A discriminative graph-based parser for the abstract meaning representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A constrained graph algebra for semantic parsing with AMRs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Groschwitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meaghan</forename><surname>Fowlie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IWCS 2017 -12th International Conference on Computational Semantics -Long papers</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Amr dependency parsing with a typed semantic algebra</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Groschwitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Lindemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meaghan</forename><surname>Fowlie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Guided neural language generation for abstractive summarization using abstract meaning representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Hardy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vlachos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">On the partition function and random maximum a-posteriori perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamir</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.6410</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Abstract meaning representation for paraphrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuad</forename><surname>Issa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Damonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<idno>abs/1611.01144</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semantics-based machine translation with hyperedge replacement grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luong</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.00887</idno>
		<title level="m">Structured attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Autoencoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neural AMR: Sequence-to-sequence models for parsing and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srini</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pushing the limits of amr parsing with self-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngsuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tahira</forename><surname>Ram?n Fern?ndez Astudillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Revanth</forename><surname>Naseem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Gangi Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roukos</surname></persName>
		</author>
		<idno>abs/2010.10673</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning executable semantic parsers for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="68" to="76" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fast semantic parsing with welltypedness guarantees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Lindemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Groschwitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Koller</surname></persName>
		</author>
		<idno>abs/2009.07365</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Toward Abstractive Summarization Using Semantic Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><forename type="middle">M</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning structured text representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="63" to="75" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">AMR parsing as graph prediction with latent alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunchuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="397" to="407" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
		<idno>abs/1611.00712</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Concise integer linear programming formulations for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>Andr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="342" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning latent permutations with gumbel-sinkhorn networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gonzalo</forename><forename type="middle">E</forename><surname>Mena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">W</forename><surname>Linderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<idno>abs/1802.08665</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Annotating Abstract Meaning Representations for Spanish</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noelia</forename><surname>Migueles-Abraira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Agerri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantza</forename><surname>Diaz De Ilarraza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)<address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Addressing a question answering challenge by combining statistical methods with inductive rule learning and reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arindam</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitta</forename><surname>Baral</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Rewarding smatch: Transition-based amr parsing with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tahira</forename><surname>Naseem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<idno>abs/1905.13370</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Perturband-map random fields: Using discrete optimization to learn and sample from energy models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="193" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Gradient estimation with stochastic softmax tricks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><forename type="middle">B</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dami</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<idno>abs/2006.08063</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A synchronous hyperedge replacement grammar based approach for AMR parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Nineteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Nineteenth Conference on Computational Natural Language Learning<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="32" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Addressing the data sparsity issue in neural AMR parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="366" to="375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unsupervised semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 conference on empirical methods in natural language processing</title>
		<meeting>the 2009 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Aligning English strings with Abstract Meaning Representation graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Pourdamghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="425" to="429" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESWC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Efficient disambiguation by means of stochastic tree substitution grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Sima&amp;apos;an</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rens</forename><surname>Bod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Krauwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remko</forename><surname>Scha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recent Advances in NLP</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">136</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Construct a sense-frame aligned predicate lexicon for Chinese AMR corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuling</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiguang</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 12th Language Resources and Evaluation Conference</title>
		<meeting>The 12th Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2962" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Semantic neural machine translation using amr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="19" to="31" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A Bayesian model for unsupervised semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Klementiev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1445" to="1455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Neural semantic parsing by character-based translation: Experiments with abstract meaning representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Van Noord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
		<idno>abs/1705.09980</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Robust subgraph generation improves abstract meaning representation parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keenon</forename><surname>Werling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Scene graph generation by iterative message passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5410" to="5419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Improving amr parsing with sequence-to-sequence pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongqin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<idno>abs/2010.01771</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Amr parsing as sequenceto-graph transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xutai</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno>abs/1905.08704</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Broad-coverage semantic parsing as transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xutai</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno>abs/1909.02607</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bregman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ussr Computational Mathematics and Mathematical Physics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="200" to="217" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Integer Programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Conforti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Cornuejols</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giacomo</forename><surname>Zambelli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer Publishing Company</publisher>
		</imprint>
	</monogr>
	<note>Incorporated</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Allennlp: A deep semantic natural language processing platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Grus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nelson</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>abs/1803.07640</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Improved variational inference with inverse autoregressive flow. ArXiv, abs/1606.04934</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>abs/1907.11692</idno>
	</analytic>
	<monogr>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">AMR parsing as graph prediction with latent alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunchuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="397" to="407" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL) System Demonstrations</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Rewarding smatch: Transition-based amr parsing with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tahira</forename><surname>Naseem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<idno>abs/1905.13370</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Gradient estimation with stochastic softmax tricks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><forename type="middle">B</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dami</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<idno>abs/2006.08063</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">The role of reentrancies in abstract meaning representation parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ida</forename><surname>Szubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Damonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of EMNLP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rush</surname></persName>
		</author>
		<idno>abs/1910.03771</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Amr parsing as sequence-tograph transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xutai</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno>abs/1905.08704</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
