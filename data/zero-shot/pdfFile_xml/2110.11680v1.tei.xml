<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Two-Stream Video Inference for Human Body Pose and Shape Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwen</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">OPPO Research Institute</orgName>
								<address>
									<addrLine>2 Xmotors</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">OPPO Research Institute</orgName>
								<address>
									<addrLine>2 Xmotors</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">OPPO Research Institute</orgName>
								<address>
									<addrLine>2 Xmotors</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">OPPO Research Institute</orgName>
								<address>
									<addrLine>2 Xmotors</addrLine>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Guo</surname></persName>
							<email>yandong.guo@live.com</email>
							<affiliation key="aff0">
								<orgName type="institution">OPPO Research Institute</orgName>
								<address>
									<addrLine>2 Xmotors</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Two-Stream Video Inference for Human Body Pose and Shape Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Several video-based 3D pose and shape estimation algorithms have been proposed to resolve the temporal inconsistency of single-image-based methods. However it still remains challenging to have stable and accurate reconstruction. In this paper, we propose a new framework Deep Two-Stream Video Inference for Human Body Pose and Shape Estimation (DTS-VIBE), to generate 3D human pose and mesh from RGB videos. We reformulate the task as a multi-modality problem that fuses RGB and optical flow for more reliable estimation. In order to fully utilize both sensory modalities (RGB or optical flow), we train a two-stream temporal network based on transformer to predict SMPL parameters. The supplementary modality, optical flow, helps to maintain temporal consistency by leveraging motion knowledge between two consecutive frames. The proposed algorithm is extensively evaluated on the Hu-man3.6 and 3DPW datasets. The experimental results show that it outperforms other state-of-the-art methods by a significant margin.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Considerable amount of research has been done on the 3D human pose and shape estimation from a single RGB image <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b15">15]</ref>. More recently, some methods try to improve 3D human reconstruction by exploiting temporal information from monocular video <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b15">15]</ref>. However, those methods still struggle to reconstruct accurate 3D human body when there is complex human joint movement or severe occlusion, largely because of limited sensory modality and training data.</p><p>To address this, we revisit the 3D human reconstruction from video with the following two beliefs. First, we argue that RGB feature alone is insufficient to interpret the high degree of freedom of human behavior, and additional sensory information is needed. Second, we argue that a much stronger temporal network should be introduced into 3D human reconstruction, considering the diverse and complex human motion. <ref type="figure">Figure 1</ref>: DTS-VIBE learns the complementation of multiply sensory modalities for 3D human mesh reconstruction. Given a RGB video (a), we first estimate its optical flow (b) by auto encoder and predict the human mesh (c) by our two-stream temporal encoder-to-decoder network based on transformer. We suggest readers view this animated figure by Adobe Reader.</p><p>Consequently, we design our reconstruction pipeline with careful reconsideration in the following three ways.</p><p>First, we propose a new two-stream architecture named Deep Two-Stream Video Inference for Human Body Pose and Shape Estimation (DTS-VIBE), which allows multimodality fusion for 3D human reconstruction. With this new architecture, we can supplement other sensory modalities, including but not limited to depth, optical flow or other motion cues, to extend RGB feature space for better 3D human pose and shape estimation. To the best of our knowledge, this is the first time such multi-modal architecture is introduced into this area to simultaneously compensate motion instability and improve temporal consistency. This architecture is highly modularized and each of its component is interchangeable, so it can easily accommodate other modalities and their own optimal seq2seq encoder/decoder. Second, we select optical flow among those candidate modalities to fit the above architecture. The optical flow has proven to be highly effective to understand human behaviour <ref type="bibr" target="#b33">[33]</ref> because it can, intuitively, help the two-stream network to bridge each two adjacent frames by understand-ing the motion. It is worth noting that our optical flow input does not require actual physical sensor such as event camera, which causes extra cost on data collection, labelling and synchronization with RGB image. Instead, we estimate the optical flow from RGB image sequence as a virtual sensory modality to provide an applicable and inexpensive solution. In other words, optical flow is directly estimated from RGB images without any additional labels.</p><p>Third, to correlate video frames in a way that best serves the pose and shape estimation, we build our temporal encoder based on transformer <ref type="bibr" target="#b35">[35]</ref> instead of Gated Recurrent Units (GRU) <ref type="bibr" target="#b5">[5]</ref> which is used as the temporal network in the state-of-the-art method VIBE <ref type="bibr" target="#b17">[17]</ref>. Despite being widely applied on various temporal tasks, GRU has its shortcomings that the temporal information is inevitably lost during the recursion. For example, when human body is partially occluded in the leading frames of a sequence, GRU cannot effectively provide reliable temporal information to those frames because referring to prior frames are impossible. However, transformer can alleviate this situation by applying global multi-head attention, particularly to the latter frames that are highly correlated to those leading frames in this case, to better estimate shapes and poses. Similarly, when human body is occluded in the middle of a sequence, transformer can combine short-term and long-term attentions simultaneously to better infer and regularize the motion in the middle. To fully exploit the proposed transformer network, we introduce a new loss named L f low which is inspired by <ref type="bibr" target="#b11">[11]</ref>. The loss is designed so that optical flow information is used to regularize the estimated pose by enforcing the trajectory of certain motion. We experimentally show that the combination of transformer network and flow loss significantly improves both accuracy and stability.</p><p>Overall, we employ dual convolutional neural networks (CNN) to extract two feature streams from RGB image/video sequence and its optical flows. The extracted image feature streams are fed into a transformer-based temporal encoder, and then combined with the corresponding optical flow streams by late fusion. The fused feature will be finally fed into a transformer-based regressor <ref type="bibr" target="#b35">[35]</ref>. Then, we follow <ref type="bibr" target="#b17">[17]</ref> to add a discriminator that tries to distinguish between the regressed body poses and samples from the AMASS <ref type="bibr" target="#b15">[15]</ref> dataset, which can provide a real/fake label for each sequence.</p><p>To justify our solutions, we conduct extensive experiments on multiple datasets. The experimental results show that our proposed method surpasses all state-of-theart video-based and single-image-based approaches. Overall, the contributions of this paper are:</p><p>? To our best knowledge, this is the first end-to-end twostream architecture which allows multi-modality fusion for video 3D human reconstruction.</p><p>? We introduce virtual optical flow which can supplement the corresponding motion information for the RGB domain to predict more accurate pose and shape.</p><p>? We propose a transformer-based temporal network to establish more robust temporal correlations and a transformer-based SMPL regressor for better body shape parameterization.</p><p>? We introduce a flow loss to regularize the predicted keypoints and reduce the acceleration error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works 2.1. Single image-based human reconstruction</head><p>Parametric human models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b29">29]</ref> are widely used in many prior studies. Pre-trained parametric models such as SMPL <ref type="bibr" target="#b22">[22]</ref>, STAR <ref type="bibr" target="#b28">[28]</ref>, SMPL-X <ref type="bibr" target="#b29">[29]</ref> are able to recover a human mesh with a few coefficients. Bogoet al. <ref type="bibr" target="#b4">[4]</ref> propose SMPLify, which first uses CNN to detect the 2D keypoints from a given image, then fits SMPL model to the predicted keypoints. Because of the high cost of 3D human data collection and annotation, people tend to use projected 2D keypoints loss as the less ideal supervision. HMR <ref type="bibr" target="#b14">[14]</ref> directly regresses SMPL parameters given an input image and projects 3D joints to 2D using the estimated camera parameters to match the ground truth 2D keypoints. It also proposes a discriminator network to distinguish the predicted and ground truth parameters, generating a more plausible mesh. Kolotouroset al. <ref type="bibr" target="#b18">[18]</ref> proposed SPIN that interatively fits the SMPL model to 2D joint and use the current estimation to supervise the network in the next iteration.</p><p>Meanwhile, some methods <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b37">37]</ref> also use intermediate output to help reconstruct human mesh. Pavlakoet al. <ref type="bibr" target="#b29">[29]</ref> first generates 2D joint heatmap and mask, then combines them with input image to predict SMPL parameters. Pengfeiet al. <ref type="bibr" target="#b37">[37]</ref> aligns 3D mesh to 2D image and use UV map to represent the 3D human mesh and uses encoder-decoder model to predict the UV map. Besides, there are plenty of other approaches which directly regress 3D human mesh from a given still image <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b32">32]</ref>. Gyeongsiket al. <ref type="bibr" target="#b27">[27]</ref> proposed an Image-to-Lixel network that directly predict three 1D-heatmaps of xyz coordinates of the human mesh veitices. Varolet al. <ref type="bibr" target="#b34">[34]</ref> proposes Bo-dyNet, which estimates voxels of human shape in the 3D volumetric space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Video-based 3D human reconstruction</head><p>Despite the progress of 3D human pose and shape estimation from single image, there are some video based methods <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b30">30]</ref> which take advantage of temporal information and subsequently achieve impressive outcomes. Hossainet al. <ref type="bibr" target="#b12">[12]</ref> propose a LSTM model</p><formula xml:id="formula_0">V-CNN Transformer Encoder O-CNN ? ? ? FC Transformer Regressor corase T corase , , 1</formula><p>Motion discriminator <ref type="figure">Figure 2</ref>: Architecture of the DTS-VIBE. The sequence of images and its corresponding flow are sent to the CNN to extract features. The transformer encoder extract temporal features and adds it with the flow feature. A fully connected layer is used to predict coarse SMPL parameters, which is processed by transformer regressor, providing final SMPL parameters. The whole network is trained with a discriminator similar to VIBE.</p><p>to predict a squence of 3D joints from given 2D joints. Pavlloet al. <ref type="bibr" target="#b30">[30]</ref> use a fully-convolutional network to process a sequence of 2D joints to estimate its 3D location, and these predicted 3D joints are used to self-supervise the network on the original 2D joints. <ref type="bibr" target="#b3">[3]</ref> first predict 2D joints and SMPL parameters for each frame, then jointly optimize them to reduce the error. Kanazawa et al. <ref type="bibr" target="#b15">[15]</ref> propose HMMR, which uses 1D CNN as temporal encoder to find features from sequence of images. It predicts 3D poses not only for the target frame but also past and future frames. Such strategy guarantees the smooth estimation. VIBE <ref type="bibr" target="#b17">[17]</ref> utilize GRU to encode features from single image into temporal feature and regress SMPL parameters. It also introduces a motion discriminator to guide the generator and encourage it to predict more reasonable poses when compared to the labels in auxiliary dataset. All of them prove to be great success, yet they still lack of temporal consistency in certain degree. In this regard, MEVA <ref type="bibr" target="#b23">[23]</ref> feed a sequence of human motion into an auto-encoder framework to first get coarse motion with VME, then refine it with MRR. Its network with encoder-decoder architecture learns the motion of human pose from AMASS <ref type="bibr" target="#b24">[24]</ref>, which helps to smooth pose estimation at inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Two Stream</head><p>Two stream architecture is widely adopted in video action recognition <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b9">9]</ref>. The feature obtained from optical flow is intuitively helpful when trying to learn the motion of people. Simonyan <ref type="bibr" target="#b33">[33]</ref> propose to extract spatial feature from images and extract temporal feature from dense optimal flow. The two types of features are combined by late fusion and sent to the classification module. Meanwhile, optical flow is also used to help 3D reconstruction. <ref type="bibr" target="#b10">[10]</ref> combine flow and 2D keypoints to predict poses over the input video and achieve decent performance under occluded scenarios. Similarly, our approach leverages the optical flow to enhance the temporal feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Architecture</head><p>In this section, we describe the deep two-stream video inference network for human body pose and shape estimation (DTS-VIBE). As summarized in <ref type="figure">Figure 2</ref>, the DTS-VIBE model consists of two-stream encoder, transformerbased regressor and motion discriminator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Two-stream encoder</head><p>The intuition behind using a two-stream encoder to process RGB and optical flow separately is that optical flow can provide additional sensory information of human motion. This is particularly useful for difficult occasions when human is severely occluded or the pose of human is irregular. Optical flow information can also help to maintain the smoothness of human prediction.</p><p>Given a sequence of continuous RGB frames v 1 , ..., v T , we first group every two consecutive video frames to obtain estimated optical flow o 1 , ..., o T using an auto en-coder <ref type="bibr" target="#b21">[21]</ref>. The RGB stream v 1 , ..., v T and optical-flow stream o 1 , ..., o T are fed into a dual CNNs (V-CNN and O-CNN), which outputs two-stream feature vectors f v i ? R 2048 and f o i ? R 2048 , i ? (1, T ). We adopt ResNet-50 <ref type="bibr" target="#b7">[7]</ref> as the backbone network of V-CNN and O-CNN, followed by the fully connected layers to reduce the dimensions from 2048 to 512. Instead of using Gated Recurrent Units (GRU), which demonstrates decent performance on video-based 3D human reconstruction <ref type="bibr" target="#b17">[17]</ref>, our temporal network is instead built on transformer <ref type="bibr" target="#b35">[35]</ref>. Although GRU is widely applied in various temporal tasks, it shows disadvantages that the temporal information is inevitably lost during the recursion. Transformer can alleviate this situation, because it directly works on every feature simultaneously with global multihead attention.</p><p>The sequence of RGB feature vectors f v i ? R 512 are fed into our transformer <ref type="bibr" target="#b35">[35]</ref>-based temporal encoder that yields latent RGB feature vectors g v i ? R 512 . Then the optical-flow feature vectors f o i ? R 512 are added on g v i and sent to a transformer-based regressor <ref type="bibr" target="#b18">[18]</ref> to predict the perframe SMPL parameters and corresponding camera parameters. Our transformer-based temporal encoder consists of N layers, each of which contains two sub-layers: a multihead attention layer and a feed forward layer. As shown in <ref type="figure" target="#fig_0">Figure 3</ref>, given a sequence of input features x 1 , ..., x T , the multi-head attention layer first gets the query (Q), key(K), and value(V) by using fully connected layer:</p><formula xml:id="formula_1">Q = W Q X, K = W K X, V = W V X<label>(1)</label></formula><p>Then the output is computed as</p><formula xml:id="formula_2">a = sof tmax( QK T ? d )V,<label>(2)</label></formula><p>where d is the dimension of key. The feed forward layer consists of 2 fully connected layer and dropout. The output of transformer is then added with the output of O-CNN, which contains the temporal information between each two frames, followed by a fully connected layer to reduce feature dimension from 512 to 157. The transformer regressor takes the low-dimension feature as input and outputs accurate SMPL parameters? 1 , ...,? T The overall loss of our module is:</p><formula xml:id="formula_3">L total = ? 1 L 3D + ? 2 L 2D + ? 3 L smpl + ? 4 L adv<label>(3)</label></formula><p>where ? 1 ,? 2 ,? 3 ,? 4 are weighted coefficients for each loss. Specifically, L 3D is the L 2 distance between ground truth 3D joints location X i,3D and 3D joints locationX i,3D from predicted SMPL model: where L 2D is the L 2 distance between ground truth 2D joints location and the weak-perspective projection of predicted 3D joints using corresponding camera parameters:</p><formula xml:id="formula_4">L 3D = T i=1 ||X i,3D ?X i,3D || 2 2 (4)</formula><formula xml:id="formula_5">L 2D = T i=1 ||X i,2D ?X i,2D || 2 2<label>(5)</label></formula><p>where L smpl is the L2 distance between ground truth and predicted SMPL parameters:</p><formula xml:id="formula_6">L smpl = T i=1 ||? i ?? i || 2 2 + ||? i ?? i || 2 2<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Motion discriminator and flow supervision</head><p>Motion discriminator. Follow <ref type="bibr" target="#b17">[17]</ref>, we use a motion discriminator D M to distinguish the predicted SMPL parameters ?(fake) and the real SMPL parameters?(real) from AMASS <ref type="bibr" target="#b24">[24]</ref>. Motion discriminator helps to produce more feasible real world poses that are aligned with 2D joint locations:</p><formula xml:id="formula_7">L adv = E ??p G [(D M (?) ? 1) 2 ]<label>(7)</label></formula><p>The objective for the discriminator is: where p G is a generated motion sequence and p R is a real motion sequence from the AMASS dataset. Flow supervision. Inspired by <ref type="bibr" target="#b11">[11]</ref>, we introduce a new flow loss to refine our DTS-VIBE network. Since optical flow represents the motion of two adjacent frames, the 2D joints location should move following its corresponding flow. So we use our flow sequence o 1 , ..., o T to supervise the movement of 2D joints location.We back trace the previous frame 2d joint position X f i?1,2D using optical flow o i and current 2d joint position X i,2D , the flow loss is calculated as L 2 distance between backward 2D joints and original 2D joints:</p><formula xml:id="formula_8">L D = E ??p R [(D M (?) ? 1) 2 + E ??p G [D M (?) 2 ]<label>(8</label></formula><formula xml:id="formula_9">L f low = T i=2 ||X i?1,2D ? X f i?1,2D || 2 2<label>(9)</label></formula><p>Note that we do not add flow loss at the beginning of the training, we only use it as supervision during refinement.</p><p>Among the dual convolutional network we use, the V-CNN is pre-trained on frame-based pose and shape estimation task <ref type="bibr" target="#b18">[18]</ref>, while the O-CNN is pre-trained on the Im-ageNet <ref type="bibr" target="#b31">[31]</ref> dataset. Similar to <ref type="bibr" target="#b17">[17]</ref>, we choose sequence length T = 16, which also leads to the best results compared with other alternative ones i.e. 8, 32, 64 and 128. For the transformer-based temporal encoder, we choose the number of layers to be 6 and number of head to be 8. The transformer-based regressor has 3 layers with 4 heads. We set the four loss term ? 1 , ? 2 , ? 3 , ? 4 in Eq. 3 to 300, 200, 120 and 60 respectively. We use Adam optimizer <ref type="bibr" target="#b16">[16]</ref> with learning rate 5 ?10 ?5 for the generator and 5 ?10 ?4 for the motion discriminator. During the refinement with the flow loss L f low as supervision, we remove the discriminator and use Adam optimizer <ref type="bibr" target="#b16">[16]</ref> with learning rate 1 ?10 ?5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first list the training/testing datasets and evaluation metrics we use for comparison. Then we compare our method with other state of the art frame-based and video-based methods. We also conduct multiple experiments to illustrate the influence of each component in our model. We provide both qualitative and quantitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>As we all know, 2D datasets (annotated by 2D joint labels) contain a large amount of in-the-wild videos and their labels are easy to obtain. 3D datasets (annotated by 3D joint labels) contain less in-the-wild videos and their labels are more difficult to acquire, but can better supervise the 3D construction model than 2D ones. Therefore we conduct hybrid-training to leverage both 2D and 3D datasets. We use InstaVariety <ref type="bibr" target="#b15">[15]</ref>, Penn Action <ref type="bibr" target="#b38">[38]</ref> and PoseTrack <ref type="bibr" target="#b0">[1]</ref> as 2D datasets and 3DPW <ref type="bibr" target="#b36">[36]</ref>, MPI-INF-3DHP <ref type="bibr" target="#b25">[25]</ref> and Human3.6M <ref type="bibr" target="#b13">[13]</ref> as 3D datasets for training and evaluation. Human3.6M is a large scale indoor dataset with 2D and 3D annotations. We use subjects S1, S5, S6, S7 and S8 as training data, and test our models on subjects S9 and S11. 3DPW is an outdoor dataset with 2D and 3D annotations. Following previous method, we also train our model on the 3DPW training set before evaluating on 3DPW test set. We also use AMASS <ref type="bibr" target="#b24">[24]</ref> dataset for adversarial training to discriminate a real/fake label for each sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metrics</head><p>We use three image-based metrics: Mean per joint position error (MPJPE), Procrustes-aligned mean per joint position error (PA-MPJPE) and per vertex error (PVE). Additionally, we report acceleration error <ref type="bibr" target="#b15">[15]</ref>, which computes the average difference between the predicted and ground ( ) </p><formula xml:id="formula_10">(b) ( ) (d) ( ) (b) ( ) (d) ( ) (b) ( ) (d) (a) (b) (c) (d) (a) (b) (c) (d) (a) (b) (c) (d) (a) (b) (c) (d) (a) (b) (c) (d) (a) (b) (c) (d) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) (f) ( ) (h) ( ) (f) ( ) (h) ( ) (f) ( ) (h) (e) (f) ( ) (h) (e) (f) (g) (h) (e) (f) (g) (h) (e) (f) (g) (h) (e) (f) (g) (h) (e) (f) (g) (h) ( ) ( ) (g) ( ) ( ) ( ) (g) ( ) ( ) (g) ( )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison results</head><p>As shown in <ref type="table" target="#tab_0">Table 1</ref>, we compare our result with previous state-of-the-art methods. Our method outperforms VIBE in all image-based metrics on 3DPW and Hu-man3.6M by a large margin. Besides, we achieve comparable result on MPI-INF-3DHP. Meanwhile, our result significantly outperforms all the image-based and video-based 3D reconstruction methods. The improvement of our model indicates that our two stream network helps to fuse spatial and temporal information, leading to more accurate and smoother 3D reconstruction. It also demonstrates that our transformer encoder and regressor are better at capturing temporal human motion information and predicting plausible SMPL parameters. <ref type="table" target="#tab_0">Table 1</ref> shows the comparison on temporal smoothness between DTS-VIBE and state-of-the-arts video-based methods. We present some visualization results in <ref type="figure">Figure 6</ref> which demonstrates that our model is better than VIBE when part of the body is self-occluded. The acceleration error of DTS-VIBE is lower than VIBE. As shown in <ref type="table" target="#tab_0">Table 1</ref>, we produce smoother result than MEVA, which is state-ofthe-art model on temporal smoothness, in terms of the ac-    <ref type="figure" target="#fig_2">Figure 5</ref> shows the acceleration error comparison between other methods and ours measured on a sample of 3DPW test set. Compared with VIBE, our model keeps acceleration error at lower level. And our result is slightly better than MEVA except some rare spikes.  <ref type="table" target="#tab_3">Table 3</ref> and <ref type="table" target="#tab_2">Table 2</ref> show the ablation study results with and without each component. For each experiment, we use 3DPW, MPI-INF-3DHP and Human3.6M for training, and 3DPW for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation study</head><p>Effectiveness of Transformer. To single out the improvement introduced by our transformer based temporal encoder, we revert the temporal encoder into GRU and compare it with our model. Similarly, we also use regressor from <ref type="bibr" target="#b14">[14]</ref> and our transformer-based regressor to prove the effectiveness of our transformer regressor. <ref type="table" target="#tab_2">Table 2</ref> shows that the use of transformer as temporal encoder not only improves the accuracy of reconstruction but also provides smoother results. It indicates that transformer can better understand the complexity and variability of real human motion. In addition, our transformer-based regressor significantly outperforms HMR regressor by 0.6% on PA-MPJPE and 37.5% on acceleration error. This demonstrates that our transformer regressor is able to predict more reasonable SMPL parameters.</p><p>Effectiveness of optical flow. We perform additional experiments to demonstrate the benefit of dense motion cues -optical flow. Here we remove human3.6M from training set. We also remove the flow-CNN feature and only use image feature to predict SMPL parameters. <ref type="table" target="#tab_3">Table 3</ref> shows the comparison between our model with and without flow feature. The accuracy of our model increases after adding optical flow features while maintaining high degree of smoothness.</p><p>Effectiveness of flow loss. We report the results with and without flow loss in <ref type="table" target="#tab_3">Table 3</ref>. It demonstrates that our acceleration error is reduced after using flow loss. Overall, the optical flow and flow loss together enhance the performance of model not only on the accuracy but also on the consistency.</p><p>Non-local Temporal Interaction. To further understand the effectiveness of our DTS-VIBE model on temporal interaction, we visualize the attention map of transformer encoder. Non-local temporal information from input video are required to stabilize pose estimation while achieving high accuracy. Self attention mechanism of transformer can catch both long-term and short-term temporal information while GRU model from VIBE <ref type="bibr" target="#b17">[17]</ref> focuses more on catching short term temporal information. <ref type="figure" target="#fig_1">Figure 4</ref> shows the average attention value of eight attention head of the first transformer encoder layer. We sample 1000 video sequences from 3DPW test set and visualize its self-attention map. Each pixel indicates the self attention value of the target frame with respect to the input frame. Note that different attention head focuses on different input frames. For instance, attention head (d) and (h) tend to catch information from frames far apart while (c) and (g) tend to catch information within nearby frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we introduce a two-stream 3D human reconstruction model named Deep Two-Stream Video Inference for Human Body Pose and Shape Estimation (DTS-  <ref type="figure">Figure 6</ref>: Comparison visualization. The first row corresponds to the origin video. The second row is the ground truth body mesh. The third and forth rows are our predicted results from camera view and alternate view. The fifth and sixth rows are VIBE results from camera view and alternate view. VIBE), which consists of two-stream encoder, transformerbased regressor and motion discriminator. DTS-VIBE avoids extra inputs other than RGB. Additionally, we introduce a new flow loss to refine our model. Extensive experi-ments demonstrate the necessity and effectiveness of virtual multi-modality fusion in 3D human reconstruction. Furthermore, our model outperforms current state-of-the-art algorithms on 3D human reconstruction by a significant margin.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Architecture of Transformer. Our temporal encoder use the same transformer structure but with different number of layers or heads.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Attention maps. Visualization of attention values among different attention heads. The x-axis and y-axis correspond to input and target frames respectively. We visualize the attention matrix value of the eight attention head of the first transformer encoder layer.(a) to (h) represent the attention value of eight attention heads separately. Lighter color indicates stronger attention value.truth acceleration of each joint in mm/s 2 . We use acceleration error as smooth indicator of temporal methods. A lower image-based metric error means a better performance in terms of accuracy of a model. And lower acceleration error means smoother results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Comparison of acceleration errors between VIBE, MEVA and our method. The top of the figure shows some samples from test images and the bottom graph is the acceleration error along the timeline for three models. Overall, our result is slightly smoother than MEVA and significantly better than VIBE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>MPJPE? MPJPE? PVE? Accel? PA-MPJPE? MPJPE? Accel? PA-MPJPE? MPJPE? Accel? Evaluation of state-of-the-art methods on 3DPW, MPI-INF-3DHP, and Human3.6M datasets. Models of VIBE,MEVA and DST-VIBE use 3DPW train set. And all methods except MEVA use Human3.6 for training. So the most fair comparison is between VIBE and ours. DST-VIBE achieves state-of-the-art results with almost all the metrics.</figDesc><table><row><cell>)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation study on transformer.</figDesc><table><row><cell>We replace our</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation study on flow. We conduct experiments that removes flow feature and flow loss from our model.</figDesc><table><row><cell>celeration error. We reduce the acceleration error by 67.3%</cell></row><row><cell>on Human3.6M, while maintaining the comparable result</cell></row><row><cell>on 3DPW and MPI-INF-3DHP.</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Posetrack: A benchmark for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5167" to="5176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scape: shape completion and animation of people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rodgers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2005</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Papers</title>
		<imprint>
			<biblScope unit="page" from="408" to="416" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploiting temporal context for 3d human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3395" to="3404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Keep it smpl: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="561" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoderdecoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pose2mesh: Graph convolutional network for 3d human pose and mesh recovery from a 2d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="769" to="787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Christoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Pinz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3468" to="3476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning 3d human pose from structure and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mundhada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Afaque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="668" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep temporal linear encoding networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2329" to="2338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Sim2real transfer learning for 3d human pose estimation: motion to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02499</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Supervision-by-registration: An unsupervised approach to improve the precision of facial landmark detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-I</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="360" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploiting temporal information for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R I</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="68" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sminchisescu. Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7122" to="7131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning 3d human dynamics from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5614" to="5623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Vibe: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5253" to="5263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3d human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2252" to="2261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Convolutional mesh regression for single-image human shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4501" to="4510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep local video feature for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Selflow: Selfsupervised learning of optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Smpl: A skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">man motion estimation via motion compression and refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Golestaneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>3d hu</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Amass: Archive of motion capture as surface shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">F</forename><surname>Troje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5442" to="5451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 international conference on 3D vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="506" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Single-shot multiperson 3d pose estimation from monocular rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="120" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">I2l-meshnet: Image-tolixel prediction network for accurate 3d human pose and mesh estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.03713</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.08535</idno>
		<title level="m">Sparse trained articulated human body regressor</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Expressive body capture: 3d hands, face, and body from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10975" to="10985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7753" to="7762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pifu: Pixel-aligned implicit function for high-resolution clothed human digitization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Natsume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Morishima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2304" to="2314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2199</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bodynet: Volumetric inference of 3d human body shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Recovering accurate 3d human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="601" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Densebody: Directly regressing dense 3d human pose and shape from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10153</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">From actemes to action: A strongly-supervised representation for detailed action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2248" to="2255" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
